<!DOCTYPE html>
<!--[if IE 8]><html class="no-js lt-ie9" lang="en" > <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="en" > <!--<![endif]-->
<head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <meta name="author" content="Sebastian Raschka">
  <link rel="shortcut icon" href="../../../img/favicon.ico">
  <title>Exhaustive Feature Selector - mlxtend</title>
  <link href='https://fonts.googleapis.com/css?family=Lato:400,700|Roboto+Slab:400,700|Inconsolata:400,700' rel='stylesheet' type='text/css'>

  <link rel="stylesheet" href="../../../css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../../../css/theme_extra.css" type="text/css" />
  <link rel="stylesheet" href="//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/styles/github.min.css">
  <link href="../../../cinder/css/base.css" rel="stylesheet">
  <link href="../../../cinder/css/bootstrap-custom.css" rel="stylesheet">
  <link href="../../../cinder/css/bootstrap-custom.min.css" rel="stylesheet">
  <link href="../../../cinder/css/cinder.css" rel="stylesheet">
  <link href="../../../cinder/css/font-awesome-4.0.3.css" rel="stylesheet">
  <link href="../../../cinder/css/highlight.css" rel="stylesheet">
  
  <script>
    // Current page data
    var mkdocs_page_name = "Exhaustive Feature Selector";
    var mkdocs_page_input_path = "user_guide/feature_selection/ExhaustiveFeatureSelector.md";
    var mkdocs_page_url = "/mlxtend/user_guide/feature_selection/ExhaustiveFeatureSelector/";
  </script>
  
  <script src="../../../js/jquery-2.1.1.min.js" defer></script>
  <script src="../../../js/modernizr-2.8.3.min.js" defer></script>
  <script src="//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/highlight.min.js"></script>
  <script>hljs.initHighlightingOnLoad();</script> 
  
  <script>
      (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
      (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
      m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
      })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');

      ga('create', 'UA-38457794-2', 'rasbt.github.io/mlxtend/');
      ga('send', 'pageview');
  </script>
  
</head>

<body class="wy-body-for-nav" role="document">

  <div class="wy-grid-for-nav">

    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side stickynav">
      <div class="wy-side-nav-search">
        <a href="../../.." class="icon icon-home"> mlxtend</a>
        <div role="search">
  <form id ="rtd-search-form" class="wy-form" action="../../../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" title="Type search term here" />
  </form>
</div>
      </div>

      <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
	<ul class="current">
	  
          
            <li class="toctree-l1">
		
    <a class="" href="../../..">Home</a>
	    </li>
          
            <li class="toctree-l1">
		
    <span class="caption-text">User Guide</span>
    <ul class="subnav">
                <li class="">
                    
    <a class="" href="../../../USER_GUIDE_INDEX/">User Guide Index</a>
                </li>
                <li class="">
                    
    <span class="caption-text">classifier</span>
    <ul class="subnav">
                <li class="toctree-l3">
                    
    <a class="" href="../../classifier/Adaline/">Adaptive Linear Neuron -- Adaline</a>
                </li>
                <li class="toctree-l3">
                    
    <a class="" href="../../classifier/EnsembleVoteClassifier/">EnsembleVoteClassifier</a>
                </li>
                <li class="toctree-l3">
                    
    <a class="" href="../../classifier/LogisticRegression/">Logistic Regression</a>
                </li>
                <li class="toctree-l3">
                    
    <a class="" href="../../classifier/MultiLayerPerceptron/">Neural Network - Multilayer Perceptron</a>
                </li>
                <li class="toctree-l3">
                    
    <a class="" href="../../classifier/Perceptron/">Perceptron</a>
                </li>
                <li class="toctree-l3">
                    
    <a class="" href="../../classifier/SoftmaxRegression/">Softmax Regression</a>
                </li>
                <li class="toctree-l3">
                    
    <a class="" href="../../classifier/StackingClassifier/">StackingClassifier</a>
                </li>
                <li class="toctree-l3">
                    
    <a class="" href="../../classifier/StackingCVClassifier/">StackingCVClassifier</a>
                </li>
    </ul>
                </li>
                <li class="">
                    
    <span class="caption-text">cluster</span>
    <ul class="subnav">
                <li class="toctree-l3">
                    
    <a class="" href="../../cluster/Kmeans/">Kmeans</a>
                </li>
    </ul>
                </li>
                <li class="">
                    
    <span class="caption-text">data</span>
    <ul class="subnav">
                <li class="toctree-l3">
                    
    <a class="" href="../../data/autompg_data/">Auto MPG</a>
                </li>
                <li class="toctree-l3">
                    
    <a class="" href="../../data/boston_housing_data/">Boston Housing Data</a>
                </li>
                <li class="toctree-l3">
                    
    <a class="" href="../../data/iris_data/">Iris Dataset</a>
                </li>
                <li class="toctree-l3">
                    
    <a class="" href="../../data/loadlocal_mnist/">Load the MNIST Dataset from Local Files</a>
                </li>
                <li class="toctree-l3">
                    
    <a class="" href="../../data/make_multiplexer_dataset/">Make Multiplexer Dataset</a>
                </li>
                <li class="toctree-l3">
                    
    <a class="" href="../../data/mnist_data/">MNIST Dataset</a>
                </li>
                <li class="toctree-l3">
                    
    <a class="" href="../../data/three_blobs_data/">Three Blobs Dataset</a>
                </li>
                <li class="toctree-l3">
                    
    <a class="" href="../../data/wine_data/">Wine Dataset</a>
                </li>
    </ul>
                </li>
                <li class="">
                    
    <span class="caption-text">evaluate</span>
    <ul class="subnav">
                <li class="toctree-l3">
                    
    <a class="" href="../../evaluate/bootstrap/">Bootstrap</a>
                </li>
                <li class="toctree-l3">
                    
    <a class="" href="../../evaluate/bootstrap_point632_score/">bootstrap_point632_score</a>
                </li>
                <li class="toctree-l3">
                    
    <a class="" href="../../evaluate/BootstrapOutOfBag/">BootstrapOutOfBag</a>
                </li>
                <li class="toctree-l3">
                    
    <a class="" href="../../evaluate/cochrans_q/">Cochran's Q Test</a>
                </li>
                <li class="toctree-l3">
                    
    <a class="" href="../../evaluate/combined_ftest_5x2cv/">5x2cv combined *F* test</a>
                </li>
                <li class="toctree-l3">
                    
    <a class="" href="../../evaluate/confusion_matrix/">Confusion Matrix</a>
                </li>
                <li class="toctree-l3">
                    
    <a class="" href="../../evaluate/feature_importance_permutation/">Feature Importance Permutation</a>
                </li>
                <li class="toctree-l3">
                    
    <a class="" href="../../evaluate/ftest/">F-Test</a>
                </li>
                <li class="toctree-l3">
                    
    <a class="" href="../../evaluate/lift_score/">Lift Score</a>
                </li>
                <li class="toctree-l3">
                    
    <a class="" href="../../evaluate/mcnemar_table/">Contigency Table for McNemar's Test</a>
                </li>
                <li class="toctree-l3">
                    
    <a class="" href="../../evaluate/mcnemar_tables/">Contigency Tables for McNemar's Test and Cochran's Q Test</a>
                </li>
                <li class="toctree-l3">
                    
    <a class="" href="../../evaluate/mcnemar/">McNemar's Test</a>
                </li>
                <li class="toctree-l3">
                    
    <a class="" href="../../evaluate/paired_ttest_5x2cv/">5x2cv paired *t* test</a>
                </li>
                <li class="toctree-l3">
                    
    <a class="" href="../../evaluate/paired_ttest_kfold_cv/">K-fold cross-validated paired *t* test</a>
                </li>
                <li class="toctree-l3">
                    
    <a class="" href="../../evaluate/paired_ttest_resampled/">Resampled paired *t* test</a>
                </li>
                <li class="toctree-l3">
                    
    <a class="" href="../../evaluate/permutation_test/">Permutation Test</a>
                </li>
                <li class="toctree-l3">
                    
    <a class="" href="../../evaluate/PredefinedHoldoutSplit/">PredefinedHoldoutSplit</a>
                </li>
                <li class="toctree-l3">
                    
    <a class="" href="../../evaluate/RandomHoldoutSplit/">RandomHoldoutSplit</a>
                </li>
                <li class="toctree-l3">
                    
    <a class="" href="../../evaluate/scoring/">Scoring</a>
                </li>
    </ul>
                </li>
                <li class="">
                    
    <span class="caption-text">feature_extraction</span>
    <ul class="subnav">
                <li class="toctree-l3">
                    
    <a class="" href="../../feature_extraction/LinearDiscriminantAnalysis/">Linear Discriminant Analysis</a>
                </li>
                <li class="toctree-l3">
                    
    <a class="" href="../../feature_extraction/PrincipalComponentAnalysis/">Principal Component Analysis</a>
                </li>
                <li class="toctree-l3">
                    
    <a class="" href="../../feature_extraction/RBFKernelPCA/">RBFKernelPCA</a>
                </li>
    </ul>
                </li>
                <li class=" current">
                    
    <span class="caption-text">feature_selection</span>
    <ul class="subnav">
                <li class="toctree-l3">
                    
    <a class="" href="../ColumnSelector/">ColumnSelector</a>
                </li>
                <li class="toctree-l3 current">
                    
    <a class="current" href="./">Exhaustive Feature Selector</a>
    <ul class="subnav">
            
    <li class="toctree-l4"><a href="#exhaustive-feature-selector">Exhaustive Feature Selector</a></li>
    
        <ul>
        
            <li><a class="toctree-l5" href="#overview">Overview</a></li>
        
            <li><a class="toctree-l5" href="#example-1-a-simple-iris-example">Example 1 - A simple Iris Example</a></li>
        
            <li><a class="toctree-l5" href="#example-2-visualizing-the-feature-selection-results">Example 2 - Visualizing the feature selection results</a></li>
        
            <li><a class="toctree-l5" href="#example-3-exhaustive-feature-selection-for-regression">Example 3 - Exhaustive Feature Selection for Regression</a></li>
        
            <li><a class="toctree-l5" href="#example-4-using-the-selected-feature-subset-for-making-new-predictions">Example 4 - Using the Selected Feature Subset For Making New Predictions</a></li>
        
            <li><a class="toctree-l5" href="#example-5-exhaustive-feature-selection-and-gridsearch">Example 5 - Exhaustive Feature Selection and GridSearch</a></li>
        
            <li><a class="toctree-l5" href="#example-6-working-with-pandas-dataframes">Example 6 - Working with pandas DataFrames</a></li>
        
            <li><a class="toctree-l5" href="#api">API</a></li>
        
        </ul>
    

    </ul>
                </li>
                <li class="toctree-l3">
                    
    <a class="" href="../SequentialFeatureSelector/">Sequential Feature Selector</a>
                </li>
    </ul>
                </li>
                <li class="">
                    
    <span class="caption-text">file_io</span>
    <ul class="subnav">
                <li class="toctree-l3">
                    
    <a class="" href="../../file_io/find_filegroups/">Find Filegroups</a>
                </li>
                <li class="toctree-l3">
                    
    <a class="" href="../../file_io/find_files/">Find Files</a>
                </li>
    </ul>
                </li>
                <li class="">
                    
    <span class="caption-text">frequent_patterns</span>
    <ul class="subnav">
                <li class="toctree-l3">
                    
    <a class="" href="../../frequent_patterns/apriori/">Apriori</a>
                </li>
                <li class="toctree-l3">
                    
    <a class="" href="../../frequent_patterns/association_rules/">Association rules</a>
                </li>
    </ul>
                </li>
                <li class="">
                    
    <span class="caption-text">general concepts</span>
    <ul class="subnav">
                <li class="toctree-l3">
                    
    <a class="" href="../../general_concepts/activation-functions/">Activation Functions for Artificial Neural Networks</a>
                </li>
                <li class="toctree-l3">
                    
    <a class="" href="../../general_concepts/gradient-optimization/">Gradient Descent and Stochastic Gradient Descent</a>
                </li>
                <li class="toctree-l3">
                    
    <a class="" href="../../general_concepts/linear-gradient-derivative/">Deriving the Gradient Descent Rule for Linear Regression and Adaline</a>
                </li>
                <li class="toctree-l3">
                    
    <a class="" href="../../general_concepts/regularization-linear/">Regularization of Generalized Linear Models</a>
                </li>
    </ul>
                </li>
                <li class="">
                    
    <span class="caption-text">image</span>
    <ul class="subnav">
                <li class="toctree-l3">
                    
    <a class="" href="../../image/extract_face_landmarks/">Extract Face Landmarks</a>
                </li>
    </ul>
                </li>
                <li class="">
                    
    <span class="caption-text">math</span>
    <ul class="subnav">
                <li class="toctree-l3">
                    
    <a class="" href="../../math/num_combinations/">Compute the Number of Combinations</a>
                </li>
                <li class="toctree-l3">
                    
    <a class="" href="../../math/num_permutations/">Compute the Number of Permutations</a>
                </li>
                <li class="toctree-l3">
                    
    <a class="" href="../../math/vectorspace_dimensionality/">Vectorspace Dimensionality</a>
                </li>
                <li class="toctree-l3">
                    
    <a class="" href="../../math/vectorspace_orthonormalization/">Vectorspace Orthonormalization</a>
                </li>
    </ul>
                </li>
                <li class="">
                    
    <span class="caption-text">plotting</span>
    <ul class="subnav">
                <li class="toctree-l3">
                    
    <a class="" href="../../plotting/category_scatter/">Scatterplot with Categories</a>
                </li>
                <li class="toctree-l3">
                    
    <a class="" href="../../plotting/checkerboard_plot/">Checkerboard Plot</a>
                </li>
                <li class="toctree-l3">
                    
    <a class="" href="../../plotting/ecdf/">Empirical Cumulative Distribution Function Plot</a>
                </li>
                <li class="toctree-l3">
                    
    <a class="" href="../../plotting/enrichment_plot/">Enrichment Plot</a>
                </li>
                <li class="toctree-l3">
                    
    <a class="" href="../../plotting/plot_confusion_matrix/">Confusion Matrix</a>
                </li>
                <li class="toctree-l3">
                    
    <a class="" href="../../plotting/plot_decision_regions/">Plotting Decision Regions</a>
                </li>
                <li class="toctree-l3">
                    
    <a class="" href="../../plotting/plot_learning_curves/">Plotting Learning Curves</a>
                </li>
                <li class="toctree-l3">
                    
    <a class="" href="../../plotting/plot_linear_regression/">Linear Regression Plot</a>
                </li>
                <li class="toctree-l3">
                    
    <a class="" href="../../plotting/plot_sequential_feature_selection/">Plot Sequential Feature Selection</a>
                </li>
                <li class="toctree-l3">
                    
    <a class="" href="../../plotting/scatterplotmatrix/">Scatter Plot Matrix</a>
                </li>
                <li class="toctree-l3">
                    
    <a class="" href="../../plotting/stacked_barplot/">Stacked Barplot</a>
                </li>
    </ul>
                </li>
                <li class="">
                    
    <span class="caption-text">preprocessing</span>
    <ul class="subnav">
                <li class="toctree-l3">
                    
    <a class="" href="../../preprocessing/CopyTransformer/">CopyTransformer</a>
                </li>
                <li class="toctree-l3">
                    
    <a class="" href="../../preprocessing/DenseTransformer/">DenseTransformer</a>
                </li>
                <li class="toctree-l3">
                    
    <a class="" href="../../preprocessing/MeanCenterer/">Mean Centerer</a>
                </li>
                <li class="toctree-l3">
                    
    <a class="" href="../../preprocessing/minmax_scaling/">MinMax Scaling</a>
                </li>
                <li class="toctree-l3">
                    
    <a class="" href="../../preprocessing/one-hot_encoding/">One hot encoding</a>
                </li>
                <li class="toctree-l3">
                    
    <a class="" href="../../preprocessing/shuffle_arrays_unison/">Shuffle Arrays in Unison</a>
                </li>
                <li class="toctree-l3">
                    
    <a class="" href="../../preprocessing/standardize/">Standardize</a>
                </li>
                <li class="toctree-l3">
                    
    <a class="" href="../../preprocessing/TransactionEncoder/">TransactionEncoder</a>
                </li>
    </ul>
                </li>
                <li class="">
                    
    <span class="caption-text">regressor</span>
    <ul class="subnav">
                <li class="toctree-l3">
                    
    <a class="" href="../../regressor/LinearRegression/">LinearRegression</a>
                </li>
                <li class="toctree-l3">
                    
    <a class="" href="../../regressor/StackingCVRegressor/">StackingCVRegressor</a>
                </li>
                <li class="toctree-l3">
                    
    <a class="" href="../../regressor/StackingRegressor/">StackingRegressor</a>
                </li>
    </ul>
                </li>
                <li class="">
                    
    <span class="caption-text">text</span>
    <ul class="subnav">
                <li class="toctree-l3">
                    
    <a class="" href="../../text/generalize_names/">Generalize Names</a>
                </li>
                <li class="toctree-l3">
                    
    <a class="" href="../../text/generalize_names_duplcheck/">Generalize Names & Duplicate Checking</a>
                </li>
                <li class="toctree-l3">
                    
    <a class="" href="../../text/tokenizer/">Tokenizer</a>
                </li>
    </ul>
                </li>
                <li class="">
                    
    <span class="caption-text">utils</span>
    <ul class="subnav">
                <li class="toctree-l3">
                    
    <a class="" href="../../utils/Counter/">Counter</a>
                </li>
    </ul>
                </li>
    </ul>
	    </li>
          
            <li class="toctree-l1">
		
    <span class="caption-text">API</span>
    <ul class="subnav">
                <li class="">
                    
    <a class="" href="../../../api_subpackages/mlxtend.classifier/">Mlxtend.classifier</a>
                </li>
                <li class="">
                    
    <a class="" href="../../../api_subpackages/mlxtend.cluster/">Mlxtend.cluster</a>
                </li>
                <li class="">
                    
    <a class="" href="../../../api_subpackages/mlxtend.data/">Mlxtend.data</a>
                </li>
                <li class="">
                    
    <a class="" href="../../../api_subpackages/mlxtend.evaluate/">Mlxtend.evaluate</a>
                </li>
                <li class="">
                    
    <a class="" href="../../../api_subpackages/mlxtend.feature_extraction/">Mlxtend.feature extraction</a>
                </li>
                <li class="">
                    
    <a class="" href="../../../api_subpackages/mlxtend.feature_selection/">Mlxtend.feature selection</a>
                </li>
                <li class="">
                    
    <a class="" href="../../../api_subpackages/mlxtend.file_io/">Mlxtend.file io</a>
                </li>
                <li class="">
                    
    <a class="" href="../../../api_subpackages/mlxtend.frequent_patterns/">Mlxtend.frequent patterns</a>
                </li>
                <li class="">
                    
    <a class="" href="../../../api_subpackages/mlxtend.plotting/">Mlxtend.plotting</a>
                </li>
                <li class="">
                    
    <a class="" href="../../../api_subpackages/mlxtend.preprocessing/">Mlxtend.preprocessing</a>
                </li>
                <li class="">
                    
    <a class="" href="../../../api_subpackages/mlxtend.regressor/">Mlxtend.regressor</a>
                </li>
                <li class="">
                    
    <a class="" href="../../../api_subpackages/mlxtend.text/">Mlxtend.text</a>
                </li>
                <li class="">
                    
    <a class="" href="../../../api_subpackages/mlxtend.utils/">Mlxtend.utils</a>
                </li>
    </ul>
	    </li>
          
            <li class="toctree-l1">
		
    <a class="" href="../../../installation/">Installation</a>
	    </li>
          
            <li class="toctree-l1">
		
    <span class="caption-text">About</span>
    <ul class="subnav">
                <li class="">
                    
    <a class="" href="../../../CHANGELOG/">Release Notes</a>
                </li>
                <li class="">
                    
    <a class="" href="../../../CONTRIBUTING/">How To Contribute</a>
                </li>
                <li class="">
                    
    <a class="" href="../../../contributors/">Contributors</a>
                </li>
                <li class="">
                    
    <a class="" href="../../../license/">License</a>
                </li>
                <li class="">
                    
    <a class="" href="../../../cite/">Citing Mlxtend</a>
                </li>
                <li class="">
                    
    <a class="" href="../../../discuss/">Discuss</a>
                </li>
    </ul>
	    </li>
          
        </ul>
      </div>
      &nbsp;
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" role="navigation" aria-label="top navigation">
        <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
        <a href="../../..">mlxtend</a>
      </nav>

      
      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="breadcrumbs navigation">
  <ul class="wy-breadcrumbs">
    <li><a href="../../..">Docs</a> &raquo;</li>
    
      
        
          <li>feature_selection &raquo;</li>
        
      
        
          <li>User Guide &raquo;</li>
        
      
    
    <li>Exhaustive Feature Selector</li>
    <li class="wy-breadcrumbs-aside">
      
        <a href="https://github.com/rasbt/mlxtend/edit/master/docs/user_guide/feature_selection/ExhaustiveFeatureSelector.md"
          class="icon icon-github"> Edit on GitHub</a>
      
    </li>
  </ul>
  <hr/>
</div>
          <div role="main">
            <div class="section">
              
                <h1 id="exhaustive-feature-selector">Exhaustive Feature Selector</h1>
<p>Implementation of an <em>exhaustive feature selector</em> for sampling and evaluating all possible feature combinations in a specified range.</p>
<blockquote>
<p>from mlxtend.feature_selection import ExhaustiveFeatureSelector</p>
</blockquote>
<h2 id="overview">Overview</h2>
<p>This exhaustive feature selection algorithm is a wrapper approach for brute-force evaluation of feature subsets; the best subset is selected by optimizing a specified performance metric given an arbitrary regressor or classifier. For instance, if the classifier is a logistic regression and the dataset consists of 4 features, the alogorithm will evaluate all 15 feature combinations (if <code>min_features=1</code> and <code>max_features=4</code>)</p>
<ul>
<li>{0}</li>
<li>{1}</li>
<li>{2}</li>
<li>{3}</li>
<li>{0, 1}</li>
<li>{0, 2}</li>
<li>{0, 3}</li>
<li>{1, 2}</li>
<li>{1, 3}</li>
<li>{2, 3}</li>
<li>{0, 1, 2}</li>
<li>{0, 1, 3}</li>
<li>{0, 2, 3}</li>
<li>{1, 2, 3}</li>
<li>{0, 1, 2, 3}</li>
</ul>
<p>and select the one that results in the best performance (e.g., classification accuracy) of the logistic regression classifier.</p>
<h2 id="example-1-a-simple-iris-example">Example 1 - A simple Iris Example</h2>
<p>Initializing a simple classifier from scikit-learn:</p>
<pre><code class="python">from sklearn.neighbors import KNeighborsClassifier
from sklearn.datasets import load_iris
from mlxtend.feature_selection import ExhaustiveFeatureSelector as EFS

iris = load_iris()
X = iris.data
y = iris.target

knn = KNeighborsClassifier(n_neighbors=3)

efs1 = EFS(knn, 
           min_features=1,
           max_features=4,
           scoring='accuracy',
           print_progress=True,
           cv=5)

efs1 = efs1.fit(X, y)

print('Best accuracy score: %.2f' % efs1.best_score_)
print('Best subset (indices):', efs1.best_idx_)
print('Best subset (corresponding names):', efs1.best_feature_names_)
</code></pre>

<pre><code>Features: 15/15

Best accuracy score: 0.97
Best subset (indices): (0, 2, 3)
Best subset (corresponding names): ('0', '2', '3')
</code></pre>
<p>Note that in the example above, the  'best_feature_names_' are simply a string equivalent of the feature indices. However, we can provide custom feature names to the <code>fit</code> function for this mapping:</p>
<pre><code class="python">feature_names = ('sepal length', 'sepal width', 'petal length', 'petal width')
efs1 = efs1.fit(X, y, custom_feature_names=feature_names)
print('Best subset (corresponding names):', efs1.best_feature_names_)
</code></pre>

<pre><code>Features: 15/15

Best subset (corresponding names): ('sepal length', 'petal length', 'petal width')
</code></pre>
<p>Via the <code>subsets_</code> attribute, we can take a look at the selected feature indices at each step:</p>
<pre><code class="python">efs1.subsets_
</code></pre>

<pre><code>{0: {'avg_score': 0.65999999999999992,
  'cv_scores': array([ 0.53333333,  0.63333333,  0.73333333,  0.76666667,  0.63333333]),
  'feature_idx': (0,),
  'feature_names': ('sepal length',)},
 1: {'avg_score': 0.56666666666666665,
  'cv_scores': array([ 0.53333333,  0.63333333,  0.6       ,  0.5       ,  0.56666667]),
  'feature_idx': (1,),
  'feature_names': ('sepal width',)},
 2: {'avg_score': 0.95333333333333337,
  'cv_scores': array([ 0.93333333,  1.        ,  0.9       ,  0.93333333,  1.        ]),
  'feature_idx': (2,),
  'feature_names': ('petal length',)},
 3: {'avg_score': 0.94666666666666666,
  'cv_scores': array([ 0.96666667,  0.96666667,  0.93333333,  0.86666667,  1.        ]),
  'feature_idx': (3,),
  'feature_names': ('petal width',)},
 4: {'avg_score': 0.72666666666666668,
  'cv_scores': array([ 0.66666667,  0.8       ,  0.63333333,  0.86666667,  0.66666667]),
  'feature_idx': (0, 1),
  'feature_names': ('sepal length', 'sepal width')},
 5: {'avg_score': 0.94666666666666666,
  'cv_scores': array([ 0.96666667,  1.        ,  0.86666667,  0.93333333,  0.96666667]),
  'feature_idx': (0, 2),
  'feature_names': ('sepal length', 'petal length')},
 6: {'avg_score': 0.95333333333333337,
  'cv_scores': array([ 0.96666667,  0.96666667,  0.9       ,  0.93333333,  1.        ]),
  'feature_idx': (0, 3),
  'feature_names': ('sepal length', 'petal width')},
 7: {'avg_score': 0.94666666666666666,
  'cv_scores': array([ 0.96666667,  1.        ,  0.9       ,  0.93333333,  0.93333333]),
  'feature_idx': (1, 2),
  'feature_names': ('sepal width', 'petal length')},
 8: {'avg_score': 0.94000000000000006,
  'cv_scores': array([ 0.96666667,  0.96666667,  0.86666667,  0.93333333,  0.96666667]),
  'feature_idx': (1, 3),
  'feature_names': ('sepal width', 'petal width')},
 9: {'avg_score': 0.95333333333333337,
  'cv_scores': array([ 0.96666667,  0.96666667,  0.9       ,  0.93333333,  1.        ]),
  'feature_idx': (2, 3),
  'feature_names': ('petal length', 'petal width')},
 10: {'avg_score': 0.94000000000000006,
  'cv_scores': array([ 0.96666667,  0.96666667,  0.86666667,  0.93333333,  0.96666667]),
  'feature_idx': (0, 1, 2),
  'feature_names': ('sepal length', 'sepal width', 'petal length')},
 11: {'avg_score': 0.94666666666666666,
  'cv_scores': array([ 0.93333333,  0.96666667,  0.9       ,  0.93333333,  1.        ]),
  'feature_idx': (0, 1, 3),
  'feature_names': ('sepal length', 'sepal width', 'petal width')},
 12: {'avg_score': 0.97333333333333338,
  'cv_scores': array([ 0.96666667,  0.96666667,  0.96666667,  0.96666667,  1.        ]),
  'feature_idx': (0, 2, 3),
  'feature_names': ('sepal length', 'petal length', 'petal width')},
 13: {'avg_score': 0.95999999999999996,
  'cv_scores': array([ 0.96666667,  0.96666667,  0.93333333,  0.93333333,  1.        ]),
  'feature_idx': (1, 2, 3),
  'feature_names': ('sepal width', 'petal length', 'petal width')},
 14: {'avg_score': 0.96666666666666679,
  'cv_scores': array([ 0.96666667,  0.96666667,  0.93333333,  0.96666667,  1.        ]),
  'feature_idx': (0, 1, 2, 3),
  'feature_names': ('sepal length',
   'sepal width',
   'petal length',
   'petal width')}}
</code></pre>
<h2 id="example-2-visualizing-the-feature-selection-results">Example 2 - Visualizing the feature selection results</h2>
<p>For our convenience, we can visualize the output from the feature selection in a pandas DataFrame format using the <code>get_metric_dict</code> method of the <code>ExhaustiveFeatureSelector</code> object. The columns <code>std_dev</code> and <code>std_err</code> represent the standard deviation and standard errors of the cross-validation scores, respectively.</p>
<p>Below, we see the DataFrame of the Sequential Forward Selector from Example 2:</p>
<pre><code class="python">import pandas as pd

iris = load_iris()
X = iris.data
y = iris.target

knn = KNeighborsClassifier(n_neighbors=3)

efs1 = EFS(knn, 
           min_features=1,
           max_features=4,
           scoring='accuracy',
           print_progress=True,
           cv=5)

feature_names = ('sepal length', 'sepal width',
                 'petal length', 'petal width')
efs1 = efs1.fit(X, y, custom_feature_names=feature_names)

df = pd.DataFrame.from_dict(efs1.get_metric_dict()).T
df.sort_values('avg_score', inplace=True, ascending=False)
df
</code></pre>

<pre><code>Features: 15/15
</code></pre>
<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>avg_score</th>
      <th>ci_bound</th>
      <th>cv_scores</th>
      <th>feature_idx</th>
      <th>feature_names</th>
      <th>std_dev</th>
      <th>std_err</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>12</th>
      <td>0.973333</td>
      <td>0.0171372</td>
      <td>[0.966666666667, 0.966666666667, 0.96666666666...</td>
      <td>(0, 2, 3)</td>
      <td>(sepal length, petal length, petal width)</td>
      <td>0.0133333</td>
      <td>0.00666667</td>
    </tr>
    <tr>
      <th>14</th>
      <td>0.966667</td>
      <td>0.0270963</td>
      <td>[0.966666666667, 0.966666666667, 0.93333333333...</td>
      <td>(0, 1, 2, 3)</td>
      <td>(sepal length, sepal width, petal length, peta...</td>
      <td>0.0210819</td>
      <td>0.0105409</td>
    </tr>
    <tr>
      <th>13</th>
      <td>0.96</td>
      <td>0.0320608</td>
      <td>[0.966666666667, 0.966666666667, 0.93333333333...</td>
      <td>(1, 2, 3)</td>
      <td>(sepal width, petal length, petal width)</td>
      <td>0.0249444</td>
      <td>0.0124722</td>
    </tr>
    <tr>
      <th>2</th>
      <td>0.953333</td>
      <td>0.0514116</td>
      <td>[0.933333333333, 1.0, 0.9, 0.933333333333, 1.0]</td>
      <td>(2,)</td>
      <td>(petal length,)</td>
      <td>0.04</td>
      <td>0.02</td>
    </tr>
    <tr>
      <th>6</th>
      <td>0.953333</td>
      <td>0.0436915</td>
      <td>[0.966666666667, 0.966666666667, 0.9, 0.933333...</td>
      <td>(0, 3)</td>
      <td>(sepal length, petal width)</td>
      <td>0.0339935</td>
      <td>0.0169967</td>
    </tr>
    <tr>
      <th>9</th>
      <td>0.953333</td>
      <td>0.0436915</td>
      <td>[0.966666666667, 0.966666666667, 0.9, 0.933333...</td>
      <td>(2, 3)</td>
      <td>(petal length, petal width)</td>
      <td>0.0339935</td>
      <td>0.0169967</td>
    </tr>
    <tr>
      <th>3</th>
      <td>0.946667</td>
      <td>0.0581151</td>
      <td>[0.966666666667, 0.966666666667, 0.93333333333...</td>
      <td>(3,)</td>
      <td>(petal width,)</td>
      <td>0.0452155</td>
      <td>0.0226078</td>
    </tr>
    <tr>
      <th>5</th>
      <td>0.946667</td>
      <td>0.0581151</td>
      <td>[0.966666666667, 1.0, 0.866666666667, 0.933333...</td>
      <td>(0, 2)</td>
      <td>(sepal length, petal length)</td>
      <td>0.0452155</td>
      <td>0.0226078</td>
    </tr>
    <tr>
      <th>7</th>
      <td>0.946667</td>
      <td>0.0436915</td>
      <td>[0.966666666667, 1.0, 0.9, 0.933333333333, 0.9...</td>
      <td>(1, 2)</td>
      <td>(sepal width, petal length)</td>
      <td>0.0339935</td>
      <td>0.0169967</td>
    </tr>
    <tr>
      <th>11</th>
      <td>0.946667</td>
      <td>0.0436915</td>
      <td>[0.933333333333, 0.966666666667, 0.9, 0.933333...</td>
      <td>(0, 1, 3)</td>
      <td>(sepal length, sepal width, petal width)</td>
      <td>0.0339935</td>
      <td>0.0169967</td>
    </tr>
    <tr>
      <th>8</th>
      <td>0.94</td>
      <td>0.0499631</td>
      <td>[0.966666666667, 0.966666666667, 0.86666666666...</td>
      <td>(1, 3)</td>
      <td>(sepal width, petal width)</td>
      <td>0.038873</td>
      <td>0.0194365</td>
    </tr>
    <tr>
      <th>10</th>
      <td>0.94</td>
      <td>0.0499631</td>
      <td>[0.966666666667, 0.966666666667, 0.86666666666...</td>
      <td>(0, 1, 2)</td>
      <td>(sepal length, sepal width, petal length)</td>
      <td>0.038873</td>
      <td>0.0194365</td>
    </tr>
    <tr>
      <th>4</th>
      <td>0.726667</td>
      <td>0.11623</td>
      <td>[0.666666666667, 0.8, 0.633333333333, 0.866666...</td>
      <td>(0, 1)</td>
      <td>(sepal length, sepal width)</td>
      <td>0.0904311</td>
      <td>0.0452155</td>
    </tr>
    <tr>
      <th>0</th>
      <td>0.66</td>
      <td>0.106334</td>
      <td>[0.533333333333, 0.633333333333, 0.73333333333...</td>
      <td>(0,)</td>
      <td>(sepal length,)</td>
      <td>0.0827312</td>
      <td>0.0413656</td>
    </tr>
    <tr>
      <th>1</th>
      <td>0.566667</td>
      <td>0.0605892</td>
      <td>[0.533333333333, 0.633333333333, 0.6, 0.5, 0.5...</td>
      <td>(1,)</td>
      <td>(sepal width,)</td>
      <td>0.0471405</td>
      <td>0.0235702</td>
    </tr>
  </tbody>
</table>
</div>

<pre><code class="python">import matplotlib.pyplot as plt

metric_dict = efs1.get_metric_dict()

fig = plt.figure()
k_feat = sorted(metric_dict.keys())
avg = [metric_dict[k]['avg_score'] for k in k_feat]

upper, lower = [], []
for k in k_feat:
    upper.append(metric_dict[k]['avg_score'] +
                 metric_dict[k]['std_dev'])
    lower.append(metric_dict[k]['avg_score'] -
                 metric_dict[k]['std_dev'])

plt.fill_between(k_feat,
                 upper,
                 lower,
                 alpha=0.2,
                 color='blue',
                 lw=1)

plt.plot(k_feat, avg, color='blue', marker='o')
plt.ylabel('Accuracy +/- Standard Deviation')
plt.xlabel('Number of Features')
feature_min = len(metric_dict[k_feat[0]]['feature_idx'])
feature_max = len(metric_dict[k_feat[-1]]['feature_idx'])
plt.xticks(k_feat, 
           [str(metric_dict[k]['feature_names']) for k in k_feat], 
           rotation=90)
plt.show()
</code></pre>

<p><img alt="png" src="../ExhaustiveFeatureSelector_files/ExhaustiveFeatureSelector_19_0.png" /></p>
<h2 id="example-3-exhaustive-feature-selection-for-regression">Example 3 - Exhaustive Feature Selection for Regression</h2>
<p>Similar to the classification examples above, the <code>SequentialFeatureSelector</code> also supports scikit-learn's estimators
for regression.</p>
<pre><code class="python">from sklearn.linear_model import LinearRegression
from sklearn.datasets import load_boston

boston = load_boston()
X, y = boston.data, boston.target

lr = LinearRegression()

efs = EFS(lr, 
          min_features=10,
          max_features=12,
          scoring='neg_mean_squared_error',
          cv=10)

efs.fit(X, y)

print('Best MSE score: %.2f' % efs.best_score_ * (-1))
print('Best subset:', efs.best_idx_)
</code></pre>

<pre><code>Features: 377/377


Best subset: (0, 1, 4, 6, 7, 8, 9, 10, 11, 12)
</code></pre>
<h2 id="example-4-using-the-selected-feature-subset-for-making-new-predictions">Example 4 - Using the Selected Feature Subset For Making New Predictions</h2>
<pre><code class="python"># Initialize the dataset

from sklearn.neighbors import KNeighborsClassifier
from sklearn.datasets import load_iris
from sklearn.model_selection import train_test_split

iris = load_iris()
X, y = iris.data, iris.target
X_train, X_test, y_train, y_test = train_test_split(
         X, y, test_size=0.33, random_state=1)

knn = KNeighborsClassifier(n_neighbors=3)
</code></pre>

<pre><code class="python"># Select the &quot;best&quot; three features via
# 5-fold cross-validation on the training set.

from mlxtend.feature_selection import ExhaustiveFeatureSelector as EFS

efs1 = EFS(knn, 
           min_features=1,
           max_features=4,
           scoring='accuracy',
           cv=5)
efs1 = efs1.fit(X_train, y_train)
</code></pre>

<pre><code>Features: 15/15
</code></pre>
<pre><code class="python">print('Selected features:', efs1.best_idx_)
</code></pre>

<pre><code>Selected features: (2, 3)
</code></pre>
<pre><code class="python"># Generate the new subsets based on the selected features
# Note that the transform call is equivalent to
# X_train[:, efs1.k_feature_idx_]

X_train_efs = efs1.transform(X_train)
X_test_efs = efs1.transform(X_test)

# Fit the estimator using the new feature subset
# and make a prediction on the test data
knn.fit(X_train_efs, y_train)
y_pred = knn.predict(X_test_efs)

# Compute the accuracy of the prediction
acc = float((y_test == y_pred).sum()) / y_pred.shape[0]
print('Test set accuracy: %.2f %%' % (acc*100))
</code></pre>

<pre><code>Test set accuracy: 96.00 %
</code></pre>
<h2 id="example-5-exhaustive-feature-selection-and-gridsearch">Example 5 - Exhaustive Feature Selection and GridSearch</h2>
<pre><code class="python"># Initialize the dataset

from sklearn.datasets import load_iris
from sklearn.model_selection import train_test_split

iris = load_iris()
X, y = iris.data, iris.target
X_train, X_test, y_train, y_test = train_test_split(
         X, y, test_size=0.33, random_state=1)
</code></pre>

<p>Use scikit-learn's <code>GridSearch</code> to tune the hyperparameters of the <code>LogisticRegression</code> estimator inside the <code>ExhaustiveFeatureSelector</code> and use it for prediction in the pipeline. <strong>Note that the <code>clone_estimator</code> attribute needs to be set to <code>False</code>.</strong></p>
<pre><code class="python">from sklearn.model_selection import GridSearchCV
from sklearn.pipeline import make_pipeline
from sklearn.linear_model import LogisticRegression
from mlxtend.feature_selection import ExhaustiveFeatureSelector as EFS

lr = LogisticRegression(multi_class='multinomial', 
                        solver='lbfgs', 
                        random_state=123)

efs1 = EFS(estimator=lr, 
           min_features=2,
           max_features=3,
           scoring='accuracy',
           print_progress=False,
           clone_estimator=False,
           cv=5,
           n_jobs=1)

pipe = make_pipeline(efs1, lr)

param_grid = {'exhaustivefeatureselector__estimator__C': [0.1, 1.0, 10.0]}

gs = GridSearchCV(estimator=pipe, 
                  param_grid=param_grid, 
                  scoring='accuracy', 
                  n_jobs=1, 
                  cv=2, 
                  verbose=1, 
                  refit=False)

# run gridearch
gs = gs.fit(X_train, y_train)
</code></pre>

<pre><code>Fitting 2 folds for each of 3 candidates, totalling 6 fits


[Parallel(n_jobs=1)]: Done   6 out of   6 | elapsed:    2.7s finished
</code></pre>
<p>... and the "best" parameters determined by GridSearch are ...</p>
<pre><code class="python">print(&quot;Best parameters via GridSearch&quot;, gs.best_params_)
</code></pre>

<pre><code>Best parameters via GridSearch {'exhaustivefeatureselector__estimator__C': 1.0}
</code></pre>
<h4 id="obtaining-the-best-k-feature-indices-after-gridsearch">Obtaining the best <em>k</em> feature indices after GridSearch</h4>
<p>If we are interested in the best <em>k</em> best feature indices via <code>SequentialFeatureSelection.best_idx_</code>, we have to initialize a <code>GridSearchCV</code> object with <code>refit=True</code>. Now, the grid search object will take the complete training dataset and the best parameters, which it found via cross-validation, to train the estimator pipeline.</p>
<pre><code class="python">gs = GridSearchCV(estimator=pipe, 
                  param_grid=param_grid, 
                  scoring='accuracy', 
                  n_jobs=1, 
                  cv=2, 
                  verbose=1, 
                  refit=True)
</code></pre>

<p>After running the grid search, we can access the individual pipeline objects of the <code>best_estimator_</code> via the <code>steps</code> attribute.</p>
<pre><code class="python">gs = gs.fit(X_train, y_train)
gs.best_estimator_.steps
</code></pre>

<pre><code>Fitting 2 folds for each of 3 candidates, totalling 6 fits


[Parallel(n_jobs=1)]: Done   6 out of   6 | elapsed:    2.9s finished





[('exhaustivefeatureselector',
  ExhaustiveFeatureSelector(clone_estimator=False, cv=5,
               estimator=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,
            intercept_scaling=1, max_iter=100, multi_class='multinomial',
            n_jobs=1, penalty='l2', random_state=123, solver='lbfgs',
            tol=0.0001, verbose=0, warm_start=False),
               max_features=3, min_features=2, n_jobs=1,
               pre_dispatch='2*n_jobs', print_progress=False,
               scoring='accuracy')),
 ('logisticregression',
  LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,
            intercept_scaling=1, max_iter=100, multi_class='multinomial',
            n_jobs=1, penalty='l2', random_state=123, solver='lbfgs',
            tol=0.0001, verbose=0, warm_start=False))]
</code></pre>
<p>Via sub-indexing, we can then obtain the best-selected feature subset:</p>
<pre><code class="python">print('Best features:', gs.best_estimator_.steps[0][1].best_idx_)
</code></pre>

<pre><code>Best features: (2, 3)
</code></pre>
<p>During cross-validation, this feature combination had a CV accuracy of:</p>
<pre><code class="python">print('Best score:', gs.best_score_)
</code></pre>

<pre><code>Best score: 0.97
</code></pre>
<pre><code class="python">gs.best_params_
</code></pre>

<pre><code>{'exhaustivefeatureselector__estimator__C': 1.0}
</code></pre>
<p><strong>Alternatively</strong>, if we can set the "best grid search parameters" in our pipeline manually if we ran <code>GridSearchCV</code> with <code>refit=False</code>. It should yield the same results:</p>
<pre><code class="python">pipe.set_params(**gs.best_params_).fit(X_train, y_train)
print('Best features:', pipe.steps[0][1].best_idx_)
</code></pre>

<pre><code>Best features: (2, 3)
</code></pre>
<h2 id="example-6-working-with-pandas-dataframes">Example 6 - Working with pandas DataFrames</h2>
<p>Optionally, we can also use pandas DataFrames and pandas Series as input to the <code>fit</code> function. In this case, the column names of the pandas DataFrame will be used as feature names. However, note that if <code>custom_feature_names</code> are provided in the fit function, these <code>custom_feature_names</code> take precedence over the DataFrame column-based feature names.</p>
<pre><code class="python">import pandas as pd
from sklearn.neighbors import KNeighborsClassifier
from sklearn.datasets import load_iris

iris = load_iris()
col_names = ('sepal length', 'sepal width',
             'petal length', 'petal width')
X_df = pd.DataFrame(iris.data, columns=col_names)
y_series = pd.Series(iris.target)
knn = KNeighborsClassifier(n_neighbors=4)
</code></pre>

<pre><code class="python">from mlxtend.feature_selection import ExhaustiveFeatureSelector as EFS

knn = KNeighborsClassifier(n_neighbors=3)

efs1 = EFS(knn, 
           min_features=1,
           max_features=4,
           scoring='accuracy',
           print_progress=True,
           cv=5)

efs1 = efs1.fit(X_df, y_series)

print('Best accuracy score: %.2f' % efs1.best_score_)
print('Best subset (indices):', efs1.best_idx_)
print('Best subset (corresponding names):', efs1.best_feature_names_)
</code></pre>

<pre><code>Features: 15/15

Best accuracy score: 0.97
Best subset (indices): (0, 2, 3)
Best subset (corresponding names): ('sepal length', 'petal length', 'petal width')
</code></pre>
<h2 id="api">API</h2>
<p><em>ExhaustiveFeatureSelector(estimator, min_features=1, max_features=1, print_progress=True, scoring='accuracy', cv=5, n_jobs=1, pre_dispatch='2</em>n_jobs', clone_estimator=True)*</p>
<p>Exhaustive Feature Selection for Classification and Regression.
(new in v0.4.3)</p>
<p><strong>Parameters</strong></p>
<ul>
<li>
<p><code>estimator</code> : scikit-learn classifier or regressor</p>
</li>
<li>
<p><code>min_features</code> : int (default: 1)</p>
<p>Minumum number of features to select</p>
</li>
<li>
<p><code>max_features</code> : int (default: 1)</p>
<p>Maximum number of features to select</p>
</li>
<li>
<p><code>print_progress</code> : bool (default: True)</p>
<p>Prints progress as the number of epochs
to stderr.</p>
</li>
<li>
<p><code>scoring</code> : str, (default='accuracy')</p>
<p>Scoring metric in {accuracy, f1, precision, recall, roc_auc}
for classifiers,
{'mean_absolute_error', 'mean_squared_error',
'median_absolute_error', 'r2'} for regressors,
or a callable object or function with
signature <code>scorer(estimator, X, y)</code>.</p>
</li>
<li>
<p><code>cv</code> : int (default: 5)</p>
<p>Scikit-learn cross-validation generator or <code>int</code>.
If estimator is a classifier (or y consists of integer class labels),
stratified k-fold is performed, and regular k-fold cross-validation
otherwise.
No cross-validation if cv is None, False, or 0.</p>
</li>
<li>
<p><code>n_jobs</code> : int (default: 1)</p>
<p>The number of CPUs to use for evaluating different feature subsets
in parallel. -1 means 'all CPUs'.</p>
</li>
<li>
<p><code>pre_dispatch</code> : int, or string (default: '2*n_jobs')</p>
<p>Controls the number of jobs that get dispatched
during parallel execution if <code>n_jobs &gt; 1</code> or <code>n_jobs=-1</code>.
Reducing this number can be useful to avoid an explosion of
memory consumption when more jobs get dispatched than CPUs can process.
This parameter can be:
None, in which case all the jobs are immediately created and spawned.
Use this for lightweight and fast-running jobs,
to avoid delays due to on-demand spawning of the jobs
An int, giving the exact number of total jobs that are spawned
A string, giving an expression as a function
of n_jobs, as in <code>2*n_jobs</code></p>
</li>
<li>
<p><code>clone_estimator</code> : bool (default: True)</p>
<p>Clones estimator if True; works with the original estimator instance
if False. Set to False if the estimator doesn't
implement scikit-learn's set_params and get_params methods.
In addition, it is required to set cv=0, and n_jobs=1.</p>
</li>
</ul>
<p><strong>Attributes</strong></p>
<ul>
<li>
<p><code>best_idx_</code> : array-like, shape = [n_predictions]</p>
<p>Feature Indices of the selected feature subsets.</p>
</li>
<li>
<p><code>best_feature_names_</code> : array-like, shape = [n_predictions]</p>
<p>Feature names of the selected feature subsets. If pandas
DataFrames are used in the <code>fit</code> method, the feature
names correspond to the column names. Otherwise, the
feature names are string representation of the feature
array indices. New in v 0.13.0.</p>
</li>
<li>
<p><code>best_score_</code> : float</p>
<p>Cross validation average score of the selected subset.</p>
</li>
<li>
<p><code>subsets_</code> : dict</p>
<p>A dictionary of selected feature subsets during the
exhaustive selection, where the dictionary keys are
the lengths k of these feature subsets. The dictionary
values are dictionaries themselves with the following
keys: 'feature_idx' (tuple of indices of the feature subset)
'feature_names' (tuple of feature names of the feat. subset)
'cv_scores' (list individual cross-validation scores)
'avg_score' (average cross-validation score)
Note that if pandas
DataFrames are used in the <code>fit</code> method, the 'feature_names'
correspond to the column names. Otherwise, the
feature names are string representation of the feature
array indices. The 'feature_names' is new in v 0.13.0.</p>
</li>
</ul>
<p><strong>Examples</strong></p>
<p>For usage examples, please see
    <a href="http://rasbt.github.io/mlxtend/user_guide/feature_selection/ExhaustiveFeatureSelector/">http://rasbt.github.io/mlxtend/user_guide/feature_selection/ExhaustiveFeatureSelector/</a></p>
<h3 id="methods">Methods</h3>
<hr>

<p><em>fit(X, y, custom_feature_names=None, </em><em>fit_params)</em></p>
<p>Perform feature selection and learn model from training data.</p>
<p><strong>Parameters</strong></p>
<ul>
<li>
<p><code>X</code> : {array-like, sparse matrix}, shape = [n_samples, n_features]</p>
<p>Training vectors, where n_samples is the number of samples and
n_features is the number of features.
New in v 0.13.0: pandas DataFrames are now also accepted as
argument for X.</p>
</li>
<li>
<p><code>y</code> : array-like, shape = [n_samples]</p>
<p>Target values.</p>
</li>
<li>
<p><code>custom_feature_names</code> : None or tuple (default: tuple)</p>
<p>Custom feature names for <code>self.k_feature_names</code> and
<code>self.subsets_[i]['feature_names']</code>.
(new in v 0.13.0)</p>
</li>
<li>
<p><code>fit_params</code> : dict of string -&gt; object, optional</p>
<p>Parameters to pass to to the fit method of classifier.</p>
</li>
</ul>
<p><strong>Returns</strong></p>
<ul>
<li><code>self</code> : object</li>
</ul>
<hr>

<p><em>fit_transform(X, y, </em><em>fit_params)</em></p>
<p>Fit to training data and return the best selected features from X.</p>
<p><strong>Parameters</strong></p>
<ul>
<li>
<p><code>X</code> : {array-like, sparse matrix}, shape = [n_samples, n_features]</p>
<p>Training vectors, where n_samples is the number of samples and
n_features is the number of features.
New in v 0.13.0: pandas DataFrames are now also accepted as
argument for X.</p>
</li>
<li>
<p><code>y</code> : array-like, shape = [n_samples]</p>
<p>Target values.</p>
</li>
<li>
<p><code>fit_params</code> : dict of string -&gt; object, optional</p>
<p>Parameters to pass to to the fit method of classifier.</p>
</li>
</ul>
<p><strong>Returns</strong></p>
<p>Feature subset of X, shape={n_samples, k_features}</p>
<hr>

<p><em>get_metric_dict(confidence_interval=0.95)</em></p>
<p>Return metric dictionary</p>
<p><strong>Parameters</strong></p>
<ul>
<li>
<p><code>confidence_interval</code> : float (default: 0.95)</p>
<p>A positive float between 0.0 and 1.0 to compute the confidence
interval bounds of the CV score averages.</p>
</li>
</ul>
<p><strong>Returns</strong></p>
<p>Dictionary with items where each dictionary value is a list
    with the number of iterations (number of feature subsets) as
    its length. The dictionary keys corresponding to these lists
    are as follows:
    'feature_idx': tuple of the indices of the feature subset
    'cv_scores': list with individual CV scores
    'avg_score': of CV average scores
    'std_dev': standard deviation of the CV score average
    'std_err': standard error of the CV score average
    'ci_bound': confidence interval bound of the CV score average</p>
<hr>

<p><em>get_params(deep=True)</em></p>
<p>Get parameters for this estimator.</p>
<p><strong>Parameters</strong></p>
<ul>
<li>
<p><code>deep</code> : boolean, optional</p>
<p>If True, will return the parameters for this estimator and
contained subobjects that are estimators.</p>
</li>
</ul>
<p><strong>Returns</strong></p>
<ul>
<li>
<p><code>params</code> : mapping of string to any</p>
<p>Parameter names mapped to their values.</p>
</li>
</ul>
<hr>

<p><em>set_params(</em><em>params)</em></p>
<p>Set the parameters of this estimator.</p>
<p>The method works on simple estimators as well as on nested objects
(such as pipelines). The latter have parameters of the form
<code>&lt;component&gt;__&lt;parameter&gt;</code> so that it's possible to update each
component of a nested object.</p>
<p><strong>Returns</strong></p>
<p>self</p>
<hr>

<p><em>transform(X)</em></p>
<p>Return the best selected features from X.</p>
<p><strong>Parameters</strong></p>
<ul>
<li>
<p><code>X</code> : {array-like, sparse matrix}, shape = [n_samples, n_features]</p>
<p>Training vectors, where n_samples is the number of samples and
n_features is the number of features.
New in v 0.13.0: pandas DataFrames are now also accepted as
argument for X.</p>
</li>
</ul>
<p><strong>Returns</strong></p>
<p>Feature subset of X, shape={n_samples, k_features}</p>
              
            </div>
          </div>
          <footer>
  
    <div class="rst-footer-buttons" role="navigation" aria-label="footer navigation">
      
        <a href="../SequentialFeatureSelector/" class="btn btn-neutral float-right" title="Sequential Feature Selector">Next <span class="icon icon-circle-arrow-right"></span></a>
      
      
        <a href="../ColumnSelector/" class="btn btn-neutral" title="ColumnSelector"><span class="icon icon-circle-arrow-left"></span> Previous</a>
      
    </div>
  

  <hr/>

  <div role="contentinfo">
    <!-- Copyright etc -->
    
      <p>Copyright &copy; 2014-2018 <a href="http://sebastianraschka.com">Sebastian Raschka</a></p>
    
  </div>

  Built with <a href="http://www.mkdocs.org">MkDocs</a> using a <a href="https://github.com/snide/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>.
</footer>
      
        </div>
      </div>

    </section>

  </div>

  <div class="rst-versions" role="note" style="cursor: pointer">
    <span class="rst-current-version" data-toggle="rst-current-version">
      
          <a href="https://github.com/rasbt/mlxtend/" class="fa fa-github" style="float: left; color: #fcfcfc"> GitHub</a>
      
      
        <span><a href="../ColumnSelector/" style="color: #fcfcfc;">&laquo; Previous</a></span>
      
      
        <span style="margin-left: 15px"><a href="../SequentialFeatureSelector/" style="color: #fcfcfc">Next &raquo;</a></span>
      
    </span>
</div>
    <script>var base_url = '../../..';</script>
    <script src="../../../js/theme.js" defer></script>
      <script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS_HTML" defer></script>
      <script src="../../../mathjaxhelper.js" defer></script>
      <script src="../../../search/main.js" defer></script>

</body>
</html>

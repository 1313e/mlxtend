<!DOCTYPE html>
<html lang="en">

<head>
    <page.page.meta charset="utf-8">
    <page.page.meta http-equiv="X-UA-Compatible" page.content="IE=edge">
    <page.page.meta name="viewport" page.content="width=device-width, initial-scale=1.0">
    <page.page.meta name="description" page.content="A library consisting of useful tools and extensions for the day-to-day data science tasks."> 
    <page.page.meta name="author" page.content="Sebastian Raschka"> 
    <link rel="canonical" href="http://rasbt.github.io/mlxtend/api_subpackages/mlxtend.classifier/"> 
    <link rel="shortcut icon" href="../../img/favicon.ico">

    <title>Mlxtend.classifier - mlxtend</title>

    <link href="../../css/bootstrap-custom.min.css" rel="stylesheet">
    <link href="//maxcdn.bootstrapcdn.com/font-awesome/4.4.0/css/font-awesome.min.css" rel="stylesheet">
    <link rel="stylesheet" href="//cdn.jsdelivr.net/font-hack/2.018/css/hack.min.css">
    <link href='//fonts.googleapis.com/css?family=PT+Sans:400,400italic,700,700italic&subset=latin-ext,latin' rel='stylesheet' type='text/css'>
    <link href='//fonts.googleapis.com/css?family=Open+Sans:300italic,400italic,700italic,400,300,600,700&subset=latin-ext,latin' rel='stylesheet' type='text/css'>
    <link href="../../css/base.css" rel="stylesheet">
    <link href="../../css/cinder.css" rel="stylesheet">
    <link rel="stylesheet" href="../../css/highlight.css">


    <link href="../../extra.css" rel="stylesheet">

    <!-- HTML5 shim and Respond.js IE8 support of HTML5 elements and media queries -->
    <!--[if lt IE 9]>
            <script src="https://oss.maxcdn.com/libs/html5shiv/3.7.0/html5shiv.js"></script>
            <script src="https://oss.maxcdn.com/libs/respond.js/1.3.0/respond.min.js"></script>
        <![endif]-->

    <script src="https://ajax.googleapis.com/ajax/libs/webfont/1.5.18/webfont.js"></script>
    <script>
    WebFont.load({
        google: {
            families: ['Open Sans', 'PT Sans']
        }
    });
    </script>

    
    <script>
    (function(i, s, o, g, r, a, m) {
        i['GoogleAnalyticsObject'] = r;
        i[r] = i[r] || function() {
            (i[r].q = i[r].q || []).push(arguments)
        }, i[r].l = 1 * new Date();
        a = s.createElement(o),
        m = s.getElementsByTagName(o)[0];
        a.async = 1;
        a.src = g;
        m.parentNode.insertBefore(a, m)
    })(window, document, 'script', '//www.google-analytics.com/analytics.js', 'ga');

    ga('create', 'UA-38457794-2', 'rasbt.github.io/mlxtend/');
    ga('send', 'pageview');
    </script>
    
</head>

<body>

    <div class="navbar navbar-default navbar-fixed-top" role="navigation">
    <div class="container">

        <!-- Collapsed navigation -->
        <div class="navbar-header">
            
            <!-- Expander button -->
            <button type="button" class="navbar-toggle" data-toggle="collapse" data-target=".navbar-collapse">
                <span class="sr-only">Toggle navigation</span>
                <span class="icon-bar"></span>
                <span class="icon-bar"></span>
                <span class="icon-bar"></span>
            </button>
            

            <!-- Main title -->
            <a class="navbar-brand" href="../..">mlxtend</a>
        </div>

        <!-- Expanded navigation -->
        <div class="navbar-collapse collapse">
            
                <!-- Main navigation -->
                <ul class="nav navbar-nav">
                
                
                    <li >
                        <a href="../..">Home</a>
                    </li>
                
                
                
                    <li class="dropdown">
                        <a href="#" class="dropdown-toggle" data-toggle="dropdown">User Guide <b class="caret"></b></a>
                        <ul class="dropdown-menu">
                        
                            
<li >
    <a href="../../USER_GUIDE_INDEX/">USER GUIDE INDEX</a>
</li>

                        
                            
  <li class="dropdown-submenu">
    <a tabindex="-1" href="">classifier</a>
    <ul class="dropdown-menu">
        
            
<li >
    <a href="../../user_guide/classifier/EnsembleVoteClassifier/">EnsembleVoteClassifier</a>
</li>

        
            
<li >
    <a href="../../user_guide/classifier/StackingClassifier/">StackingClassifier</a>
</li>

        
            
<li >
    <a href="../../user_guide/classifier/StackingCVClassifier/">StackingCVClassifier</a>
</li>

        
            
<li >
    <a href="../../user_guide/classifier/Perceptron/">Perceptron</a>
</li>

        
            
<li >
    <a href="../../user_guide/classifier/Adaline/">Adaline</a>
</li>

        
            
<li >
    <a href="../../user_guide/classifier/LogisticRegression/">LogisticRegression</a>
</li>

        
            
<li >
    <a href="../../user_guide/classifier/SoftmaxRegression/">SoftmaxRegression</a>
</li>

        
            
<li >
    <a href="../../user_guide/classifier/MultiLayerPerceptron/">MultiLayerPerceptron</a>
</li>

        
    </ul>
  </li>

                        
                            
  <li class="dropdown-submenu">
    <a tabindex="-1" href="">tf_classifier</a>
    <ul class="dropdown-menu">
        
            
<li >
    <a href="../../user_guide/tf_classifier/TfMultiLayerPerceptron/">TfMultiLayerPerceptron</a>
</li>

        
            
<li >
    <a href="../../user_guide/tf_classifier/TfSoftmaxRegression/">TfSoftmaxRegression</a>
</li>

        
    </ul>
  </li>

                        
                            
  <li class="dropdown-submenu">
    <a tabindex="-1" href="">regressor</a>
    <ul class="dropdown-menu">
        
            
<li >
    <a href="../../user_guide/regressor/LinearRegression/">LinearRegression</a>
</li>

        
            
<li >
    <a href="../../user_guide/regressor/StackingRegressor/">StackingRegressor</a>
</li>

        
    </ul>
  </li>

                        
                            
  <li class="dropdown-submenu">
    <a tabindex="-1" href="">tf_regressor</a>
    <ul class="dropdown-menu">
        
            
<li >
    <a href="../../user_guide/tf_regressor/TfLinearRegression/">TfLinearRegression</a>
</li>

        
    </ul>
  </li>

                        
                            
  <li class="dropdown-submenu">
    <a tabindex="-1" href="">feature_selection</a>
    <ul class="dropdown-menu">
        
            
<li >
    <a href="../../user_guide/feature_selection/SequentialFeatureSelector/">SequentialFeatureSelector</a>
</li>

        
            
<li >
    <a href="../../user_guide/feature_selection/ExhaustiveFeatureSelector/">ExhaustiveFeatureSelector</a>
</li>

        
            
<li >
    <a href="../../user_guide/feature_selection/ColumnSelector/">ColumnSelector</a>
</li>

        
    </ul>
  </li>

                        
                            
  <li class="dropdown-submenu">
    <a tabindex="-1" href="">feature_extraction</a>
    <ul class="dropdown-menu">
        
            
<li >
    <a href="../../user_guide/feature_extraction/PrincipalComponentAnalysis/">PrincipalComponentAnalysis</a>
</li>

        
            
<li >
    <a href="../../user_guide/feature_extraction/LinearDiscriminantAnalysis/">LinearDiscriminantAnalysis</a>
</li>

        
            
<li >
    <a href="../../user_guide/feature_extraction/RBFKernelPCA/">RBFKernelPCA</a>
</li>

        
    </ul>
  </li>

                        
                            
  <li class="dropdown-submenu">
    <a tabindex="-1" href="">cluster</a>
    <ul class="dropdown-menu">
        
            
<li >
    <a href="../../user_guide/cluster/Kmeans/">Kmeans</a>
</li>

        
    </ul>
  </li>

                        
                            
  <li class="dropdown-submenu">
    <a tabindex="-1" href="">tf_cluster</a>
    <ul class="dropdown-menu">
        
            
<li >
    <a href="../../user_guide/tf_cluster/TfKmeans/">TfKmeans</a>
</li>

        
    </ul>
  </li>

                        
                            
  <li class="dropdown-submenu">
    <a tabindex="-1" href="">evaluate</a>
    <ul class="dropdown-menu">
        
            
<li >
    <a href="../../user_guide/evaluate/confusion_matrix/">Confusion matrix</a>
</li>

        
            
<li >
    <a href="../../user_guide/evaluate/mcnemar_table/">Mcnemar table</a>
</li>

        
            
<li >
    <a href="../../user_guide/evaluate/mcnemar/">Mcnemar</a>
</li>

        
            
<li >
    <a href="../../user_guide/evaluate/lift_score/">Lift score</a>
</li>

        
            
<li >
    <a href="../../user_guide/evaluate/scoring/">Scoring</a>
</li>

        
    </ul>
  </li>

                        
                            
  <li class="dropdown-submenu">
    <a tabindex="-1" href="">preprocessing</a>
    <ul class="dropdown-menu">
        
            
<li >
    <a href="../../user_guide/preprocessing/MeanCenterer/">MeanCenterer</a>
</li>

        
            
<li >
    <a href="../../user_guide/preprocessing/minmax_scaling/">Minmax scaling</a>
</li>

        
            
<li >
    <a href="../../user_guide/preprocessing/shuffle_arrays_unison/">Shuffle arrays unison</a>
</li>

        
            
<li >
    <a href="../../user_guide/preprocessing/standardize/">Standardize</a>
</li>

        
            
<li >
    <a href="../../user_guide/preprocessing/one-hot_encoding/">One hot encoding</a>
</li>

        
            
<li >
    <a href="../../user_guide/preprocessing/OnehotTransactions/">OnehotTransactions</a>
</li>

        
            
<li >
    <a href="../../user_guide/preprocessing/DenseTransformer/">DenseTransformer</a>
</li>

        
            
<li >
    <a href="../../user_guide/preprocessing/CopyTransformer/">CopyTransformer</a>
</li>

        
    </ul>
  </li>

                        
                            
  <li class="dropdown-submenu">
    <a tabindex="-1" href="">data</a>
    <ul class="dropdown-menu">
        
            
<li >
    <a href="../../user_guide/data/autompg_data/">Autompg data</a>
</li>

        
            
<li >
    <a href="../../user_guide/data/boston_housing_data/">Boston housing data</a>
</li>

        
            
<li >
    <a href="../../user_guide/data/iris_data/">Iris data</a>
</li>

        
            
<li >
    <a href="../../user_guide/data/mnist_data/">Mnist data</a>
</li>

        
            
<li >
    <a href="../../user_guide/data/loadlocal_mnist/">Loadlocal mnist</a>
</li>

        
            
<li >
    <a href="../../user_guide/data/wine_data/">Wine data</a>
</li>

        
            
<li >
    <a href="../../user_guide/data/three_blobs_data/">Three blobs data</a>
</li>

        
    </ul>
  </li>

                        
                            
  <li class="dropdown-submenu">
    <a tabindex="-1" href="">association</a>
    <ul class="dropdown-menu">
        
            
<li >
    <a href="../../user_guide/association/apriori/">Apriori</a>
</li>

        
    </ul>
  </li>

                        
                            
  <li class="dropdown-submenu">
    <a tabindex="-1" href="">file_io</a>
    <ul class="dropdown-menu">
        
            
<li >
    <a href="../../user_guide/file_io/find_filegroups/">Find filegroups</a>
</li>

        
            
<li >
    <a href="../../user_guide/file_io/find_files/">Find files</a>
</li>

        
    </ul>
  </li>

                        
                            
  <li class="dropdown-submenu">
    <a tabindex="-1" href="">plotting</a>
    <ul class="dropdown-menu">
        
            
<li >
    <a href="../../user_guide/plotting/plot_decision_regions/">Plot decision regions</a>
</li>

        
            
<li >
    <a href="../../user_guide/plotting/plot_learning_curves/">Plot learning curves</a>
</li>

        
            
<li >
    <a href="../../user_guide/plotting/plot_sequential_feature_selection/">Plot sequential feature selection</a>
</li>

        
            
<li >
    <a href="../../user_guide/plotting/plot_confusion_matrix/">Plot confusion matrix</a>
</li>

        
            
<li >
    <a href="../../user_guide/plotting/plot_linear_regression/">Plot linear regression</a>
</li>

        
            
<li >
    <a href="../../user_guide/plotting/category_scatter/">Category scatter</a>
</li>

        
            
<li >
    <a href="../../user_guide/plotting/enrichment_plot/">Enrichment plot</a>
</li>

        
            
<li >
    <a href="../../user_guide/plotting/stacked_barplot/">Stacked barplot</a>
</li>

        
            
<li >
    <a href="../../user_guide/plotting/checkerboard_plot/">Checkerboard plot</a>
</li>

        
    </ul>
  </li>

                        
                            
  <li class="dropdown-submenu">
    <a tabindex="-1" href="">math</a>
    <ul class="dropdown-menu">
        
            
<li >
    <a href="../../user_guide/math/num_combinations/">Num combinations</a>
</li>

        
            
<li >
    <a href="../../user_guide/math/num_permutations/">Num permutations</a>
</li>

        
    </ul>
  </li>

                        
                            
  <li class="dropdown-submenu">
    <a tabindex="-1" href="">text</a>
    <ul class="dropdown-menu">
        
            
<li >
    <a href="../../user_guide/text/generalize_names/">Generalize names</a>
</li>

        
            
<li >
    <a href="../../user_guide/text/generalize_names_duplcheck/">Generalize names duplcheck</a>
</li>

        
            
<li >
    <a href="../../user_guide/text/tokenizer/">Tokenizer</a>
</li>

        
    </ul>
  </li>

                        
                            
  <li class="dropdown-submenu">
    <a tabindex="-1" href="">utils</a>
    <ul class="dropdown-menu">
        
            
<li >
    <a href="../../user_guide/utils/Counter/">Counter</a>
</li>

        
    </ul>
  </li>

                        
                            
  <li class="dropdown-submenu">
    <a tabindex="-1" href="">general concepts</a>
    <ul class="dropdown-menu">
        
            
<li >
    <a href="../../user_guide/general_concepts/activation-functions/">Activation functions</a>
</li>

        
            
<li >
    <a href="../../user_guide/general_concepts/gradient-optimization/">Gradient optimization</a>
</li>

        
            
<li >
    <a href="../../user_guide/general_concepts/linear-gradient-derivative/">Linear gradient derivative</a>
</li>

        
            
<li >
    <a href="../../user_guide/general_concepts/regularization-linear/">Regularization linear</a>
</li>

        
    </ul>
  </li>

                        
                        </ul>
                    </li>
                
                
                
                    <li class="dropdown active">
                        <a href="#" class="dropdown-toggle" data-toggle="dropdown">API <b class="caret"></b></a>
                        <ul class="dropdown-menu">
                        
                            
<li class="active">
    <a href="./">Mlxtend.classifier</a>
</li>

                        
                            
<li >
    <a href="../mlxtend.tf_classifier/">Mlxtend.tf classifier</a>
</li>

                        
                            
<li >
    <a href="../mlxtend.data/">Mlxtend.data</a>
</li>

                        
                            
<li >
    <a href="../mlxtend.evaluate/">Mlxtend.evaluate</a>
</li>

                        
                            
<li >
    <a href="../mlxtend.feature_selection/">Mlxtend.feature selection</a>
</li>

                        
                            
<li >
    <a href="../mlxtend.feature_extraction/">Mlxtend.feature extraction</a>
</li>

                        
                            
<li >
    <a href="../mlxtend.cluster/">Mlxtend.cluster</a>
</li>

                        
                            
<li >
    <a href="../mlxtend.tf_cluster/">Mlxtend.tf cluster</a>
</li>

                        
                            
<li >
    <a href="../mlxtend.file_io/">Mlxtend.file io</a>
</li>

                        
                            
<li >
    <a href="../mlxtend.plotting/">Mlxtend.plotting</a>
</li>

                        
                            
<li >
    <a href="../mlxtend.preprocessing/">Mlxtend.preprocessing</a>
</li>

                        
                            
<li >
    <a href="../mlxtend.regressor/">Mlxtend.regressor</a>
</li>

                        
                            
<li >
    <a href="../mlxtend.tf_regressor/">Mlxtend.tf regressor</a>
</li>

                        
                            
<li >
    <a href="../mlxtend.text/">Mlxtend.text</a>
</li>

                        
                            
<li >
    <a href="../mlxtend.utils/">Mlxtend.utils</a>
</li>

                        
                        </ul>
                    </li>
                
                
                
                    <li >
                        <a href="../../installation/">Installation</a>
                    </li>
                
                
                
                    <li class="dropdown">
                        <a href="#" class="dropdown-toggle" data-toggle="dropdown">About <b class="caret"></b></a>
                        <ul class="dropdown-menu">
                        
                            
<li >
    <a href="../../changelog/">Release Notes</a>
</li>

                        
                            
<li >
    <a href="../../contributing/">How To Contribute</a>
</li>

                        
                            
<li >
    <a href="../../contributors/">Contributors</a>
</li>

                        
                            
<li >
    <a href="../../license/">License</a>
</li>

                        
                            
<li >
    <a href="../../cite/">Citing Mlxtend</a>
</li>

                        
                        </ul>
                    </li>
                
                
                </ul>
            

            <ul class="nav navbar-nav navbar-right">
                <li>
                    <a href="#" data-toggle="modal" data-target="#mkdocs_search_modal">
                        <i class="fa fa-search"></i> Search
                    </a>
                </li>
                
                    <li >
                        <a rel="next" href="../../user_guide/general_concepts/regularization-linear/">
                            <i class="fa fa-arrow-left"></i> Previous
                        </a>
                    </li>
                    <li >
                        <a rel="prev" href="../mlxtend.tf_classifier/">
                            Next <i class="fa fa-arrow-right"></i>
                        </a>
                    </li>
                


                    <li>
                        <a href="https://github.com/rasbt/mlxtend">
                                <i class="fa fa-github"></i>
                            GitHub
                        </a>
                    </li>



                
            </ul>
        </div>
    </div>
</div>

    <div class="container">
        
        <div class="col-md-3"><div class="bs-sidebar hidden-print affix well" role="complementary">
    <ul class="nav bs-sidenav">
    
        <li class="main active"><a href="#adaline">Adaline</a></li>
        
            <li><a href="#methods">Methods</a></li>
                <!--  -->
        
    
        <li class="main "><a href="#ensemblevoteclassifier">EnsembleVoteClassifier</a></li>
        
            <li><a href="#methods_1">Methods</a></li>
                <!--  -->
        
    
        <li class="main "><a href="#logisticregression">LogisticRegression</a></li>
        
            <li><a href="#methods_2">Methods</a></li>
                <!--  -->
        
    
        <li class="main "><a href="#multilayerperceptron">MultiLayerPerceptron</a></li>
        
            <li><a href="#methods_3">Methods</a></li>
                <!--  -->
        
    
        <li class="main "><a href="#perceptron">Perceptron</a></li>
        
            <li><a href="#methods_4">Methods</a></li>
                <!--  -->
        
    
        <li class="main "><a href="#softmaxregression">SoftmaxRegression</a></li>
        
            <li><a href="#methods_5">Methods</a></li>
                <!--  -->
        
    
        <li class="main "><a href="#stackingclassifier">StackingClassifier</a></li>
        
            <li><a href="#methods_6">Methods</a></li>
                <!--  -->
        
    
        <li class="main "><a href="#stackingcvclassifier">StackingCVClassifier</a></li>
        
            <li><a href="#methods_7">Methods</a></li>
                <!--  -->
        
    
    </ul>
</div></div>
        <div class="col-md-9" role="main">

<p>mlxtend version: 0.5.1.dev0 </p>
<h2 id="adaline">Adaline</h2>
<p><em>Adaline(eta=0.01, epochs=50, minibatches=None, random_seed=None, print_progress=0)</em></p>
<p>ADAptive LInear NEuron classifier.</p>
<p>Note that this implementation of Adaline expects binary class labels
in {0, 1}.</p>
<p><strong>Parameters</strong></p>
<ul>
<li>
<p><code>eta</code> : float (default: 0.01)</p>
<p>solver rate (between 0.0 and 1.0)</p>
</li>
<li>
<p><code>epochs</code> : int (default: 50)</p>
<p>Passes over the training dataset.
Prior to each epoch, the dataset is shuffled
if <code>minibatches &gt; 1</code> to prevent cycles in stochastic gradient descent.</p>
</li>
<li>
<p><code>minibatches</code> : int (default: None)</p>
<p>The number of minibatches for gradient-based optimization.
If None: Normal Equations (closed-form solution)
If 1: Gradient Descent learning
If len(y): Stochastic Gradient Descent (SGD) online learning
If 1 &lt; minibatches &lt; len(y): SGD Minibatch learning</p>
</li>
<li>
<p><code>random_seed</code> : int (default: None)</p>
<p>Set random state for shuffling and initializing the weights.</p>
</li>
<li>
<p><code>print_progress</code> : int (default: 0)</p>
<p>Prints progress in fitting to stderr if not solver='normal equation'
0: No output
1: Epochs elapsed and cost
2: 1 plus time elapsed
3: 2 plus estimated time until completion</p>
</li>
</ul>
<p><strong>Attributes</strong></p>
<ul>
<li>
<p><code>w_</code> : 2d-array, shape={n_features, 1}</p>
<p>Model weights after fitting.</p>
</li>
<li>
<p><code>b_</code> : 1d-array, shape={1,}</p>
<p>Bias unit after fitting.</p>
</li>
<li>
<p><code>cost_</code> : list</p>
<p>Sum of squared errors after each epoch.</p>
</li>
</ul>
<h3 id="methods">Methods</h3>
<hr>

<p><em>fit(X, y, init_params=True)</em></p>
<p>Learn model from training data.</p>
<p><strong>Parameters</strong></p>
<ul>
<li>
<p><code>X</code> : {array-like, sparse matrix}, shape = [n_samples, n_features]</p>
<p>Training vectors, where n_samples is the number of samples and
n_features is the number of features.</p>
</li>
<li>
<p><code>y</code> : array-like, shape = [n_samples]</p>
<p>Target values.</p>
</li>
<li>
<p><code>init_params</code> : bool (default: True)</p>
<p>Re-initializes model parameters prior to fitting.
Set False to continue training with weights from
a previous model fitting.</p>
</li>
</ul>
<p><strong>Returns</strong></p>
<ul>
<li><code>self</code> : object</li>
</ul>
<hr>

<p><em>predict(X)</em></p>
<p>Predict targets from X.</p>
<p><strong>Parameters</strong></p>
<ul>
<li>
<p><code>X</code> : {array-like, sparse matrix}, shape = [n_samples, n_features]</p>
<p>Training vectors, where n_samples is the number of samples and
n_features is the number of features.</p>
</li>
</ul>
<p><strong>Returns</strong></p>
<ul>
<li>
<p><code>target_values</code> : array-like, shape = [n_samples]</p>
<p>Predicted target values.</p>
</li>
</ul>
<hr>

<p><em>score(X, y)</em></p>
<p>Compute the prediction accuracy</p>
<p><strong>Parameters</strong></p>
<ul>
<li>
<p><code>X</code> : {array-like, sparse matrix}, shape = [n_samples, n_features]</p>
<p>Training vectors, where n_samples is the number of samples and
n_features is the number of features.</p>
</li>
<li>
<p><code>y</code> : array-like, shape = [n_samples]</p>
<p>Target values (true class labels).</p>
</li>
</ul>
<p><strong>Returns</strong></p>
<ul>
<li>
<p><code>acc</code> : float</p>
<p>The prediction accuracy as a float
between 0.0 and 1.0 (perfect score).</p>
</li>
</ul>
<h2 id="ensemblevoteclassifier">EnsembleVoteClassifier</h2>
<p><em>EnsembleVoteClassifier(clfs, voting='hard', weights=None, verbose=0, refit=True)</em></p>
<p>Soft Voting/Majority Rule classifier for scikit-learn estimators.</p>
<p><strong>Parameters</strong></p>
<ul>
<li>
<p><code>clfs</code> : array-like, shape = [n_classifiers]</p>
<p>A list of classifiers.
Invoking the <code>fit</code> method on the <code>VotingClassifier</code> will fit clones
of those original classifiers that will
be stored in the class attribute
<code>self.clfs_</code> if <code>refit=True</code> (default).</p>
</li>
<li>
<p><code>voting</code> : str, {'hard', 'soft'} (default='hard')</p>
<p>If 'hard', uses predicted class labels for majority rule voting.
Else if 'soft', predicts the class label based on the argmax of
the sums of the predicted probalities, which is recommended for
an ensemble of well-calibrated classifiers.</p>
</li>
<li>
<p><code>weights</code> : array-like, shape = [n_classifiers], optional (default=<code>None</code>)</p>
<p>Sequence of weights (<code>float</code> or <code>int</code>) to weight the occurances of
predicted class labels (<code>hard</code> voting) or class probabilities
before averaging (<code>soft</code> voting). Uses uniform weights if <code>None</code>.</p>
</li>
<li>
<p><code>verbose</code> : int, optional (default=0)</p>
<p>Controls the verbosity of the building process.
- <code>verbose=0</code> (default): Prints nothing
- <code>verbose=1</code>: Prints the number &amp; name of the clf being fitted
- <code>verbose=2</code>: Prints info about the parameters of the clf being fitted
- <code>verbose&gt;2</code>: Changes <code>verbose</code> param of the underlying clf to
self.verbose - 2</p>
</li>
<li>
<p><code>refit</code> : bool (default: True)</p>
<p>Refits classifiers in <code>clfs</code> if True; uses references to the <code>clfs</code>,
otherwise (assumes that the classifiers were already fit).
Note: refit=False is incompatible to mist scikit-learn wrappers!
For instance, if any form of cross-validation is performed
this would require the re-fitting classifiers to training folds, which
would raise a NotFitterError if refit=False.
(New in mlxtend v0.6.)</p>
</li>
</ul>
<p><strong>Attributes</strong></p>
<ul>
<li>
<p><code>classes_</code> : array-like, shape = [n_predictions]</p>
</li>
<li>
<p><code>clf</code> : array-like, shape = [n_predictions]</p>
<p>The unmodified input classifiers</p>
</li>
<li>
<p><code>clf_</code> : array-like, shape = [n_predictions]</p>
<p>Fitted clones of the input classifiers</p>
</li>
</ul>
<p><strong>Examples</strong></p>
<pre><code>&gt;&gt;&gt; import numpy as np
&gt;&gt;&gt; from sklearn.linear_model import LogisticRegression
&gt;&gt;&gt; from sklearn.naive_bayes import GaussianNB
&gt;&gt;&gt; from sklearn.ensemble import RandomForestClassifier
&gt;&gt;&gt; from mlxtend.sklearn import EnsembleVoteClassifier
&gt;&gt;&gt; clf1 = LogisticRegression(random_seed=1)
&gt;&gt;&gt; clf2 = RandomForestClassifier(random_seed=1)
&gt;&gt;&gt; clf3 = GaussianNB()
&gt;&gt;&gt; X = np.array([[-1, -1], [-2, -1], [-3, -2], [1, 1], [2, 1], [3, 2]])
&gt;&gt;&gt; y = np.array([1, 1, 1, 2, 2, 2])
&gt;&gt;&gt; eclf1 = EnsembleVoteClassifier(clfs=[clf1, clf2, clf3],
... voting='hard', verbose=1)
&gt;&gt;&gt; eclf1 = eclf1.fit(X, y)
&gt;&gt;&gt; print(eclf1.predict(X))
[1 1 1 2 2 2]
&gt;&gt;&gt; eclf2 = EnsembleVoteClassifier(clfs=[clf1, clf2, clf3], voting='soft')
&gt;&gt;&gt; eclf2 = eclf2.fit(X, y)
&gt;&gt;&gt; print(eclf2.predict(X))
[1 1 1 2 2 2]
&gt;&gt;&gt; eclf3 = EnsembleVoteClassifier(clfs=[clf1, clf2, clf3],
...                          voting='soft', weights=[2,1,1])
&gt;&gt;&gt; eclf3 = eclf3.fit(X, y)
&gt;&gt;&gt; print(eclf3.predict(X))
[1 1 1 2 2 2]
&gt;&gt;&gt;
</code></pre>
<h3 id="methods_1">Methods</h3>
<hr>

<p><em>fit(X, y)</em></p>
<p>Learn weight coefficients from training data for each classifier.</p>
<p><strong>Parameters</strong></p>
<ul>
<li>
<p><code>X</code> : {array-like, sparse matrix}, shape = [n_samples, n_features]</p>
<p>Training vectors, where n_samples is the number of samples and
n_features is the number of features.</p>
</li>
<li>
<p><code>y</code> : array-like, shape = [n_samples]</p>
<p>Target values.</p>
</li>
</ul>
<p><strong>Returns</strong></p>
<ul>
<li><code>self</code> : object</li>
</ul>
<hr>

<p><em>fit_transform(X, y=None, </em><em>fit_params)</em></p>
<p>Fit to data, then transform it.</p>
<p>Fits transformer to X and y with optional parameters fit_params
and returns a transformed version of X.</p>
<p><strong>Parameters</strong></p>
<ul>
<li>
<p><code>X</code> : numpy array of shape [n_samples, n_features]</p>
<p>Training set.</p>
</li>
<li>
<p><code>y</code> : numpy array of shape [n_samples]</p>
<p>Target values.</p>
</li>
</ul>
<p><strong>Returns</strong></p>
<ul>
<li>
<p><code>X_new</code> : numpy array of shape [n_samples, n_features_new]</p>
<p>Transformed array.</p>
</li>
</ul>
<hr>

<p><em>get_params(deep=True)</em></p>
<p>Return estimator parameter names for GridSearch support.</p>
<hr>

<p><em>predict(X)</em></p>
<p>Predict class labels for X.</p>
<p><strong>Parameters</strong></p>
<ul>
<li>
<p><code>X</code> : {array-like, sparse matrix}, shape = [n_samples, n_features]</p>
<p>Training vectors, where n_samples is the number of samples and
n_features is the number of features.</p>
</li>
</ul>
<p><strong>Returns</strong></p>
<ul>
<li>
<p><code>maj</code> : array-like, shape = [n_samples]</p>
<p>Predicted class labels.</p>
</li>
</ul>
<hr>

<p><em>predict_proba(X)</em></p>
<p>Predict class probabilities for X.</p>
<p><strong>Parameters</strong></p>
<ul>
<li>
<p><code>X</code> : {array-like, sparse matrix}, shape = [n_samples, n_features]</p>
<p>Training vectors, where n_samples is the number of samples and
n_features is the number of features.</p>
</li>
</ul>
<p><strong>Returns</strong></p>
<ul>
<li>
<p><code>avg</code> : array-like, shape = [n_samples, n_classes]</p>
<p>Weighted average probability for each class per sample.</p>
</li>
</ul>
<hr>

<p><em>score(X, y, sample_weight=None)</em></p>
<p>Returns the mean accuracy on the given test data and labels.</p>
<p>In multi-label classification, this is the subset accuracy
which is a harsh metric since you require for each sample that
each label set be correctly predicted.</p>
<p><strong>Parameters</strong></p>
<ul>
<li>
<p><code>X</code> : array-like, shape = (n_samples, n_features)</p>
<p>Test samples.</p>
</li>
<li>
<p><code>y</code> : array-like, shape = (n_samples) or (n_samples, n_outputs)</p>
<p>True labels for X.</p>
</li>
<li>
<p><code>sample_weight</code> : array-like, shape = [n_samples], optional</p>
<p>Sample weights.</p>
</li>
</ul>
<p><strong>Returns</strong></p>
<ul>
<li>
<p><code>score</code> : float</p>
<p>Mean accuracy of self.predict(X) wrt. y.</p>
</li>
</ul>
<hr>

<p><em>set_params(</em><em>params)</em></p>
<p>Set the parameters of this estimator.</p>
<p>The method works on simple estimators as well as on nested objects
(such as pipelines). The latter have parameters of the form
<code>&lt;component&gt;__&lt;parameter&gt;</code> so that it's possible to update each
component of a nested object.</p>
<p><strong>Returns</strong></p>
<p>self</p>
<hr>

<p><em>transform(X)</em></p>
<p>Return class labels or probabilities for X for each estimator.</p>
<p><strong>Parameters</strong></p>
<ul>
<li>
<p><code>X</code> : {array-like, sparse matrix}, shape = [n_samples, n_features]</p>
<p>Training vectors, where n_samples is the number of samples and
n_features is the number of features.</p>
</li>
</ul>
<p><strong>Returns</strong></p>
<ul>
<li>
<p><code>If</code>voting='soft'`` : array-like = [n_classifiers, n_samples, n_classes]</p>
<p>Class probabilties calculated by each classifier.</p>
</li>
<li>
<p><code>If</code>voting='hard'`` : array-like = [n_classifiers, n_samples]</p>
<p>Class labels predicted by each classifier.</p>
</li>
</ul>
<h2 id="logisticregression">LogisticRegression</h2>
<p><em>LogisticRegression(eta=0.01, epochs=50, l2_lambda=0.0, minibatches=1, random_seed=None, print_progress=0)</em></p>
<p>Logistic regression classifier.</p>
<p>Note that this implementation of Logistic Regression
expects binary class labels in {0, 1}.</p>
<p><strong>Parameters</strong></p>
<ul>
<li>
<p><code>eta</code> : float (default: 0.01)</p>
<p>Learning rate (between 0.0 and 1.0)</p>
</li>
<li>
<p><code>epochs</code> : int (default: 50)</p>
<p>Passes over the training dataset.
Prior to each epoch, the dataset is shuffled
if <code>minibatches &gt; 1</code> to prevent cycles in stochastic gradient descent.</p>
</li>
<li>
<p><code>l2_lambda</code> : float</p>
<p>Regularization parameter for L2 regularization.
No regularization if l2_lambda=0.0.</p>
</li>
<li>
<p><code>minibatches</code> : int (default: 1)</p>
<p>The number of minibatches for gradient-based optimization.
If 1: Gradient Descent learning
If len(y): Stochastic Gradient Descent (SGD) online learning
If 1 &lt; minibatches &lt; len(y): SGD Minibatch learning</p>
</li>
<li>
<p><code>random_seed</code> : int (default: None)</p>
<p>Set random state for shuffling and initializing the weights.</p>
</li>
<li>
<p><code>print_progress</code> : int (default: 0)</p>
<p>Prints progress in fitting to stderr.
0: No output
1: Epochs elapsed and cost
2: 1 plus time elapsed
3: 2 plus estimated time until completion</p>
</li>
</ul>
<p><strong>Attributes</strong></p>
<ul>
<li>
<p><code>w_</code> : 2d-array, shape={n_features, 1}</p>
<p>Model weights after fitting.</p>
</li>
<li>
<p><code>b_</code> : 1d-array, shape={1,}</p>
<p>Bias unit after fitting.</p>
</li>
<li>
<p><code>cost_</code> : list</p>
<p>List of floats with cross_entropy cost (sgd or gd) for every
epoch.</p>
</li>
</ul>
<h3 id="methods_2">Methods</h3>
<hr>

<p><em>fit(X, y, init_params=True)</em></p>
<p>Learn model from training data.</p>
<p><strong>Parameters</strong></p>
<ul>
<li>
<p><code>X</code> : {array-like, sparse matrix}, shape = [n_samples, n_features]</p>
<p>Training vectors, where n_samples is the number of samples and
n_features is the number of features.</p>
</li>
<li>
<p><code>y</code> : array-like, shape = [n_samples]</p>
<p>Target values.</p>
</li>
<li>
<p><code>init_params</code> : bool (default: True)</p>
<p>Re-initializes model parameters prior to fitting.
Set False to continue training with weights from
a previous model fitting.</p>
</li>
</ul>
<p><strong>Returns</strong></p>
<ul>
<li><code>self</code> : object</li>
</ul>
<hr>

<p><em>predict(X)</em></p>
<p>Predict targets from X.</p>
<p><strong>Parameters</strong></p>
<ul>
<li>
<p><code>X</code> : {array-like, sparse matrix}, shape = [n_samples, n_features]</p>
<p>Training vectors, where n_samples is the number of samples and
n_features is the number of features.</p>
</li>
</ul>
<p><strong>Returns</strong></p>
<ul>
<li>
<p><code>target_values</code> : array-like, shape = [n_samples]</p>
<p>Predicted target values.</p>
</li>
</ul>
<hr>

<p><em>predict_proba(X)</em></p>
<p>Predict class probabilities of X from the net input.</p>
<p><strong>Parameters</strong></p>
<ul>
<li>
<p><code>X</code> : {array-like, sparse matrix}, shape = [n_samples, n_features]</p>
<p>Training vectors, where n_samples is the number of samples and
n_features is the number of features.</p>
</li>
</ul>
<p><strong>Returns</strong></p>
<ul>
<li><code>Class 1 probability</code> : float</li>
</ul>
<hr>

<p><em>score(X, y)</em></p>
<p>Compute the prediction accuracy</p>
<p><strong>Parameters</strong></p>
<ul>
<li>
<p><code>X</code> : {array-like, sparse matrix}, shape = [n_samples, n_features]</p>
<p>Training vectors, where n_samples is the number of samples and
n_features is the number of features.</p>
</li>
<li>
<p><code>y</code> : array-like, shape = [n_samples]</p>
<p>Target values (true class labels).</p>
</li>
</ul>
<p><strong>Returns</strong></p>
<ul>
<li>
<p><code>acc</code> : float</p>
<p>The prediction accuracy as a float
between 0.0 and 1.0 (perfect score).</p>
</li>
</ul>
<h2 id="multilayerperceptron">MultiLayerPerceptron</h2>
<p><em>MultiLayerPerceptron(eta=0.5, epochs=50, hidden_layers=[50], n_classes=None, momentum=0.0, l1=0.0, l2=0.0, dropout=1.0, decrease_const=0.0, minibatches=1, random_seed=None, print_progress=0)</em></p>
<p>Multi-layer perceptron classifier with logistic sigmoid activations</p>
<p><strong>Parameters</strong></p>
<ul>
<li>
<p><code>eta</code> : float (default: 0.5)</p>
<p>Learning rate (between 0.0 and 1.0)</p>
</li>
<li>
<p><code>epochs</code> : int (default: 50)</p>
<p>Passes over the training dataset.
Prior to each epoch, the dataset is shuffled
if <code>minibatches &gt; 1</code> to prevent cycles in stochastic gradient descent.</p>
</li>
<li>
<p><code>hidden_layers</code> : list (default: [50])</p>
<p>Number of units per hidden layer. By default 50 units in the
first hidden layer. At the moment only 1 hidden layer is supported</p>
</li>
<li>
<p><code>n_classes</code> : int (default: None)</p>
<p>A positive integer to declare the number of class labels
if not all class labels are present in a partial training set.
Gets the number of class labels automatically if None.</p>
</li>
<li>
<p><code>l1</code> : float (default: 0.0)</p>
<p>L1 regularization strength</p>
</li>
<li>
<p><code>l2</code> : float (default: 0.0)</p>
<p>L2 regularization strength</p>
</li>
<li>
<p><code>momentum</code> : float (default: 0.0)</p>
<p>Momentum constant. Factor multiplied with the
gradient of the previous epoch t-1 to improve
learning speed
w(t) := w(t) - (grad(t) + momentum * grad(t-1))</p>
</li>
<li>
<p><code>decrease_const</code> : float (default: 0.0)</p>
<p>Decrease constant. Shrinks the learning rate
after each epoch via eta / (1 + epoch*decrease_const)</p>
</li>
<li>
<p><code>minibatches</code> : int (default: 1)</p>
<p>Divide the training data into <em>k</em> minibatches
for accelerated stochastic gradient descent learning.
Gradient Descent Learning if <code>minibatches</code> = 1
Stochastic Gradient Descent learning if <code>minibatches</code> = len(y)
Minibatch learning if <code>minibatches</code> &gt; 1</p>
</li>
<li>
<p><code>random_seed</code> : int (default: None)</p>
<p>Set random state for shuffling and initializing the weights.</p>
</li>
<li>
<p><code>print_progress</code> : int (default: 0)</p>
<p>Prints progress in fitting to stderr.
0: No output
1: Epochs elapsed and cost
2: 1 plus time elapsed
3: 2 plus estimated time until completion</p>
</li>
</ul>
<p><strong>Attributes</strong></p>
<ul>
<li>
<p><code>w_</code> : 2d-array, shape=[n_features, n_classes]</p>
<p>Weights after fitting.</p>
</li>
<li>
<p><code>b_</code> : 1D-array, shape=[n_classes]</p>
<p>Bias units after fitting.</p>
</li>
<li>
<p><code>cost_</code> : list</p>
<p>List of floats; the mean categorical cross entropy
cost after each epoch.</p>
</li>
</ul>
<h3 id="methods_3">Methods</h3>
<hr>

<p><em>fit(X, y, init_params=True)</em></p>
<p>Learn model from training data.</p>
<p><strong>Parameters</strong></p>
<ul>
<li>
<p><code>X</code> : {array-like, sparse matrix}, shape = [n_samples, n_features]</p>
<p>Training vectors, where n_samples is the number of samples and
n_features is the number of features.</p>
</li>
<li>
<p><code>y</code> : array-like, shape = [n_samples]</p>
<p>Target values.</p>
</li>
<li>
<p><code>init_params</code> : bool (default: True)</p>
<p>Re-initializes model parameters prior to fitting.
Set False to continue training with weights from
a previous model fitting.</p>
</li>
</ul>
<p><strong>Returns</strong></p>
<ul>
<li><code>self</code> : object</li>
</ul>
<hr>

<p><em>predict(X)</em></p>
<p>Predict targets from X.</p>
<p><strong>Parameters</strong></p>
<ul>
<li>
<p><code>X</code> : {array-like, sparse matrix}, shape = [n_samples, n_features]</p>
<p>Training vectors, where n_samples is the number of samples and
n_features is the number of features.</p>
</li>
</ul>
<p><strong>Returns</strong></p>
<ul>
<li>
<p><code>target_values</code> : array-like, shape = [n_samples]</p>
<p>Predicted target values.</p>
</li>
</ul>
<hr>

<p><em>predict_proba(X)</em></p>
<p>Predict class probabilities of X from the net input.</p>
<p><strong>Parameters</strong></p>
<ul>
<li>
<p><code>X</code> : {array-like, sparse matrix}, shape = [n_samples, n_features]</p>
<p>Training vectors, where n_samples is the number of samples and
n_features is the number of features.</p>
</li>
</ul>
<p><strong>Returns</strong></p>
<ul>
<li><code>Class probabilties</code> : array-like, shape= [n_samples, n_classes]</li>
</ul>
<hr>

<p><em>score(X, y)</em></p>
<p>Compute the prediction accuracy</p>
<p><strong>Parameters</strong></p>
<ul>
<li>
<p><code>X</code> : {array-like, sparse matrix}, shape = [n_samples, n_features]</p>
<p>Training vectors, where n_samples is the number of samples and
n_features is the number of features.</p>
</li>
<li>
<p><code>y</code> : array-like, shape = [n_samples]</p>
<p>Target values (true class labels).</p>
</li>
</ul>
<p><strong>Returns</strong></p>
<ul>
<li>
<p><code>acc</code> : float</p>
<p>The prediction accuracy as a float
between 0.0 and 1.0 (perfect score).</p>
</li>
</ul>
<h2 id="perceptron">Perceptron</h2>
<p><em>Perceptron(eta=0.1, epochs=50, random_seed=None, print_progress=0)</em></p>
<p>Perceptron classifier.</p>
<p>Note that this implementation of the Perceptron expects binary class labels
in {0, 1}.</p>
<p><strong>Parameters</strong></p>
<ul>
<li>
<p><code>eta</code> : float (default: 0.1)</p>
<p>Learning rate (between 0.0 and 1.0)</p>
</li>
<li>
<p><code>epochs</code> : int (default: 50)</p>
<p>Number of passes over the training dataset.
Prior to each epoch, the dataset is shuffled to prevent cycles.</p>
</li>
<li>
<p><code>random_seed</code> : int</p>
<p>Random state for initializing random weights and shuffling.</p>
</li>
<li>
<p><code>print_progress</code> : int (default: 0)</p>
<p>Prints progress in fitting to stderr.
0: No output
1: Epochs elapsed and cost
2: 1 plus time elapsed
3: 2 plus estimated time until completion</p>
</li>
</ul>
<p><strong>Attributes</strong></p>
<ul>
<li>
<p><code>w_</code> : 2d-array, shape={n_features, 1}</p>
<p>Model weights after fitting.</p>
</li>
<li>
<p><code>b_</code> : 1d-array, shape={1,}</p>
<p>Bias unit after fitting.</p>
</li>
<li>
<p><code>cost_</code> : list</p>
<p>Number of misclassifications in every epoch.</p>
</li>
</ul>
<h3 id="methods_4">Methods</h3>
<hr>

<p><em>fit(X, y, init_params=True)</em></p>
<p>Learn model from training data.</p>
<p><strong>Parameters</strong></p>
<ul>
<li>
<p><code>X</code> : {array-like, sparse matrix}, shape = [n_samples, n_features]</p>
<p>Training vectors, where n_samples is the number of samples and
n_features is the number of features.</p>
</li>
<li>
<p><code>y</code> : array-like, shape = [n_samples]</p>
<p>Target values.</p>
</li>
<li>
<p><code>init_params</code> : bool (default: True)</p>
<p>Re-initializes model parameters prior to fitting.
Set False to continue training with weights from
a previous model fitting.</p>
</li>
</ul>
<p><strong>Returns</strong></p>
<ul>
<li><code>self</code> : object</li>
</ul>
<hr>

<p><em>predict(X)</em></p>
<p>Predict targets from X.</p>
<p><strong>Parameters</strong></p>
<ul>
<li>
<p><code>X</code> : {array-like, sparse matrix}, shape = [n_samples, n_features]</p>
<p>Training vectors, where n_samples is the number of samples and
n_features is the number of features.</p>
</li>
</ul>
<p><strong>Returns</strong></p>
<ul>
<li>
<p><code>target_values</code> : array-like, shape = [n_samples]</p>
<p>Predicted target values.</p>
</li>
</ul>
<hr>

<p><em>score(X, y)</em></p>
<p>Compute the prediction accuracy</p>
<p><strong>Parameters</strong></p>
<ul>
<li>
<p><code>X</code> : {array-like, sparse matrix}, shape = [n_samples, n_features]</p>
<p>Training vectors, where n_samples is the number of samples and
n_features is the number of features.</p>
</li>
<li>
<p><code>y</code> : array-like, shape = [n_samples]</p>
<p>Target values (true class labels).</p>
</li>
</ul>
<p><strong>Returns</strong></p>
<ul>
<li>
<p><code>acc</code> : float</p>
<p>The prediction accuracy as a float
between 0.0 and 1.0 (perfect score).</p>
</li>
</ul>
<h2 id="softmaxregression">SoftmaxRegression</h2>
<p><em>SoftmaxRegression(eta=0.01, epochs=50, l2=0.0, minibatches=1, n_classes=None, random_seed=None, print_progress=0)</em></p>
<p>Softmax regression classifier.</p>
<p><strong>Parameters</strong></p>
<ul>
<li>
<p><code>eta</code> : float (default: 0.01)</p>
<p>Learning rate (between 0.0 and 1.0)</p>
</li>
<li>
<p><code>epochs</code> : int (default: 50)</p>
<p>Passes over the training dataset.
Prior to each epoch, the dataset is shuffled
if <code>minibatches &gt; 1</code> to prevent cycles in stochastic gradient descent.</p>
</li>
<li>
<p><code>l2</code> : float</p>
<p>Regularization parameter for L2 regularization.
No regularization if l2=0.0.</p>
</li>
<li>
<p><code>minibatches</code> : int (default: 1)</p>
<p>The number of minibatches for gradient-based optimization.
If 1: Gradient Descent learning
If len(y): Stochastic Gradient Descent (SGD) online learning
If 1 &lt; minibatches &lt; len(y): SGD Minibatch learning</p>
</li>
<li>
<p><code>n_classes</code> : int (default: None)</p>
<p>A positive integer to declare the number of class labels
if not all class labels are present in a partial training set.
Gets the number of class labels automatically if None.</p>
</li>
<li>
<p><code>random_seed</code> : int (default: None)</p>
<p>Set random state for shuffling and initializing the weights.</p>
</li>
<li>
<p><code>print_progress</code> : int (default: 0)</p>
<p>Prints progress in fitting to stderr.
0: No output
1: Epochs elapsed and cost
2: 1 plus time elapsed
3: 2 plus estimated time until completion</p>
</li>
</ul>
<p><strong>Attributes</strong></p>
<ul>
<li>
<p><code>w_</code> : 2d-array, shape={n_features, 1}</p>
<p>Model weights after fitting.</p>
</li>
<li>
<p><code>b_</code> : 1d-array, shape={1,}</p>
<p>Bias unit after fitting.</p>
</li>
<li>
<p><code>cost_</code> : list</p>
<p>List of floats, the average cross_entropy for each epoch.</p>
</li>
</ul>
<h3 id="methods_5">Methods</h3>
<hr>

<p><em>fit(X, y, init_params=True)</em></p>
<p>Learn model from training data.</p>
<p><strong>Parameters</strong></p>
<ul>
<li>
<p><code>X</code> : {array-like, sparse matrix}, shape = [n_samples, n_features]</p>
<p>Training vectors, where n_samples is the number of samples and
n_features is the number of features.</p>
</li>
<li>
<p><code>y</code> : array-like, shape = [n_samples]</p>
<p>Target values.</p>
</li>
<li>
<p><code>init_params</code> : bool (default: True)</p>
<p>Re-initializes model parameters prior to fitting.
Set False to continue training with weights from
a previous model fitting.</p>
</li>
</ul>
<p><strong>Returns</strong></p>
<ul>
<li><code>self</code> : object</li>
</ul>
<hr>

<p><em>predict(X)</em></p>
<p>Predict targets from X.</p>
<p><strong>Parameters</strong></p>
<ul>
<li>
<p><code>X</code> : {array-like, sparse matrix}, shape = [n_samples, n_features]</p>
<p>Training vectors, where n_samples is the number of samples and
n_features is the number of features.</p>
</li>
</ul>
<p><strong>Returns</strong></p>
<ul>
<li>
<p><code>target_values</code> : array-like, shape = [n_samples]</p>
<p>Predicted target values.</p>
</li>
</ul>
<hr>

<p><em>predict_proba(X)</em></p>
<p>Predict class probabilities of X from the net input.</p>
<p><strong>Parameters</strong></p>
<ul>
<li>
<p><code>X</code> : {array-like, sparse matrix}, shape = [n_samples, n_features]</p>
<p>Training vectors, where n_samples is the number of samples and
n_features is the number of features.</p>
</li>
</ul>
<p><strong>Returns</strong></p>
<ul>
<li><code>Class probabilties</code> : array-like, shape= [n_samples, n_classes]</li>
</ul>
<hr>

<p><em>score(X, y)</em></p>
<p>Compute the prediction accuracy</p>
<p><strong>Parameters</strong></p>
<ul>
<li>
<p><code>X</code> : {array-like, sparse matrix}, shape = [n_samples, n_features]</p>
<p>Training vectors, where n_samples is the number of samples and
n_features is the number of features.</p>
</li>
<li>
<p><code>y</code> : array-like, shape = [n_samples]</p>
<p>Target values (true class labels).</p>
</li>
</ul>
<p><strong>Returns</strong></p>
<ul>
<li>
<p><code>acc</code> : float</p>
<p>The prediction accuracy as a float
between 0.0 and 1.0 (perfect score).</p>
</li>
</ul>
<h2 id="stackingclassifier">StackingClassifier</h2>
<p><em>StackingClassifier(classifiers, meta_classifier, use_probas=False, average_probas=False, verbose=0)</em></p>
<p>A Stacking classifier for scikit-learn estimators for classification.</p>
<p><strong>Parameters</strong></p>
<ul>
<li>
<p><code>classifiers</code> : array-like, shape = [n_classifiers]</p>
<p>A list of classifiers.
Invoking the <code>fit</code> method on the <code>StackingClassifer</code> will fit clones
of these original classifiers that will
be stored in the class attribute
<code>self.clfs_</code>.</p>
</li>
<li>
<p><code>meta_classifier</code> : object</p>
<p>The meta-classifier to be fitted on the ensemble of
classifiers</p>
</li>
<li>
<p><code>use_probas</code> : bool (default: False)</p>
<p>If True, trains meta-classifier based on predicted probabilities
instead of class labels.</p>
</li>
<li>
<p><code>average_probas</code> : bool (default: False)</p>
<p>Averages the probabilities as meta features if True.</p>
</li>
<li>
<p><code>verbose</code> : int, optional (default=0)</p>
<p>Controls the verbosity of the building process.
- <code>verbose=0</code> (default): Prints nothing
- <code>verbose=1</code>: Prints the number &amp; name of the regressor being fitted
- <code>verbose=2</code>: Prints info about the parameters of the
regressor being fitted
- <code>verbose&gt;2</code>: Changes <code>verbose</code> param of the underlying regressor to
self.verbose - 2</p>
</li>
</ul>
<p><strong>Attributes</strong></p>
<ul>
<li>
<p><code>clfs_</code> : list, shape=[n_classifiers]</p>
<p>Fitted classifiers (clones of the original classifiers)</p>
</li>
<li>
<p><code>meta_clf_</code> : estimator</p>
<p>Fitted meta-classifier (clone of the original meta-estimator)</p>
</li>
</ul>
<h3 id="methods_6">Methods</h3>
<hr>

<p><em>fit(X, y)</em></p>
<p>Fit ensemble classifers and the meta-classifier.</p>
<p><strong>Parameters</strong></p>
<ul>
<li>
<p><code>X</code> : {array-like, sparse matrix}, shape = [n_samples, n_features]</p>
<p>Training vectors, where n_samples is the number of samples and
n_features is the number of features.</p>
</li>
<li>
<p><code>y</code> : array-like, shape = [n_samples]</p>
<p>Target values.</p>
</li>
</ul>
<p><strong>Returns</strong></p>
<ul>
<li><code>self</code> : object</li>
</ul>
<hr>

<p><em>fit_transform(X, y=None, </em><em>fit_params)</em></p>
<p>Fit to data, then transform it.</p>
<p>Fits transformer to X and y with optional parameters fit_params
and returns a transformed version of X.</p>
<p><strong>Parameters</strong></p>
<ul>
<li>
<p><code>X</code> : numpy array of shape [n_samples, n_features]</p>
<p>Training set.</p>
</li>
<li>
<p><code>y</code> : numpy array of shape [n_samples]</p>
<p>Target values.</p>
</li>
</ul>
<p><strong>Returns</strong></p>
<ul>
<li>
<p><code>X_new</code> : numpy array of shape [n_samples, n_features_new]</p>
<p>Transformed array.</p>
</li>
</ul>
<hr>

<p><em>get_params(deep=True)</em></p>
<p>Return estimator parameter names for GridSearch support.</p>
<hr>

<p><em>predict(X)</em></p>
<p>Predict target values for X.</p>
<p><strong>Parameters</strong></p>
<ul>
<li>
<p><code>X</code> : {array-like, sparse matrix}, shape = [n_samples, n_features]</p>
<p>Training vectors, where n_samples is the number of samples and
n_features is the number of features.</p>
</li>
</ul>
<p><strong>Returns</strong></p>
<ul>
<li>
<p><code>labels</code> : array-like, shape = [n_samples]</p>
<p>Predicted class labels.</p>
</li>
</ul>
<hr>

<p><em>predict_proba(X)</em></p>
<p>Predict class probabilities for X.</p>
<p><strong>Parameters</strong></p>
<ul>
<li>
<p><code>X</code> : {array-like, sparse matrix}, shape = [n_samples, n_features]</p>
<p>Training vectors, where n_samples is the number of samples and
n_features is the number of features.</p>
</li>
</ul>
<p><strong>Returns</strong></p>
<ul>
<li>
<p><code>proba</code> : array-like, shape = [n_samples, n_classes]</p>
<p>Probability for each class per sample.</p>
</li>
</ul>
<hr>

<p><em>score(X, y, sample_weight=None)</em></p>
<p>Returns the mean accuracy on the given test data and labels.</p>
<p>In multi-label classification, this is the subset accuracy
which is a harsh metric since you require for each sample that
each label set be correctly predicted.</p>
<p><strong>Parameters</strong></p>
<ul>
<li>
<p><code>X</code> : array-like, shape = (n_samples, n_features)</p>
<p>Test samples.</p>
</li>
<li>
<p><code>y</code> : array-like, shape = (n_samples) or (n_samples, n_outputs)</p>
<p>True labels for X.</p>
</li>
<li>
<p><code>sample_weight</code> : array-like, shape = [n_samples], optional</p>
<p>Sample weights.</p>
</li>
</ul>
<p><strong>Returns</strong></p>
<ul>
<li>
<p><code>score</code> : float</p>
<p>Mean accuracy of self.predict(X) wrt. y.</p>
</li>
</ul>
<hr>

<p><em>set_params(</em><em>params)</em></p>
<p>Set the parameters of this estimator.</p>
<p>The method works on simple estimators as well as on nested objects
(such as pipelines). The latter have parameters of the form
<code>&lt;component&gt;__&lt;parameter&gt;</code> so that it's possible to update each
component of a nested object.</p>
<p><strong>Returns</strong></p>
<p>self</p>
<h2 id="stackingcvclassifier">StackingCVClassifier</h2>
<p><em>StackingCVClassifier(classifiers, meta_classifier, use_probas=False, n_folds=2, use_features_in_secondary=False, stratify=True, random_state=None, shuffle=True, verbose=0)</em></p>
<p>A 'Stacking Cross-Validation' classifier for scikit-learn estimators.</p>
<p>New in mlxtend v0.4.3</p>
<p><strong>Parameters</strong></p>
<ul>
<li>
<p><code>classifiers</code> : array-like, shape = [n_classifiers]</p>
<p>A list of classifiers.
Invoking the <code>fit</code> method on the <code>StackingClassifer</code> will fit clones
of these original classifiers that will
be stored in the class attribute
<code>self.clfs_</code>.</p>
</li>
<li>
<p><code>meta_classifier</code> : object</p>
<p>The meta-classifier to be fitted on the ensemble of
classifiers</p>
</li>
<li>
<p><code>use_probas</code> : bool (default: False)</p>
<p>If True, trains meta-classifier based on predicted probabilities
instead of class labels.</p>
</li>
<li>
<p><code>n_folds</code> : int (default=2)</p>
<p>The number of folds used for while creating training data for the
meta-classifier during fitting.</p>
</li>
<li>
<p><code>use_features_in_secondary</code> : bool (default: False)</p>
<p>If True, the meta-classifier will be trained both on the predictions
of the original classifiers and the original dataset.
If False, the meta-classifier will be trained only on the predictions
of the original classifiers.</p>
</li>
<li>
<p><code>stratify</code> : bool (default: True)</p>
<p>If True, the cross-validation technique used for fitting the classifier
will be Stratified K-Fold.
If False, the cross-validation technique used will be Regular K-Fold.
It is highly recommended to use Stratified K-Fold.</p>
</li>
<li>
<p><code>shuffle</code> : bool (default: True)</p>
<p>If True, when fitting, the training data will be shuffled prior to
cross-validation.
random_state: None, int, or RandomState
When shuffle=True, pseudo-random number generator state used for
shuffling. If None, use default numpy RNG for shuffling.</p>
</li>
<li>
<p><code>verbose</code> : int, optional (default=0)</p>
<p>Controls the verbosity of the building process.
- <code>verbose=0</code> (default): Prints nothing
- <code>verbose=1</code>: Prints the number &amp; name of the regressor being fitted
and which fold is currently being used for fitting
- <code>verbose=2</code>: Prints info about the parameters of the
regressor being fitted
- <code>verbose&gt;2</code>: Changes <code>verbose</code> param of the underlying regressor to
self.verbose - 2</p>
</li>
</ul>
<p><strong>Attributes</strong></p>
<ul>
<li>
<p><code>clfs_</code> : list, shape=[n_classifiers]</p>
<p>Fitted classifiers (clones of the original classifiers)</p>
</li>
<li>
<p><code>meta_clf_</code> : estimator</p>
<p>Fitted meta-classifier (clone of the original meta-estimator)</p>
</li>
</ul>
<h3 id="methods_7">Methods</h3>
<hr>

<p><em>fit(X, y)</em></p>
<p>Fit ensemble classifers and the meta-classifier.</p>
<p><strong>Parameters</strong></p>
<ul>
<li>
<p><code>X</code> : {array-like, sparse matrix}, shape = [n_samples, n_features]</p>
<p>Training vectors, where n_samples is the number of samples and
n_features is the number of features.</p>
</li>
<li>
<p><code>y</code> : array-like, shape = [n_samples]</p>
<p>Target values.</p>
</li>
</ul>
<p><strong>Returns</strong></p>
<ul>
<li><code>self</code> : object</li>
</ul>
<hr>

<p><em>fit_transform(X, y=None, </em><em>fit_params)</em></p>
<p>Fit to data, then transform it.</p>
<p>Fits transformer to X and y with optional parameters fit_params
and returns a transformed version of X.</p>
<p><strong>Parameters</strong></p>
<ul>
<li>
<p><code>X</code> : numpy array of shape [n_samples, n_features]</p>
<p>Training set.</p>
</li>
<li>
<p><code>y</code> : numpy array of shape [n_samples]</p>
<p>Target values.</p>
</li>
</ul>
<p><strong>Returns</strong></p>
<ul>
<li>
<p><code>X_new</code> : numpy array of shape [n_samples, n_features_new]</p>
<p>Transformed array.</p>
</li>
</ul>
<hr>

<p><em>get_params(deep=True)</em></p>
<p>Return estimator parameter names for GridSearch support.</p>
<hr>

<p><em>predict(X)</em></p>
<p>Predict target values for X.</p>
<p><strong>Parameters</strong></p>
<ul>
<li>
<p><code>X</code> : {array-like, sparse matrix}, shape = [n_samples, n_features]</p>
<p>Training vectors, where n_samples is the number of samples and
n_features is the number of features.</p>
</li>
</ul>
<p><strong>Returns</strong></p>
<ul>
<li>
<p><code>labels</code> : array-like, shape = [n_samples]</p>
<p>Predicted class labels.</p>
</li>
</ul>
<hr>

<p><em>predict_proba(X)</em></p>
<p>Predict class probabilities for X.</p>
<p><strong>Parameters</strong></p>
<ul>
<li>
<p><code>X</code> : {array-like, sparse matrix}, shape = [n_samples, n_features]</p>
<p>Training vectors, where n_samples is the number of samples and
n_features is the number of features.</p>
</li>
</ul>
<p><strong>Returns</strong></p>
<ul>
<li>
<p><code>proba</code> : array-like, shape = [n_samples, n_classes]</p>
<p>Probability for each class per sample.</p>
</li>
</ul>
<hr>

<p><em>score(X, y, sample_weight=None)</em></p>
<p>Returns the mean accuracy on the given test data and labels.</p>
<p>In multi-label classification, this is the subset accuracy
which is a harsh metric since you require for each sample that
each label set be correctly predicted.</p>
<p><strong>Parameters</strong></p>
<ul>
<li>
<p><code>X</code> : array-like, shape = (n_samples, n_features)</p>
<p>Test samples.</p>
</li>
<li>
<p><code>y</code> : array-like, shape = (n_samples) or (n_samples, n_outputs)</p>
<p>True labels for X.</p>
</li>
<li>
<p><code>sample_weight</code> : array-like, shape = [n_samples], optional</p>
<p>Sample weights.</p>
</li>
</ul>
<p><strong>Returns</strong></p>
<ul>
<li>
<p><code>score</code> : float</p>
<p>Mean accuracy of self.predict(X) wrt. y.</p>
</li>
</ul>
<hr>

<p><em>set_params(</em><em>params)</em></p>
<p>Set the parameters of this estimator.</p>
<p>The method works on simple estimators as well as on nested objects
(such as pipelines). The latter have parameters of the form
<code>&lt;component&gt;__&lt;parameter&gt;</code> so that it's possible to update each
component of a nested object.</p>
<p><strong>Returns</strong></p>
<p>self</p></div>
        
    </div>

    <footer class="col-md-12 text-center">
        <hr>
        <p>
        <small>Copyright &copy; 2014-2017 <a href="http://sebastianraschka.com">Sebastian Raschka</a><br></small>
        
        <small>Documentation built with <a href="http://www.mkdocs.org/">MkDocs</a>.</p></small>
    </footer>

    <script src="../../js/jquery-1.10.2.min.js"></script>
    <script src="../../js/bootstrap-3.0.3.min.js"></script>
    <script src="../../js/highlight.pack.js"></script>
    <script>hljs.initHighlightingOnLoad();</script>
    <script>
    var base_url = '../..';
    </script>
    <script data-main="../../mkdocs/js/search.js" src="../../mkdocs/js/require.js"></script>
    <script src="../../js/base.js"></script>
    <script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS_HTML"></script>
    <script src="../../mathjaxhelper.js"></script>

    <div class="modal" id="mkdocs_search_modal" tabindex="-1" role="dialog" aria-labelledby="Search Modal" aria-hidden="true">
        <div class="modal-dialog">
            <div class="modal-content">
                <div class="modal-header">
                    <button type="button" class="close" data-dismiss="modal">
                        <span aria-hidden="true">&times;</span>
                        <span class="sr-only">Close</span>
                    </button>
                    <h4 class="modal-title" id="exampleModalLabel">Search</h4>
                </div>
                <div class="modal-body">
                    <p>
                        From here you can search these documents. Enter your search terms below.
                    </p>
                    <form role="form">
                        <div class="form-group">
                            <input type="text" class="form-control" placeholder="Search..." id="mkdocs-search-query">
                        </div>
                    </form>
                    <div id="mkdocs-search-results"></div>
                </div>
                <div class="modal-footer">
                </div>
            </div>
        </div>
    </div>

    </body>

</html>

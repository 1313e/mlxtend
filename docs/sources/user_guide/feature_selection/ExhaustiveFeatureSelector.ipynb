{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sebastian Raschka, 2015-2016  \n",
    "`mlxtend`, a library of extension and helper modules for Python's data analysis and machine learning libraries\n",
    "\n",
    "- GitHub repository: https://github.com/rasbt/mlxtend\n",
    "- Documentation: http://rasbt.github.io/mlxtend/\n",
    "\n",
    "View this page in [jupyter nbviewer](http://nbviewer.ipython.org/github/rasbt/mlxtend/blob/master/docs/sources/_ipynb_templates/math/num_permutations.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sebastian Raschka \n",
      "last updated: 2016-09-08 \n",
      "\n",
      "CPython 3.5.2\n",
      "IPython 5.1.0\n",
      "\n",
      "matplotlib 1.5.1\n",
      "numpy 1.11.1\n",
      "scipy 0.17.1\n",
      "mlxtend 0.4.3dev0\n"
     ]
    }
   ],
   "source": [
    "%load_ext watermark\n",
    "%watermark -a 'Sebastian Raschka' -u -d -v -p matplotlib,numpy,scipy,mlxtend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exhaustive Feature Selector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implementation of an *exhaustive feature selector* for sampling and evaluating all possible feature combinations in a specified range."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> from mlxtend.feature_selection import ExhaustiveFeatureSelector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This exhaustive feature selection algorithm is a wrapper approach for brute-force evaluation of feature subsets; the best subset is selected by optimizing a specified performance metric given an arbitrary regressor or classifier. For instance, if the classifier is a logistic regression and the dataset consists of 4 features, the alogorithm will evaluate all 15 feature combinations (if `min_features=1` and `max_features=4`)\n",
    "\n",
    "- {0}\n",
    "- {1}\n",
    "- {2}\n",
    "- {3}\n",
    "- {0, 1}\n",
    "- {0, 2}\n",
    "- {0, 3}\n",
    "- {1, 2}\n",
    "- {1, 3}\n",
    "- {2, 3}\n",
    "- {0, 1, 2}\n",
    "- {0, 1, 3}\n",
    "- {0, 2, 3}\n",
    "- {1, 2, 3}\n",
    "- {0, 1, 2, 3}\n",
    "\n",
    "and select the one that results in the best performance (e.g., classification accuracy) of the logistic regression classifier.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Examples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 1 - A simple Iris Example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initializing a simple classifier from scikit-learn:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Features: 15/15"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best accuracy score: 0.97\n",
      "Best subset: (0, 2, 3)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.datasets import load_iris\n",
    "from mlxtend.feature_selection import ExhaustiveFeatureSelector as EFS\n",
    "\n",
    "iris = load_iris()\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "\n",
    "knn = KNeighborsClassifier(n_neighbors=3)\n",
    "\n",
    "efs1 = EFS(knn, \n",
    "           min_features=1,\n",
    "           max_features=4,\n",
    "           scoring='accuracy',\n",
    "           print_progress=True,\n",
    "           cv=5)\n",
    "\n",
    "efs1 = efs1.fit(X, y)\n",
    "\n",
    "print('Best accuracy score: %.2f' % efs1.best_score_)\n",
    "print('Best subset:', efs1.best_idx_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Via the `subsets_` attribute, we can take a look at the selected feature indices at each step:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: {'avg_score': 0.65999999999999992,\n",
       "  'cv_scores': array([ 0.53333333,  0.63333333,  0.73333333,  0.76666667,  0.63333333]),\n",
       "  'feature_idx': (0,)},\n",
       " 1: {'avg_score': 0.56666666666666665,\n",
       "  'cv_scores': array([ 0.53333333,  0.63333333,  0.6       ,  0.5       ,  0.56666667]),\n",
       "  'feature_idx': (1,)},\n",
       " 2: {'avg_score': 0.95333333333333337,\n",
       "  'cv_scores': array([ 0.93333333,  1.        ,  0.9       ,  0.93333333,  1.        ]),\n",
       "  'feature_idx': (2,)},\n",
       " 3: {'avg_score': 0.94666666666666666,\n",
       "  'cv_scores': array([ 0.96666667,  0.96666667,  0.93333333,  0.86666667,  1.        ]),\n",
       "  'feature_idx': (3,)},\n",
       " 4: {'avg_score': 0.72666666666666668,\n",
       "  'cv_scores': array([ 0.66666667,  0.8       ,  0.63333333,  0.86666667,  0.66666667]),\n",
       "  'feature_idx': (0, 1)},\n",
       " 5: {'avg_score': 0.94666666666666666,\n",
       "  'cv_scores': array([ 0.96666667,  1.        ,  0.86666667,  0.93333333,  0.96666667]),\n",
       "  'feature_idx': (0, 2)},\n",
       " 6: {'avg_score': 0.95333333333333337,\n",
       "  'cv_scores': array([ 0.96666667,  0.96666667,  0.9       ,  0.93333333,  1.        ]),\n",
       "  'feature_idx': (0, 3)},\n",
       " 7: {'avg_score': 0.94666666666666666,\n",
       "  'cv_scores': array([ 0.96666667,  1.        ,  0.9       ,  0.93333333,  0.93333333]),\n",
       "  'feature_idx': (1, 2)},\n",
       " 8: {'avg_score': 0.94000000000000006,\n",
       "  'cv_scores': array([ 0.96666667,  0.96666667,  0.86666667,  0.93333333,  0.96666667]),\n",
       "  'feature_idx': (1, 3)},\n",
       " 9: {'avg_score': 0.95333333333333337,\n",
       "  'cv_scores': array([ 0.96666667,  0.96666667,  0.9       ,  0.93333333,  1.        ]),\n",
       "  'feature_idx': (2, 3)},\n",
       " 10: {'avg_score': 0.94000000000000006,\n",
       "  'cv_scores': array([ 0.96666667,  0.96666667,  0.86666667,  0.93333333,  0.96666667]),\n",
       "  'feature_idx': (0, 1, 2)},\n",
       " 11: {'avg_score': 0.94666666666666666,\n",
       "  'cv_scores': array([ 0.93333333,  0.96666667,  0.9       ,  0.93333333,  1.        ]),\n",
       "  'feature_idx': (0, 1, 3)},\n",
       " 12: {'avg_score': 0.97333333333333338,\n",
       "  'cv_scores': array([ 0.96666667,  0.96666667,  0.96666667,  0.96666667,  1.        ]),\n",
       "  'feature_idx': (0, 2, 3)},\n",
       " 13: {'avg_score': 0.95999999999999996,\n",
       "  'cv_scores': array([ 0.96666667,  0.96666667,  0.93333333,  0.93333333,  1.        ]),\n",
       "  'feature_idx': (1, 2, 3)},\n",
       " 14: {'avg_score': 0.96666666666666679,\n",
       "  'cv_scores': array([ 0.96666667,  0.96666667,  0.93333333,  0.96666667,  1.        ]),\n",
       "  'feature_idx': (0, 1, 2, 3)}}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "efs1.subsets_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 2 - Visualizing the feature selection results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " For our convenience, we can visualize the output from the feature selection in a pandas DataFrame format using the `get_metric_dict` method of the `ExhaustiveFeatureSelector` object. The columns `std_dev` and `std_err` represent the standard deviation and standard errors of the cross-validation scores, respectively."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below, we see the DataFrame of the Sequential Forward Selector from Example 2:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Features: 15/15"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>avg_score</th>\n",
       "      <th>ci_bound</th>\n",
       "      <th>cv_scores</th>\n",
       "      <th>feature_idx</th>\n",
       "      <th>std_dev</th>\n",
       "      <th>std_err</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.973333</td>\n",
       "      <td>0.0171372</td>\n",
       "      <td>[0.966666666667, 0.966666666667, 0.96666666666...</td>\n",
       "      <td>(0, 2, 3)</td>\n",
       "      <td>0.0133333</td>\n",
       "      <td>0.00666667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.966667</td>\n",
       "      <td>0.0270963</td>\n",
       "      <td>[0.966666666667, 0.966666666667, 0.93333333333...</td>\n",
       "      <td>(0, 1, 2, 3)</td>\n",
       "      <td>0.0210819</td>\n",
       "      <td>0.0105409</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.96</td>\n",
       "      <td>0.0320608</td>\n",
       "      <td>[0.966666666667, 0.966666666667, 0.93333333333...</td>\n",
       "      <td>(1, 2, 3)</td>\n",
       "      <td>0.0249444</td>\n",
       "      <td>0.0124722</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.953333</td>\n",
       "      <td>0.0514116</td>\n",
       "      <td>[0.933333333333, 1.0, 0.9, 0.933333333333, 1.0]</td>\n",
       "      <td>(2,)</td>\n",
       "      <td>0.04</td>\n",
       "      <td>0.02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.953333</td>\n",
       "      <td>0.0436915</td>\n",
       "      <td>[0.966666666667, 0.966666666667, 0.9, 0.933333...</td>\n",
       "      <td>(0, 3)</td>\n",
       "      <td>0.0339935</td>\n",
       "      <td>0.0169967</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.953333</td>\n",
       "      <td>0.0436915</td>\n",
       "      <td>[0.966666666667, 0.966666666667, 0.9, 0.933333...</td>\n",
       "      <td>(2, 3)</td>\n",
       "      <td>0.0339935</td>\n",
       "      <td>0.0169967</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.946667</td>\n",
       "      <td>0.0581151</td>\n",
       "      <td>[0.966666666667, 0.966666666667, 0.93333333333...</td>\n",
       "      <td>(3,)</td>\n",
       "      <td>0.0452155</td>\n",
       "      <td>0.0226078</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.946667</td>\n",
       "      <td>0.0581151</td>\n",
       "      <td>[0.966666666667, 1.0, 0.866666666667, 0.933333...</td>\n",
       "      <td>(0, 2)</td>\n",
       "      <td>0.0452155</td>\n",
       "      <td>0.0226078</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.946667</td>\n",
       "      <td>0.0436915</td>\n",
       "      <td>[0.966666666667, 1.0, 0.9, 0.933333333333, 0.9...</td>\n",
       "      <td>(1, 2)</td>\n",
       "      <td>0.0339935</td>\n",
       "      <td>0.0169967</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.946667</td>\n",
       "      <td>0.0436915</td>\n",
       "      <td>[0.933333333333, 0.966666666667, 0.9, 0.933333...</td>\n",
       "      <td>(0, 1, 3)</td>\n",
       "      <td>0.0339935</td>\n",
       "      <td>0.0169967</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.94</td>\n",
       "      <td>0.0499631</td>\n",
       "      <td>[0.966666666667, 0.966666666667, 0.86666666666...</td>\n",
       "      <td>(1, 3)</td>\n",
       "      <td>0.038873</td>\n",
       "      <td>0.0194365</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.94</td>\n",
       "      <td>0.0499631</td>\n",
       "      <td>[0.966666666667, 0.966666666667, 0.86666666666...</td>\n",
       "      <td>(0, 1, 2)</td>\n",
       "      <td>0.038873</td>\n",
       "      <td>0.0194365</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.726667</td>\n",
       "      <td>0.11623</td>\n",
       "      <td>[0.666666666667, 0.8, 0.633333333333, 0.866666...</td>\n",
       "      <td>(0, 1)</td>\n",
       "      <td>0.0904311</td>\n",
       "      <td>0.0452155</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.66</td>\n",
       "      <td>0.106334</td>\n",
       "      <td>[0.533333333333, 0.633333333333, 0.73333333333...</td>\n",
       "      <td>(0,)</td>\n",
       "      <td>0.0827312</td>\n",
       "      <td>0.0413656</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.566667</td>\n",
       "      <td>0.0605892</td>\n",
       "      <td>[0.533333333333, 0.633333333333, 0.6, 0.5, 0.5...</td>\n",
       "      <td>(1,)</td>\n",
       "      <td>0.0471405</td>\n",
       "      <td>0.0235702</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   avg_score   ci_bound                                          cv_scores  \\\n",
       "12  0.973333  0.0171372  [0.966666666667, 0.966666666667, 0.96666666666...   \n",
       "14  0.966667  0.0270963  [0.966666666667, 0.966666666667, 0.93333333333...   \n",
       "13      0.96  0.0320608  [0.966666666667, 0.966666666667, 0.93333333333...   \n",
       "2   0.953333  0.0514116    [0.933333333333, 1.0, 0.9, 0.933333333333, 1.0]   \n",
       "6   0.953333  0.0436915  [0.966666666667, 0.966666666667, 0.9, 0.933333...   \n",
       "9   0.953333  0.0436915  [0.966666666667, 0.966666666667, 0.9, 0.933333...   \n",
       "3   0.946667  0.0581151  [0.966666666667, 0.966666666667, 0.93333333333...   \n",
       "5   0.946667  0.0581151  [0.966666666667, 1.0, 0.866666666667, 0.933333...   \n",
       "7   0.946667  0.0436915  [0.966666666667, 1.0, 0.9, 0.933333333333, 0.9...   \n",
       "11  0.946667  0.0436915  [0.933333333333, 0.966666666667, 0.9, 0.933333...   \n",
       "8       0.94  0.0499631  [0.966666666667, 0.966666666667, 0.86666666666...   \n",
       "10      0.94  0.0499631  [0.966666666667, 0.966666666667, 0.86666666666...   \n",
       "4   0.726667    0.11623  [0.666666666667, 0.8, 0.633333333333, 0.866666...   \n",
       "0       0.66   0.106334  [0.533333333333, 0.633333333333, 0.73333333333...   \n",
       "1   0.566667  0.0605892  [0.533333333333, 0.633333333333, 0.6, 0.5, 0.5...   \n",
       "\n",
       "     feature_idx    std_dev     std_err  \n",
       "12     (0, 2, 3)  0.0133333  0.00666667  \n",
       "14  (0, 1, 2, 3)  0.0210819   0.0105409  \n",
       "13     (1, 2, 3)  0.0249444   0.0124722  \n",
       "2           (2,)       0.04        0.02  \n",
       "6         (0, 3)  0.0339935   0.0169967  \n",
       "9         (2, 3)  0.0339935   0.0169967  \n",
       "3           (3,)  0.0452155   0.0226078  \n",
       "5         (0, 2)  0.0452155   0.0226078  \n",
       "7         (1, 2)  0.0339935   0.0169967  \n",
       "11     (0, 1, 3)  0.0339935   0.0169967  \n",
       "8         (1, 3)   0.038873   0.0194365  \n",
       "10     (0, 1, 2)   0.038873   0.0194365  \n",
       "4         (0, 1)  0.0904311   0.0452155  \n",
       "0           (0,)  0.0827312   0.0413656  \n",
       "1           (1,)  0.0471405   0.0235702  "
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "iris = load_iris()\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "\n",
    "knn = KNeighborsClassifier(n_neighbors=3)\n",
    "\n",
    "efs1 = EFS(knn, \n",
    "           min_features=1,\n",
    "           max_features=4,\n",
    "           scoring='accuracy',\n",
    "           print_progress=True,\n",
    "           cv=5)\n",
    "\n",
    "efs1 = efs1.fit(X, y)\n",
    "\n",
    "df = pd.DataFrame.from_dict(efs1.get_metric_dict()).T\n",
    "df.sort_values('avg_score', inplace=True, ascending=False)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAE5CAYAAAB/KzxGAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1191ef320>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "metric_dict = efs1.get_metric_dict()\n",
    "\n",
    "fig = plt.figure()\n",
    "k_feat = sorted(metric_dict.keys())\n",
    "avg = [metric_dict[k]['avg_score'] for k in k_feat]\n",
    "\n",
    "upper, lower = [], []\n",
    "for k in k_feat:\n",
    "    upper.append(metric_dict[k]['avg_score'] +\n",
    "                 metric_dict[k]['std_dev'])\n",
    "    lower.append(metric_dict[k]['avg_score'] -\n",
    "                 metric_dict[k]['std_dev'])\n",
    "    \n",
    "plt.fill_between(k_feat,\n",
    "                 upper,\n",
    "                 lower,\n",
    "                 alpha=0.2,\n",
    "                 color='blue',\n",
    "                 lw=1)\n",
    "\n",
    "plt.plot(k_feat, avg, color='blue', marker='o')\n",
    "plt.ylabel('Accuracy +/- Standard Deviation')\n",
    "plt.xlabel('Number of Features')\n",
    "feature_min = len(metric_dict[k_feat[0]]['feature_idx'])\n",
    "feature_max = len(metric_dict[k_feat[-1]]['feature_idx'])\n",
    "plt.xticks(k_feat, \n",
    "           [str(metric_dict[k]['feature_idx']) for k in k_feat], \n",
    "           rotation=90)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 3 - Exhaustive Feature Selection for Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similar to the classification examples above, the `SequentialFeatureSelector` also supports scikit-learn's estimators\n",
    "for regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Features: 5720/5720"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best accuracy score: -30.50\n",
      "Best subset: (1, 4, 7, 8, 9, 10, 12)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.datasets import load_boston\n",
    "\n",
    "boston = load_boston()\n",
    "X, y = boston.data, boston.target\n",
    "\n",
    "lr = LinearRegression()\n",
    "\n",
    "efs = EFS(lr, \n",
    "          min_features=3,\n",
    "          max_features=7,\n",
    "          scoring='mean_squared_error',\n",
    "          cv=10)\n",
    "\n",
    "efs.fit(X, y)\n",
    "\n",
    "print('Best accuracy score: %.2f' % efs.best_score_)\n",
    "print('Best subset:', efs.best_idx_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Example 4 - Using the Selected Feature Subset For Making New Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Initialize the dataset\n",
    "\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.cross_validation import train_test_split\n",
    "\n",
    "iris = load_iris()\n",
    "X, y = iris.data, iris.target\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "         X, y, test_size=0.33, random_state=1)\n",
    "\n",
    "knn = KNeighborsClassifier(n_neighbors=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Features: 15/15"
     ]
    }
   ],
   "source": [
    "# Select the \"best\" three features via\n",
    "# 5-fold cross-validation on the training set.\n",
    "\n",
    "from mlxtend.feature_selection import ExhaustiveFeatureSelector as EFS\n",
    "\n",
    "efs1 = EFS(knn, \n",
    "           min_features=1,\n",
    "           max_features=4,\n",
    "           scoring='accuracy',\n",
    "           cv=5)\n",
    "efs1 = efs1.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected features: (2, 3)\n"
     ]
    }
   ],
   "source": [
    "print('Selected features:', efs1.best_idx_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set accuracy: 96.00 %\n"
     ]
    }
   ],
   "source": [
    "# Generate the new subsets based on the selected features\n",
    "# Note that the transform call is equivalent to\n",
    "# X_train[:, efs1.k_feature_idx_]\n",
    "\n",
    "X_train_efs = efs1.transform(X_train)\n",
    "X_test_efs = efs1.transform(X_test)\n",
    "\n",
    "# Fit the estimator using the new feature subset\n",
    "# and make a prediction on the test data\n",
    "knn.fit(X_train_efs, y_train)\n",
    "y_pred = knn.predict(X_test_efs)\n",
    "\n",
    "# Compute the accuracy of the prediction\n",
    "acc = float((y_test == y_pred).sum()) / y_pred.shape[0]\n",
    "print('Test set accuracy: %.2f %%' % (acc*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 5 - Exhaustive Feature Selection and GridSearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Initialize the dataset\n",
    "\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.cross_validation import train_test_split\n",
    "\n",
    "iris = load_iris()\n",
    "X, y = iris.data, iris.target\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "         X, y, test_size=0.33, random_state=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use scikit-learn's `GridSearch` to tune the hyperparameters of the `LogisticRegression` estimator inside the `ExhaustiveFeatureSelector` and use it for prediction in the pipeline. **Note that the `clone_estimator` attribute needs to be set to `False`.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 2 folds for each of 3 candidates, totalling 6 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=2)]: Done   6 out of   6 | elapsed:  1.4min finished\n"
     ]
    }
   ],
   "source": [
    "from sklearn.grid_search import GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from mlxtend.feature_selection import ExhaustiveFeatureSelector as EFS\n",
    "\n",
    "lr = LogisticRegression(multi_class='multinomial', \n",
    "                        solver='lbfgs', \n",
    "                        random_state=123)\n",
    "\n",
    "efs1 = EFS(estimator=lr, \n",
    "           min_features=2,\n",
    "           max_features=3,\n",
    "           scoring='accuracy',\n",
    "           print_progress=False,\n",
    "           clone_estimator=False,\n",
    "           cv=5,\n",
    "           n_jobs=1)\n",
    "\n",
    "pipe = make_pipeline(efs1, lr)\n",
    "\n",
    "param_grid = {'exhaustivefeatureselector__estimator__C': [0.1, 1.0, 10.0]}\n",
    "    \n",
    "gs = GridSearchCV(estimator=pipe, \n",
    "                  param_grid=param_grid, \n",
    "                  scoring='accuracy', \n",
    "                  n_jobs=2, \n",
    "                  cv=2, \n",
    "                  verbose=1, \n",
    "                  refit=False)\n",
    "\n",
    "# run gridearch\n",
    "gs = gs.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "... and the \"best\" parameters determined by GridSearch are ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters via GridSearch {'exhaustivefeatureselector__estimator__C': 10.0}\n"
     ]
    }
   ],
   "source": [
    "print(\"Best parameters via GridSearch\", gs.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Obtaining the best *k* feature indices after GridSearch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we are interested in the best *k* best feature indices via `SequentialFeatureSelection.best_idx_`, we have to initialize a `GridSearchCV` object with `refit=True`. Now, the grid search object will take the complete training dataset and the best parameters, which it found via cross-validation, to train the estimator pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "gs = GridSearchCV(estimator=pipe, \n",
    "                  param_grid=param_grid, \n",
    "                  scoring='accuracy', \n",
    "                  n_jobs=2, \n",
    "                  cv=2, \n",
    "                  verbose=1, \n",
    "                  refit=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After running the grid search, we can access the individual pipeline objects of the `best_estimator_` via the `steps` attribute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 2 folds for each of 3 candidates, totalling 6 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=2)]: Done   6 out of   6 | elapsed:  1.5min finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('exhaustivefeatureselector',\n",
       "  ExhaustiveFeatureSelector(clone_estimator=False, cv=5,\n",
       "               estimator=LogisticRegression(C=10.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "            intercept_scaling=1, max_iter=100, multi_class='multinomial',\n",
       "            n_jobs=1, penalty='l2', random_state=123, solver='lbfgs',\n",
       "            tol=0.0001, verbose=0, warm_start=False),\n",
       "               max_features=3, min_features=2, n_jobs=1,\n",
       "               pre_dispatch='2*n_jobs', print_progress=False,\n",
       "               scoring='accuracy')),\n",
       " ('logisticregression',\n",
       "  LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "            intercept_scaling=1, max_iter=100, multi_class='multinomial',\n",
       "            n_jobs=1, penalty='l2', random_state=123, solver='lbfgs',\n",
       "            tol=0.0001, verbose=0, warm_start=False))]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gs = gs.fit(X_train, y_train)\n",
    "gs.best_estimator_.steps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Via sub-indexing, we can then obtain the best-selected feature subset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best features: (0, 2, 6)\n"
     ]
    }
   ],
   "source": [
    "print('Best features:', gs.best_estimator_.steps[0][1].best_idx_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "During cross-validation, this feature combination had a CV accuracy of:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best score: 0.934959349593\n"
     ]
    }
   ],
   "source": [
    "print('Best score:', gs.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'exhaustivefeatureselector__estimator__C': 10.0}"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gs.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Alternatively**, if we can set the \"best grid search parameters\" in our pipeline manually if we ran `GridSearchCV` with `refit=False`. It should yield the same results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best features: (0, 2, 6)\n"
     ]
    }
   ],
   "source": [
    "pipe.set_params(**gs.best_params_).fit(X_train, y_train)\n",
    "print('Best features:', pipe.steps[0][1].best_idx_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "## ExhaustiveFeatureSelector\n",
      "\n",
      "*ExhaustiveFeatureSelector(estimator, min_features=1, max_features=1, print_progress=True, scoring='accuracy', cv=5, n_jobs=1, pre_dispatch='2*n_jobs', clone_estimator=True)*\n",
      "\n",
      "Exhaustive Feature Selection for Classification and Regression.\n",
      "    (new in v0.4.3)\n",
      "\n",
      "**Parameters**\n",
      "\n",
      "- `estimator` : scikit-learn classifier or regressor\n",
      "\n",
      "\n",
      "- `min_features` : int (default: 1)\n",
      "\n",
      "    Minumum number of features to select\n",
      "\n",
      "- `max_features` : int (default: 1)\n",
      "\n",
      "    Maximum number of features to select\n",
      "\n",
      "- `print_progress` : bool (default: True)\n",
      "\n",
      "    Prints progress as the number of epochs\n",
      "    to stderr.\n",
      "\n",
      "- `scoring` : str, (default='accuracy')\n",
      "\n",
      "    Scoring metric in {accuracy, f1, precision, recall, roc_auc}\n",
      "    for classifiers,\n",
      "    {'mean_absolute_error', 'mean_squared_error',\n",
      "    'median_absolute_error', 'r2'} for regressors,\n",
      "    or a callable object or function with\n",
      "    signature ``scorer(estimator, X, y)``.\n",
      "\n",
      "- `cv` : int (default: 5)\n",
      "\n",
      "    Scikit-learn cross-validation generator or `int`.\n",
      "    If estimator is a classifier (or y consists of integer class labels),\n",
      "    stratified k-fold is performed, and regular k-fold cross-validation\n",
      "    otherwise.\n",
      "    No cross-validation if cv is None, False, or 0.\n",
      "\n",
      "- `n_jobs` : int (default: 1)\n",
      "\n",
      "    The number of CPUs to use for cross validation. -1 means 'all CPUs'.\n",
      "\n",
      "- `pre_dispatch` : int, or string (default: '2*n_jobs')\n",
      "\n",
      "    Controls the number of jobs that get dispatched\n",
      "    during parallel execution in cross_val_score.\n",
      "    Reducing this number can be useful to avoid an explosion of\n",
      "    memory consumption when more jobs get dispatched than CPUs can process.\n",
      "    This parameter can be:\n",
      "    None, in which case all the jobs are immediately created and spawned.\n",
      "    Use this for lightweight and fast-running jobs,\n",
      "    to avoid delays due to on-demand spawning of the jobs\n",
      "    An int, giving the exact number of total jobs that are spawned\n",
      "    A string, giving an expression as a function\n",
      "    of n_jobs, as in `2*n_jobs`\n",
      "\n",
      "- `clone_estimator` : bool (default: True)\n",
      "\n",
      "    Clones estimator if True; works with the original estimator instance\n",
      "    if False. Set to False if the estimator doesn't\n",
      "    implement scikit-learn's set_params and get_params methods.\n",
      "    In addition, it is required to set cv=0, and n_jobs=1.\n",
      "\n",
      "**Attributes**\n",
      "\n",
      "- `best_idx_` : array-like, shape = [n_predictions]\n",
      "\n",
      "    Feature Indices of the selected feature subsets.\n",
      "\n",
      "- `best_score_` : float\n",
      "\n",
      "    Cross validation average score of the selected subset.\n",
      "\n",
      "- `subsets_` : dict\n",
      "\n",
      "    A dictionary of selected feature subsets during the\n",
      "    sequential selection, where the dictionary keys are\n",
      "    the lengths k of these feature subsets. The dictionary\n",
      "    values are dictionaries themselves with the following\n",
      "    keys: 'feature_idx' (tuple of indices of the feature subset)\n",
      "    'cv_scores' (list individual cross-validation scores)\n",
      "    'avg_score' (average cross-validation score)\n",
      "\n",
      "### Methods\n",
      "\n",
      "<hr>\n",
      "\n",
      "*fit(X, y)*\n",
      "\n",
      "Perform feature selection and learn model from training data.\n",
      "\n",
      "**Parameters**\n",
      "\n",
      "- `X` : {array-like, sparse matrix}, shape = [n_samples, n_features]\n",
      "\n",
      "    Training vectors, where n_samples is the number of samples and\n",
      "    n_features is the number of features.\n",
      "\n",
      "- `y` : array-like, shape = [n_samples]\n",
      "\n",
      "    Target values.\n",
      "\n",
      "**Returns**\n",
      "\n",
      "- `self` : object\n",
      "\n",
      "\n",
      "<hr>\n",
      "\n",
      "*fit_transform(X, y)*\n",
      "\n",
      "Fit to training data and return the best selected features from X.\n",
      "\n",
      "**Parameters**\n",
      "\n",
      "- `X` : {array-like, sparse matrix}, shape = [n_samples, n_features]\n",
      "\n",
      "    Training vectors, where n_samples is the number of samples and\n",
      "    n_features is the number of features.\n",
      "\n",
      "**Returns**\n",
      "\n",
      "Feature subset of X, shape={n_samples, k_features}\n",
      "\n",
      "<hr>\n",
      "\n",
      "*get_metric_dict(confidence_interval=0.95)*\n",
      "\n",
      "Return metric dictionary\n",
      "\n",
      "**Parameters**\n",
      "\n",
      "- `confidence_interval` : float (default: 0.95)\n",
      "\n",
      "    A positive float between 0.0 and 1.0 to compute the confidence\n",
      "    interval bounds of the CV score averages.\n",
      "\n",
      "**Returns**\n",
      "\n",
      "Dictionary with items where each dictionary value is a list\n",
      "    with the number of iterations (number of feature subsets) as\n",
      "    its length. The dictionary keys corresponding to these lists\n",
      "    are as follows:\n",
      "    'feature_idx': tuple of the indices of the feature subset\n",
      "    'cv_scores': list with individual CV scores\n",
      "    'avg_score': of CV average scores\n",
      "    'std_dev': standard deviation of the CV score average\n",
      "    'std_err': standard error of the CV score average\n",
      "    'ci_bound': confidence interval bound of the CV score average\n",
      "\n",
      "<hr>\n",
      "\n",
      "*get_params(deep=True)*\n",
      "\n",
      "Get parameters for this estimator.\n",
      "\n",
      "**Parameters**\n",
      "\n",
      "deep: boolean, optional\n",
      "    If True, will return the parameters for this estimator and\n",
      "    contained subobjects that are estimators.\n",
      "\n",
      "**Returns**\n",
      "\n",
      "- `params` : mapping of string to any\n",
      "\n",
      "    Parameter names mapped to their values.\n",
      "\n",
      "<hr>\n",
      "\n",
      "*set_params(**params)*\n",
      "\n",
      "Set the parameters of this estimator.\n",
      "\n",
      "    The method works on simple estimators as well as on nested objects\n",
      "    (such as pipelines). The former have parameters of the form\n",
      "    ``<component>__<parameter>`` so that it's possible to update each\n",
      "    component of a nested object.\n",
      "\n",
      "**Returns**\n",
      "\n",
      "self\n",
      "\n",
      "<hr>\n",
      "\n",
      "*transform(X)*\n",
      "\n",
      "Return the best selected features from X.\n",
      "\n",
      "**Parameters**\n",
      "\n",
      "- `X` : {array-like, sparse matrix}, shape = [n_samples, n_features]\n",
      "\n",
      "    Training vectors, where n_samples is the number of samples and\n",
      "    n_features is the number of features.\n",
      "\n",
      "**Returns**\n",
      "\n",
      "Feature subset of X, shape={n_samples, k_features}\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "with open('../../api_modules/mlxtend.feature_selection/ExhaustiveFeatureSelector.md', 'r') as f:\n",
    "    print(f.read())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}

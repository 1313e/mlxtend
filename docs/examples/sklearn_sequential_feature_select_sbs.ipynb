{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sebastian Raschka 27/04/2015 \n",
      "\n",
      "CPython 3.4.3\n",
      "IPython 3.1.0\n"
     ]
    }
   ],
   "source": [
    "%load_ext watermark\n",
    "%watermark -a 'Sebastian Raschka' -d -v"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Sequential Backward Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to avoid the Curse of Dimensionality, pattern classification is often accompanied by Dimensionality Reduction, which also has the nice side-effect of increasing the computational performance. Common techniques are projection-based, such as Principal Component Analysis (PCA) (unsupervised) or Linear Discriminant (LDA) (supervised). It shall be noted though that regularization in classification models such as Logistic Regression, Support Vector Machines, or Neural Networks is to be preferred over using dimensionality reduction to avoid overfitting. However, dimensionality reduction is still a useful data compression technique to increase computational efficiency and data storage problems.\n",
    "\n",
    "An alternative to a projection-based dimensionality reduction approach is the so-called Feature Selection, and here, we will take a look at some of the established algorithms to tackle this combinatorial search problem: Sequential Backward Selection (SBS). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's summarize its mechanics in words:\n",
    "SBS starts with the original $d$-dimensional feature set and sequentially removes features from this set until the subset reaches a desired (user-specified) size $k$ where $k < d$. In every iteration $i$, the subset $d-i$ dimensional subset is evaluated using a criterion function to determine the least informative feature to be removed.\n",
    "\n",
    "The criterion function is typically the performance of the classifier measured via cross validation. \n",
    "\n",
    "Let's consider the following example where we have a dataset that consists of 3 features:\n",
    "\n",
    "\n",
    "- Original feature set: $\\{x_1, x_2, x_3\\}$\n",
    "\n",
    "In order to determine the least informative feature, we create 2-dimensional feature subsets and measure the performance (e.g., accuracy) of the classifier on each of those subset:\n",
    "\n",
    "- 1: $\\{x_1, x_2\\}$ -> 0.96\n",
    "- 2: $\\{x_1, x_3\\}$ -> 0.87\n",
    "- 3: $\\{x_2, x_3\\}$ -> 0.77\n",
    "\n",
    "Based on the accuracy measures, we would then eliminate feature $x_3$ and repeat this procedure until we reached the number of features to select. E.g., if we'd want to select 2 features, we'd stop at this point and select the feature subset $\\{x_1, x_2$\\}.\n",
    "\n",
    "Note that this algorithm is considered as \"subpoptimal\" in contrast to an exhaustive search, which is often computationally not feasible, though."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Further Reading"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "F. Ferri, P. Pudil, M. Hatef, and J. Kittler investigated the performance of different Sequential Selection Algorithms for Feature Selection on different scales and reported their results in\n",
    "\n",
    "- [\"Comparative Study of Techniques for Large Scale Feature Selection,\"](https://books.google.com/books?id=sbajBQAAQBAJ&pg=PA403&lpg=PA403&dq=Comparative+Study+of+Techniques+for+Large+Scale+Feature+Selection&source=bl&ots=KdGKWqzbzj&sig=5I9nhy-TrRmKyAiLDfy5ML_m578&hl=en&sa=X&ei=i7w-VYnoPMyXsAWm2IGgCw&ved=0CD4Q6AEwBA#v=onepage&q=Comparative%20Study%20of%20Techniques%20for%20Large%20Scale%20Feature%20Selection&f=false) Pattern Recognition in Practice IV, E. Gelsema and L. Kanal, eds., pp. 403-413. Elsevier Science B.V., 1994.\n",
    "\n",
    "Choosing an \"appropriate\" algorithm really depends on the problem - the size and desired recognition rate and computational performance. Thus, I want to encourage you to take (at least) a brief look at their paper and the results they obtained from experimenting with different problems feature space dimensions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Iris Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Indices of selected features: (0, 3)\n",
      "CV score of selected subset: 0.96\n",
      "New feature subset:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[ 5.1,  0.2],\n",
       "       [ 4.9,  0.2],\n",
       "       [ 4.7,  0.2],\n",
       "       [ 4.6,  0.2],\n",
       "       [ 5. ,  0.2]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from mlxtend.sklearn import SBS\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.datasets import load_iris\n",
    "\n",
    "iris = load_iris()\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "\n",
    "knn = KNeighborsClassifier(n_neighbors=4)\n",
    "\n",
    "sbs = SBS(knn, k_features=2, scoring='accuracy', cv=5)\n",
    "sbs.fit(X, y)\n",
    "\n",
    "print('Indices of selected features:', sbs.indices_)\n",
    "print('CV score of selected subset:', sbs.k_score_)\n",
    "print('New feature subset:')\n",
    "sbs.transform(X)[0:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Wine Data Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div style=\"max-height:1000px;max-width:1500px;overflow:auto;\">\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>14.23</td>\n",
       "      <td>1.71</td>\n",
       "      <td>2.43</td>\n",
       "      <td>15.6</td>\n",
       "      <td>127</td>\n",
       "      <td>2.80</td>\n",
       "      <td>3.06</td>\n",
       "      <td>0.28</td>\n",
       "      <td>2.29</td>\n",
       "      <td>5.64</td>\n",
       "      <td>1.04</td>\n",
       "      <td>3.92</td>\n",
       "      <td>1065</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>13.20</td>\n",
       "      <td>1.78</td>\n",
       "      <td>2.14</td>\n",
       "      <td>11.2</td>\n",
       "      <td>100</td>\n",
       "      <td>2.65</td>\n",
       "      <td>2.76</td>\n",
       "      <td>0.26</td>\n",
       "      <td>1.28</td>\n",
       "      <td>4.38</td>\n",
       "      <td>1.05</td>\n",
       "      <td>3.40</td>\n",
       "      <td>1050</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>13.16</td>\n",
       "      <td>2.36</td>\n",
       "      <td>2.67</td>\n",
       "      <td>18.6</td>\n",
       "      <td>101</td>\n",
       "      <td>2.80</td>\n",
       "      <td>3.24</td>\n",
       "      <td>0.30</td>\n",
       "      <td>2.81</td>\n",
       "      <td>5.68</td>\n",
       "      <td>1.03</td>\n",
       "      <td>3.17</td>\n",
       "      <td>1185</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>14.37</td>\n",
       "      <td>1.95</td>\n",
       "      <td>2.50</td>\n",
       "      <td>16.8</td>\n",
       "      <td>113</td>\n",
       "      <td>3.85</td>\n",
       "      <td>3.49</td>\n",
       "      <td>0.24</td>\n",
       "      <td>2.18</td>\n",
       "      <td>7.80</td>\n",
       "      <td>0.86</td>\n",
       "      <td>3.45</td>\n",
       "      <td>1480</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>13.24</td>\n",
       "      <td>2.59</td>\n",
       "      <td>2.87</td>\n",
       "      <td>21.0</td>\n",
       "      <td>118</td>\n",
       "      <td>2.80</td>\n",
       "      <td>2.69</td>\n",
       "      <td>0.39</td>\n",
       "      <td>1.82</td>\n",
       "      <td>4.32</td>\n",
       "      <td>1.04</td>\n",
       "      <td>2.93</td>\n",
       "      <td>735</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   0      1     2     3     4    5     6     7     8     9     10    11    12  \\\n",
       "0   1  14.23  1.71  2.43  15.6  127  2.80  3.06  0.28  2.29  5.64  1.04  3.92   \n",
       "1   1  13.20  1.78  2.14  11.2  100  2.65  2.76  0.26  1.28  4.38  1.05  3.40   \n",
       "2   1  13.16  2.36  2.67  18.6  101  2.80  3.24  0.30  2.81  5.68  1.03  3.17   \n",
       "3   1  14.37  1.95  2.50  16.8  113  3.85  3.49  0.24  2.18  7.80  0.86  3.45   \n",
       "4   1  13.24  2.59  2.87  21.0  118  2.80  2.69  0.39  1.82  4.32  1.04  2.93   \n",
       "\n",
       "     13  \n",
       "0  1065  \n",
       "1  1050  \n",
       "2  1185  \n",
       "3  1480  \n",
       "4   735  "
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('https://archive.ics.uci.edu/ml/machine-learning-databases/wine/wine.data', header=None)\n",
    "\n",
    "X = df.iloc[:, 1:].values\n",
    "y = df.iloc[:, 0].values\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYQAAAEPCAYAAABCyrPIAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x10acd15f8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "knn = KNeighborsClassifier(n_neighbors=4)\n",
    "\n",
    "# selecting features\n",
    "sbs = SBS(knn, k_features=1, scoring='accuracy', cv=5)\n",
    "sbs.fit(X, y)\n",
    "\n",
    "# plotting performance of feature subsets\n",
    "k_feat = [len(k) for k in sbs.subsets_]\n",
    "\n",
    "plt.bar(k_feat, sbs.scores_, align='center', alpha=0.5)\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Number of features')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}

{
 "metadata": {
  "name": "",
  "signature": "sha256:4c61f4fba64c0d402cb3a7ea9b62aab626f621388edd7169fc11ad80de8809ab"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%load_ext watermark"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 1
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%watermark -d -a 'Sebastian Raschka' -v -p scikit-learn,numpy,pandas"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Sebastian Raschka 23/02/2015 \n",
        "\n",
        "CPython 3.4.2\n",
        "IPython 2.3.1\n",
        "\n",
        "scikit-learn 0.15.2\n",
        "numpy 1.9.1\n",
        "pandas 0.15.2\n"
       ]
      }
     ],
     "prompt_number": 2
    },
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "Implementing a Weighted Majority Rule Ensemble Classifier in scikit-learn"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<br>\n",
      "<br>"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Here, I want to present a simple and conservative approach of implementing a weighted majority rule ensemble classifier in [scikit-learn](http://scikit-learn.org/stable/) that yielded remarkably good results when I tried it in a [kaggle](http://www.kaggle.com) competition. For me personally, kaggle competitions are just a nice way to try out and compare different approaches and ideas -- basically an opportunity to learn in a controlled environment with nice datasets.  \n",
      "\n",
      "Of course, there are other implementations of more sophisticated [ensemble methods](http://scikit-learn.org/stable/modules/ensemble.html) in scikit-learn, such as [bagging classifiers](http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.BaggingClassifier.html), [random forests](http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html), or the famous [AdaBoost](http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.AdaBoostClassifier.html) algorithm. However, as far as I am concerned, they all require the usage of a common \"base classifier.\"\n",
      "\n",
      "In contrast, my motivation for the following approach was to combine conceptually different machine learning classifiers and use a majority vote rule. The reason for this was that I had trained a set of equally well performing models, and I wanted to balance out their individual weaknesses."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<br>\n",
      "<br>"
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Sections"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "- [Classifying Iris Flowers Using Different Classification Models](#Classifying-Iris-Flowers-Using-Different-Classification-Models)\n",
      "- [Implementing the Majority Voting Rule Ensemble Classifier](#Implementing-the-Majority-Voting-Rule-Ensemble-Classifier)\n",
      "- [Additional Note About the EnsembleClassifier Implementation: Class Labels vs. Probabilities](#Additional-Note-About-the-EnsembleClassifier-Implementation:-Class-Labels-vs.-Probabilities)\n",
      "- [EnsembleClassifier - Tuning Weights](#EnsembleClassifier---Tuning-Weights)\n",
      "- [EnsembleClassifier - Pipelines](#EnsembleClassifier---Pipelines)\n",
      "- [Some Final Words](#Some-Final-Words)"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<br>\n",
      "<br>"
     ]
    },
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "Classifying Iris Flowers Using Different Classification Models"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "[[back to top](#Sections)]"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "For a simple example, let us use three different classification models to classify the samples in the [Iris dataset](http://en.wikipedia.org/wiki/Iris_flower_data_set): Logistic regression, a naive Bayes classifier with a Gaussian kernel, and a random forest classifier -- an ensemble method itself. At this point, let's not worry about preprocessing the data and training and test sets. Also, we will only use 2 feature columns (sepal width and petal height) to make the classification problem harder. "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn import datasets\n",
      "\n",
      "iris = datasets.load_iris()\n",
      "X, y = iris.data[:, 1:3], iris.target"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 3
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn import cross_validation\n",
      "from sklearn.linear_model import LogisticRegression\n",
      "from sklearn.naive_bayes import GaussianNB \n",
      "from sklearn.ensemble import RandomForestClassifier\n",
      "import numpy as np\n",
      "\n",
      "np.random.seed(123)\n",
      "\n",
      "clf1 = LogisticRegression()\n",
      "clf2 = RandomForestClassifier()\n",
      "clf3 = GaussianNB()\n",
      "\n",
      "print('5-fold cross validation:\\n')\n",
      "\n",
      "for clf, label in zip([clf1, clf2, clf3], ['Logistic Regression', 'Random Forest', 'naive Bayes']):\n",
      "\n",
      "    scores = cross_validation.cross_val_score(clf, X, y, cv=5, scoring='accuracy')\n",
      "    print(\"Accuracy: %0.2f (+/- %0.2f) [%s]\" % (scores.mean(), scores.std(), label))\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "5-fold cross validation:\n",
        "\n",
        "Accuracy: 0.90 (+/- 0.05) [Logistic Regression]\n",
        "Accuracy: 0.92 (+/- 0.05) [Random Forest]"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Accuracy: 0.91 (+/- 0.04) [naive Bayes]\n"
       ]
      }
     ],
     "prompt_number": 4
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "As we can see from the cross-validation results above, the performance of the three models is almost equal."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<br>\n",
      "<br>"
     ]
    },
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "Implementing the Majority Voting Rule Ensemble Classifier"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "[[back to top](#Sections)]"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Now, we will implement a simple `EnsembleClassifier` class that allows us to combine the three different classifiers. We define a `predict` method that let's us simply take the majority rule of the predictions by the classifiers.\n",
      "E.g., if the prediction for a sample is\n",
      "\n",
      "- classifier 1 -> class 1\n",
      "- classifier 2 -> class 1\n",
      "- classifier 3 -> class 2\n",
      "\n",
      "we would classify the sample as \"class 1.\""
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Furthermore, we add a `weights` parameter, which let's us assign a specific weight to each classifier. In order to work with the weights, we collect the predicted class probabilities for each classifier, multiply it by the classifier weight, and take the average. Based on these weighted average probabilties, we can then assign the class label.\n",
      "\n",
      "To illustrate this with a simple example, let's assume we have 3 classifiers and a 3-class classification problems where we assign equal weights to all classifiers (the default): w1=1, w2=1, w3=1.\n",
      "\n",
      "The weighted average probabilities for a sample would then be calculated as follows:\n",
      "\n",
      "| classifier      | class 1  | class 2  | class 3  |\n",
      "|-----------------|----------|----------|----------|\n",
      "| classifier 1    | w1 * 0.2 | w1 * 0.5 | w1 * 0.3 |\n",
      "| classifier 2    | w2 * 0.6 | w2 * 0.3 | w2 * 0.1 |\n",
      "| classifier 3    | w3 * 0.3 | w3 * 0.4 | w3 * 0.3 |\n",
      "| weighted average| 0.37     | 0.4      | 0.3      |\n",
      "\n",
      "We can see in the table above that class 2 has the highest weighted average probability, thus we classify the sample as class 2."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Now, let's put it into code and apply it to our Iris classification."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "\"\"\"\n",
      "Soft Voting/Majority Rule classifier\n",
      "\n",
      "This module contains a Soft Voting/Majority Rule classifier for \n",
      "classification clfs.\n",
      "\n",
      "\n",
      "\"\"\"\n",
      "from sklearn.base import BaseEstimator\n",
      "from sklearn.base import ClassifierMixin\n",
      "from sklearn.base import TransformerMixin\n",
      "from sklearn.preprocessing import LabelEncoder\n",
      "import numpy as np\n",
      "import operator\n",
      "\n",
      "class EnsembleClassifier(BaseEstimator, ClassifierMixin, TransformerMixin):\n",
      "    \"\"\"\n",
      "    Soft Voting/Majority Rule classifier for classification clfs.\n",
      "    Parameters\n",
      "    ----------\n",
      "    clfs : array-like, shape = [n_classifiers]\n",
      "      A list of clfs for classification.\n",
      "    voting : str, {'hard', 'soft'} (default='hard')\n",
      "      If 'hard', uses predicted class labels for majority rule voting.\n",
      "      Else if 'soft', predicts the class label based on the argmax of\n",
      "      the sums of the predicted probalities.\n",
      "    weights : array-like, shape = [n_classifiers], optional (default=`None`)\n",
      "      Sequence of weights (`float` or `int`) to weight the occurances of\n",
      "      predicted class labels (`hard` voting) or class probabilities\n",
      "      before averaging (`soft` voting). Uses uniform weights if `None`.\n",
      "    Attributes\n",
      "    ----------\n",
      "    classes_ : array-like, shape = [n_predictions]\n",
      "    Examples\n",
      "    --------\n",
      "    >>> import numpy as np\n",
      "    >>> from sklearn.linear_model import LogisticRegression\n",
      "    >>> from sklearn.naive_bayes import GaussianNB\n",
      "    >>> from sklearn.ensemble import RandomForestClassifier\n",
      "    >>> clf1 = LogisticRegression(random_state=1)\n",
      "    >>> clf2 = RandomForestClassifier(random_state=1)\n",
      "    >>> clf3 = GaussianNB()\n",
      "    >>> X = np.array([[-1, -1], [-2, -1], [-3, -2], [1, 1], [2, 1], [3, 2]])\n",
      "    >>> y = np.array([1, 1, 1, 2, 2, 2])\n",
      "    >>> eclf1 = EnsembleClassifier(clfs=[clf1, clf2, clf3], voting='hard')\n",
      "    >>> eclf1 = eclf1.fit(X, y)\n",
      "    >>> print(eclf1.predict(X))\n",
      "    [1 1 1 2 2 2]\n",
      "    >>> eclf2 = EnsembleClassifier(clfs=[clf1, clf2, clf3], voting='soft')\n",
      "    >>> eclf2 = eclf2.fit(X, y)\n",
      "    >>> print(eclf2.predict(X))\n",
      "    [1 1 1 2 2 2]\n",
      "    >>> eclf3 = EnsembleClassifier(clfs=[clf1, clf2, clf3],\n",
      "    ...                          voting='soft', weights=[2,1,1])\n",
      "    >>> eclf3 = eclf3.fit(X, y)\n",
      "    >>> print(eclf3.predict(X))\n",
      "    [1 1 1 2 2 2]\n",
      "    >>>\n",
      "    \"\"\"\n",
      "    def __init__(self, clfs, voting='hard', weights=None):\n",
      "        self.clfs = clfs\n",
      "        self.voting = voting\n",
      "        self.weights = weights\n",
      "\n",
      "    def fit(self, X, y):\n",
      "        \"\"\"\n",
      "        Fits the clfs.\n",
      "        Parameters\n",
      "        ----------\n",
      "        X : {array-like, sparse matrix}, shape = [n_samples, n_features]\n",
      "            Training vectors, where n_samples is the number of samples and\n",
      "            n_features is the number of features.\n",
      "        y : array-like, shape = [n_samples]\n",
      "            Target values.\n",
      "        Returns\n",
      "        -------\n",
      "        self : object\n",
      "        \"\"\"\n",
      "\n",
      "        if isinstance(y, np.ndarray) and len(y.shape) > 1 and y.shape[1] > 1:\n",
      "            raise NotImplementedError('Multilabel classification'\n",
      "                                      ' not implemented, yet.')\n",
      "\n",
      "        if self.voting not in ('soft', 'hard'):\n",
      "            raise ValueError(\"Voting must be 'soft' or 'hard'; got (voting=%r)\"\n",
      "                             % voting)\n",
      "\n",
      "        if self.weights and len(self.weights) != len(self.clfs):\n",
      "            raise ValueError('Number of classifiers and weights must be equal'\n",
      "                             '; got %d weights, %d clfs'\n",
      "                             % (len(self.weights), len(self.clfs)))\n",
      "\n",
      "        self.le_ = LabelEncoder()\n",
      "        self.le_.fit(y)\n",
      "        self.classes_ = self.le_.classes_\n",
      "\n",
      "        for clf in self.clfs:\n",
      "            clf.fit(X, self.le_.transform(y))\n",
      "                   \n",
      "        return self\n",
      "            \n",
      "    def predict(self, X):\n",
      "        \"\"\"\n",
      "        Predict class labels for X.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        X : {array-like, sparse matrix}, shape = [n_samples, n_features]\n",
      "            Training vectors, where n_samples is the number of samples and\n",
      "            n_features is the number of features.\n",
      "        \n",
      "        Returns\n",
      "        ----------\n",
      "        maj : array-like, shape = [n_samples]\n",
      "            Predicted class labels.\n",
      "        \n",
      "        \"\"\"\n",
      "        if self.voting == 'soft':\n",
      "            \n",
      "            maj = np.argmax(self.predict_proba(X), axis=1)\n",
      "        \n",
      "        else:  # 'hard' voting\n",
      "            predictions = self._predict(X)\n",
      "\n",
      "            maj = np.apply_along_axis(lambda x:\n",
      "                                      np.argmax(np.bincount(x, weights=self.weights)),\n",
      "                                      axis=1,\n",
      "                                      arr=predictions)\n",
      "        \n",
      "        maj = self.le_.inverse_transform(maj)\n",
      "        \n",
      "        return maj\n",
      "            \n",
      "    def predict_proba(self, X):\n",
      "        \"\"\" \n",
      "        Predict class probabilities for X.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        X : {array-like, sparse matrix}, shape = [n_samples, n_features]\n",
      "            Training vectors, where n_samples is the number of samples and\n",
      "            n_features is the number of features.\n",
      "        \n",
      "        Returns\n",
      "        ----------\n",
      "        avg : array-like, shape = [n_samples, n_classes]\n",
      "            Weighted average probability for each class per sample.\n",
      "        \n",
      "        \"\"\"\n",
      "        avg = np.average(self._predict_probas(X), axis=0, weights=self.weights)\n",
      "        \n",
      "        return avg\n",
      "  \n",
      "    def transform(self, X):\n",
      "        \"\"\"\n",
      "        Return class labels or probabilities for X for each estimator.\n",
      "                 \n",
      "        Parameters\n",
      "        ----------\n",
      "        X : {array-like, sparse matrix}, shape = [n_samples, n_features]\n",
      "            Training vectors, where n_samples is the number of samples and\n",
      "            n_features is the number of features.\n",
      "      \n",
      "        Returns\n",
      "        -------\n",
      "        If `voting='soft'`:\n",
      "          array-like = [n_classifiers, n_samples, n_classes]\n",
      "            Class probabilties calculated by each classifier.\n",
      "        If `voting='hard'`:\n",
      "          array-like = [n_classifiers, n_samples]\n",
      "            Class labels predicted by each classifier.\n",
      "        \n",
      "        \"\"\"\n",
      "        if self.voting == 'soft':\n",
      "            return self._predict_probas(X)\n",
      "        else:\n",
      "            return self._predict(X)  \n",
      "    \n",
      "    def _predict(self, X):\n",
      "        \"\"\" Collects results from clf.predict calls. \"\"\"\n",
      "        \n",
      "        return np.asarray([clf.predict(X) for clf in self.clfs]).T\n",
      "        \n",
      "    def _predict_probas(self, X):\n",
      "        \"\"\" Collects results from clf.predict calls. \"\"\"\n",
      "        \n",
      "        return np.asarray([clf.predict_proba(X) for clf in self.clfs])\n",
      "        \n",
      "if __name__ == \"__main__\":\n",
      "    import doctest\n",
      "    doctest.testmod()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 3
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Majority Rule (hard) Voting\n",
      "\n",
      "np.random.seed(123)\n",
      "eclf = EnsembleClassifier(clfs=[clf1, clf2, clf3], voting='hard')\n",
      "\n",
      "for clf, label in zip([clf1, clf2, clf3, eclf], ['Logistic Regression', 'Random Forest', 'naive Bayes', 'Ensemble']):\n",
      "\n",
      "    scores = cross_validation.cross_val_score(clf, X, y, cv=5, scoring='accuracy')\n",
      "    print(\"Accuracy: %0.2f (+/- %0.2f) [%s]\" % (scores.mean(), scores.std(), label))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Accuracy: 0.90 (+/- 0.05) [Logistic Regression]\n",
        "Accuracy: 0.92 (+/- 0.05) [Random Forest]"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Accuracy: 0.91 (+/- 0.04) [naive Bayes]\n",
        "Accuracy: 0.95 (+/- 0.05) [Ensemble]"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n"
       ]
      }
     ],
     "prompt_number": 6
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Average Probabilities (soft) Voting\n",
      "\n",
      "np.random.seed(123)\n",
      "eclf = EnsembleClassifier(clfs=[clf1, clf2, clf3], voting='soft', weights=[2,1,5])\n",
      "\n",
      "for clf, label in zip([clf1, clf2, clf3, eclf], ['Logistic Regression', 'Random Forest', 'naive Bayes', 'Ensemble']):\n",
      "\n",
      "    scores = cross_validation.cross_val_score(clf, X, y, cv=5, scoring='accuracy')\n",
      "    print(\"Accuracy: %0.2f (+/- %0.2f) [%s]\" % (scores.mean(), scores.std(), label))\n",
      "\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Accuracy: 0.90 (+/- 0.05) [Logistic Regression]\n",
        "Accuracy: 0.92 (+/- 0.05) [Random Forest]"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Accuracy: 0.91 (+/- 0.04) [naive Bayes]\n",
        "Accuracy: 0.94 (+/- 0.04) [Ensemble]"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n"
       ]
      }
     ],
     "prompt_number": 7
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<br>\n",
      "<br>"
     ]
    },
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "EnsembleClassifier - Tuning Weights"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "[[back to top](#Sections)]"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Let's get back to our `weights` parameter. Here, we will use a naive brute-force approach to find the optimal weights for each classifier to increase the prediction accuracy."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import pandas as pd\n",
      "\n",
      "np.random.seed(123)\n",
      "\n",
      "df = pd.DataFrame(columns=('w1', 'w2', 'w3', 'mean', 'std'))\n",
      "\n",
      "i = 0\n",
      "for w1 in range(1,4):\n",
      "    for w2 in range(1,4):\n",
      "        for w3 in range(1,4):\n",
      "            \n",
      "            if len(set((w1,w2,w3))) == 1: # skip if all weights are equal\n",
      "                continue\n",
      "            \n",
      "            eclf = EnsembleClassifier(clfs=[clf1, clf2, clf3], voting='soft', weights=[w1,w2,w3])\n",
      "            scores = cross_validation.cross_val_score(\n",
      "                                            estimator=eclf,\n",
      "                                            X=X, \n",
      "                                            y=y, \n",
      "                                            cv=5, \n",
      "                                            scoring='accuracy',\n",
      "                                            n_jobs=1)\n",
      "            \n",
      "            df.loc[i] = [w1, w2, w3, scores.mean(), scores.std()]\n",
      "            i += 1\n",
      "            \n",
      "df.sort(columns=['mean', 'std'], ascending=False)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "html": [
        "<div style=\"max-height:1000px;max-width:1500px;overflow:auto;\">\n",
        "<table border=\"1\" class=\"dataframe\">\n",
        "  <thead>\n",
        "    <tr style=\"text-align: right;\">\n",
        "      <th></th>\n",
        "      <th>w1</th>\n",
        "      <th>w2</th>\n",
        "      <th>w3</th>\n",
        "      <th>mean</th>\n",
        "      <th>std</th>\n",
        "    </tr>\n",
        "  </thead>\n",
        "  <tbody>\n",
        "    <tr>\n",
        "      <th>2 </th>\n",
        "      <td> 1</td>\n",
        "      <td> 2</td>\n",
        "      <td> 1</td>\n",
        "      <td> 0.953333</td>\n",
        "      <td> 0.033993</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>17</th>\n",
        "      <td> 3</td>\n",
        "      <td> 1</td>\n",
        "      <td> 2</td>\n",
        "      <td> 0.953333</td>\n",
        "      <td> 0.033993</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>16</th>\n",
        "      <td> 3</td>\n",
        "      <td> 1</td>\n",
        "      <td> 1</td>\n",
        "      <td> 0.946667</td>\n",
        "      <td> 0.045216</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>20</th>\n",
        "      <td> 3</td>\n",
        "      <td> 2</td>\n",
        "      <td> 2</td>\n",
        "      <td> 0.946667</td>\n",
        "      <td> 0.045216</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>1 </th>\n",
        "      <td> 1</td>\n",
        "      <td> 1</td>\n",
        "      <td> 3</td>\n",
        "      <td> 0.946667</td>\n",
        "      <td> 0.040000</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>6 </th>\n",
        "      <td> 1</td>\n",
        "      <td> 3</td>\n",
        "      <td> 2</td>\n",
        "      <td> 0.946667</td>\n",
        "      <td> 0.033993</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>7 </th>\n",
        "      <td> 1</td>\n",
        "      <td> 3</td>\n",
        "      <td> 3</td>\n",
        "      <td> 0.946667</td>\n",
        "      <td> 0.033993</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>11</th>\n",
        "      <td> 2</td>\n",
        "      <td> 2</td>\n",
        "      <td> 1</td>\n",
        "      <td> 0.946667</td>\n",
        "      <td> 0.033993</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>13</th>\n",
        "      <td> 2</td>\n",
        "      <td> 3</td>\n",
        "      <td> 1</td>\n",
        "      <td> 0.946667</td>\n",
        "      <td> 0.033993</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>14</th>\n",
        "      <td> 2</td>\n",
        "      <td> 3</td>\n",
        "      <td> 2</td>\n",
        "      <td> 0.946667</td>\n",
        "      <td> 0.033993</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>18</th>\n",
        "      <td> 3</td>\n",
        "      <td> 1</td>\n",
        "      <td> 3</td>\n",
        "      <td> 0.946667</td>\n",
        "      <td> 0.033993</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>22</th>\n",
        "      <td> 3</td>\n",
        "      <td> 3</td>\n",
        "      <td> 1</td>\n",
        "      <td> 0.946667</td>\n",
        "      <td> 0.033993</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>23</th>\n",
        "      <td> 3</td>\n",
        "      <td> 3</td>\n",
        "      <td> 2</td>\n",
        "      <td> 0.946667</td>\n",
        "      <td> 0.033993</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>19</th>\n",
        "      <td> 3</td>\n",
        "      <td> 2</td>\n",
        "      <td> 1</td>\n",
        "      <td> 0.940000</td>\n",
        "      <td> 0.057349</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>5 </th>\n",
        "      <td> 1</td>\n",
        "      <td> 3</td>\n",
        "      <td> 1</td>\n",
        "      <td> 0.940000</td>\n",
        "      <td> 0.044222</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>8 </th>\n",
        "      <td> 2</td>\n",
        "      <td> 1</td>\n",
        "      <td> 1</td>\n",
        "      <td> 0.940000</td>\n",
        "      <td> 0.044222</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>9 </th>\n",
        "      <td> 2</td>\n",
        "      <td> 1</td>\n",
        "      <td> 2</td>\n",
        "      <td> 0.940000</td>\n",
        "      <td> 0.044222</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>12</th>\n",
        "      <td> 2</td>\n",
        "      <td> 2</td>\n",
        "      <td> 3</td>\n",
        "      <td> 0.940000</td>\n",
        "      <td> 0.044222</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>21</th>\n",
        "      <td> 3</td>\n",
        "      <td> 2</td>\n",
        "      <td> 3</td>\n",
        "      <td> 0.940000</td>\n",
        "      <td> 0.044222</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>4 </th>\n",
        "      <td> 1</td>\n",
        "      <td> 2</td>\n",
        "      <td> 3</td>\n",
        "      <td> 0.940000</td>\n",
        "      <td> 0.038873</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>3 </th>\n",
        "      <td> 1</td>\n",
        "      <td> 2</td>\n",
        "      <td> 2</td>\n",
        "      <td> 0.940000</td>\n",
        "      <td> 0.032660</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>10</th>\n",
        "      <td> 2</td>\n",
        "      <td> 1</td>\n",
        "      <td> 3</td>\n",
        "      <td> 0.940000</td>\n",
        "      <td> 0.032660</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>0 </th>\n",
        "      <td> 1</td>\n",
        "      <td> 1</td>\n",
        "      <td> 2</td>\n",
        "      <td> 0.933333</td>\n",
        "      <td> 0.047140</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>15</th>\n",
        "      <td> 2</td>\n",
        "      <td> 3</td>\n",
        "      <td> 3</td>\n",
        "      <td> 0.933333</td>\n",
        "      <td> 0.047140</td>\n",
        "    </tr>\n",
        "  </tbody>\n",
        "</table>\n",
        "</div>"
       ],
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 8,
       "text": [
        "    w1  w2  w3      mean       std\n",
        "2    1   2   1  0.953333  0.033993\n",
        "17   3   1   2  0.953333  0.033993\n",
        "16   3   1   1  0.946667  0.045216\n",
        "20   3   2   2  0.946667  0.045216\n",
        "1    1   1   3  0.946667  0.040000\n",
        "6    1   3   2  0.946667  0.033993\n",
        "7    1   3   3  0.946667  0.033993\n",
        "11   2   2   1  0.946667  0.033993\n",
        "13   2   3   1  0.946667  0.033993\n",
        "14   2   3   2  0.946667  0.033993\n",
        "18   3   1   3  0.946667  0.033993\n",
        "22   3   3   1  0.946667  0.033993\n",
        "23   3   3   2  0.946667  0.033993\n",
        "19   3   2   1  0.940000  0.057349\n",
        "5    1   3   1  0.940000  0.044222\n",
        "8    2   1   1  0.940000  0.044222\n",
        "9    2   1   2  0.940000  0.044222\n",
        "12   2   2   3  0.940000  0.044222\n",
        "21   3   2   3  0.940000  0.044222\n",
        "4    1   2   3  0.940000  0.038873\n",
        "3    1   2   2  0.940000  0.032660\n",
        "10   2   1   3  0.940000  0.032660\n",
        "0    1   1   2  0.933333  0.047140\n",
        "15   2   3   3  0.933333  0.047140"
       ]
      }
     ],
     "prompt_number": 8
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<br>\n",
      "<br>"
     ]
    },
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "EnsembleClassifier - Pipelines"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "[[back to top](#Sections)]"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Of course, we can also use the `EnsembleClassifier` in `Pipelines`. This is especially useful if a certain classifier does a pretty good job on a certain feature subset or requires different `preprocessing` steps. For demonstration purposes, let us implement a simple `ColumnSelector` class."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "class ColumnSelector(object):\n",
      "    \"\"\" \n",
      "    A feature selector for scikit-learn's Pipeline class that returns\n",
      "    specified columns from a numpy array.\n",
      "    \n",
      "    \"\"\"\n",
      "    \n",
      "    def __init__(self, cols):\n",
      "        self.cols = cols\n",
      "        \n",
      "    def transform(self, X, y=None):\n",
      "        return X[:, self.cols]\n",
      "\n",
      "    def fit(self, X, y=None):\n",
      "        return self"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 9
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn.pipeline import Pipeline \n",
      "from sklearn.lda import LDA\n",
      "\n",
      "pipe1 = Pipeline([\n",
      "               ('sel', ColumnSelector([1])),    # use only the 1st feature\n",
      "               ('clf', GaussianNB())])\n",
      "\n",
      "pipe2 = Pipeline([\n",
      "               ('sel', ColumnSelector([0, 1])), # use the 1st and 2nd feature\n",
      "               ('dim', LDA(n_components=1)),    # Dimensionality reduction via LDA\n",
      "               ('clf', LogisticRegression())])\n",
      "\n",
      "eclf = EnsembleClassifier([pipe1, pipe2])\n",
      "scores = cross_validation.cross_val_score(eclf, X, y, cv=5, scoring='accuracy')\n",
      "print(\"Accuracy: %0.2f (+/- %0.2f) [%s]\" % (scores.mean(), scores.std(), label))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Accuracy: 0.95 (+/- 0.03) [Ensemble]\n"
       ]
      }
     ],
     "prompt_number": 10
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<br>\n",
      "<br>"
     ]
    },
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "Ensemble EnsembleClassifier"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "[[back to top](#Sections)]"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "If one `EnsembleClassifier` is not yet enough, we can also build an ensemble classifier of ensemble classifiers. Just like the other examples above, the following code is just meant to be a technical demonstration:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "eclf1 = EnsembleClassifier(clfs=[clf1, clf2, clf3], voting='soft', weights=[5,2,1])\n",
      "eclf2 = EnsembleClassifier(clfs=[clf1, clf2, clf3], voting='soft', weights=[4,2,1])\n",
      "eclf3 = EnsembleClassifier(clfs=[clf1, clf2, clf3], voting='soft', weights=[1,2,4])\n",
      "\n",
      "eclf = EnsembleClassifier(clfs=[eclf1, eclf2, eclf3], voting='soft', weights=[2,1,1])\n",
      "\n",
      "scores = cross_validation.cross_val_score(eclf, X, y, cv=5, scoring='accuracy')\n",
      "print(\"Accuracy: %0.2f (+/- %0.2f) [%s]\" % (scores.mean(), scores.std(), label))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Accuracy: 0.95 (+/- 0.03) [Ensemble]\n"
       ]
      }
     ],
     "prompt_number": 11
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<br>\n",
      "<br>"
     ]
    },
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "Some Final Words"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "[[back to top](#Sections)]"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "When we applied the `EnsembleClassifier` to the iris example above, the results surely looked nice. But we have to keep in mind that this is just a toy example. The majority rule voting approach might not always work so well in practice, especially if the ensemble consists of more \"weak\" than \"strong\" classification models. Also, although we used a cross-validation approach to overcome the overfitting challenge, please always keep a spare validation dataset to evaluate the results."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Anyway, if you are interested in those approaches, I added them to my [`mlxtend`](https://github.com/rasbt/mlxtend) Python module; in `mlxtend` (short for \"machine learning library extensions\"), I collect certain things that I personally find useful but are not available in other packages yet.\n",
      "\n",
      "You can install `mlxtend` via\n",
      "\n",
      "    pip install mlxtend\n",
      "\n",
      "and then load the `ColumnSelector` or `EnsembleClassifier` via\n",
      "\n",
      "    from mlxtend.sklearn import ColumnSelector\n",
      "    from mlxtend.sklearn import EnsembleClassifier"
     ]
    },
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "<br>\n",
      "<br>"
     ]
    },
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "Example"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn.linear_model import LogisticRegression\n",
      "from sklearn.naive_bayes import GaussianNB\n",
      "from sklearn.ensemble import RandomForestClassifier\n",
      "import matplotlib.pyplot as plt\n",
      "%matplotlib inline\n",
      "\n",
      "clf1 = LogisticRegression(random_state=123)\n",
      "clf2 = RandomForestClassifier(random_state=123)\n",
      "clf3 = GaussianNB()\n",
      "X = np.array([[-1.0, -1.0], [-1.2, -1.4], [-3.4, -2.2], [1.1, 1.2]])\n",
      "y = np.array([1, 1, 2, 2])\n",
      "\n",
      "eclf = EnsembleClassifier(clfs=[clf1, clf2, clf3], \n",
      "                        voting='soft', \n",
      "                        weights=[1, 1, 5])\n",
      "\n",
      "# predict class probabilities for all classifiers\n",
      "probas = [c.fit(X, y).predict_proba(X) for c in (clf1, clf2, clf3, eclf)]\n",
      "\n",
      "# get class probabilities for the first sample in the dataset\n",
      "class1_1 = [pr[0,0] for pr in probas]\n",
      "class2_1 = [pr[0,1] for pr in probas]\n",
      "\n",
      "#####################\n",
      "# plotting \n",
      "\n",
      "N = 4  # number of groups\n",
      "ind = np.arange(N)  # group positions\n",
      "width = 0.35  # bar width\n",
      "\n",
      "fig, ax = plt.subplots(figsize=(7,5))\n",
      "\n",
      "# bars for classifier 1-3\n",
      "p1 = ax.bar(ind, np.hstack(([class1_1[:-1],[0]])), width, color='green')\n",
      "p2 = ax.bar(ind + width, np.hstack(([class2_1[:-1],[0]])), width, color='lightgreen')\n",
      "\n",
      "# bars for VotingClassifier\n",
      "p3 = ax.bar(ind, [0, 0, 0, class1_1[-1]], width, color='blue')\n",
      "p4 = ax.bar(ind + width, [0, 0, 0, class2_1[-1]], width, color='steelblue')\n",
      "\n",
      "# plot annotations\n",
      "plt.axvline(2.8, color='k', linestyle='dashed')\n",
      "ax.set_xticks(ind + width)\n",
      "ax.set_xticklabels(['LogisticRegression\\nweight 1', \n",
      "                    'GaussianNB\\nweight 1', \n",
      "                    'RandomForestClassifier\\nweight 5', \n",
      "                    'VotingClassifier\\n(average probabilities)'],\n",
      "                   rotation=40,\n",
      "                   ha='right')\n",
      "plt.ylim([0,1])\n",
      "plt.ylabel('probability')\n",
      "plt.title('Class probabilities for sample 1 by different classifiers')\n",
      "plt.legend([p1[0], p2[0]], ['class 1', 'class 2'] , loc='upper left')\n",
      "plt.tight_layout()\n",
      "#plt.savefig('../../images/sklean_ensemble_probas.png')\n",
      "plt.show()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "display_data",
       "png": "iVBORw0KGgoAAAANSUhEUgAAAe4AAAFjCAYAAAD7OehQAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\npUEFaW8AAAAASUVORK5CYII=\n",
       "text": [
        "<matplotlib.figure.Figure at 0x107661898>"
       ]
      }
     ],
     "prompt_number": 11
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}
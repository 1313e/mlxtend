{
    "docs": [
        {
            "location": "/",
            "text": "Welcome to mlxtend\u2019s documentation!\n\n\nMlxtend is a library consisting of useful tools and extensions for the day-to-day data science tasks.\n\n\n\n\nThis open source project is released under a permissive new BSD open source \nlicense\n and commercially usable\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLinks\n\n\n\n\nDocumentation: \nhttp://rasbt.github.io/mlxtend/\n\n\nSource code repository: \nhttps://github.com/rasbt/mlxtend\n\n\nPyPI: \nhttps://pypi.python.org/pypi/mlxtend\n\n\nQuestions? Check out the \nGoogle Groups mailing list\n\n\n\n\n\n\n\nExamples\n\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport matplotlib.gridspec as gridspec\nimport itertools\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import SVC\nfrom sklearn.ensemble import RandomForestClassifier\nfrom mlxtend.classifier import EnsembleVoteClassifier\nfrom mlxtend.data import iris_data\nfrom mlxtend.evaluate import plot_decision_regions\n\n# Initializing Classifiers\nclf1 = LogisticRegression(random_state=0)\nclf2 = RandomForestClassifier(random_state=0)\nclf3 = SVC(random_state=0, probability=True)\neclf = EnsembleVoteClassifier(clfs=[clf1, clf2, clf3], weights=[2, 1, 1], voting='soft')\n\n# Loading some example data\nX, y = iris_data()\nX = X[:,[0, 2]]\n\n# Plotting Decision Regions\ngs = gridspec.GridSpec(2, 2)\nfig = plt.figure(figsize=(10, 8))\n\nfor clf, lab, grd in zip([clf1, clf2, clf3, eclf],\n                         ['Logistic Regression', 'Random Forest', 'Naive Bayes', 'Ensemble'],\n                         itertools.product([0, 1], repeat=2)):\n    clf.fit(X, y)\n    ax = plt.subplot(gs[grd[0], grd[1]])\n    fig = plot_decision_regions(X=X, y=y, clf=clf, legend=2)\n    plt.title(lab)\nplt.show()",
            "title": "Home"
        },
        {
            "location": "/#welcome-to-mlxtends-documentation",
            "text": "Mlxtend is a library consisting of useful tools and extensions for the day-to-day data science tasks.   This open source project is released under a permissive new BSD open source  license  and commercially usable",
            "title": "Welcome to mlxtend\u2019s documentation!"
        },
        {
            "location": "/#links",
            "text": "Documentation:  http://rasbt.github.io/mlxtend/  Source code repository:  https://github.com/rasbt/mlxtend  PyPI:  https://pypi.python.org/pypi/mlxtend  Questions? Check out the  Google Groups mailing list",
            "title": "Links"
        },
        {
            "location": "/#examples",
            "text": "import numpy as np\nimport matplotlib.pyplot as plt\nimport matplotlib.gridspec as gridspec\nimport itertools\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import SVC\nfrom sklearn.ensemble import RandomForestClassifier\nfrom mlxtend.classifier import EnsembleVoteClassifier\nfrom mlxtend.data import iris_data\nfrom mlxtend.evaluate import plot_decision_regions\n\n# Initializing Classifiers\nclf1 = LogisticRegression(random_state=0)\nclf2 = RandomForestClassifier(random_state=0)\nclf3 = SVC(random_state=0, probability=True)\neclf = EnsembleVoteClassifier(clfs=[clf1, clf2, clf3], weights=[2, 1, 1], voting='soft')\n\n# Loading some example data\nX, y = iris_data()\nX = X[:,[0, 2]]\n\n# Plotting Decision Regions\ngs = gridspec.GridSpec(2, 2)\nfig = plt.figure(figsize=(10, 8))\n\nfor clf, lab, grd in zip([clf1, clf2, clf3, eclf],\n                         ['Logistic Regression', 'Random Forest', 'Naive Bayes', 'Ensemble'],\n                         itertools.product([0, 1], repeat=2)):\n    clf.fit(X, y)\n    ax = plt.subplot(gs[grd[0], grd[1]])\n    fig = plot_decision_regions(X=X, y=y, clf=clf, legend=2)\n    plt.title(lab)\nplt.show()",
            "title": "Examples"
        },
        {
            "location": "/user_guide/classifier/EnsembleVoteClassifier/",
            "text": "EnsembleVoteClassifier\n\n\nImplementation of a majority voting \nEnsembleVoteClassifier\n for classification.\n\n\n\n\nfrom mlxtend.classifier import EnsembleVoteClassifier\n\n\n\n\nOverview\n\n\nThe \nEnsembleVoteClassifier\n is a meta-classifier for combining similar or conceptually different machine learning classifiers for classification via majority or plurality voting. (For simplicity, we will refer to both majority and plurality voting as majority voting.)\n\n\n\n\nThe \nEnsembleVoteClassifier\n implements \"hard\" and \"soft\" voting. In hard voting, we predict the final class label as the class label that has been predicted most frequently by the classification models. In soft voting, we predict the class labels by averaging the class-probabilities (only recommended if the classifiers are well-calibrated).\n\n\n\n\nNote\n\n\nIf you are interested in using the \nEnsembleVoteClassifier\n, please note that it is now also available through scikit learn (>0.17) as \nVotingClassifier\n.\n\n\nMajority Voting / Hard Voting\n\n\nHard voting is the simplest case of majority voting. Here, we predict the class label \n$\\hat{y}$\n via majority (plurality) voting of each classifier \n$C_j$\n:\n\n\n$$\\hat{y}=mode\\{C_1(\\mathbf{x}), C_2(\\mathbf{x}), ..., C_m(\\mathbf{x})\\}$$\n\n\nAssuming that we combine three classifiers that classify a training sample as follows:\n\n\n\n\nclassifier 1 -> class 0\n\n\nclassifier 2 -> class 0\n\n\nclassifier 3 -> class 1\n\n\n\n\n$$\\hat{y}=mode\\{0, 0, 1\\} = 0$$\n\n\nVia majority vote, we would we would classify the sample as \"class 0.\"\n\n\nWeighted Majority Vote\n\n\nIn addition to the simple majority vote (hard voting) as described in the previous section, we can compute a weighted majority vote by associating a weight \n$w_j$\n with classifier \n$C_j$\n:\n\n\n$$\\hat{y} = \\arg \\max_i \\sum^{m}_{j=1} w_j \\chi_A \\big(C_j(\\mathbf{x})=i\\big),$$\n\n\nwhere \n$\\chi_A$\n is the characteristic function \n$[C_j(\\mathbf{x}) = i \\; \\in A]$\n, and \n$A$\n is the set of unique class labels. \n\n\nContinuing with the example from the previous section\n\n\n\n\nclassifier 1 -> class 0\n\n\nclassifier 2 -> class 0\n\n\nclassifier 3 -> class 1\n\n\n\n\nassigning the weights {0.2, 0.2, 0.6} would yield a prediction \n$\\hat{y} = 1$\n:\n\n\n$$\\arg \\max_i [0.2 \\times i_0 + 0.2 \\times i_0 + 0.6 \\times i_1] = 1$$\n\n\nSoft Voting\n\n\nIn soft voting, we predict the class labels based on the predicted probabilities \n$p$\n for classifier -- this approach is only recommended if the classifiers are well-calibrated.\n\n\n$$\\hat{y} = \\arg \\max_i \\sum^{m}_{j=1} w_j p_{ij},$$\n\n\nwhere \n$w_j$\n is the weight that can be assigned to the \n$j$\nth classifier.\n\n\nAssuming the example in the previous section was a binary classification task with class labels \n$i \\in \\{0, 1\\}$\n, our ensemble could make the following prediction:\n\n\n\n\n$C_1(\\mathbf{x}) \\rightarrow [0.9, 0.1]$\n\n\n$C_2(\\mathbf{x}) \\rightarrow [0.8, 0.2]$\n\n\n$C_3(\\mathbf{x}) \\rightarrow [0.4, 0.6]$\n\n\n\n\nUsing uniform weights, we compute the average probabilities:\n\n\n$$p(i_0 \\mid \\mathbf{x}) = \\frac{0.9 + 0.8 + 0.4}{3} = 0.7 \\\\\\\\\np(i_1 \\mid \\mathbf{x}) = \\frac{0.1 + 0.2 + 0.6}{3} = 0.3$$\n\n\n$$\\hat{y} = \\arg \\max_i \\big[p(i_0 \\mid \\mathbf{x}), p(i_1 \\mid \\mathbf{x}) \\big] = 0$$\n\n\nHowever, assigning the weights {0.1, 0.1, 0.8} would yield a prediction \n$\\hat{y} = 1$\n:\n\n\n$$p(i_0 \\mid \\mathbf{x}) = {0.1 \\times 0.9 + 0.1 \\times 0.8 + 0.8 \\times  0.4} = 0.49 \\\\\\\\\np(i_1 \\mid \\mathbf{x}) = {0.1 \\times 0.1 + 0.2 \\times 0.1 + 0.8 \\times 0.6} = 0.51$$\n\n\n$$\\hat{y} = \\arg \\max_i \\big[p(i_0 \\mid \\mathbf{x}), p(i_1 \\mid \\mathbf{x}) \\big] = 1$$\n\n\nReferences\n\n\n\n\n[1] S. Raschka. \nPython Machine Learning\n. Packt Publishing Ltd., 2015.\n\n\n\n\nExamples\n\n\nExample 1 -  Classifying Iris Flowers Using Different Classification Models\n\n\nfrom sklearn import datasets\n\niris = datasets.load_iris()\nX, y = iris.data[:, 1:3], iris.target\n\n\n\n\nfrom sklearn import cross_validation\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.naive_bayes import GaussianNB \nfrom sklearn.ensemble import RandomForestClassifier\nimport numpy as np\n\nclf1 = LogisticRegression(random_state=1)\nclf2 = RandomForestClassifier(random_state=1)\nclf3 = GaussianNB()\n\nprint('5-fold cross validation:\\n')\n\nfor clf, label in zip([clf1, clf2, clf3], ['Logistic Regression', 'Random Forest', 'Naive Bayes']):\n\n    scores = cross_validation.cross_val_score(clf, X, y, cv=5, scoring='accuracy')\n    print(\"Accuracy: %0.2f (+/- %0.2f) [%s]\" % (scores.mean(), scores.std(), label))\n\n\n\n\n5-fold cross validation:\n\nAccuracy: 0.90 (+/- 0.05) [Logistic Regression]\nAccuracy: 0.93 (+/- 0.05) [Random Forest]\nAccuracy: 0.91 (+/- 0.04) [Naive Bayes]\n\n\n\nfrom mlxtend.classifier import EnsembleVoteClassifier\n\neclf = EnsembleVoteClassifier(clfs=[clf1, clf2, clf3], weights=[1,1,1])\n\nfor clf, label in zip([clf1, clf2, clf3, eclf], ['Logistic Regression', 'Random Forest', 'Naive Bayes', 'Ensemble']):\n\n    scores = cross_validation.cross_val_score(clf, X, y, cv=5, scoring='accuracy')\n    print(\"Accuracy: %0.2f (+/- %0.2f) [%s]\" % (scores.mean(), scores.std(), label))\n\n\n\n\nAccuracy: 0.90 (+/- 0.05) [Logistic Regression]\nAccuracy: 0.93 (+/- 0.05) [Random Forest]\nAccuracy: 0.91 (+/- 0.04) [Naive Bayes]\nAccuracy: 0.95 (+/- 0.05) [Ensemble]\n\n\n\nPlotting Decision Regions\n\n\nimport matplotlib.pyplot as plt\nfrom mlxtend.evaluate import plot_decision_regions\nimport matplotlib.gridspec as gridspec\nimport itertools\n\ngs = gridspec.GridSpec(2, 2)\n\nfig = plt.figure(figsize=(10,8))\n\nfor clf, lab, grd in zip([clf1, clf2, clf3, eclf],\n                         ['Logistic Regression', 'Random Forest', 'Naive Bayes', 'Ensemble'],\n                         itertools.product([0, 1], repeat=2)):\n\n    clf.fit(X, y)\n    ax = plt.subplot(gs[grd[0], grd[1]])\n    fig = plot_decision_regions(X=X, y=y, clf=clf)\n    plt.title(lab)\n\n\n\n\n\n\nExample 2 - Grid Search\n\n\nfrom sklearn import datasets\n\niris = datasets.load_iris()\nX, y = iris.data[:, 1:3], iris.target\n\n\n\n\nfrom sklearn.grid_search import GridSearchCV\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.naive_bayes import GaussianNB \nfrom sklearn.ensemble import RandomForestClassifier\nfrom mlxtend.classifier import EnsembleVoteClassifier\n\nclf1 = LogisticRegression(random_state=1)\nclf2 = RandomForestClassifier(random_state=1)\nclf3 = GaussianNB()\neclf = EnsembleVoteClassifier(clfs=[clf1, clf2, clf3], voting='soft')\n\nparams = {'logisticregression__C': [1.0, 100.0],\n          'randomforestclassifier__n_estimators': [20, 200],}\n\ngrid = GridSearchCV(estimator=eclf, param_grid=params, cv=5)\ngrid.fit(iris.data, iris.target)\n\nfor params, mean_score, scores in grid.grid_scores_:\n    print(\"%0.3f (+/-%0.03f) for %r\"\n        % (mean_score, scores.std() / 2, params))\n\n\n\n\n0.953 (+/-0.013) for {'logisticregression__C': 1.0, 'randomforestclassifier__n_estimators': 20}\n0.960 (+/-0.012) for {'logisticregression__C': 1.0, 'randomforestclassifier__n_estimators': 200}\n0.960 (+/-0.012) for {'logisticregression__C': 100.0, 'randomforestclassifier__n_estimators': 20}\n0.953 (+/-0.017) for {'logisticregression__C': 100.0, 'randomforestclassifier__n_estimators': 200}\n\n\n\nNote\n: If the \nEnsembleClassifier\n is initialized with multiple similar estimator objects, the estimator names are modified with consecutive integer indices, for example:\n\n\nclf1 = LogisticRegression(random_state=1)\nclf2 = RandomForestClassifier(random_state=1)\neclf = EnsembleVoteClassifier(clfs=[clf1, clf1, clf2], voting='soft')\n\nparams = {'logisticregression-1__C': [1.0, 100.0],\n          'logisticregression-2__C': [1.0, 100.0],\n          'randomforestclassifier__n_estimators': [20, 200],}\n\ngrid = GridSearchCV(estimator=eclf, param_grid=params, cv=5)\ngrid = grid.fit(iris.data, iris.target)\n\n\n\n\nAPI\n\n\nEnsembleVoteClassifier(clfs, voting='hard', weights=None, verbose=0)\n\n\nSoft Voting/Majority Rule classifier for scikit-learn estimators.\n\n\nParameters\n\n\n\n\n\n\nclfs\n : array-like, shape = [n_classifiers]\n\n\nA list of classifiers.\nInvoking the \nfit\n method on the \nVotingClassifier\n will fit clones\nof those original classifiers that will\nbe stored in the class attribute\n\nself.clfs_\n.\n\n\n\n\n\n\nvoting\n : str, {'hard', 'soft'} (default='hard')\n\n\nIf 'hard', uses predicted class labels for majority rule voting.\nElse if 'soft', predicts the class label based on the argmax of\nthe sums of the predicted probalities, which is recommended for\nan ensemble of well-calibrated classifiers.\n\n\n\n\n\n\nweights\n : array-like, shape = [n_classifiers], optional (default=\nNone\n)\n\n\nSequence of weights (\nfloat\n or \nint\n) to weight the occurances of\npredicted class labels (\nhard\n voting) or class probabilities\nbefore averaging (\nsoft\n voting). Uses uniform weights if \nNone\n.\n\n\n\n\n\n\nverbose\n : int, optional (default=0)\n\n\nControls the verbosity of the building process.\n- \nverbose=0\n (default): Prints nothing\n- \nverbose=1\n: Prints the number & name of the clf being fitted\n- \nverbose=2\n: Prints info about the parameters of the clf being fitted\n- \nverbose>2\n: Changes \nverbose\n param of the underlying clf to\nself.verbose - 2\n\n\n\n\n\n\nAttributes\n\n\n\n\n\n\nclasses_\n : array-like, shape = [n_predictions]\n\n\n\n\n\n\nclf\n : array-like, shape = [n_predictions]\n\n\nThe unmodified input classifiers\n\n\n\n\n\n\nclf_\n : array-like, shape = [n_predictions]\n\n\nFitted clones of the input classifiers\n\n\n\n\n\n\nExamples\n\n\n>>> import numpy as np\n>>> from sklearn.linear_model import LogisticRegression\n>>> from sklearn.naive_bayes import GaussianNB\n>>> from sklearn.ensemble import RandomForestClassifier\n>>> from mlxtend.sklearn import EnsembleVoteClassifier\n>>> clf1 = LogisticRegression(random_seed=1)\n>>> clf2 = RandomForestClassifier(random_seed=1)\n>>> clf3 = GaussianNB()\n>>> X = np.array([[-1, -1], [-2, -1], [-3, -2], [1, 1], [2, 1], [3, 2]])\n>>> y = np.array([1, 1, 1, 2, 2, 2])\n>>> eclf1 = EnsembleVoteClassifier(clfs=[clf1, clf2, clf3],\n... voting='hard', verbose=1)\n>>> eclf1 = eclf1.fit(X, y)\n>>> print(eclf1.predict(X))\n[1 1 1 2 2 2]\n>>> eclf2 = EnsembleVoteClassifier(clfs=[clf1, clf2, clf3], voting='soft')\n>>> eclf2 = eclf2.fit(X, y)\n>>> print(eclf2.predict(X))\n[1 1 1 2 2 2]\n>>> eclf3 = EnsembleVoteClassifier(clfs=[clf1, clf2, clf3],\n...                          voting='soft', weights=[2,1,1])\n>>> eclf3 = eclf3.fit(X, y)\n>>> print(eclf3.predict(X))\n[1 1 1 2 2 2]\n>>>\n\n\n\nMethods\n\n\n\n\n\nfit(X, y)\n\n\nLearn weight coefficients from training data for each classifier.\n\n\nParameters\n\n\n\n\n\n\nX\n : {array-like, sparse matrix}, shape = [n_samples, n_features]\n\n\nTraining vectors, where n_samples is the number of samples and\nn_features is the number of features.\n\n\n\n\n\n\ny\n : array-like, shape = [n_samples]\n\n\nTarget values.\n\n\n\n\n\n\nReturns\n\n\n\n\nself\n : object\n\n\n\n\n\n\n\nfit_transform(X, y=None, \nfit_params)\n\n\nFit to data, then transform it.\n\n\nFits transformer to X and y with optional parameters fit_params\nand returns a transformed version of X.\n\n\n\nParameters\n\n\n\n\n\n\nX\n : numpy array of shape [n_samples, n_features]\n\n\nTraining set.\n\n\n\n\n\n\ny\n : numpy array of shape [n_samples]\n\n\nTarget values.\n\n\n\n\n\n\nReturns\n\n\n\n\n\n\nX_new\n : numpy array of shape [n_samples, n_features_new]\n\n\nTransformed array.\n\n\n\n\n\n\n\n\n\nget_params(deep=True)\n\n\nReturn estimator parameter names for GridSearch support.\n\n\n\n\n\npredict(X)\n\n\nPredict class labels for X.\n\n\nParameters\n\n\n\n\n\n\nX\n : {array-like, sparse matrix}, shape = [n_samples, n_features]\n\n\nTraining vectors, where n_samples is the number of samples and\nn_features is the number of features.\n\n\n\n\n\n\nReturns\n\n\n\n\n\n\nmaj\n : array-like, shape = [n_samples]\n\n\nPredicted class labels.\n\n\n\n\n\n\n\n\n\npredict_proba(X)\n\n\nPredict class probabilities for X.\n\n\nParameters\n\n\n\n\n\n\nX\n : {array-like, sparse matrix}, shape = [n_samples, n_features]\n\n\nTraining vectors, where n_samples is the number of samples and\nn_features is the number of features.\n\n\n\n\n\n\nReturns\n\n\n\n\n\n\navg\n : array-like, shape = [n_samples, n_classes]\n\n\nWeighted average probability for each class per sample.\n\n\n\n\n\n\n\n\n\nscore(X, y, sample_weight=None)\n\n\nReturns the mean accuracy on the given test data and labels.\n\n\nIn multi-label classification, this is the subset accuracy\nwhich is a harsh metric since you require for each sample that\neach label set be correctly predicted.\n\n\n\nParameters\n\n\n\n\n\n\nX\n : array-like, shape = (n_samples, n_features)\n\n\nTest samples.\n\n\n\n\n\n\ny\n : array-like, shape = (n_samples) or (n_samples, n_outputs)\n\n\nTrue labels for X.\n\n\n\n\n\n\nsample_weight\n : array-like, shape = [n_samples], optional\n\n\nSample weights.\n\n\n\n\n\n\nReturns\n\n\n\n\n\n\nscore\n : float\n\n\nMean accuracy of self.predict(X) wrt. y.\n\n\n\n\n\n\n\n\n\nset_params(\nparams)\n\n\nSet the parameters of this estimator.\n\n\nThe method works on simple estimators as well as on nested objects\n(such as pipelines). The former have parameters of the form\n``<component>__<parameter>`` so that it's possible to update each\ncomponent of a nested object.\n\n\n\nReturns\n\n\nself\n\n\n\n\n\ntransform(X)\n\n\nReturn class labels or probabilities for X for each estimator.\n\n\nParameters\n\n\n\n\n\n\nX\n : {array-like, sparse matrix}, shape = [n_samples, n_features]\n\n\nTraining vectors, where n_samples is the number of samples and\nn_features is the number of features.\n\n\n\n\n\n\nReturns\n\n\n\n\n\n\nIf\nvoting='soft'`` : array-like = [n_classifiers, n_samples, n_classes]\n\n\nClass probabilties calculated by each classifier.\n\n\n\n\n\n\nIf\nvoting='hard'`` : array-like = [n_classifiers, n_samples]\n\n\nClass labels predicted by each classifier.",
            "title": "EnsembleVoteClassifier"
        },
        {
            "location": "/user_guide/classifier/EnsembleVoteClassifier/#ensemblevoteclassifier",
            "text": "Implementation of a majority voting  EnsembleVoteClassifier  for classification.   from mlxtend.classifier import EnsembleVoteClassifier",
            "title": "EnsembleVoteClassifier"
        },
        {
            "location": "/user_guide/classifier/EnsembleVoteClassifier/#overview",
            "text": "The  EnsembleVoteClassifier  is a meta-classifier for combining similar or conceptually different machine learning classifiers for classification via majority or plurality voting. (For simplicity, we will refer to both majority and plurality voting as majority voting.)   The  EnsembleVoteClassifier  implements \"hard\" and \"soft\" voting. In hard voting, we predict the final class label as the class label that has been predicted most frequently by the classification models. In soft voting, we predict the class labels by averaging the class-probabilities (only recommended if the classifiers are well-calibrated).   Note  If you are interested in using the  EnsembleVoteClassifier , please note that it is now also available through scikit learn (>0.17) as  VotingClassifier .",
            "title": "Overview"
        },
        {
            "location": "/user_guide/classifier/EnsembleVoteClassifier/#majority-voting-hard-voting",
            "text": "Hard voting is the simplest case of majority voting. Here, we predict the class label  $\\hat{y}$  via majority (plurality) voting of each classifier  $C_j$ :  $$\\hat{y}=mode\\{C_1(\\mathbf{x}), C_2(\\mathbf{x}), ..., C_m(\\mathbf{x})\\}$$  Assuming that we combine three classifiers that classify a training sample as follows:   classifier 1 -> class 0  classifier 2 -> class 0  classifier 3 -> class 1   $$\\hat{y}=mode\\{0, 0, 1\\} = 0$$  Via majority vote, we would we would classify the sample as \"class 0.\"",
            "title": "Majority Voting / Hard Voting"
        },
        {
            "location": "/user_guide/classifier/EnsembleVoteClassifier/#weighted-majority-vote",
            "text": "In addition to the simple majority vote (hard voting) as described in the previous section, we can compute a weighted majority vote by associating a weight  $w_j$  with classifier  $C_j$ :  $$\\hat{y} = \\arg \\max_i \\sum^{m}_{j=1} w_j \\chi_A \\big(C_j(\\mathbf{x})=i\\big),$$  where  $\\chi_A$  is the characteristic function  $[C_j(\\mathbf{x}) = i \\; \\in A]$ , and  $A$  is the set of unique class labels.   Continuing with the example from the previous section   classifier 1 -> class 0  classifier 2 -> class 0  classifier 3 -> class 1   assigning the weights {0.2, 0.2, 0.6} would yield a prediction  $\\hat{y} = 1$ :  $$\\arg \\max_i [0.2 \\times i_0 + 0.2 \\times i_0 + 0.6 \\times i_1] = 1$$",
            "title": "Weighted Majority Vote"
        },
        {
            "location": "/user_guide/classifier/EnsembleVoteClassifier/#soft-voting",
            "text": "In soft voting, we predict the class labels based on the predicted probabilities  $p$  for classifier -- this approach is only recommended if the classifiers are well-calibrated.  $$\\hat{y} = \\arg \\max_i \\sum^{m}_{j=1} w_j p_{ij},$$  where  $w_j$  is the weight that can be assigned to the  $j$ th classifier.  Assuming the example in the previous section was a binary classification task with class labels  $i \\in \\{0, 1\\}$ , our ensemble could make the following prediction:   $C_1(\\mathbf{x}) \\rightarrow [0.9, 0.1]$  $C_2(\\mathbf{x}) \\rightarrow [0.8, 0.2]$  $C_3(\\mathbf{x}) \\rightarrow [0.4, 0.6]$   Using uniform weights, we compute the average probabilities:  $$p(i_0 \\mid \\mathbf{x}) = \\frac{0.9 + 0.8 + 0.4}{3} = 0.7 \\\\\\\\\np(i_1 \\mid \\mathbf{x}) = \\frac{0.1 + 0.2 + 0.6}{3} = 0.3$$  $$\\hat{y} = \\arg \\max_i \\big[p(i_0 \\mid \\mathbf{x}), p(i_1 \\mid \\mathbf{x}) \\big] = 0$$  However, assigning the weights {0.1, 0.1, 0.8} would yield a prediction  $\\hat{y} = 1$ :  $$p(i_0 \\mid \\mathbf{x}) = {0.1 \\times 0.9 + 0.1 \\times 0.8 + 0.8 \\times  0.4} = 0.49 \\\\\\\\\np(i_1 \\mid \\mathbf{x}) = {0.1 \\times 0.1 + 0.2 \\times 0.1 + 0.8 \\times 0.6} = 0.51$$  $$\\hat{y} = \\arg \\max_i \\big[p(i_0 \\mid \\mathbf{x}), p(i_1 \\mid \\mathbf{x}) \\big] = 1$$",
            "title": "Soft Voting"
        },
        {
            "location": "/user_guide/classifier/EnsembleVoteClassifier/#references",
            "text": "[1] S. Raschka.  Python Machine Learning . Packt Publishing Ltd., 2015.",
            "title": "References"
        },
        {
            "location": "/user_guide/classifier/EnsembleVoteClassifier/#examples",
            "text": "",
            "title": "Examples"
        },
        {
            "location": "/user_guide/classifier/EnsembleVoteClassifier/#example-1-classifying-iris-flowers-using-different-classification-models",
            "text": "from sklearn import datasets\n\niris = datasets.load_iris()\nX, y = iris.data[:, 1:3], iris.target  from sklearn import cross_validation\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.naive_bayes import GaussianNB \nfrom sklearn.ensemble import RandomForestClassifier\nimport numpy as np\n\nclf1 = LogisticRegression(random_state=1)\nclf2 = RandomForestClassifier(random_state=1)\nclf3 = GaussianNB()\n\nprint('5-fold cross validation:\\n')\n\nfor clf, label in zip([clf1, clf2, clf3], ['Logistic Regression', 'Random Forest', 'Naive Bayes']):\n\n    scores = cross_validation.cross_val_score(clf, X, y, cv=5, scoring='accuracy')\n    print(\"Accuracy: %0.2f (+/- %0.2f) [%s]\" % (scores.mean(), scores.std(), label))  5-fold cross validation:\n\nAccuracy: 0.90 (+/- 0.05) [Logistic Regression]\nAccuracy: 0.93 (+/- 0.05) [Random Forest]\nAccuracy: 0.91 (+/- 0.04) [Naive Bayes]  from mlxtend.classifier import EnsembleVoteClassifier\n\neclf = EnsembleVoteClassifier(clfs=[clf1, clf2, clf3], weights=[1,1,1])\n\nfor clf, label in zip([clf1, clf2, clf3, eclf], ['Logistic Regression', 'Random Forest', 'Naive Bayes', 'Ensemble']):\n\n    scores = cross_validation.cross_val_score(clf, X, y, cv=5, scoring='accuracy')\n    print(\"Accuracy: %0.2f (+/- %0.2f) [%s]\" % (scores.mean(), scores.std(), label))  Accuracy: 0.90 (+/- 0.05) [Logistic Regression]\nAccuracy: 0.93 (+/- 0.05) [Random Forest]\nAccuracy: 0.91 (+/- 0.04) [Naive Bayes]\nAccuracy: 0.95 (+/- 0.05) [Ensemble]",
            "title": "Example 1 -  Classifying Iris Flowers Using Different Classification Models"
        },
        {
            "location": "/user_guide/classifier/EnsembleVoteClassifier/#plotting-decision-regions",
            "text": "import matplotlib.pyplot as plt\nfrom mlxtend.evaluate import plot_decision_regions\nimport matplotlib.gridspec as gridspec\nimport itertools\n\ngs = gridspec.GridSpec(2, 2)\n\nfig = plt.figure(figsize=(10,8))\n\nfor clf, lab, grd in zip([clf1, clf2, clf3, eclf],\n                         ['Logistic Regression', 'Random Forest', 'Naive Bayes', 'Ensemble'],\n                         itertools.product([0, 1], repeat=2)):\n\n    clf.fit(X, y)\n    ax = plt.subplot(gs[grd[0], grd[1]])\n    fig = plot_decision_regions(X=X, y=y, clf=clf)\n    plt.title(lab)",
            "title": "Plotting Decision Regions"
        },
        {
            "location": "/user_guide/classifier/EnsembleVoteClassifier/#example-2-grid-search",
            "text": "from sklearn import datasets\n\niris = datasets.load_iris()\nX, y = iris.data[:, 1:3], iris.target  from sklearn.grid_search import GridSearchCV\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.naive_bayes import GaussianNB \nfrom sklearn.ensemble import RandomForestClassifier\nfrom mlxtend.classifier import EnsembleVoteClassifier\n\nclf1 = LogisticRegression(random_state=1)\nclf2 = RandomForestClassifier(random_state=1)\nclf3 = GaussianNB()\neclf = EnsembleVoteClassifier(clfs=[clf1, clf2, clf3], voting='soft')\n\nparams = {'logisticregression__C': [1.0, 100.0],\n          'randomforestclassifier__n_estimators': [20, 200],}\n\ngrid = GridSearchCV(estimator=eclf, param_grid=params, cv=5)\ngrid.fit(iris.data, iris.target)\n\nfor params, mean_score, scores in grid.grid_scores_:\n    print(\"%0.3f (+/-%0.03f) for %r\"\n        % (mean_score, scores.std() / 2, params))  0.953 (+/-0.013) for {'logisticregression__C': 1.0, 'randomforestclassifier__n_estimators': 20}\n0.960 (+/-0.012) for {'logisticregression__C': 1.0, 'randomforestclassifier__n_estimators': 200}\n0.960 (+/-0.012) for {'logisticregression__C': 100.0, 'randomforestclassifier__n_estimators': 20}\n0.953 (+/-0.017) for {'logisticregression__C': 100.0, 'randomforestclassifier__n_estimators': 200}  Note : If the  EnsembleClassifier  is initialized with multiple similar estimator objects, the estimator names are modified with consecutive integer indices, for example:  clf1 = LogisticRegression(random_state=1)\nclf2 = RandomForestClassifier(random_state=1)\neclf = EnsembleVoteClassifier(clfs=[clf1, clf1, clf2], voting='soft')\n\nparams = {'logisticregression-1__C': [1.0, 100.0],\n          'logisticregression-2__C': [1.0, 100.0],\n          'randomforestclassifier__n_estimators': [20, 200],}\n\ngrid = GridSearchCV(estimator=eclf, param_grid=params, cv=5)\ngrid = grid.fit(iris.data, iris.target)",
            "title": "Example 2 - Grid Search"
        },
        {
            "location": "/user_guide/classifier/EnsembleVoteClassifier/#api",
            "text": "EnsembleVoteClassifier(clfs, voting='hard', weights=None, verbose=0)  Soft Voting/Majority Rule classifier for scikit-learn estimators.  Parameters    clfs  : array-like, shape = [n_classifiers]  A list of classifiers.\nInvoking the  fit  method on the  VotingClassifier  will fit clones\nof those original classifiers that will\nbe stored in the class attribute self.clfs_ .    voting  : str, {'hard', 'soft'} (default='hard')  If 'hard', uses predicted class labels for majority rule voting.\nElse if 'soft', predicts the class label based on the argmax of\nthe sums of the predicted probalities, which is recommended for\nan ensemble of well-calibrated classifiers.    weights  : array-like, shape = [n_classifiers], optional (default= None )  Sequence of weights ( float  or  int ) to weight the occurances of\npredicted class labels ( hard  voting) or class probabilities\nbefore averaging ( soft  voting). Uses uniform weights if  None .    verbose  : int, optional (default=0)  Controls the verbosity of the building process.\n-  verbose=0  (default): Prints nothing\n-  verbose=1 : Prints the number & name of the clf being fitted\n-  verbose=2 : Prints info about the parameters of the clf being fitted\n-  verbose>2 : Changes  verbose  param of the underlying clf to\nself.verbose - 2    Attributes    classes_  : array-like, shape = [n_predictions]    clf  : array-like, shape = [n_predictions]  The unmodified input classifiers    clf_  : array-like, shape = [n_predictions]  Fitted clones of the input classifiers    Examples  >>> import numpy as np\n>>> from sklearn.linear_model import LogisticRegression\n>>> from sklearn.naive_bayes import GaussianNB\n>>> from sklearn.ensemble import RandomForestClassifier\n>>> from mlxtend.sklearn import EnsembleVoteClassifier\n>>> clf1 = LogisticRegression(random_seed=1)\n>>> clf2 = RandomForestClassifier(random_seed=1)\n>>> clf3 = GaussianNB()\n>>> X = np.array([[-1, -1], [-2, -1], [-3, -2], [1, 1], [2, 1], [3, 2]])\n>>> y = np.array([1, 1, 1, 2, 2, 2])\n>>> eclf1 = EnsembleVoteClassifier(clfs=[clf1, clf2, clf3],\n... voting='hard', verbose=1)\n>>> eclf1 = eclf1.fit(X, y)\n>>> print(eclf1.predict(X))\n[1 1 1 2 2 2]\n>>> eclf2 = EnsembleVoteClassifier(clfs=[clf1, clf2, clf3], voting='soft')\n>>> eclf2 = eclf2.fit(X, y)\n>>> print(eclf2.predict(X))\n[1 1 1 2 2 2]\n>>> eclf3 = EnsembleVoteClassifier(clfs=[clf1, clf2, clf3],\n...                          voting='soft', weights=[2,1,1])\n>>> eclf3 = eclf3.fit(X, y)\n>>> print(eclf3.predict(X))\n[1 1 1 2 2 2]\n>>>",
            "title": "API"
        },
        {
            "location": "/user_guide/classifier/EnsembleVoteClassifier/#methods",
            "text": "fit(X, y)  Learn weight coefficients from training data for each classifier.  Parameters    X  : {array-like, sparse matrix}, shape = [n_samples, n_features]  Training vectors, where n_samples is the number of samples and\nn_features is the number of features.    y  : array-like, shape = [n_samples]  Target values.    Returns   self  : object    fit_transform(X, y=None,  fit_params)  Fit to data, then transform it.  Fits transformer to X and y with optional parameters fit_params\nand returns a transformed version of X.  Parameters    X  : numpy array of shape [n_samples, n_features]  Training set.    y  : numpy array of shape [n_samples]  Target values.    Returns    X_new  : numpy array of shape [n_samples, n_features_new]  Transformed array.     get_params(deep=True)  Return estimator parameter names for GridSearch support.   predict(X)  Predict class labels for X.  Parameters    X  : {array-like, sparse matrix}, shape = [n_samples, n_features]  Training vectors, where n_samples is the number of samples and\nn_features is the number of features.    Returns    maj  : array-like, shape = [n_samples]  Predicted class labels.     predict_proba(X)  Predict class probabilities for X.  Parameters    X  : {array-like, sparse matrix}, shape = [n_samples, n_features]  Training vectors, where n_samples is the number of samples and\nn_features is the number of features.    Returns    avg  : array-like, shape = [n_samples, n_classes]  Weighted average probability for each class per sample.     score(X, y, sample_weight=None)  Returns the mean accuracy on the given test data and labels.  In multi-label classification, this is the subset accuracy\nwhich is a harsh metric since you require for each sample that\neach label set be correctly predicted.  Parameters    X  : array-like, shape = (n_samples, n_features)  Test samples.    y  : array-like, shape = (n_samples) or (n_samples, n_outputs)  True labels for X.    sample_weight  : array-like, shape = [n_samples], optional  Sample weights.    Returns    score  : float  Mean accuracy of self.predict(X) wrt. y.     set_params( params)  Set the parameters of this estimator.  The method works on simple estimators as well as on nested objects\n(such as pipelines). The former have parameters of the form\n``<component>__<parameter>`` so that it's possible to update each\ncomponent of a nested object.  Returns  self   transform(X)  Return class labels or probabilities for X for each estimator.  Parameters    X  : {array-like, sparse matrix}, shape = [n_samples, n_features]  Training vectors, where n_samples is the number of samples and\nn_features is the number of features.    Returns    If voting='soft'`` : array-like = [n_classifiers, n_samples, n_classes]  Class probabilties calculated by each classifier.    If voting='hard'`` : array-like = [n_classifiers, n_samples]  Class labels predicted by each classifier.",
            "title": "Methods"
        },
        {
            "location": "/user_guide/classifier/Perceptron/",
            "text": "Perceptron\n\n\nImplementation of a Perceptron learning algorithm for classification.\n\n\n\n\nfrom mlxtend.classifier import Perceptron\n\n\n\n\nOverview\n\n\nThe idea behind this \"thresholded\" perceptron was to mimic how a single neuron in the brain works: It either \"fires\" or not. \nA perceptron receives multiple input signals, and if the sum of the input signals exceed a certain threshold it either returns a signal or remains \"silent\" otherwise. What made this a \"machine learning\" algorithm was Frank Rosenblatt's idea of the perceptron learning rule: The perceptron algorithm is about learning the weights for the input signals in order to draw linear decision boundary that allows us to discriminate between the two linearly separable classes +1 and -1.\n\n\n\n\nBasic Notation\n\n\nBefore we dive deeper into the algorithm(s) for learning the weights of the perceptron classifier, let us take a brief look at the basic notation. In the following sections, we will label the \npositive\n and \nnegative\n class in our binary classification setting as \"1\" and \"-1\", respectively. Next, we define an activation function \n$g(\\mathbf{z})$\n that takes a linear combination of the input values \n$\\mathbf{x}$\n and weights \n$\\mathbf{w}$\n as input (\n$\\mathbf{z} = w_1x_{1} + \\dots + w_mx_{m}$\n), and if \n$g(\\mathbf{z})$\n is greater than a defined threshold \n$\\theta$\n we predict 1 and -1 otherwise; in this case, this activation function \n$g$\n is a simple \"unit step function,\" which is sometimes also called \"Heaviside step function.\" \n\n\n$$\n g(z) =\\begin{cases}\n    1 & \\text{if $z \\ge \\theta$}\\\\\n    -1 & \\text{otherwise}.\n  \\end{cases}\n$$\n\n\nwhere\n\n\n$$z =  w_1x_{1} + \\dots + w_mx_{m} = \\sum_{j=1}^{m} x_{j}w_{j} \\\\ = \\mathbf{w}^T\\mathbf{x}$$\n\n\n$\\mathbf{w}$\n is the feature vector, and \n$\\mathbf{x}$\n is an \n$m$\n-dimensional sample from the training dataset:\n\n\n$$ \n\\mathbf{w} = \\begin{bmatrix}\n    w_{1}  \\\\\n    \\vdots \\\\\n    w_{m}\n\\end{bmatrix}\n\\quad  \\mathbf{x} = \\begin{bmatrix}\n    x_{1}  \\\\\n    \\vdots \\\\\n    x_{m}\n\\end{bmatrix}$$\n\n\nIn order to simplify the notation, we bring \n$\\theta$\n to the left side of the equation and define \n$w_0 = -\\theta  \\text{ and } x_0=1$\n \n\n\nso that \n\n\n$$\\begin{equation}\n g({z}) =\\begin{cases}\n    1 & \\text{if $z \\ge 0$}\\\\\n    -1 & \\text{otherwise}.\n  \\end{cases}\n\\end{equation}$$\n\n\nand\n\n\n$$z = w_0x_{0} + w_1x_{1} + \\dots + w_mx_{m} = \\sum_{j=0}^{m} x_{j}w_{j} \\\\ = \\mathbf{w}^T\\mathbf{x}.$$\n\n\nPerceptron Rule\n\n\nRosenblatt's initial perceptron rule is fairly simple and can be summarized by the following steps: \n\n\n\n\nInitialize the weights to 0 or small random numbers.\n\n\nFor each training sample \n$\\mathbf{x^{(i)}}$\n:\n\n\nCalculate the \noutput\n value.\n\n\nUpdate the weights.\n\n\n\n\n\n\n\n\nThe output value is the class label predicted by the unit step function that we defined earlier (output \n$=g(\\mathbf{z})$\n) and the weight update can be written more formally as  \n$w_j := w_j + \\Delta w_j$\n.\n\n\nThe value for updating the weights at each increment is calculated by the learning rule\n\n\n$\\Delta w_j = \\eta \\; (\\text{target}^{(i)} - \\text{output}^{(i)})\\;x^{(i)}_{j}$\n\n\nwhere \n$\\eta$\n is the learning rate (a constant between 0.0 and 1.0), \"target\" is the true class label, and the \"output\" is the predicted class label.\n\n\naIt is important to note that all weights in the weight vector are being updated simultaneously. Concretely, for a 2-dimensional dataset, we would write the update as:\n\n\n$\\Delta w_0 = \\eta(\\text{target}^{(i)} - \\text{output}^{(i)})$\n\n\n$\\Delta w_1 = \\eta(\\text{target}^{(i)} - \\text{output}^{(i)})\\;x^{(i)}_{1}$\n\n\n$\\Delta w_2 = \\eta(\\text{target}^{(i)} - \\text{output}^{(i)})\\;x^{(i)}_{2}$\n  \n\n\nBefore we implement the perceptron rule in Python, let us make a simple thought experiment to illustrate how beautifully simple this learning rule really is. In the two scenarios where the perceptron predicts the class label correctly, the weights remain unchanged:\n\n\n\n\n$\\Delta w_j = \\eta(-1^{(i)} - -1^{(i)})\\;x^{(i)}_{j} = 0$\n \n\n\n$\\Delta w_j = \\eta(1^{(i)} - 1^{(i)})\\;x^{(i)}_{j} = 0$\n \n\n\n\n\nHowever, in case of a wrong prediction, the weights are being \"pushed\" towards the direction of the positive or negative target class, respectively:\n\n\n\n\n$\\Delta w_j = \\eta(1^{(i)} - -1^{(i)})\\;x^{(i)}_{j} = \\eta(2)\\;x^{(i)}_{j}$\n \n\n\n$\\Delta w_j = \\eta(-1^{(i)} - 1^{(i)})\\;x^{(i)}_{j} = \\eta(-2)\\;x^{(i)}_{j}$\n \n\n\n\n\nIt is important to note that the convergence of the perceptron is only guaranteed if the two classes are linearly separable. If the two classes can't be separated by a linear decision boundary, we can set a maximum number of passes over the training dataset (\"epochs\") and/or a threshold for the number of tolerated misclassifications.\n\n\nReferences\n\n\n\n\nF. Rosenblatt. The perceptron, a perceiving and recognizing automaton Project Para. Cornell Aeronautical Laboratory, 1957.\n\n\n\n\nExamples\n\n\nExample 1 - Classification of Iris Flowers\n\n\nfrom mlxtend.data import iris_data\nfrom mlxtend.evaluate import plot_decision_regions\nfrom mlxtend.classifier import Perceptron\nimport matplotlib.pyplot as plt\n\n# Loading Data\n\nX, y = iris_data()\nX = X[:, [0, 3]] # sepal length and petal width\nX = X[0:100] # class 0 and class 1\ny = y[0:100] # class 0 and class 1\n\n# standardize\nX[:,0] = (X[:,0] - X[:,0].mean()) / X[:,0].std()\nX[:,1] = (X[:,1] - X[:,1].mean()) / X[:,1].std()\n\n\n# Rosenblatt Perceptron\n\nppn = Perceptron(epochs=5, eta=0.05, random_seed=0)\nppn.fit(X, y)\n\nplot_decision_regions(X, y, clf=ppn)\nplt.title('Perceptron - Rosenblatt Perceptron Rule')\nplt.show()\n\nprint(ppn.w_)\n\nplt.plot(range(len(ppn.cost_)), ppn.cost_)\nplt.xlabel('Iterations')\nplt.ylabel('Missclassifications')\nplt.show()\n\n\n\n\n[-0.3902373  -0.35696213 -0.37944732]\n\n\n\n\n\n[ 0.0097627   0.08408412  0.31449279]\n\n\n\n\n\nAPI\n\n\nPerceptron(eta=0.1, epochs=50, shuffle=False, random_seed=None, zero_init_weight=False)\n\n\nPerceptron classifier.\n\n\nParameters\n\n\n\n\n\n\neta\n : float (default: 0.1)\n\n\nLearning rate (between 0.0 and 1.0)\n\n\n\n\n\n\nepochs\n : int (default: 50)\n\n\nNumber of passes over the training dataset.\n\n\n\n\n\n\nshuffle\n : bool (default: False)\n\n\nShuffles training data every epoch if True to prevent circles.\n\n\n\n\n\n\nrandom_seed\n : int\n\n\nRandom state for initializing random weights.\n\n\n\n\n\n\nzero_init_weight\n : bool (default: False)\n\n\nIf True, weights are initialized to zero instead of small random\nnumbers in the interval [-0.1, 0.1];\nignored if solver='normal equation'\n\n\n\n\n\n\nAttributes\n\n\n\n\n\n\nw_\n : 1d-array\n\n\nWeights after fitting.\n\n\n\n\n\n\ncost_\n : list\n\n\nNumber of misclassifications in every epoch.\n\n\n\n\n\n\nMethods\n\n\n\n\n\nfit(X, y, init_weights=True)\n\n\nLearn weight coefficients from training data.\n\n\nParameters\n\n\n\n\n\n\nX\n : {array-like, sparse matrix}, shape = [n_samples, n_features]\n\n\nTraining vectors, where n_samples is the number of samples and\nn_features is the number of features.\n\n\n\n\n\n\ny\n : array-like, shape = [n_samples]\n\n\nTarget values.\n\n\n\n\n\n\ninit_weights\n : bool (default: True)\n\n\nRe-initializes weights prior to fitting. Set False to continue\ntraining with weights from a previous fitting.\n\n\n\n\n\n\nReturns\n\n\n\n\nself\n : object\n\n\n\n\n\n\n\nnet_input(X)\n\n\nNet input function\n\n\n\n\n\npredict(X)\n\n\nPredict class labels for X.\n\n\nParameters\n\n\n\n\n\n\nX\n : {array-like, sparse matrix}, shape = [n_samples, n_features]\n\n\nTraining vectors, where n_samples is the number of samples and\nn_features is the number of features.\n\n\n\n\n\n\nReturns\n\n\n\n\n\n\nclass\n : int\n\n\nPredicted class label.",
            "title": "Perceptron"
        },
        {
            "location": "/user_guide/classifier/Perceptron/#perceptron",
            "text": "Implementation of a Perceptron learning algorithm for classification.   from mlxtend.classifier import Perceptron",
            "title": "Perceptron"
        },
        {
            "location": "/user_guide/classifier/Perceptron/#overview",
            "text": "The idea behind this \"thresholded\" perceptron was to mimic how a single neuron in the brain works: It either \"fires\" or not. \nA perceptron receives multiple input signals, and if the sum of the input signals exceed a certain threshold it either returns a signal or remains \"silent\" otherwise. What made this a \"machine learning\" algorithm was Frank Rosenblatt's idea of the perceptron learning rule: The perceptron algorithm is about learning the weights for the input signals in order to draw linear decision boundary that allows us to discriminate between the two linearly separable classes +1 and -1.",
            "title": "Overview"
        },
        {
            "location": "/user_guide/classifier/Perceptron/#basic-notation",
            "text": "Before we dive deeper into the algorithm(s) for learning the weights of the perceptron classifier, let us take a brief look at the basic notation. In the following sections, we will label the  positive  and  negative  class in our binary classification setting as \"1\" and \"-1\", respectively. Next, we define an activation function  $g(\\mathbf{z})$  that takes a linear combination of the input values  $\\mathbf{x}$  and weights  $\\mathbf{w}$  as input ( $\\mathbf{z} = w_1x_{1} + \\dots + w_mx_{m}$ ), and if  $g(\\mathbf{z})$  is greater than a defined threshold  $\\theta$  we predict 1 and -1 otherwise; in this case, this activation function  $g$  is a simple \"unit step function,\" which is sometimes also called \"Heaviside step function.\"   $$\n g(z) =\\begin{cases}\n    1 & \\text{if $z \\ge \\theta$}\\\\\n    -1 & \\text{otherwise}.\n  \\end{cases}\n$$  where  $$z =  w_1x_{1} + \\dots + w_mx_{m} = \\sum_{j=1}^{m} x_{j}w_{j} \\\\ = \\mathbf{w}^T\\mathbf{x}$$  $\\mathbf{w}$  is the feature vector, and  $\\mathbf{x}$  is an  $m$ -dimensional sample from the training dataset:  $$ \n\\mathbf{w} = \\begin{bmatrix}\n    w_{1}  \\\\\n    \\vdots \\\\\n    w_{m}\n\\end{bmatrix}\n\\quad  \\mathbf{x} = \\begin{bmatrix}\n    x_{1}  \\\\\n    \\vdots \\\\\n    x_{m}\n\\end{bmatrix}$$  In order to simplify the notation, we bring  $\\theta$  to the left side of the equation and define  $w_0 = -\\theta  \\text{ and } x_0=1$    so that   $$\\begin{equation}\n g({z}) =\\begin{cases}\n    1 & \\text{if $z \\ge 0$}\\\\\n    -1 & \\text{otherwise}.\n  \\end{cases}\n\\end{equation}$$  and  $$z = w_0x_{0} + w_1x_{1} + \\dots + w_mx_{m} = \\sum_{j=0}^{m} x_{j}w_{j} \\\\ = \\mathbf{w}^T\\mathbf{x}.$$",
            "title": "Basic Notation"
        },
        {
            "location": "/user_guide/classifier/Perceptron/#perceptron-rule",
            "text": "Rosenblatt's initial perceptron rule is fairly simple and can be summarized by the following steps:    Initialize the weights to 0 or small random numbers.  For each training sample  $\\mathbf{x^{(i)}}$ :  Calculate the  output  value.  Update the weights.     The output value is the class label predicted by the unit step function that we defined earlier (output  $=g(\\mathbf{z})$ ) and the weight update can be written more formally as   $w_j := w_j + \\Delta w_j$ .  The value for updating the weights at each increment is calculated by the learning rule  $\\Delta w_j = \\eta \\; (\\text{target}^{(i)} - \\text{output}^{(i)})\\;x^{(i)}_{j}$  where  $\\eta$  is the learning rate (a constant between 0.0 and 1.0), \"target\" is the true class label, and the \"output\" is the predicted class label.  aIt is important to note that all weights in the weight vector are being updated simultaneously. Concretely, for a 2-dimensional dataset, we would write the update as:  $\\Delta w_0 = \\eta(\\text{target}^{(i)} - \\text{output}^{(i)})$  $\\Delta w_1 = \\eta(\\text{target}^{(i)} - \\text{output}^{(i)})\\;x^{(i)}_{1}$  $\\Delta w_2 = \\eta(\\text{target}^{(i)} - \\text{output}^{(i)})\\;x^{(i)}_{2}$     Before we implement the perceptron rule in Python, let us make a simple thought experiment to illustrate how beautifully simple this learning rule really is. In the two scenarios where the perceptron predicts the class label correctly, the weights remain unchanged:   $\\Delta w_j = \\eta(-1^{(i)} - -1^{(i)})\\;x^{(i)}_{j} = 0$    $\\Delta w_j = \\eta(1^{(i)} - 1^{(i)})\\;x^{(i)}_{j} = 0$     However, in case of a wrong prediction, the weights are being \"pushed\" towards the direction of the positive or negative target class, respectively:   $\\Delta w_j = \\eta(1^{(i)} - -1^{(i)})\\;x^{(i)}_{j} = \\eta(2)\\;x^{(i)}_{j}$    $\\Delta w_j = \\eta(-1^{(i)} - 1^{(i)})\\;x^{(i)}_{j} = \\eta(-2)\\;x^{(i)}_{j}$     It is important to note that the convergence of the perceptron is only guaranteed if the two classes are linearly separable. If the two classes can't be separated by a linear decision boundary, we can set a maximum number of passes over the training dataset (\"epochs\") and/or a threshold for the number of tolerated misclassifications.",
            "title": "Perceptron Rule"
        },
        {
            "location": "/user_guide/classifier/Perceptron/#references",
            "text": "F. Rosenblatt. The perceptron, a perceiving and recognizing automaton Project Para. Cornell Aeronautical Laboratory, 1957.",
            "title": "References"
        },
        {
            "location": "/user_guide/classifier/Perceptron/#examples",
            "text": "",
            "title": "Examples"
        },
        {
            "location": "/user_guide/classifier/Perceptron/#example-1-classification-of-iris-flowers",
            "text": "from mlxtend.data import iris_data\nfrom mlxtend.evaluate import plot_decision_regions\nfrom mlxtend.classifier import Perceptron\nimport matplotlib.pyplot as plt\n\n# Loading Data\n\nX, y = iris_data()\nX = X[:, [0, 3]] # sepal length and petal width\nX = X[0:100] # class 0 and class 1\ny = y[0:100] # class 0 and class 1\n\n# standardize\nX[:,0] = (X[:,0] - X[:,0].mean()) / X[:,0].std()\nX[:,1] = (X[:,1] - X[:,1].mean()) / X[:,1].std()\n\n\n# Rosenblatt Perceptron\n\nppn = Perceptron(epochs=5, eta=0.05, random_seed=0)\nppn.fit(X, y)\n\nplot_decision_regions(X, y, clf=ppn)\nplt.title('Perceptron - Rosenblatt Perceptron Rule')\nplt.show()\n\nprint(ppn.w_)\n\nplt.plot(range(len(ppn.cost_)), ppn.cost_)\nplt.xlabel('Iterations')\nplt.ylabel('Missclassifications')\nplt.show()  [-0.3902373  -0.35696213 -0.37944732]   [ 0.0097627   0.08408412  0.31449279]",
            "title": "Example 1 - Classification of Iris Flowers"
        },
        {
            "location": "/user_guide/classifier/Perceptron/#api",
            "text": "Perceptron(eta=0.1, epochs=50, shuffle=False, random_seed=None, zero_init_weight=False)  Perceptron classifier.  Parameters    eta  : float (default: 0.1)  Learning rate (between 0.0 and 1.0)    epochs  : int (default: 50)  Number of passes over the training dataset.    shuffle  : bool (default: False)  Shuffles training data every epoch if True to prevent circles.    random_seed  : int  Random state for initializing random weights.    zero_init_weight  : bool (default: False)  If True, weights are initialized to zero instead of small random\nnumbers in the interval [-0.1, 0.1];\nignored if solver='normal equation'    Attributes    w_  : 1d-array  Weights after fitting.    cost_  : list  Number of misclassifications in every epoch.",
            "title": "API"
        },
        {
            "location": "/user_guide/classifier/Perceptron/#methods",
            "text": "fit(X, y, init_weights=True)  Learn weight coefficients from training data.  Parameters    X  : {array-like, sparse matrix}, shape = [n_samples, n_features]  Training vectors, where n_samples is the number of samples and\nn_features is the number of features.    y  : array-like, shape = [n_samples]  Target values.    init_weights  : bool (default: True)  Re-initializes weights prior to fitting. Set False to continue\ntraining with weights from a previous fitting.    Returns   self  : object    net_input(X)  Net input function   predict(X)  Predict class labels for X.  Parameters    X  : {array-like, sparse matrix}, shape = [n_samples, n_features]  Training vectors, where n_samples is the number of samples and\nn_features is the number of features.    Returns    class  : int  Predicted class label.",
            "title": "Methods"
        },
        {
            "location": "/user_guide/classifier/Adaline/",
            "text": "Adaptive Linear Neuron -- Adaline\n\n\nAn implementation of the ADAptive LInear NEuron, Adaline, for binary classification tasks.\n\n\n\n\nfrom mlxtend.classifier import Adaline\n\n\n\n\nOverview\n\n\nAn illustration of the ADAptive LInear NEuron (Adaline) -- a single-layer artificial linear neuron with a threshold unit:\n\n\n\n\nThe Adaline classifier is closely related to the Ordinary Least Squares (OLS) Linear Regression algorithm; in OLS regression we find the line (or hyperplane) that minimizes the vertical offsets. Or in other words, we define the best-fitting line as the line that minimizes the sum of squared errors (SSE) or mean squared error (MSE) between our target variable (y) and our predicted output over all samples \n$i$\n in our dataset of size \n$n$\n.\n\n\n$$ SSE =  \\sum_i (\\text{target}^{(i)} - \\text{output}^{(i)})^2$$\n\n\n$$MSE = \\frac{1}{n} \\times SSE$$\n\n\nLinearRegression\n implements a linear regression model for performing ordinary least squares regression, and in Adaline, we add a threshold function \n$g(\\cdot)$\n to convert the continuous outcome to a categorical class label:\n\n\n$$y = g({z}) = \\left\\{\n    \\begin{array}{l}\n    1 & \\text{if z $\\ge$ 0}\\\\\n    -1 & \\text{otherwise}.\n    \\end{array}\n    \\\\  \n  \\right.$$\n\n\nAn Adaline model can be trained by one of the following three approaches:\n\n\n\n\nNormal Equations\n\n\nGradient Descent\n\n\nStochastic Gradient Descent\n\n\n\n\nNormal Equations (closed-form solution)\n\n\nThe closed-form solution should be preferred for \"smaller\" datasets where calculating (a \"costly\") matrix inverse is not a concern. For very large datasets, or datasets where the inverse of \n$[X^T X]$\n may not exist (the matrix is non-invertible or singular, e.g., in case of perfect multicollinearity), the gradient descent or stochastic gradient descent approaches are to be preferred.\n\n\nThe linear function (linear regression model) is defined as:\n\n\n$$z = w_0x_0 + w_1x_1 + ... + w_mx_m = \\sum_{j=1}^{m} w_j x_j = \\mathbf{w}^T\\mathbf{x}$$\n\n\nwhere \n$y$\n is the response variable, \n$\\mathbf{x}$\n is an \n$m$\n-dimensional sample vector, and \n$\\mathbf{w}$\n is the weight vector (vector of coefficients). Note that \n$w_0$\n represents the y-axis intercept of the model and therefore \n$x_0=1$\n.  \n\n\nUsing the closed-form solution (normal equation), we compute the weights of the model as follows:\n\n\n$$ \\mathbf{w} = (\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{X}^Ty$$\n\n\nGradient Descent (GD)  and Stochastic Gradient Descent (SGD)\n\n\nIn the current implementation, the Adaline model is learned via Gradient Descent or Stochastic Gradient Descent.\n\n\nSee \nGradient Descent and Stochastic Gradient Descent\n and \nDeriving the Gradient Descent Rule for Linear Regression and Adaline\n for details.\n\n\nRandom shuffling is implemented as:\n\n\n\n\nfor one or more epochs\n\n\nrandomly shuffle samples in the training set\n\n\nfor training sample \ni\n\n\ncompute gradients and perform weight updates\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nReferences\n\n\n\n\nB. Widrow, M. E. Hoff, et al. \nAdaptive switching circuits\n. 1960.\n\n\n\n\nExamples\n\n\nExample 1 - Closed Form Solution\n\n\nfrom mlxtend.data import iris_data\nfrom mlxtend.evaluate import plot_decision_regions\nfrom mlxtend.classifier import Adaline\nimport matplotlib.pyplot as plt\n\n# Loading Data\n\nX, y = iris_data()\nX = X[:, [0, 3]] # sepal length and petal width\nX = X[0:100] # class 0 and class 1\ny = y[0:100] # class 0 and class 1\n\n# standardize\nX[:,0] = (X[:,0] - X[:,0].mean()) / X[:,0].std()\nX[:,1] = (X[:,1] - X[:,1].mean()) / X[:,1].std()\n\n\nada = Adaline(epochs=30, eta=0.01, solver='normal equation', random_seed=1)\nada.fit(X, y)\nplot_decision_regions(X, y, clf=ada)\nplt.title('Adaline - Stochastic Gradient Descent')\n\nplt.show()\n\n\n\n\n\n\nExample 2 - Gradient Descent\n\n\nfrom mlxtend.data import iris_data\nfrom mlxtend.evaluate import plot_decision_regions\nfrom mlxtend.classifier import Adaline\nimport matplotlib.pyplot as plt\n\n# Loading Data\n\nX, y = iris_data()\nX = X[:, [0, 3]] # sepal length and petal width\nX = X[0:100] # class 0 and class 1\ny = y[0:100] # class 0 and class 1\n\n# standardize\nX[:,0] = (X[:,0] - X[:,0].mean()) / X[:,0].std()\nX[:,1] = (X[:,1] - X[:,1].mean()) / X[:,1].std()\n\n\nada = Adaline(epochs=30, eta=0.01, solver='gd', random_seed=1)\nada.fit(X, y)\nplot_decision_regions(X, y, clf=ada)\nplt.title('Adaline - Stochastic Gradient Descent')\nplt.show()\n\nplt.plot(range(len(ada.cost_)), ada.cost_, marker='o')\nplt.xlabel('Iterations')\nplt.ylabel('Cost')\n\n\n\n\n\n\n<matplotlib.text.Text at 0x1095e0a90>\n\n\n\n\n\nExample 3 - Stochastic Gradient Descent\n\n\nfrom mlxtend.data import iris_data\nfrom mlxtend.evaluate import plot_decision_regions\nfrom mlxtend.classifier import Adaline\nimport matplotlib.pyplot as plt\n\n# Loading Data\n\nX, y = iris_data()\nX = X[:, [0, 3]] # sepal length and petal width\nX = X[0:100] # class 0 and class 1\ny = y[0:100] # class 0 and class 1\n\n# standardize\nX[:,0] = (X[:,0] - X[:,0].mean()) / X[:,0].std()\nX[:,1] = (X[:,1] - X[:,1].mean()) / X[:,1].std()\n\n\nada = Adaline(epochs=15, eta=0.01, solver='sgd', random_seed=1)\nada.fit(X, y)\nplot_decision_regions(X, y, clf=ada)\nplt.title('Adaline - Stochastic Gradient Descent')\nplt.show()\n\nplt.plot(range(len(ada.cost_)), ada.cost_, marker='o')\nplt.xlabel('Iterations')\nplt.ylabel('Cost')\nplt.show()\n\n\n\n\n\n\n\n\nAPI\n\n\nAdaline(eta=0.01, epochs=50, solver='sgd', random_seed=None, shuffle=False, zero_init_weight=False)\n\n\nADAptive LInear NEuron classifier.\n\n\nParameters\n\n\n\n\n\n\neta\n : float (default: 0.01)\n\n\nsolver rate (between 0.0 and 1.0)\n\n\n\n\n\n\nepochs\n : int (default: 50)\n\n\nPasses over the training dataset.\n\n\n\n\n\n\nsolver\n : {'gd', 'sgd', 'normal equation'} (default: 'sgd')\n\n\nMethod for solving the cost function. 'gd' for gradient descent,\n'sgd' for stochastic gradient descent, or 'normal equation' (default)\nto solve the cost function analytically.\n\n\n\n\n\n\nshuffle\n : bool (default: False)\n\n\nShuffles training data every epoch if True to prevent circles.\n\n\n\n\n\n\nrandom_seed\n : int (default: None)\n\n\nSet random state for shuffling and initializing the weights.\n\n\n\n\n\n\nzero_init_weight\n : bool (default: False)\n\n\nIf True, weights are initialized to zero instead of small random\nnumbers in the interval [-0.1, 0.1];\nignored if solver='normal equation'\n\n\n\n\n\n\nAttributes\n\n\n\n\n\n\nw_\n : 1d-array\n\n\nWeights after fitting.\n\n\n\n\n\n\ncost_\n : list\n\n\nSum of squared errors after each epoch.\n\n\n\n\n\n\nMethods\n\n\n\n\n\nactivation(X)\n\n\nCompute the linear activation from the net input.\n\n\n\n\n\nfit(X, y, init_weights=True)\n\n\nLearn weight coefficients from training data.\n\n\nParameters\n\n\n\n\n\n\nX\n : {array-like, sparse matrix}, shape = [n_samples, n_features]\n\n\nTraining vectors, where n_samples is the number of samples and\nn_features is the number of features.\n\n\n\n\n\n\ny\n : array-like, shape = [n_samples]\n\n\nTarget values.\n\n\n\n\n\n\ninit_weights\n : bool (default: True)\n\n\nRe-initializes weights prior to fitting. Set False to continue\ntraining with weights from a previous fitting.\n\n\n\n\n\n\nReturns\n\n\n\n\nself\n : object\n\n\n\n\n\n\n\nnet_input(X)\n\n\nCompute the linear net input.\n\n\n\n\n\npredict(X)\n\n\nPredict class labels of X.\n\n\nParameters\n\n\n\n\n\n\nX\n : {array-like, sparse matrix}, shape = [n_samples, n_features]\n\n\nTraining vectors, where n_samples is the number of samples and\nn_features is the number of features.\n\n\n\n\n\n\nReturns\n\n\n\n\n\n\nclass\n : int\n\n\nPredicted class label.",
            "title": "Adaline"
        },
        {
            "location": "/user_guide/classifier/Adaline/#adaptive-linear-neuron-adaline",
            "text": "An implementation of the ADAptive LInear NEuron, Adaline, for binary classification tasks.   from mlxtend.classifier import Adaline",
            "title": "Adaptive Linear Neuron -- Adaline"
        },
        {
            "location": "/user_guide/classifier/Adaline/#overview",
            "text": "An illustration of the ADAptive LInear NEuron (Adaline) -- a single-layer artificial linear neuron with a threshold unit:   The Adaline classifier is closely related to the Ordinary Least Squares (OLS) Linear Regression algorithm; in OLS regression we find the line (or hyperplane) that minimizes the vertical offsets. Or in other words, we define the best-fitting line as the line that minimizes the sum of squared errors (SSE) or mean squared error (MSE) between our target variable (y) and our predicted output over all samples  $i$  in our dataset of size  $n$ .  $$ SSE =  \\sum_i (\\text{target}^{(i)} - \\text{output}^{(i)})^2$$  $$MSE = \\frac{1}{n} \\times SSE$$  LinearRegression  implements a linear regression model for performing ordinary least squares regression, and in Adaline, we add a threshold function  $g(\\cdot)$  to convert the continuous outcome to a categorical class label:  $$y = g({z}) = \\left\\{\n    \\begin{array}{l}\n    1 & \\text{if z $\\ge$ 0}\\\\\n    -1 & \\text{otherwise}.\n    \\end{array}\n    \\\\  \n  \\right.$$  An Adaline model can be trained by one of the following three approaches:   Normal Equations  Gradient Descent  Stochastic Gradient Descent",
            "title": "Overview"
        },
        {
            "location": "/user_guide/classifier/Adaline/#normal-equations-closed-form-solution",
            "text": "The closed-form solution should be preferred for \"smaller\" datasets where calculating (a \"costly\") matrix inverse is not a concern. For very large datasets, or datasets where the inverse of  $[X^T X]$  may not exist (the matrix is non-invertible or singular, e.g., in case of perfect multicollinearity), the gradient descent or stochastic gradient descent approaches are to be preferred.  The linear function (linear regression model) is defined as:  $$z = w_0x_0 + w_1x_1 + ... + w_mx_m = \\sum_{j=1}^{m} w_j x_j = \\mathbf{w}^T\\mathbf{x}$$  where  $y$  is the response variable,  $\\mathbf{x}$  is an  $m$ -dimensional sample vector, and  $\\mathbf{w}$  is the weight vector (vector of coefficients). Note that  $w_0$  represents the y-axis intercept of the model and therefore  $x_0=1$ .    Using the closed-form solution (normal equation), we compute the weights of the model as follows:  $$ \\mathbf{w} = (\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{X}^Ty$$",
            "title": "Normal Equations (closed-form solution)"
        },
        {
            "location": "/user_guide/classifier/Adaline/#gradient-descent-gd-and-stochastic-gradient-descent-sgd",
            "text": "In the current implementation, the Adaline model is learned via Gradient Descent or Stochastic Gradient Descent.  See  Gradient Descent and Stochastic Gradient Descent  and  Deriving the Gradient Descent Rule for Linear Regression and Adaline  for details.  Random shuffling is implemented as:   for one or more epochs  randomly shuffle samples in the training set  for training sample  i  compute gradients and perform weight updates",
            "title": "Gradient Descent (GD)  and Stochastic Gradient Descent (SGD)"
        },
        {
            "location": "/user_guide/classifier/Adaline/#references",
            "text": "B. Widrow, M. E. Hoff, et al.  Adaptive switching circuits . 1960.",
            "title": "References"
        },
        {
            "location": "/user_guide/classifier/Adaline/#examples",
            "text": "",
            "title": "Examples"
        },
        {
            "location": "/user_guide/classifier/Adaline/#example-1-closed-form-solution",
            "text": "from mlxtend.data import iris_data\nfrom mlxtend.evaluate import plot_decision_regions\nfrom mlxtend.classifier import Adaline\nimport matplotlib.pyplot as plt\n\n# Loading Data\n\nX, y = iris_data()\nX = X[:, [0, 3]] # sepal length and petal width\nX = X[0:100] # class 0 and class 1\ny = y[0:100] # class 0 and class 1\n\n# standardize\nX[:,0] = (X[:,0] - X[:,0].mean()) / X[:,0].std()\nX[:,1] = (X[:,1] - X[:,1].mean()) / X[:,1].std()\n\n\nada = Adaline(epochs=30, eta=0.01, solver='normal equation', random_seed=1)\nada.fit(X, y)\nplot_decision_regions(X, y, clf=ada)\nplt.title('Adaline - Stochastic Gradient Descent')\n\nplt.show()",
            "title": "Example 1 - Closed Form Solution"
        },
        {
            "location": "/user_guide/classifier/Adaline/#example-2-gradient-descent",
            "text": "from mlxtend.data import iris_data\nfrom mlxtend.evaluate import plot_decision_regions\nfrom mlxtend.classifier import Adaline\nimport matplotlib.pyplot as plt\n\n# Loading Data\n\nX, y = iris_data()\nX = X[:, [0, 3]] # sepal length and petal width\nX = X[0:100] # class 0 and class 1\ny = y[0:100] # class 0 and class 1\n\n# standardize\nX[:,0] = (X[:,0] - X[:,0].mean()) / X[:,0].std()\nX[:,1] = (X[:,1] - X[:,1].mean()) / X[:,1].std()\n\n\nada = Adaline(epochs=30, eta=0.01, solver='gd', random_seed=1)\nada.fit(X, y)\nplot_decision_regions(X, y, clf=ada)\nplt.title('Adaline - Stochastic Gradient Descent')\nplt.show()\n\nplt.plot(range(len(ada.cost_)), ada.cost_, marker='o')\nplt.xlabel('Iterations')\nplt.ylabel('Cost')   <matplotlib.text.Text at 0x1095e0a90>",
            "title": "Example 2 - Gradient Descent"
        },
        {
            "location": "/user_guide/classifier/Adaline/#example-3-stochastic-gradient-descent",
            "text": "from mlxtend.data import iris_data\nfrom mlxtend.evaluate import plot_decision_regions\nfrom mlxtend.classifier import Adaline\nimport matplotlib.pyplot as plt\n\n# Loading Data\n\nX, y = iris_data()\nX = X[:, [0, 3]] # sepal length and petal width\nX = X[0:100] # class 0 and class 1\ny = y[0:100] # class 0 and class 1\n\n# standardize\nX[:,0] = (X[:,0] - X[:,0].mean()) / X[:,0].std()\nX[:,1] = (X[:,1] - X[:,1].mean()) / X[:,1].std()\n\n\nada = Adaline(epochs=15, eta=0.01, solver='sgd', random_seed=1)\nada.fit(X, y)\nplot_decision_regions(X, y, clf=ada)\nplt.title('Adaline - Stochastic Gradient Descent')\nplt.show()\n\nplt.plot(range(len(ada.cost_)), ada.cost_, marker='o')\nplt.xlabel('Iterations')\nplt.ylabel('Cost')\nplt.show()",
            "title": "Example 3 - Stochastic Gradient Descent"
        },
        {
            "location": "/user_guide/classifier/Adaline/#api",
            "text": "Adaline(eta=0.01, epochs=50, solver='sgd', random_seed=None, shuffle=False, zero_init_weight=False)  ADAptive LInear NEuron classifier.  Parameters    eta  : float (default: 0.01)  solver rate (between 0.0 and 1.0)    epochs  : int (default: 50)  Passes over the training dataset.    solver  : {'gd', 'sgd', 'normal equation'} (default: 'sgd')  Method for solving the cost function. 'gd' for gradient descent,\n'sgd' for stochastic gradient descent, or 'normal equation' (default)\nto solve the cost function analytically.    shuffle  : bool (default: False)  Shuffles training data every epoch if True to prevent circles.    random_seed  : int (default: None)  Set random state for shuffling and initializing the weights.    zero_init_weight  : bool (default: False)  If True, weights are initialized to zero instead of small random\nnumbers in the interval [-0.1, 0.1];\nignored if solver='normal equation'    Attributes    w_  : 1d-array  Weights after fitting.    cost_  : list  Sum of squared errors after each epoch.",
            "title": "API"
        },
        {
            "location": "/user_guide/classifier/Adaline/#methods",
            "text": "activation(X)  Compute the linear activation from the net input.   fit(X, y, init_weights=True)  Learn weight coefficients from training data.  Parameters    X  : {array-like, sparse matrix}, shape = [n_samples, n_features]  Training vectors, where n_samples is the number of samples and\nn_features is the number of features.    y  : array-like, shape = [n_samples]  Target values.    init_weights  : bool (default: True)  Re-initializes weights prior to fitting. Set False to continue\ntraining with weights from a previous fitting.    Returns   self  : object    net_input(X)  Compute the linear net input.   predict(X)  Predict class labels of X.  Parameters    X  : {array-like, sparse matrix}, shape = [n_samples, n_features]  Training vectors, where n_samples is the number of samples and\nn_features is the number of features.    Returns    class  : int  Predicted class label.",
            "title": "Methods"
        },
        {
            "location": "/user_guide/classifier/LogisticRegression/",
            "text": "Logistic Regression\n\n\nA logistic regression class for binary classification tasks.\n\n\n\n\nfrom mlxtend.classifier import LogisticRegression\n\n\n\n\nOverview\n\n\n\n\nRelated to the \nPerceptron\n and \n'Adaline'\n, a Logistic Regression model is a linear model for binary classification. However, instead of minimizing a linear cost function such as the sum of squared errors (SSE) in Adaline, we minimize a sigmoid function, i.e., the logistic function:\n\n\n$$\\phi(z) = \\frac{1}{1 + e^{-z}},$$\n\n\nwhere \n$z$\n is defined as the net input\n\n\n$$z = w_0x_0 + w_1x_1 + ... + w_mx_m = \\sum_{j=0}^{m} w_j x_j= \\mathbf{w}^T\\mathbf{x}.$$\n \n\n\nThe net input is in turn based on the logit function\n\n\n$$logit(p(y=1 \\mid \\mathbf{x})) = z.$$\n\n\nHere, \n$p(y=1 \\mid \\mathbf{x})$\n is the conditional probability that a particular sample belongs to class 1 given its features \n$\\mathbf{x}$\n. The logit function takes inputs in the range [0, 1] and transform them to values over the entire real number range. In contrast, the logistic function takes input values over the entire real number range and transforms them to values in the range [0, 1]. In other words, the logistic function is the inverse of the logit function, and it lets us predict the conditional probability that a certain sample belongs to class 1 (or class 0).\n\n\n\n\n\\documentclass{article}\n\\begin{document}\nThis is your only binary choices\n\\begin{math}\n  \\left\\{\n    \\begin{array}{l}\n      0\\\\\n      1\n    \\end{array}\n  \\right.\n\\end{math}\n\\end{document}\n\n\n\n\nAfter model fitting, the conditional probability \n$p(y=1 \\mid \\mathbf{x})$\n is converted to a binary class label via a threshold function \n$g(\\cdot)$\n:\n\n\n$$y = g({z}) =  \\left\\{\n     \\begin{array}{l}\n      1 & \\text{if $\\phi(z) \\ge 0.5$}\\\\\n      0 & \\text{otherwise.}\n    \\end{array}\n    \\\\\n  \\right.$$\n\n\nor eqivalently:\n\n\n$$y = g({z}) = \\left\\{\n    \\begin{array}{l}\n    1 & \\text{if z $\\ge$ 0}\\\\\n    0 & \\text{otherwise}.\n    \\end{array}\n    \\\\  \n  \\right.$$\n\n\nObjective Function -- Log-Likelihood\n\n\nIn order to parameterize a logistic regression model, we maximize the likelihood \n$L(\\cdot)$\n (or minimize the logistic cost function).\n\n\nWe write the likelihood as \n\n\n$$L(\\mathbf{w}) = P(\\mathbf{y} \\mid \\mathbf{x};\\mathbf{w}) = \\prod_{i=1}^{n} P\\big(y^{(i)} \\mid x^{(i)}; \\mathbf{w}\\big) = \\prod^{n}_{i=1}\\bigg(\\phi\\big(z^{(i)}\\big)\\bigg)^{y^{(i)}} \\bigg(1-\\phi\\big(z^{(i)}\\big)\\bigg)^{1-y^{(i)}},$$\n\n\nunder the assumption that the training samples are independent of each other.\n\n\nIn practice, it is easier to maximize the (natural) log of this equation, which is called\nthe log-likelihood function:\n\n\n$$l(\\mathbf{w}) = \\log L(\\mathbf{w}) = \\sum^{n}_{i=1} y^{(i)} \\log \\bigg(\\phi\\big(z^{(i)}\\big)\\bigg) + \\big( 1 - y^{(i)}\\big) \\log \\big(1-\\phi\\big(z^{(i)}\\big)\\big)$$\n\n\nOne advantage of taking the log is to avoid numeric underflow (and challenges with floating point math) for very small likelihoods. Another advantage is that we can obtain the derivative more easily, using the addition trick to rewrite the product of factors as a summation term, which we can then maximize using optimization algorithms such as gradient ascent.\n\n\nObjective Function -- Logistic Cost Function\n\n\nAn alternative to maximizing the log-likelihood, we can define a cost function \n$J(\\cdot)$\n to be minimized; we rewrite the log-likelihood as:\n\n\n$$J(\\mathbf{w}) = \\sum_{i=1}^{m} - y^{(i)} log \\bigg( \\phi\\big(z^{(i)}\\big) \\bigg) - \\big(1 - y^{(i)}\\big) log\\bigg(1-\\phi\\big(z^{(i)}\\big)\\bigg)$$\n\n\n$$\n J\\big(\\phi(z), y; \\mathbf{w}\\big) =\\begin{cases}\n    -log\\big(\\phi(z) \\big) & \\text{if $y = 1$}\\\\\n    -log\\big(1- \\phi(z) \\big) & \\text{if $y = 0$}\n  \\end{cases}\n$$\n\n\n\n\nAs we can see in the figure above, we penalize wrong predictions with an increasingly larger cost.\n\n\nGradient Descent (GD)  and Stochastic Gradient Descent (SGD) Optimization\n\n\nGradient Ascent and the log-likelihood\n\n\nTo learn the weight coefficient of a logistic regression model via gradient-based optimization, we compute the partial derivative of the log-likelihood function -- w.r.t. the \nj\nth weight -- as follows:\n\n\n$$\\frac{\\partial}{\\partial w_j} l(\\mathbf{w}) = \\bigg(y \\frac{1}{\\phi(z)} - (1-y) \\frac{1}{1-\\phi{(z)}} \\bigg) \\frac{\\partial}{\\partial w_j}\\phi(z)$$\n\n\nAs an intermediate step, we compute the partial derivative of the sigmoid function, which will come in handy later:\n\n\n\\begin{align}\n&\\frac{\\partial}{\\partial z} \\phi(z) = \\frac{\\partial}{{\\partial z}} \\frac{1}{1+e^{-z}} \\\\\n&= \\frac{1}{(1 + e^{-z})^{2}} e^{-z}\\\\\n&= \\frac{1}{1+e^{-z}} \\bigg(1 - \\frac{1}{1+e^{-z}} \\bigg)\\\\\n&= \\phi(z)\\big(1-\\phi(z)\\big)\n\\end{align}\n\n\nNow, we re-substitute \n$$\\frac{\\partial}{\\partial z} \\phi(z) = \\phi(z) \\big(1 - \\phi(z)\\big)$$\n back into in the log-likelihood partial derivative equation and obtain the equation shown below:\n\n\n\\begin{align}\n& \\bigg(y \\frac{1}{\\phi{(z)}} - (1 - y) \\frac{1}{1 - \\phi(z)} \\bigg) \\frac{\\partial}{\\partial w_j} \\phi(z) \\\\\n&= \\bigg(y \\frac{1}{\\phi{(z)}} - (1 - y) \\frac{1}{1 - \\phi(z)} \\bigg) \\phi(z) \\big(1 - \\phi(z)\\big) \\frac{\\partial}{\\partial w_j}z\\\\\n&= \\big(y(1-\\phi(z)\\big) - (1 - y) \\phi(z)\\big)x_j\\\\\n&=\\big(y - \\phi(z)\\big)x_j\n\\end{align}\n\n\nNow, in order to find the weights of the model, we take a step proportional to the positive direction of the gradient to maximize the log-likelihood. Futhermore, we add a coefficient, the learning rate \n$\\eta$\n to the weight update:\n\n\n\\begin{align}\n& w_j := w_j + \\eta \\frac{\\partial}{\\partial w_j} l(\\mathbf{w})\\\\\n& w_j := w_j + \\eta \\sum^{n}_{i=1} \\big( y^{(i)} - \\phi\\big(z^{(i)}\\big)\\big)x_j^{(i)}\n\\end{align}\n\n\nNote that the gradient (and weight update) is computed from all samples in the training set in gradient ascent/descent in contrast to stochastic gradient ascent/descent. For more information about the differences between gradient descent and stochastic gradient descent, please see the related article \nGradient Descent and Stochastic Gradient Descent\n.\n\n\nThe previous equation shows the weight update for a single weight \n$j$\n. In gradient-based optimization, all weight coefficients are updated simultaneously; the weight update can be written more compactly as \n\n\n$$\\mathbf{w} := \\mathbf{w} + \\Delta\\mathbf{w},$$\n\nwhere\n\n\n$$\\Delta{\\mathbf{w}} = \\eta \\nabla l(\\mathbf{w})$$\n\n\nGradient Descent and the logistic cost function\n\n\nIn the previous section, we derived the gradient of the log-likelihood function, which can be optimized via gradient ascent. Similarly, we can obtain the cost gradient of the logistic cost function \n$J(\\cdot)$\n and minimize it via gradient descent in order to learn the logistic regression model.\n\n\nThe update rule for a single weight:\n\n\n\\begin{align}\n& \\Delta{w_j} = -\\eta \\frac{\\partial J}{\\partial w_j} \\\n& = - \\eta \\sum_{i=1}^{n}\\big(y^{(i)} - \\phi\\big(z^{(i)}\\big) x^{(i)} \\big)\n\\end{align}\n\n\nThe simultaneous weight update:\n\n\n$$\\mathbf{w} := \\mathbf{w} + \\Delta\\mathbf{w}$$\n\n\nwhere\n\n\n$$\\Delta{\\mathbf{w}} = - \\eta \\nabla J(\\mathbf{w}).$$\n\n\nShuffling\n\n\nRandom shuffling is implemented as:\n\n\n\n\nfor one or more epochs\n\n\nrandomly shuffle samples in the training set\n\n\nfor training sample \ni\n\n\ncompute gradients and perform weight updates\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nRegularization\n\n\nAs a way to tackle overfitting, we can add additional bias to the logistic regression model via a regularization terms. Via the L2 regularization term, we reduce the complexity of the model by penalizing large weight coefficients:\n\n\n$$L2: \\frac{\\lambda}{2}\\lVert \\mathbf{w} \\lVert_2 = \\frac{\\lambda}{2} \\sum_{j=1}^{m} w_j^2$$\n\n\nIn order to apply regularization, we just need to add the regularization term to the cost function that we defined for logistic regression to shrink the weights:\n\n\n$$J(\\mathbf{w}) =  \\sum_{i=1}^{m} \\Bigg[ - y^{(i)} log \\bigg( \\phi\\big(z^{(i)}\\big) \\bigg) - \\big(1 - y^{(i)}\\big) log\\bigg(1-\\phi\\big(z^{(i)}\\big)\\bigg) \\Bigg] + \\frac{\\lambda}{2} \\sum_{j=1}^{m} w_j^2$$\n\n\nThe update rule for a single weight:\n\n\n\\begin{align}\n& \\Delta{w_j} = -\\eta \\bigg( \\frac{\\partial J}{\\partial w_j} + \\lambda w_j\\bigg)\\\n& = - \\eta \\sum_{i=1}^{n}\\big(y^{(i)} - \\phi\\big(z^{(i)}\\big) x^{(i)} \\big) - \\eta \\lambda w_j\n\\end{align}\n\n\nThe simultaneous weight update:\n\n\n$$\\mathbf{w} := \\mathbf{w} + \\Delta\\mathbf{w}$$\n\n\nwhere\n\n\n$$\\Delta{\\mathbf{w}} = - \\eta \\big( \\nabla J(\\mathbf{w}) + \\lambda \\mathbf{w}\\big).$$\n\n\nFor more information on regularization, please see \nRegularization of Generalized Linear Models\n.\n\n\nReferences\n\n\n\n\nBishop, Christopher M. \nPattern recognition and machine learning\n. Springer, 2006. pp. 203-213\n\n\n\n\nExamples\n\n\nExample 1 - Gradient Descent\n\n\nfrom mlxtend.data import iris_data\nfrom mlxtend.evaluate import plot_decision_regions\nfrom mlxtend.classifier import LogisticRegression\nimport matplotlib.pyplot as plt\n\n# Loading Data\n\nX, y = iris_data()\nX = X[:, [0, 3]] # sepal length and petal width\nX = X[0:100] # class 0 and class 1\ny = y[0:100] # class 0 and class 1\n\n# standardize\nX[:,0] = (X[:,0] - X[:,0].mean()) / X[:,0].std()\nX[:,1] = (X[:,1] - X[:,1].mean()) / X[:,1].std()\n\nlr = LogisticRegression(eta=0.1, l2_lambda=0.0, epochs=500, learning='gd', random_seed=1)\nlr.fit(X, y)\n\nplot_decision_regions(X, y, clf=lr)\nplt.title('Logistic Regression - Stochastic Gradient Descent')\nplt.show()\n\nplt.plot(range(len(lr.cost_)), lr.cost_)\nplt.xlabel('Iterations')\nplt.ylabel('Cost')\nplt.show()\n\n\n\n\n\n\n\n\nPredicting Class Labels\n\n\ny_pred = lr.predict(X)\nprint('Last 3 Class Labels: %s' % y_pred[-3:])\n\n\n\n\nLast 3 Class Labels: [1 1 1]\n\n\n\ny_pred = lr.activation(X)\nprint('Last 3 Class Labels: %s' % y_pred[-3:])\n\n\n\n\nLast 3 Class Labels: [ 0.99999934  0.99898689  0.99999708]\n\n\n\nPredicting Class Probabilities\n\n\nExample 2 - Stochastic Gradient Descent\n\n\nfrom mlxtend.data import iris_data\nfrom mlxtend.evaluate import plot_decision_regions\nfrom mlxtend.classifier import LogisticRegression\nimport matplotlib.pyplot as plt\n\n# Loading Data\n\nX, y = iris_data()\nX = X[:, [0, 3]] # sepal length and petal width\nX = X[0:100] # class 0 and class 1\ny = y[0:100] # class 0 and class 1\n\n# standardize\nX[:,0] = (X[:,0] - X[:,0].mean()) / X[:,0].std()\nX[:,1] = (X[:,1] - X[:,1].mean()) / X[:,1].std()\n\nlr = LogisticRegression(eta=0.05, epochs=100, l2_lambda=0.0, learning='sgd', random_seed=1)\nlr.fit(X, y)\n\nplot_decision_regions(X, y, clf=lr)\nplt.title('Logistic Regression - Stochastic Gradient Descent')\nplt.show()\n\nplt.plot(range(len(lr.cost_)), lr.cost_)\nplt.xlabel('Iterations')\nplt.ylabel('Cost')\nplt.show()\n\n\n\n\n\n\n\n\nAPI\n\n\nLogisticRegression(eta=0.01, epochs=50, regularization=None, l2_lambda=0.0, learning='sgd', shuffle=False, random_seed=None, zero_init_weight=False)\n\n\nLogistic regression classifier.\n\n\nParameters\n\n\n\n\n\n\neta\n : float (default: 0.01)\n\n\nLearning rate (between 0.0 and 1.0)\n\n\n\n\n\n\nepochs\n : int (default: 50)\n\n\nPasses over the training dataset.\n\n\n\n\n\n\nlearning\n : str (default: sgd)\n\n\nLearning rule, sgd (stochastic gradient descent)\nor gd (gradient descent).\n\n\n\n\n\n\nregularization\n : {None, 'l2'} (default: None)\n\n\nType of regularization. No regularization if\n\nregularization=None\n.\n\n\n\n\n\n\nl2_lambda\n : float\n\n\nRegularization parameter for L2 regularization.\nNo regularization if l2_lambda=0.0.\n\n\n\n\n\n\nshuffle\n : bool (default: False)\n\n\nShuffles training data every epoch if True to prevent circles.\n\n\n\n\n\n\nrandom_seed\n : int (default: None)\n\n\nSet random state for shuffling and initializing the weights.\n\n\n\n\n\n\nzero_init_weight\n : bool (default: False)\n\n\nIf True, weights are initialized to zero instead of small random\nnumbers in the interval [-0.1, 0.1];\nignored if solver='normal equation'\n\n\n\n\n\n\nAttributes\n\n\n\n\n\n\nw_\n : 1d-array\n\n\nWeights after fitting.\n\n\n\n\n\n\ncost_\n : list\n\n\nList of floats with sum of squared error cost (sgd or gd) for every\nepoch.\n\n\n\n\n\n\nMethods\n\n\n\n\n\nactivation(X)\n\n\nPredict class probabilities of X from the net input.\n\n\nParameters\n\n\n\n\n\n\nX\n : {array-like, sparse matrix}, shape = [n_samples, n_features]\n\n\nTraining vectors, where n_samples is the number of samples and\nn_features is the number of features.\n\n\n\n\n\n\nReturns\n\n\n\n\nClass 1 probability\n : float\n\n\n\n\n\n\n\nfit(X, y, init_weights=True)\n\n\nLearn weight coefficients from training data.\n\n\nParameters\n\n\n\n\n\n\nX\n : {array-like, sparse matrix}, shape = [n_samples, n_features]\n\n\nTraining vectors, where n_samples is the number of samples and\nn_features is the number of features.\n\n\n\n\n\n\ny\n : array-like, shape = [n_samples]\n\n\nTarget values.\n\n\n\n\n\n\ninit_weights\n : bool (default: True)\n\n\n(Re)initializes weights to small random floats if True.\n\n\n\n\n\n\nReturns\n\n\n\n\nself\n : object\n\n\n\n\n\n\n\nnet_input(X)\n\n\nCompute the linear net input.\n\n\n\n\n\npredict(X)\n\n\nPredict class labels of X.\n\n\nParameters\n\n\n\n\n\n\nX\n : {array-like, sparse matrix}, shape = [n_samples, n_features]\n\n\nTraining vectors, where n_samples is the number of samples and\nn_features is the number of features.\n\n\n\n\n\n\nReturns\n\n\n\n\n\n\nclass\n : int\n\n\nPredicted class label(s).",
            "title": "LogisticRegression"
        },
        {
            "location": "/user_guide/classifier/LogisticRegression/#logistic-regression",
            "text": "A logistic regression class for binary classification tasks.   from mlxtend.classifier import LogisticRegression",
            "title": "Logistic Regression"
        },
        {
            "location": "/user_guide/classifier/LogisticRegression/#overview",
            "text": "Related to the  Perceptron  and  'Adaline' , a Logistic Regression model is a linear model for binary classification. However, instead of minimizing a linear cost function such as the sum of squared errors (SSE) in Adaline, we minimize a sigmoid function, i.e., the logistic function:  $$\\phi(z) = \\frac{1}{1 + e^{-z}},$$  where  $z$  is defined as the net input  $$z = w_0x_0 + w_1x_1 + ... + w_mx_m = \\sum_{j=0}^{m} w_j x_j= \\mathbf{w}^T\\mathbf{x}.$$    The net input is in turn based on the logit function  $$logit(p(y=1 \\mid \\mathbf{x})) = z.$$  Here,  $p(y=1 \\mid \\mathbf{x})$  is the conditional probability that a particular sample belongs to class 1 given its features  $\\mathbf{x}$ . The logit function takes inputs in the range [0, 1] and transform them to values over the entire real number range. In contrast, the logistic function takes input values over the entire real number range and transforms them to values in the range [0, 1]. In other words, the logistic function is the inverse of the logit function, and it lets us predict the conditional probability that a certain sample belongs to class 1 (or class 0).   \\documentclass{article}\n\\begin{document}\nThis is your only binary choices\n\\begin{math}\n  \\left\\{\n    \\begin{array}{l}\n      0\\\\\n      1\n    \\end{array}\n  \\right.\n\\end{math}\n\\end{document}  After model fitting, the conditional probability  $p(y=1 \\mid \\mathbf{x})$  is converted to a binary class label via a threshold function  $g(\\cdot)$ :  $$y = g({z}) =  \\left\\{\n     \\begin{array}{l}\n      1 & \\text{if $\\phi(z) \\ge 0.5$}\\\\\n      0 & \\text{otherwise.}\n    \\end{array}\n    \\\\\n  \\right.$$  or eqivalently:  $$y = g({z}) = \\left\\{\n    \\begin{array}{l}\n    1 & \\text{if z $\\ge$ 0}\\\\\n    0 & \\text{otherwise}.\n    \\end{array}\n    \\\\  \n  \\right.$$",
            "title": "Overview"
        },
        {
            "location": "/user_guide/classifier/LogisticRegression/#objective-function-log-likelihood",
            "text": "In order to parameterize a logistic regression model, we maximize the likelihood  $L(\\cdot)$  (or minimize the logistic cost function).  We write the likelihood as   $$L(\\mathbf{w}) = P(\\mathbf{y} \\mid \\mathbf{x};\\mathbf{w}) = \\prod_{i=1}^{n} P\\big(y^{(i)} \\mid x^{(i)}; \\mathbf{w}\\big) = \\prod^{n}_{i=1}\\bigg(\\phi\\big(z^{(i)}\\big)\\bigg)^{y^{(i)}} \\bigg(1-\\phi\\big(z^{(i)}\\big)\\bigg)^{1-y^{(i)}},$$  under the assumption that the training samples are independent of each other.  In practice, it is easier to maximize the (natural) log of this equation, which is called\nthe log-likelihood function:  $$l(\\mathbf{w}) = \\log L(\\mathbf{w}) = \\sum^{n}_{i=1} y^{(i)} \\log \\bigg(\\phi\\big(z^{(i)}\\big)\\bigg) + \\big( 1 - y^{(i)}\\big) \\log \\big(1-\\phi\\big(z^{(i)}\\big)\\big)$$  One advantage of taking the log is to avoid numeric underflow (and challenges with floating point math) for very small likelihoods. Another advantage is that we can obtain the derivative more easily, using the addition trick to rewrite the product of factors as a summation term, which we can then maximize using optimization algorithms such as gradient ascent.",
            "title": "Objective Function -- Log-Likelihood"
        },
        {
            "location": "/user_guide/classifier/LogisticRegression/#objective-function-logistic-cost-function",
            "text": "An alternative to maximizing the log-likelihood, we can define a cost function  $J(\\cdot)$  to be minimized; we rewrite the log-likelihood as:  $$J(\\mathbf{w}) = \\sum_{i=1}^{m} - y^{(i)} log \\bigg( \\phi\\big(z^{(i)}\\big) \\bigg) - \\big(1 - y^{(i)}\\big) log\\bigg(1-\\phi\\big(z^{(i)}\\big)\\bigg)$$  $$\n J\\big(\\phi(z), y; \\mathbf{w}\\big) =\\begin{cases}\n    -log\\big(\\phi(z) \\big) & \\text{if $y = 1$}\\\\\n    -log\\big(1- \\phi(z) \\big) & \\text{if $y = 0$}\n  \\end{cases}\n$$   As we can see in the figure above, we penalize wrong predictions with an increasingly larger cost.",
            "title": "Objective Function -- Logistic Cost Function"
        },
        {
            "location": "/user_guide/classifier/LogisticRegression/#gradient-descent-gd-and-stochastic-gradient-descent-sgd-optimization",
            "text": "",
            "title": "Gradient Descent (GD)  and Stochastic Gradient Descent (SGD) Optimization"
        },
        {
            "location": "/user_guide/classifier/LogisticRegression/#gradient-ascent-and-the-log-likelihood",
            "text": "To learn the weight coefficient of a logistic regression model via gradient-based optimization, we compute the partial derivative of the log-likelihood function -- w.r.t. the  j th weight -- as follows:  $$\\frac{\\partial}{\\partial w_j} l(\\mathbf{w}) = \\bigg(y \\frac{1}{\\phi(z)} - (1-y) \\frac{1}{1-\\phi{(z)}} \\bigg) \\frac{\\partial}{\\partial w_j}\\phi(z)$$  As an intermediate step, we compute the partial derivative of the sigmoid function, which will come in handy later:  \\begin{align}\n&\\frac{\\partial}{\\partial z} \\phi(z) = \\frac{\\partial}{{\\partial z}} \\frac{1}{1+e^{-z}} \\\\\n&= \\frac{1}{(1 + e^{-z})^{2}} e^{-z}\\\\\n&= \\frac{1}{1+e^{-z}} \\bigg(1 - \\frac{1}{1+e^{-z}} \\bigg)\\\\\n&= \\phi(z)\\big(1-\\phi(z)\\big)\n\\end{align}  Now, we re-substitute  $$\\frac{\\partial}{\\partial z} \\phi(z) = \\phi(z) \\big(1 - \\phi(z)\\big)$$  back into in the log-likelihood partial derivative equation and obtain the equation shown below:  \\begin{align}\n& \\bigg(y \\frac{1}{\\phi{(z)}} - (1 - y) \\frac{1}{1 - \\phi(z)} \\bigg) \\frac{\\partial}{\\partial w_j} \\phi(z) \\\\\n&= \\bigg(y \\frac{1}{\\phi{(z)}} - (1 - y) \\frac{1}{1 - \\phi(z)} \\bigg) \\phi(z) \\big(1 - \\phi(z)\\big) \\frac{\\partial}{\\partial w_j}z\\\\\n&= \\big(y(1-\\phi(z)\\big) - (1 - y) \\phi(z)\\big)x_j\\\\\n&=\\big(y - \\phi(z)\\big)x_j\n\\end{align}  Now, in order to find the weights of the model, we take a step proportional to the positive direction of the gradient to maximize the log-likelihood. Futhermore, we add a coefficient, the learning rate  $\\eta$  to the weight update:  \\begin{align}\n& w_j := w_j + \\eta \\frac{\\partial}{\\partial w_j} l(\\mathbf{w})\\\\\n& w_j := w_j + \\eta \\sum^{n}_{i=1} \\big( y^{(i)} - \\phi\\big(z^{(i)}\\big)\\big)x_j^{(i)}\n\\end{align}  Note that the gradient (and weight update) is computed from all samples in the training set in gradient ascent/descent in contrast to stochastic gradient ascent/descent. For more information about the differences between gradient descent and stochastic gradient descent, please see the related article  Gradient Descent and Stochastic Gradient Descent .  The previous equation shows the weight update for a single weight  $j$ . In gradient-based optimization, all weight coefficients are updated simultaneously; the weight update can be written more compactly as   $$\\mathbf{w} := \\mathbf{w} + \\Delta\\mathbf{w},$$ \nwhere  $$\\Delta{\\mathbf{w}} = \\eta \\nabla l(\\mathbf{w})$$",
            "title": "Gradient Ascent and the log-likelihood"
        },
        {
            "location": "/user_guide/classifier/LogisticRegression/#gradient-descent-and-the-logistic-cost-function",
            "text": "In the previous section, we derived the gradient of the log-likelihood function, which can be optimized via gradient ascent. Similarly, we can obtain the cost gradient of the logistic cost function  $J(\\cdot)$  and minimize it via gradient descent in order to learn the logistic regression model.  The update rule for a single weight:  \\begin{align}\n& \\Delta{w_j} = -\\eta \\frac{\\partial J}{\\partial w_j} \\\n& = - \\eta \\sum_{i=1}^{n}\\big(y^{(i)} - \\phi\\big(z^{(i)}\\big) x^{(i)} \\big)\n\\end{align}  The simultaneous weight update:  $$\\mathbf{w} := \\mathbf{w} + \\Delta\\mathbf{w}$$  where  $$\\Delta{\\mathbf{w}} = - \\eta \\nabla J(\\mathbf{w}).$$",
            "title": "Gradient Descent and the logistic cost function"
        },
        {
            "location": "/user_guide/classifier/LogisticRegression/#shuffling",
            "text": "Random shuffling is implemented as:   for one or more epochs  randomly shuffle samples in the training set  for training sample  i  compute gradients and perform weight updates",
            "title": "Shuffling"
        },
        {
            "location": "/user_guide/classifier/LogisticRegression/#regularization",
            "text": "As a way to tackle overfitting, we can add additional bias to the logistic regression model via a regularization terms. Via the L2 regularization term, we reduce the complexity of the model by penalizing large weight coefficients:  $$L2: \\frac{\\lambda}{2}\\lVert \\mathbf{w} \\lVert_2 = \\frac{\\lambda}{2} \\sum_{j=1}^{m} w_j^2$$  In order to apply regularization, we just need to add the regularization term to the cost function that we defined for logistic regression to shrink the weights:  $$J(\\mathbf{w}) =  \\sum_{i=1}^{m} \\Bigg[ - y^{(i)} log \\bigg( \\phi\\big(z^{(i)}\\big) \\bigg) - \\big(1 - y^{(i)}\\big) log\\bigg(1-\\phi\\big(z^{(i)}\\big)\\bigg) \\Bigg] + \\frac{\\lambda}{2} \\sum_{j=1}^{m} w_j^2$$  The update rule for a single weight:  \\begin{align}\n& \\Delta{w_j} = -\\eta \\bigg( \\frac{\\partial J}{\\partial w_j} + \\lambda w_j\\bigg)\\\n& = - \\eta \\sum_{i=1}^{n}\\big(y^{(i)} - \\phi\\big(z^{(i)}\\big) x^{(i)} \\big) - \\eta \\lambda w_j\n\\end{align}  The simultaneous weight update:  $$\\mathbf{w} := \\mathbf{w} + \\Delta\\mathbf{w}$$  where  $$\\Delta{\\mathbf{w}} = - \\eta \\big( \\nabla J(\\mathbf{w}) + \\lambda \\mathbf{w}\\big).$$  For more information on regularization, please see  Regularization of Generalized Linear Models .",
            "title": "Regularization"
        },
        {
            "location": "/user_guide/classifier/LogisticRegression/#references",
            "text": "Bishop, Christopher M.  Pattern recognition and machine learning . Springer, 2006. pp. 203-213",
            "title": "References"
        },
        {
            "location": "/user_guide/classifier/LogisticRegression/#examples",
            "text": "",
            "title": "Examples"
        },
        {
            "location": "/user_guide/classifier/LogisticRegression/#example-1-gradient-descent",
            "text": "from mlxtend.data import iris_data\nfrom mlxtend.evaluate import plot_decision_regions\nfrom mlxtend.classifier import LogisticRegression\nimport matplotlib.pyplot as plt\n\n# Loading Data\n\nX, y = iris_data()\nX = X[:, [0, 3]] # sepal length and petal width\nX = X[0:100] # class 0 and class 1\ny = y[0:100] # class 0 and class 1\n\n# standardize\nX[:,0] = (X[:,0] - X[:,0].mean()) / X[:,0].std()\nX[:,1] = (X[:,1] - X[:,1].mean()) / X[:,1].std()\n\nlr = LogisticRegression(eta=0.1, l2_lambda=0.0, epochs=500, learning='gd', random_seed=1)\nlr.fit(X, y)\n\nplot_decision_regions(X, y, clf=lr)\nplt.title('Logistic Regression - Stochastic Gradient Descent')\nplt.show()\n\nplt.plot(range(len(lr.cost_)), lr.cost_)\nplt.xlabel('Iterations')\nplt.ylabel('Cost')\nplt.show()",
            "title": "Example 1 - Gradient Descent"
        },
        {
            "location": "/user_guide/classifier/LogisticRegression/#predicting-class-labels",
            "text": "y_pred = lr.predict(X)\nprint('Last 3 Class Labels: %s' % y_pred[-3:])  Last 3 Class Labels: [1 1 1]  y_pred = lr.activation(X)\nprint('Last 3 Class Labels: %s' % y_pred[-3:])  Last 3 Class Labels: [ 0.99999934  0.99898689  0.99999708]",
            "title": "Predicting Class Labels"
        },
        {
            "location": "/user_guide/classifier/LogisticRegression/#predicting-class-probabilities",
            "text": "",
            "title": "Predicting Class Probabilities"
        },
        {
            "location": "/user_guide/classifier/LogisticRegression/#example-2-stochastic-gradient-descent",
            "text": "from mlxtend.data import iris_data\nfrom mlxtend.evaluate import plot_decision_regions\nfrom mlxtend.classifier import LogisticRegression\nimport matplotlib.pyplot as plt\n\n# Loading Data\n\nX, y = iris_data()\nX = X[:, [0, 3]] # sepal length and petal width\nX = X[0:100] # class 0 and class 1\ny = y[0:100] # class 0 and class 1\n\n# standardize\nX[:,0] = (X[:,0] - X[:,0].mean()) / X[:,0].std()\nX[:,1] = (X[:,1] - X[:,1].mean()) / X[:,1].std()\n\nlr = LogisticRegression(eta=0.05, epochs=100, l2_lambda=0.0, learning='sgd', random_seed=1)\nlr.fit(X, y)\n\nplot_decision_regions(X, y, clf=lr)\nplt.title('Logistic Regression - Stochastic Gradient Descent')\nplt.show()\n\nplt.plot(range(len(lr.cost_)), lr.cost_)\nplt.xlabel('Iterations')\nplt.ylabel('Cost')\nplt.show()",
            "title": "Example 2 - Stochastic Gradient Descent"
        },
        {
            "location": "/user_guide/classifier/LogisticRegression/#api",
            "text": "LogisticRegression(eta=0.01, epochs=50, regularization=None, l2_lambda=0.0, learning='sgd', shuffle=False, random_seed=None, zero_init_weight=False)  Logistic regression classifier.  Parameters    eta  : float (default: 0.01)  Learning rate (between 0.0 and 1.0)    epochs  : int (default: 50)  Passes over the training dataset.    learning  : str (default: sgd)  Learning rule, sgd (stochastic gradient descent)\nor gd (gradient descent).    regularization  : {None, 'l2'} (default: None)  Type of regularization. No regularization if regularization=None .    l2_lambda  : float  Regularization parameter for L2 regularization.\nNo regularization if l2_lambda=0.0.    shuffle  : bool (default: False)  Shuffles training data every epoch if True to prevent circles.    random_seed  : int (default: None)  Set random state for shuffling and initializing the weights.    zero_init_weight  : bool (default: False)  If True, weights are initialized to zero instead of small random\nnumbers in the interval [-0.1, 0.1];\nignored if solver='normal equation'    Attributes    w_  : 1d-array  Weights after fitting.    cost_  : list  List of floats with sum of squared error cost (sgd or gd) for every\nepoch.",
            "title": "API"
        },
        {
            "location": "/user_guide/classifier/LogisticRegression/#methods",
            "text": "activation(X)  Predict class probabilities of X from the net input.  Parameters    X  : {array-like, sparse matrix}, shape = [n_samples, n_features]  Training vectors, where n_samples is the number of samples and\nn_features is the number of features.    Returns   Class 1 probability  : float    fit(X, y, init_weights=True)  Learn weight coefficients from training data.  Parameters    X  : {array-like, sparse matrix}, shape = [n_samples, n_features]  Training vectors, where n_samples is the number of samples and\nn_features is the number of features.    y  : array-like, shape = [n_samples]  Target values.    init_weights  : bool (default: True)  (Re)initializes weights to small random floats if True.    Returns   self  : object    net_input(X)  Compute the linear net input.   predict(X)  Predict class labels of X.  Parameters    X  : {array-like, sparse matrix}, shape = [n_samples, n_features]  Training vectors, where n_samples is the number of samples and\nn_features is the number of features.    Returns    class  : int  Predicted class label(s).",
            "title": "Methods"
        },
        {
            "location": "/user_guide/classifier/NeuralNetMLP/",
            "text": "Neural Network - Multilayer Perceptron\n\n\nImplementation of a multilayer perceptron, a feedforward artificial neural network.\n\n\n\n\nfrom mlxtend.classifier import NeuralNetMLP\n\n\n\n\nOverview\n\n\nAlthough the code is fully working and can be used for common classification tasks, this implementation is not geared towards efficiency but clarity \u2013 the original code was written for demonstration purposes.\n\n\nBasic Architecture\n\n\n  \n\n\nThe neurons \n$x_0$\n and \n$a_0$\n represent the bias units (\n$x_0=1$\n, \n$a_0=1$\n). \n\n\nThe \n$i$\nth superscript denotes the \n$i$\nth layer, and the \nj\nth subscripts stands for the index of the respective unit. For example, \n$a_{1}^{(2)}$\n refers to the first activation unit \nafter\n the bias unit (i.e., 2nd activation unit) in the 2nd layer (here: the hidden layer)\n\n\n\\begin{align}\n    \\mathbf{a^{(2)}} &= \\begin{bmatrix}\n           a_{0}^{(2)} \\\n           a_{1}^{(2)} \\\n           \\vdots \\\n           a_{m}^{(2)}\n         \\end{bmatrix}.\n  \\end{align}\n\n\nEach layer \n$(l)$\n in a multi-layer perceptron, a directed graph, is fully connected to the next layer \n$(l+1)$\n. We write the weight coefficient that connects the \n$k$\nth unit in the \n$l$\nth layer to the \n$j$\nth unit in layer \n$l+1$\n as \n$w^{(l)}_{j, k}$\n.\n\n\nFor example, the weight coefficient that connects the units\n\n\n$a_0^{(2)} \\rightarrow a_1^{(3)}$\n\n\nwould be written as \n$w_{1,0}^{(2)}$\n.\n\n\nActivation\n\n\nIn the current implementation, the activations of the hidden and output layers are computed via the logistic (sigmoid) function \n$\\phi(z) = \\frac{1}{1 + e^{-z}}.$\n\n\n\n\n(For more details on the logistic function, please see \nclassifier.LogisticRegression\n; a general overview of different activation function can be found \nhere\n.)\n\n\nReferences\n\n\n\n\nD. R. G. H. R. Williams and G. Hinton. \nLearning representations by back-propagating errors\n. Nature, pages 323\u2013533, 1986.\n\n\nC. M. Bishop. \nNeural networks for pattern recognition\n. Oxford University Press, 1995.\n\n\nT. Hastie, J. Friedman, and R. Tibshirani. \nThe Elements of Statistical Learning\n, Volume 2. Springer, 2009.\n\n\n\n\nExamples\n\n\nExample 1 - Classifying Iris Flowers\n\n\nLoad 2 features from Iris (petal length and petal width) for visualization purposes:\n\n\nfrom mlxtend.data import iris_data\nX, y = iris_data()\nX = X[:, [0, 2]]    \n\n# standardize training data\nX_std = (X - X.mean(axis=0)) / X.std(axis=0)\n\n\n\n\nTrain neural network for 3 output flower classes ('Setosa', 'Versicolor', 'Virginica'), regular gradient decent (\nminibatches=1\n), 30 hidden units, and no regularization.\n\n\nGradient Descent\n\n\nSetting the \nminibatches\n to \n1\n will result in gradient descent training; please see \nGradient Descent vs. Stochastic Gradient Descent\n for details.\n\n\nfrom mlxtend.classifier import NeuralNetMLP\n\nimport numpy as np\nnn1 = NeuralNetMLP(n_output=len(np.unique(y)), \n                   n_features=X_std.shape[1], \n                   n_hidden=50, \n                   l2=0.00, \n                   l1=0.0, \n                   epochs=300, \n                   eta=0.01, \n                   alpha=0.0,\n                   decrease_const=0.0,\n                   minibatches=1, \n                   shuffle_init=False,\n                   shuffle_epoch=False,\n                   random_seed=1,\n                   print_progress=3)\n\nnn1 = nn1.fit(X_std, y)\n\n\n\n\nEpoch: 300/300, Elapsed: 0:00:00, ETA: 0:00:00\n\n\n\ny_pred = nn1.predict(X_std)\nacc = np.sum(y == y_pred, axis=0) / X_std.shape[0]\nprint('Accuracy: %.2f%%' % (acc * 100))\n\n\n\n\nAccuracy: 96.67%\n\n\n\nimport matplotlib.pyplot as plt\nplt.plot(range(len(nn1.cost_)), nn1.cost_)\nplt.ylim([0, 300])\nplt.ylabel('Cost')\nplt.xlabel('Epochs')\nplt.show()\n\n\n\n\n\n\nfrom mlxtend.evaluate import plot_decision_regions\nfig = plot_decision_regions(X=X_std, y=y, clf=nn1, legend=2)\n\n\n\n\n\n\nStochastic Gradient Descent\n\n\nSetting \nminibatches\n to \nn_samples\n will result in stochastic gradient descent training; please see \nGradient Descent vs. Stochastic Gradient Descent\n for details.\n\n\nfrom mlxtend.classifier import NeuralNetMLP\n\nimport numpy as np\nnn2 = NeuralNetMLP(n_output=len(np.unique(y)), \n                   n_features=X_std.shape[1], \n                   n_hidden=50, \n                   l2=0.00, \n                   l1=0.0, \n                   epochs=30, \n                   eta=0.01, \n                   alpha=0.2,\n                   decrease_const=0.0,\n                   minibatches=X_std.shape[0], \n                   shuffle_init=True,\n                   shuffle_epoch=True,\n                   random_seed=1,\n                   print_progress=3)\n\nnn2 = nn2.fit(X_std, y)\n\n\n\n\nEpoch: 30/30, Elapsed: 0:00:01, ETA: 0:00:00\n\n\n\nbatches = np.array_split(range(len(nn2.cost_)), nn2.epochs+1)\ncost_ary = np.array(nn2.cost_)\ncost_avgs = [np.mean(cost_ary[i]) for i in batches]\nplt.plot(range(len(cost_avgs)),\n         cost_avgs,\n         color='red')\nplt.ylim([0, 2])\nplt.ylabel('Cost')\nplt.xlabel('Epochs')\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\nContinue the training for 200 epochs...\n\n\nnn2.epochs = 200\nnn2 = nn2.fit(X_std, y)\n\n\n\n\nEpoch: 200/200, Elapsed: 0:00:05, ETA: 0:00:00\n\n\n\nbatches = np.array_split(range(len(nn2.cost_)), nn2.epochs+1)\ncost_ary = np.array(nn2.cost_)\ncost_avgs = [np.mean(cost_ary[i]) for i in batches]\nplt.plot(range(30, len(cost_avgs)+30),\n         cost_avgs,\n         color='red')\nplt.ylim([0, 2])\nplt.xlim([30, 30+nn2.epochs])\nplt.ylabel('Cost')\nplt.xlabel('Epochs')\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\ny_pred = nn2.predict(X_std)\nacc = np.sum(y == y_pred, axis=0) / X_std.shape[0]\nprint('Accuracy: %.2f%%' % (acc * 100))\n\n\n\n\nAccuracy: 96.67%\n\n\n\nfig = plot_decision_regions(X=X_std, y=y, clf=nn2, legend=2)\nplt.show()\n\n\n\n\n\n\nExample 2 - Classifying Handwritten Digits from a 10% MNIST Subset\n\n\nLoad a \n5000-sample subset\n of the \nMNIST dataset\n (please see \ndata.load_mnist\n if you want to download and read in the complete MNIST dataset).\n\n\nfrom mlxtend.data import mnist_data\nfrom mlxtend.preprocessing import shuffle_arrays_unison\n\nX, y = mnist_data()\nX, y = shuffle_arrays_unison((X, y), random_seed=1)\nX_train, y_train = X[:500], y[:500]\nX_test, y_test = X[500:], y[500:]\n\n\n\n\nVisualize a sample from the MNIST dataset to check if it was loaded correctly:\n\n\nimport matplotlib.pyplot as plt\n\ndef plot_digit(X, y, idx):\n    img = X[idx].reshape(28,28)\n    plt.imshow(img, cmap='Greys',  interpolation='nearest')\n    plt.title('true label: %d' % y[idx])\n    plt.show()\n\nplot_digit(X, y, 3500)    \n\n\n\n\n\n\nStandardize pixel values:\n\n\nimport numpy as np\nfrom mlxtend.preprocessing import standardize\n\nX_train_std, params = standardize(X_train, \n                                  columns=range(X_train.shape[1]), \n                                  return_params=True)\n\nX_test_std = standardize(X_test,\n                         columns=range(X_test.shape[1]),\n                         params=params)\n\n\n\n\nInitialize the neural network to recognize the 10 different digits (0-10) using 300 epochs and mini-batch learning.\n\n\nnn = NeuralNetMLP(n_output=10, \n                  n_features=X_train_std.shape[1],\n                  n_hidden=50,\n                  l2=0.5,\n                  l1=0.0,\n                  epochs=300,\n                  eta=0.001,\n                  minibatches=25,\n                  alpha=0.001,\n                  decrease_const=0.0,\n                  random_seed=1,\n                  print_progress=3)\n\n\n\n\nLearn the features while printing the progress to get an idea about how long it may take.\n\n\nimport matplotlib.pyplot as plt\n\nnn.fit(X_train_std, y_train)\n\nplt.plot(range(len(nn.cost_)), nn.cost_)\nplt.ylim([0, 500])\nplt.ylabel('Cost')\nplt.xlabel('Epochs')\nplt.show()\n\n\n\n\nEpoch: 300/300, Elapsed: 0:00:12, ETA: 0:00:00\n\n\n\n\n\ny_train_pred = nn.predict(X_train_std)\ny_test_pred = nn.predict(X_test_std)\n\ntrain_acc = np.sum(y_train == y_train_pred, axis=0) / X_train_std.shape[0]\ntest_acc = np.sum(y_test == y_test_pred, axis=0) / X_test_std.shape[0]\n\nprint('Train Accuracy: %.2f%%' % (train_acc * 100))\nprint('Test Accuracy: %.2f%%' % (test_acc * 100))\n\n\n\n\nTrain Accuracy: 98.80%\nTest Accuracy: 84.00%\n\n\n\nPlease note\n that this neural network has been trained on only 10% of the MNIST data for technical demonstration purposes, hence, the lousy predictive performance.\n\n\nAPI\n\n\nNeuralNetMLP(n_output, n_features, n_hidden=30, l1=0.0, l2=0.0, epochs=500, eta=0.001, alpha=0.0, decrease_const=0.0, random_weights=[-1.0, 1.0], shuffle_init=True, shuffle_epoch=True, minibatches=1, random_seed=None, print_progress=0)\n\n\nFeedforward neural network / Multi-layer perceptron classifier.\n\n\nParameters\n\n\n\n\n\n\nn_output\n : int\n\n\nNumber of output units, should be equal to the\nnumber of unique class labels.\n\n\n\n\n\n\nn_features\n : int\n\n\nNumber of features (dimensions) in the target dataset.\nShould be equal to the number of columns in the X array.\n\n\n\n\n\n\nn_hidden\n : int (default: 30)\n\n\nNumber of hidden units.\n\n\n\n\n\n\nl1\n : float (default: 0.0)\n\n\nLambda value for L1-regularization.\nNo regularization if l1=0.0 (default)\n\n\n\n\n\n\nl2\n : float (default: 0.0)\n\n\nLambda value for L2-regularization.\nNo regularization if l2=0.0 (default)\n\n\n\n\n\n\nepochs\n : int (default: 500)\n\n\nNumber of passes over the training set.\n\n\n\n\n\n\neta\n : float (default: 0.001)\n\n\nLearning rate.\n\n\n\n\n\n\nalpha\n : float (default: 0.0)\n\n\nMomentum constant. Factor multiplied with the\ngradient of the previous epoch t-1 to improve\nlearning speed\nw(t) := w(t) - (grad(t) + alpha*grad(t-1))\n\n\n\n\n\n\ndecrease_const\n : float (default: 0.0)\n\n\nDecrease constant. Shrinks the learning rate\nafter each epoch via eta / (1 + epoch*decrease_const)\n\n\n\n\n\n\nrandom_weights\n : list (default: [-1.0, 1.0])\n\n\nMin and max values for initializing the random weights.\nInitializes weights to 0 if None or False.\n\n\n\n\n\n\nshuffle_init\n : bool (default: True)\n\n\nShuffles (a copy of the) training data before training.\n\n\n\n\n\n\nshuffle_epoch\n : bool (default: True)\n\n\nShuffles training data before every epoch if True to prevent circles.\n\n\n\n\n\n\nminibatches\n : int (default: 1)\n\n\nDivides training data into k minibatches for efficiency.\nNormal gradient descent learning if k=1 (default).\n\n\n\n\n\n\nrandom_seed\n : int (default: None)\n\n\nSet random seed for shuffling and initializing the weights.\n\n\n\n\n\n\nprint_progress\n : int (default: 0)\n\n\nPrints progress in fitting to stderr.\n0: No output\n1: Epochs elapsed\n2: 1 plus time elapsed\n3: 2 plus estimated time until completion\n\n\n\n\n\n\nAttributes\n\n\n\n\n\n\ncost_\n : list\n\n\nSum of squared errors after each epoch.\n\n\n\n\n\n\nMethods\n\n\n\n\n\nfit(X, y)\n\n\nLearn weight coefficients from training data.\n\n\nParameters\n\n\n\n\n\n\nX\n : array, shape = [n_samples, n_features]\n\n\nInput layer with original features.\n\n\n\n\n\n\ny\n : array, shape = [n_samples]\n\n\nTarget class labels.\n\n\n\n\n\n\nReturns:\n\n\nself\n\n\n\n\n\npredict(X)\n\n\nPredict class labels\n\n\nParameters\n\n\n\n\n\n\nX\n : array, shape = [n_samples, n_features]\n\n\nInput layer with original features.\n\n\n\n\n\n\nReturns:\n\n\n\n\n\n\ny_pred\n : array, shape = [n_samples]\n\n\nPredicted class labels.",
            "title": "NeuralNetMLP"
        },
        {
            "location": "/user_guide/classifier/NeuralNetMLP/#neural-network-multilayer-perceptron",
            "text": "Implementation of a multilayer perceptron, a feedforward artificial neural network.   from mlxtend.classifier import NeuralNetMLP",
            "title": "Neural Network - Multilayer Perceptron"
        },
        {
            "location": "/user_guide/classifier/NeuralNetMLP/#overview",
            "text": "Although the code is fully working and can be used for common classification tasks, this implementation is not geared towards efficiency but clarity \u2013 the original code was written for demonstration purposes.",
            "title": "Overview"
        },
        {
            "location": "/user_guide/classifier/NeuralNetMLP/#basic-architecture",
            "text": "The neurons  $x_0$  and  $a_0$  represent the bias units ( $x_0=1$ ,  $a_0=1$ ).   The  $i$ th superscript denotes the  $i$ th layer, and the  j th subscripts stands for the index of the respective unit. For example,  $a_{1}^{(2)}$  refers to the first activation unit  after  the bias unit (i.e., 2nd activation unit) in the 2nd layer (here: the hidden layer)  \\begin{align}\n    \\mathbf{a^{(2)}} &= \\begin{bmatrix}\n           a_{0}^{(2)} \\\n           a_{1}^{(2)} \\\n           \\vdots \\\n           a_{m}^{(2)}\n         \\end{bmatrix}.\n  \\end{align}  Each layer  $(l)$  in a multi-layer perceptron, a directed graph, is fully connected to the next layer  $(l+1)$ . We write the weight coefficient that connects the  $k$ th unit in the  $l$ th layer to the  $j$ th unit in layer  $l+1$  as  $w^{(l)}_{j, k}$ .  For example, the weight coefficient that connects the units  $a_0^{(2)} \\rightarrow a_1^{(3)}$  would be written as  $w_{1,0}^{(2)}$ .",
            "title": "Basic Architecture"
        },
        {
            "location": "/user_guide/classifier/NeuralNetMLP/#activation",
            "text": "In the current implementation, the activations of the hidden and output layers are computed via the logistic (sigmoid) function  $\\phi(z) = \\frac{1}{1 + e^{-z}}.$   (For more details on the logistic function, please see  classifier.LogisticRegression ; a general overview of different activation function can be found  here .)",
            "title": "Activation"
        },
        {
            "location": "/user_guide/classifier/NeuralNetMLP/#references",
            "text": "D. R. G. H. R. Williams and G. Hinton.  Learning representations by back-propagating errors . Nature, pages 323\u2013533, 1986.  C. M. Bishop.  Neural networks for pattern recognition . Oxford University Press, 1995.  T. Hastie, J. Friedman, and R. Tibshirani.  The Elements of Statistical Learning , Volume 2. Springer, 2009.",
            "title": "References"
        },
        {
            "location": "/user_guide/classifier/NeuralNetMLP/#examples",
            "text": "",
            "title": "Examples"
        },
        {
            "location": "/user_guide/classifier/NeuralNetMLP/#example-1-classifying-iris-flowers",
            "text": "Load 2 features from Iris (petal length and petal width) for visualization purposes:  from mlxtend.data import iris_data\nX, y = iris_data()\nX = X[:, [0, 2]]    \n\n# standardize training data\nX_std = (X - X.mean(axis=0)) / X.std(axis=0)  Train neural network for 3 output flower classes ('Setosa', 'Versicolor', 'Virginica'), regular gradient decent ( minibatches=1 ), 30 hidden units, and no regularization.",
            "title": "Example 1 - Classifying Iris Flowers"
        },
        {
            "location": "/user_guide/classifier/NeuralNetMLP/#gradient-descent",
            "text": "Setting the  minibatches  to  1  will result in gradient descent training; please see  Gradient Descent vs. Stochastic Gradient Descent  for details.  from mlxtend.classifier import NeuralNetMLP\n\nimport numpy as np\nnn1 = NeuralNetMLP(n_output=len(np.unique(y)), \n                   n_features=X_std.shape[1], \n                   n_hidden=50, \n                   l2=0.00, \n                   l1=0.0, \n                   epochs=300, \n                   eta=0.01, \n                   alpha=0.0,\n                   decrease_const=0.0,\n                   minibatches=1, \n                   shuffle_init=False,\n                   shuffle_epoch=False,\n                   random_seed=1,\n                   print_progress=3)\n\nnn1 = nn1.fit(X_std, y)  Epoch: 300/300, Elapsed: 0:00:00, ETA: 0:00:00  y_pred = nn1.predict(X_std)\nacc = np.sum(y == y_pred, axis=0) / X_std.shape[0]\nprint('Accuracy: %.2f%%' % (acc * 100))  Accuracy: 96.67%  import matplotlib.pyplot as plt\nplt.plot(range(len(nn1.cost_)), nn1.cost_)\nplt.ylim([0, 300])\nplt.ylabel('Cost')\nplt.xlabel('Epochs')\nplt.show()   from mlxtend.evaluate import plot_decision_regions\nfig = plot_decision_regions(X=X_std, y=y, clf=nn1, legend=2)",
            "title": "Gradient Descent"
        },
        {
            "location": "/user_guide/classifier/NeuralNetMLP/#stochastic-gradient-descent",
            "text": "Setting  minibatches  to  n_samples  will result in stochastic gradient descent training; please see  Gradient Descent vs. Stochastic Gradient Descent  for details.  from mlxtend.classifier import NeuralNetMLP\n\nimport numpy as np\nnn2 = NeuralNetMLP(n_output=len(np.unique(y)), \n                   n_features=X_std.shape[1], \n                   n_hidden=50, \n                   l2=0.00, \n                   l1=0.0, \n                   epochs=30, \n                   eta=0.01, \n                   alpha=0.2,\n                   decrease_const=0.0,\n                   minibatches=X_std.shape[0], \n                   shuffle_init=True,\n                   shuffle_epoch=True,\n                   random_seed=1,\n                   print_progress=3)\n\nnn2 = nn2.fit(X_std, y)  Epoch: 30/30, Elapsed: 0:00:01, ETA: 0:00:00  batches = np.array_split(range(len(nn2.cost_)), nn2.epochs+1)\ncost_ary = np.array(nn2.cost_)\ncost_avgs = [np.mean(cost_ary[i]) for i in batches]\nplt.plot(range(len(cost_avgs)),\n         cost_avgs,\n         color='red')\nplt.ylim([0, 2])\nplt.ylabel('Cost')\nplt.xlabel('Epochs')\nplt.tight_layout()\nplt.show()   Continue the training for 200 epochs...  nn2.epochs = 200\nnn2 = nn2.fit(X_std, y)  Epoch: 200/200, Elapsed: 0:00:05, ETA: 0:00:00  batches = np.array_split(range(len(nn2.cost_)), nn2.epochs+1)\ncost_ary = np.array(nn2.cost_)\ncost_avgs = [np.mean(cost_ary[i]) for i in batches]\nplt.plot(range(30, len(cost_avgs)+30),\n         cost_avgs,\n         color='red')\nplt.ylim([0, 2])\nplt.xlim([30, 30+nn2.epochs])\nplt.ylabel('Cost')\nplt.xlabel('Epochs')\nplt.tight_layout()\nplt.show()   y_pred = nn2.predict(X_std)\nacc = np.sum(y == y_pred, axis=0) / X_std.shape[0]\nprint('Accuracy: %.2f%%' % (acc * 100))  Accuracy: 96.67%  fig = plot_decision_regions(X=X_std, y=y, clf=nn2, legend=2)\nplt.show()",
            "title": "Stochastic Gradient Descent"
        },
        {
            "location": "/user_guide/classifier/NeuralNetMLP/#example-2-classifying-handwritten-digits-from-a-10-mnist-subset",
            "text": "Load a  5000-sample subset  of the  MNIST dataset  (please see  data.load_mnist  if you want to download and read in the complete MNIST dataset).  from mlxtend.data import mnist_data\nfrom mlxtend.preprocessing import shuffle_arrays_unison\n\nX, y = mnist_data()\nX, y = shuffle_arrays_unison((X, y), random_seed=1)\nX_train, y_train = X[:500], y[:500]\nX_test, y_test = X[500:], y[500:]  Visualize a sample from the MNIST dataset to check if it was loaded correctly:  import matplotlib.pyplot as plt\n\ndef plot_digit(X, y, idx):\n    img = X[idx].reshape(28,28)\n    plt.imshow(img, cmap='Greys',  interpolation='nearest')\n    plt.title('true label: %d' % y[idx])\n    plt.show()\n\nplot_digit(X, y, 3500)       Standardize pixel values:  import numpy as np\nfrom mlxtend.preprocessing import standardize\n\nX_train_std, params = standardize(X_train, \n                                  columns=range(X_train.shape[1]), \n                                  return_params=True)\n\nX_test_std = standardize(X_test,\n                         columns=range(X_test.shape[1]),\n                         params=params)  Initialize the neural network to recognize the 10 different digits (0-10) using 300 epochs and mini-batch learning.  nn = NeuralNetMLP(n_output=10, \n                  n_features=X_train_std.shape[1],\n                  n_hidden=50,\n                  l2=0.5,\n                  l1=0.0,\n                  epochs=300,\n                  eta=0.001,\n                  minibatches=25,\n                  alpha=0.001,\n                  decrease_const=0.0,\n                  random_seed=1,\n                  print_progress=3)  Learn the features while printing the progress to get an idea about how long it may take.  import matplotlib.pyplot as plt\n\nnn.fit(X_train_std, y_train)\n\nplt.plot(range(len(nn.cost_)), nn.cost_)\nplt.ylim([0, 500])\nplt.ylabel('Cost')\nplt.xlabel('Epochs')\nplt.show()  Epoch: 300/300, Elapsed: 0:00:12, ETA: 0:00:00   y_train_pred = nn.predict(X_train_std)\ny_test_pred = nn.predict(X_test_std)\n\ntrain_acc = np.sum(y_train == y_train_pred, axis=0) / X_train_std.shape[0]\ntest_acc = np.sum(y_test == y_test_pred, axis=0) / X_test_std.shape[0]\n\nprint('Train Accuracy: %.2f%%' % (train_acc * 100))\nprint('Test Accuracy: %.2f%%' % (test_acc * 100))  Train Accuracy: 98.80%\nTest Accuracy: 84.00%  Please note  that this neural network has been trained on only 10% of the MNIST data for technical demonstration purposes, hence, the lousy predictive performance.",
            "title": "Example 2 - Classifying Handwritten Digits from a 10% MNIST Subset"
        },
        {
            "location": "/user_guide/classifier/NeuralNetMLP/#api",
            "text": "NeuralNetMLP(n_output, n_features, n_hidden=30, l1=0.0, l2=0.0, epochs=500, eta=0.001, alpha=0.0, decrease_const=0.0, random_weights=[-1.0, 1.0], shuffle_init=True, shuffle_epoch=True, minibatches=1, random_seed=None, print_progress=0)  Feedforward neural network / Multi-layer perceptron classifier.  Parameters    n_output  : int  Number of output units, should be equal to the\nnumber of unique class labels.    n_features  : int  Number of features (dimensions) in the target dataset.\nShould be equal to the number of columns in the X array.    n_hidden  : int (default: 30)  Number of hidden units.    l1  : float (default: 0.0)  Lambda value for L1-regularization.\nNo regularization if l1=0.0 (default)    l2  : float (default: 0.0)  Lambda value for L2-regularization.\nNo regularization if l2=0.0 (default)    epochs  : int (default: 500)  Number of passes over the training set.    eta  : float (default: 0.001)  Learning rate.    alpha  : float (default: 0.0)  Momentum constant. Factor multiplied with the\ngradient of the previous epoch t-1 to improve\nlearning speed\nw(t) := w(t) - (grad(t) + alpha*grad(t-1))    decrease_const  : float (default: 0.0)  Decrease constant. Shrinks the learning rate\nafter each epoch via eta / (1 + epoch*decrease_const)    random_weights  : list (default: [-1.0, 1.0])  Min and max values for initializing the random weights.\nInitializes weights to 0 if None or False.    shuffle_init  : bool (default: True)  Shuffles (a copy of the) training data before training.    shuffle_epoch  : bool (default: True)  Shuffles training data before every epoch if True to prevent circles.    minibatches  : int (default: 1)  Divides training data into k minibatches for efficiency.\nNormal gradient descent learning if k=1 (default).    random_seed  : int (default: None)  Set random seed for shuffling and initializing the weights.    print_progress  : int (default: 0)  Prints progress in fitting to stderr.\n0: No output\n1: Epochs elapsed\n2: 1 plus time elapsed\n3: 2 plus estimated time until completion    Attributes    cost_  : list  Sum of squared errors after each epoch.",
            "title": "API"
        },
        {
            "location": "/user_guide/classifier/NeuralNetMLP/#methods",
            "text": "fit(X, y)  Learn weight coefficients from training data.  Parameters    X  : array, shape = [n_samples, n_features]  Input layer with original features.    y  : array, shape = [n_samples]  Target class labels.    Returns:  self   predict(X)  Predict class labels  Parameters    X  : array, shape = [n_samples, n_features]  Input layer with original features.    Returns:    y_pred  : array, shape = [n_samples]  Predicted class labels.",
            "title": "Methods"
        },
        {
            "location": "/user_guide/regressor/LinearRegression/",
            "text": "LinearRegression\n\n\nA implementation of Ordinary Least Squares simple and multiple linear regression.\n\n\n\n\nfrom mlxtend.regressor import LinearRegression\n\n\n\n\nOverview\n\n\nIllustration of a simple linear regression model:\n\n\n\n\nIn Ordinary Least Squares (OLS) Linear Regression, our goal is to find the line (or hyperplane) that minimizes the vertical offsets. Or in other words, we define the best-fitting line as the line that minimizes the sum of squared errors (SSE) or mean squared error (MSE) between our target variable (y) and our predicted output over all samples \n$i$\n in our dataset of size \n$n$\n.\n\n\n$$SSE =  \\sum_i \\big(\\text{target}^{(i)} - \\text{output}^{(i)}\\big)^2$$\n\n\n$$MSE = \\frac{1}{n} \\times SSE$$\n\n\nNow, \nLinearRegression\n implements a linear regression model for performing ordinary least squares regression using one of the following three approaches:\n\n\n\n\nNormal Equations\n\n\nGradient Descent\n\n\nStochastic Gradient Descent\n\n\n\n\nNormal Equations (closed-form solution)\n\n\nThe closed-form solution should be preferred for \"smaller\" datasets where calculating (a \"costly\") matrix inverse is not a concern. For very large datasets, or datasets where the inverse of \n$[X^T X]$\n may not exist (the matrix is non-invertible or singular, e.g., in case of perfect multicollinearity), the gradient descent or stochastic gradient descent approaches are to be preferred.\n\n\nThe linear function (linear regression model) is defined as:\n\n\n$$y = w_0x_0 + w_1x_1 + ... + w_mx_m = \\sum_{i=1}^{n} = \\mathbf{w}^T\\mathbf{x}$$\n\n\nwhere \n$y$\n is the response variable, \n$\\mathbf{x}$\n is an \n$m$\n-dimensional sample vector, and \n$\\mathbf{w}$\n is the weight vector (vector of coefficients). Note that \n$w_0$\n represents the y-axis intercept of the model and therefore \n$x_0=1$\n.  \n\n\nUsing the closed-form solution (normal equation), we compute the weights of the model as follows:\n\n\n$$ \\mathbf{w} = (\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{X}^Ty$$\n\n\nGradient Descent (GD)  and Stochastic Gradient Descent (SGD)\n\n\nSee \nGradient Descent and Stochastic Gradient Descent\n and \nDeriving the Gradient Descent Rule for Linear Regression and Adaline\n for details.\n\n\nRandom shuffling is implemented as:\n\n\n\n\nfor one or more epochs\n\n\nrandomly shuffle samples in the training set\n\n\nfor training sample \ni\n\n\ncompute gradients and perform weight updates\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nReferences\n\n\n\n\nF. Galton. \nRegression towards mediocrity in hereditary stature\n. Journal of the Anthropological Institute of Great Britain and Ireland, pages 246\u2013263, 1886.\n\n\nA. I. Khuri. \nIntroduction to linear regression analysis\n, by Douglas C. Montgomery, Elizabeth A. Peck, G. Geoffrey Vining. International Statistical Review, 81(2):318\u2013319, 2013.\n\n\nD. S. G. Pollock. \nThe Classical Linear Regression Model\n.\n\n\n\n\nExamples\n\n\nExample 1 - Closed Form Solution\n\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom mlxtend.regressor import LinearRegression\n\nX = np.array([ 1, 2.1, 3.6, 4.2, 6])[:, np.newaxis]\ny = np.array([ 1, 2, 3, 4, 5])\n\nne_lr = LinearRegression(solver='normal equation')\nne_lr.fit(X, y)\n\nprint('Intercept: %.2f' % ne_lr.w_[0])\nprint('Slope: %.2f' % ne_lr.w_[1])\n\ndef lin_regplot(X, y, model):\n    plt.scatter(X, y, c='blue')\n    plt.plot(X, model.predict(X), color='red')    \n    return\n\nlin_regplot(X, y, ne_lr)\nplt.show()\n\n\n\n\nIntercept: 0.25\nSlope: 0.81\n\n\n\n\n\nExample 2 - Gradient Descent\n\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom mlxtend.regressor import LinearRegression\n\nX = np.array([ 1, 2.1, 3.6, 4.2, 6])[:, np.newaxis]\ny = np.array([ 1, 2, 3, 4, 5])\n\ngd_lr = LinearRegression(solver='gd', eta=0.005, epochs=1000, random_seed=123)\ngd_lr.fit(X, y)\n\nprint('Intercept: %.2f' % gd_lr.w_[0])\nprint('Slope: %.2f' % gd_lr.w_[1])\n\ndef lin_regplot(X, y, model):\n    plt.scatter(X, y, c='blue')\n    plt.plot(X, model.predict(X), color='red')    \n    return\n\nlin_regplot(X, y, gd_lr)\nplt.show()\n\n\n\n\nIntercept: 0.25\nSlope: 0.81\n\n\n\n\n\n# Visualizing the cost to check for convergence and plotting the linear model:\n\nplt.plot(range(1, gd_lr.epochs+1), gd_lr.cost_)\nplt.xlabel('Epochs')\nplt.ylabel('Cost')\nplt.ylim([0, 0.2])\nplt.tight_layout()\nplt.show()    \n\n\n\n\n\n\nTip\n: I you are using gradient descent, consider standardizing the variables for better convergence of the algorithm.\n\n\nX_std = (X - np.mean(X)) / X.std()\ny_std = (y - np.mean(y)) / y.std()\n\ngd_lr = LinearRegression(solver='gd', eta=0.1, epochs=15, random_seed=0)\ngd_lr.fit(X_std, y_std)\n\nplt.plot(range(1, gd_lr.epochs+1), gd_lr.cost_)\nplt.xlabel('Epochs')\nplt.ylabel('Cost')\nplt.tight_layout()\nplt.show()  \n\n\n\n\n\n\nExample 3 - Stochastic Gradient Descent\n\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom mlxtend.regressor import LinearRegression\n\nX = np.array([ 1, 2.1, 3.6, 4.2, 6])[:, np.newaxis]\ny = np.array([ 1, 2, 3, 4, 5])\n\nsgd_lr = LinearRegression(solver='sgd', \n                          eta=0.01, \n                          epochs=100, \n                          random_seed=0, \n                          zero_init_weight=True)\nsgd_lr.fit(X, y)\n\nprint('Intercept: %.2f' % sgd_lr.w_[0])\nprint('Slope: %.2f' % sgd_lr.w_[1])\n\ndef lin_regplot(X, y, model):\n    plt.scatter(X, y, c='blue')\n    plt.plot(X, model.predict(X), color='red')    \n    return\n\nlin_regplot(X, y, sgd_lr)\nplt.show()\n\n\n\n\nIntercept: 0.25\nSlope: 0.81\n\n\n\n\n\nplt.plot(range(1, sgd_lr.epochs+1), sgd_lr.cost_)\nplt.xlabel('Epochs')\nplt.ylabel('Cost')\nplt.ylim([0, 0.2])\nplt.tight_layout()\nplt.show()  \n\n\n\n\n\n\nAPI\n\n\nLinearRegression(solver='normal equation', eta=0.01, epochs=50, random_seed=None, shuffle=False, zero_init_weight=False)\n\n\nOrdinary least squares linear regression.\n\n\nParameters\n\n\n\n\n\n\nsolver\n : {'gd', 'sgd', 'normal equation'} (default: 'normal equation')\n\n\nMethod for solving the cost function. 'gd' for gradient descent,\n'sgd' for stochastic gradient descent, or 'normal equation' (default)\nto solve the cost function analytically.\n\n\n\n\n\n\neta\n : float (default: 0.1)\n\n\nLearning rate (between 0.0 and 1.0);\nignored if solver='normal equation'.\n\n\n\n\n\n\nepochs\n : int (default: 50)\n\n\nPasses over the training dataset;\nignored if solver='normal equation'.\n\n\n\n\n\n\nshuffle\n : bool (default: False)\n\n\nShuffles training data every epoch if True to prevent circles;\nignored if solver='normal equation'.\n\n\n\n\n\n\nrandom_seed\n : int (default: None)\n\n\nSet random state for shuffling and initializing the weights;\nignored if solver='normal equation'.\n\n\n\n\n\n\nzero_init_weight\n : bool (default: False)\n\n\nIf True, weights are initialized to zero instead of small random\nnumbers in the interval [-0.1, 0.1];\nignored if solver='normal equation'\n\n\n\n\n\n\nAttributes\n\n\n\n\n\n\nw_\n : 1d-array\n\n\nWeights after fitting.\n\n\n\n\n\n\ncost_\n : list\n\n\nSum of squared errors after each epoch;\nignored if solver='normal equation'\n\n\n\n\n\n\nMethods\n\n\n\n\n\nfit(X, y, init_weights=True)\n\n\nLearn weight coefficients from training data.\n\n\nParameters\n\n\n\n\n\n\nX\n : {array-like, sparse matrix}, shape = [n_samples, n_features]\n\n\nTraining vectors, where n_samples is the number of samples and\nn_features is the number of features.\n\n\n\n\n\n\ny\n : array-like, shape = [n_samples]\n\n\nTarget values.\n\n\n\n\n\n\ninit_weights\n : bool (default: True)\n\n\nRe-initializes weights prior to fitting. Set False to continue\ntraining with weights from a previous fitting.\n\n\n\n\n\n\nReturns\n\n\n\n\nself\n : object\n\n\n\n\n\n\n\nnet_input(X)\n\n\nCompute the linear net input.\n\n\n\n\n\npredict(X)\n\n\nPredict class labels of X.\n\n\nParameters\n\n\n\n\n\n\nX\n : {array-like, sparse matrix}, shape = [n_samples, n_features]\n\n\nTraining vectors, where n_samples is the number of samples and\nn_features is the number of features.\n\n\n\n\n\n\nReturns\n\n\n\n\nfloat\n : Predicted target value.",
            "title": "LinearRegression"
        },
        {
            "location": "/user_guide/regressor/LinearRegression/#linearregression",
            "text": "A implementation of Ordinary Least Squares simple and multiple linear regression.   from mlxtend.regressor import LinearRegression",
            "title": "LinearRegression"
        },
        {
            "location": "/user_guide/regressor/LinearRegression/#overview",
            "text": "Illustration of a simple linear regression model:   In Ordinary Least Squares (OLS) Linear Regression, our goal is to find the line (or hyperplane) that minimizes the vertical offsets. Or in other words, we define the best-fitting line as the line that minimizes the sum of squared errors (SSE) or mean squared error (MSE) between our target variable (y) and our predicted output over all samples  $i$  in our dataset of size  $n$ .  $$SSE =  \\sum_i \\big(\\text{target}^{(i)} - \\text{output}^{(i)}\\big)^2$$  $$MSE = \\frac{1}{n} \\times SSE$$  Now,  LinearRegression  implements a linear regression model for performing ordinary least squares regression using one of the following three approaches:   Normal Equations  Gradient Descent  Stochastic Gradient Descent",
            "title": "Overview"
        },
        {
            "location": "/user_guide/regressor/LinearRegression/#normal-equations-closed-form-solution",
            "text": "The closed-form solution should be preferred for \"smaller\" datasets where calculating (a \"costly\") matrix inverse is not a concern. For very large datasets, or datasets where the inverse of  $[X^T X]$  may not exist (the matrix is non-invertible or singular, e.g., in case of perfect multicollinearity), the gradient descent or stochastic gradient descent approaches are to be preferred.  The linear function (linear regression model) is defined as:  $$y = w_0x_0 + w_1x_1 + ... + w_mx_m = \\sum_{i=1}^{n} = \\mathbf{w}^T\\mathbf{x}$$  where  $y$  is the response variable,  $\\mathbf{x}$  is an  $m$ -dimensional sample vector, and  $\\mathbf{w}$  is the weight vector (vector of coefficients). Note that  $w_0$  represents the y-axis intercept of the model and therefore  $x_0=1$ .    Using the closed-form solution (normal equation), we compute the weights of the model as follows:  $$ \\mathbf{w} = (\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{X}^Ty$$",
            "title": "Normal Equations (closed-form solution)"
        },
        {
            "location": "/user_guide/regressor/LinearRegression/#gradient-descent-gd-and-stochastic-gradient-descent-sgd",
            "text": "See  Gradient Descent and Stochastic Gradient Descent  and  Deriving the Gradient Descent Rule for Linear Regression and Adaline  for details.  Random shuffling is implemented as:   for one or more epochs  randomly shuffle samples in the training set  for training sample  i  compute gradients and perform weight updates",
            "title": "Gradient Descent (GD)  and Stochastic Gradient Descent (SGD)"
        },
        {
            "location": "/user_guide/regressor/LinearRegression/#references",
            "text": "F. Galton.  Regression towards mediocrity in hereditary stature . Journal of the Anthropological Institute of Great Britain and Ireland, pages 246\u2013263, 1886.  A. I. Khuri.  Introduction to linear regression analysis , by Douglas C. Montgomery, Elizabeth A. Peck, G. Geoffrey Vining. International Statistical Review, 81(2):318\u2013319, 2013.  D. S. G. Pollock.  The Classical Linear Regression Model .",
            "title": "References"
        },
        {
            "location": "/user_guide/regressor/LinearRegression/#examples",
            "text": "",
            "title": "Examples"
        },
        {
            "location": "/user_guide/regressor/LinearRegression/#example-1-closed-form-solution",
            "text": "import numpy as np\nimport matplotlib.pyplot as plt\nfrom mlxtend.regressor import LinearRegression\n\nX = np.array([ 1, 2.1, 3.6, 4.2, 6])[:, np.newaxis]\ny = np.array([ 1, 2, 3, 4, 5])\n\nne_lr = LinearRegression(solver='normal equation')\nne_lr.fit(X, y)\n\nprint('Intercept: %.2f' % ne_lr.w_[0])\nprint('Slope: %.2f' % ne_lr.w_[1])\n\ndef lin_regplot(X, y, model):\n    plt.scatter(X, y, c='blue')\n    plt.plot(X, model.predict(X), color='red')    \n    return\n\nlin_regplot(X, y, ne_lr)\nplt.show()  Intercept: 0.25\nSlope: 0.81",
            "title": "Example 1 - Closed Form Solution"
        },
        {
            "location": "/user_guide/regressor/LinearRegression/#example-2-gradient-descent",
            "text": "import numpy as np\nimport matplotlib.pyplot as plt\nfrom mlxtend.regressor import LinearRegression\n\nX = np.array([ 1, 2.1, 3.6, 4.2, 6])[:, np.newaxis]\ny = np.array([ 1, 2, 3, 4, 5])\n\ngd_lr = LinearRegression(solver='gd', eta=0.005, epochs=1000, random_seed=123)\ngd_lr.fit(X, y)\n\nprint('Intercept: %.2f' % gd_lr.w_[0])\nprint('Slope: %.2f' % gd_lr.w_[1])\n\ndef lin_regplot(X, y, model):\n    plt.scatter(X, y, c='blue')\n    plt.plot(X, model.predict(X), color='red')    \n    return\n\nlin_regplot(X, y, gd_lr)\nplt.show()  Intercept: 0.25\nSlope: 0.81   # Visualizing the cost to check for convergence and plotting the linear model:\n\nplt.plot(range(1, gd_lr.epochs+1), gd_lr.cost_)\nplt.xlabel('Epochs')\nplt.ylabel('Cost')\nplt.ylim([0, 0.2])\nplt.tight_layout()\nplt.show()       Tip : I you are using gradient descent, consider standardizing the variables for better convergence of the algorithm.  X_std = (X - np.mean(X)) / X.std()\ny_std = (y - np.mean(y)) / y.std()\n\ngd_lr = LinearRegression(solver='gd', eta=0.1, epochs=15, random_seed=0)\ngd_lr.fit(X_std, y_std)\n\nplt.plot(range(1, gd_lr.epochs+1), gd_lr.cost_)\nplt.xlabel('Epochs')\nplt.ylabel('Cost')\nplt.tight_layout()\nplt.show()",
            "title": "Example 2 - Gradient Descent"
        },
        {
            "location": "/user_guide/regressor/LinearRegression/#example-3-stochastic-gradient-descent",
            "text": "import numpy as np\nimport matplotlib.pyplot as plt\nfrom mlxtend.regressor import LinearRegression\n\nX = np.array([ 1, 2.1, 3.6, 4.2, 6])[:, np.newaxis]\ny = np.array([ 1, 2, 3, 4, 5])\n\nsgd_lr = LinearRegression(solver='sgd', \n                          eta=0.01, \n                          epochs=100, \n                          random_seed=0, \n                          zero_init_weight=True)\nsgd_lr.fit(X, y)\n\nprint('Intercept: %.2f' % sgd_lr.w_[0])\nprint('Slope: %.2f' % sgd_lr.w_[1])\n\ndef lin_regplot(X, y, model):\n    plt.scatter(X, y, c='blue')\n    plt.plot(X, model.predict(X), color='red')    \n    return\n\nlin_regplot(X, y, sgd_lr)\nplt.show()  Intercept: 0.25\nSlope: 0.81   plt.plot(range(1, sgd_lr.epochs+1), sgd_lr.cost_)\nplt.xlabel('Epochs')\nplt.ylabel('Cost')\nplt.ylim([0, 0.2])\nplt.tight_layout()\nplt.show()",
            "title": "Example 3 - Stochastic Gradient Descent"
        },
        {
            "location": "/user_guide/regressor/LinearRegression/#api",
            "text": "LinearRegression(solver='normal equation', eta=0.01, epochs=50, random_seed=None, shuffle=False, zero_init_weight=False)  Ordinary least squares linear regression.  Parameters    solver  : {'gd', 'sgd', 'normal equation'} (default: 'normal equation')  Method for solving the cost function. 'gd' for gradient descent,\n'sgd' for stochastic gradient descent, or 'normal equation' (default)\nto solve the cost function analytically.    eta  : float (default: 0.1)  Learning rate (between 0.0 and 1.0);\nignored if solver='normal equation'.    epochs  : int (default: 50)  Passes over the training dataset;\nignored if solver='normal equation'.    shuffle  : bool (default: False)  Shuffles training data every epoch if True to prevent circles;\nignored if solver='normal equation'.    random_seed  : int (default: None)  Set random state for shuffling and initializing the weights;\nignored if solver='normal equation'.    zero_init_weight  : bool (default: False)  If True, weights are initialized to zero instead of small random\nnumbers in the interval [-0.1, 0.1];\nignored if solver='normal equation'    Attributes    w_  : 1d-array  Weights after fitting.    cost_  : list  Sum of squared errors after each epoch;\nignored if solver='normal equation'",
            "title": "API"
        },
        {
            "location": "/user_guide/regressor/LinearRegression/#methods",
            "text": "fit(X, y, init_weights=True)  Learn weight coefficients from training data.  Parameters    X  : {array-like, sparse matrix}, shape = [n_samples, n_features]  Training vectors, where n_samples is the number of samples and\nn_features is the number of features.    y  : array-like, shape = [n_samples]  Target values.    init_weights  : bool (default: True)  Re-initializes weights prior to fitting. Set False to continue\ntraining with weights from a previous fitting.    Returns   self  : object    net_input(X)  Compute the linear net input.   predict(X)  Predict class labels of X.  Parameters    X  : {array-like, sparse matrix}, shape = [n_samples, n_features]  Training vectors, where n_samples is the number of samples and\nn_features is the number of features.    Returns   float  : Predicted target value.",
            "title": "Methods"
        },
        {
            "location": "/user_guide/regression_utils/plot_linear_regression/",
            "text": "Linear Regression Plot\n\n\nA function to plot linear regression fits. \n\n\n\n\nfrom mlxtend.regression_utils import plot_linear_regression\n\n\n\n\nOverview\n\n\nThe \nplot_linear_regression\n is a convenience function that uses scikit-learn's \nlinear_model.LinearRegression\n to fit a linear model and SciPy's \nstats.pearsonr\n to calculate the correlation coefficient. \n\n\nReferences\n\n\n\n\n-\n\n\n\n\nExamples\n\n\nExample 1 - Ordinary Least Squares Simple Linear Regression\n\n\nimport matplotlib.pyplot as plt\nfrom mlxtend.regression_utils import plot_linear_regression\nimport numpy as np\n\nX = np.array([4, 8, 13, 26, 31, 10, 8, 30, 18, 12, 20, 5, 28, 18, 6, 31, 12,\n   12, 27, 11, 6, 14, 25, 7, 13,4, 15, 21, 15])\n\ny = np.array([14, 24, 22, 59, 66, 25, 18, 60, 39, 32, 53, 18, 55, 41, 28, 61, 35,\n   36, 52, 23, 19, 25, 73, 16, 32, 14, 31, 43, 34])\n\nintercept, slope, corr_coeff = plot_linear_regression(X, y)\nplt.show()\n\n\n\n\n\n\nAPI\n\n\nplot_linear_regression(X, y, model=LinearRegression(copy_X=True, fit_intercept=True, n_jobs=1, normalize=False), corr_func='pearsonr', scattercolor='blue', fit_style='k--', legend=True, xlim='auto')\n\n\nPlot a linear regression line fit.\n\n\nParameters\n\n\n\n\n\n\nX\n : numpy array, shape = [n_samples,]\n\n\nSamples.\n\n\n\n\n\n\ny\n : numpy array, shape (n_samples,)\n\n\nTarget values\nmodel: object (default: sklearn.linear_model.LinearRegression)\nEstimator object for regression. Must implement\na .fit() and .predict() method.\ncorr_func: str or function (default: 'pearsonr')\nUses \npearsonr\n from scipy.stats if corr_func='pearsonr'.\nto compute the regression slope. If not 'pearsonr', the \ncorr_func\n,\nthe \ncorr_func\n parameter expects a function of the form\nfunc(\n, \n) as inputs, which is expected to return\na tuple \n(<correlation_coefficient>, <some_unused_value>)\n.\nscattercolor: string (default: blue)\nColor of scatter plot points.\nfit_style: string (default: k--)\nStyle for the line fit.\nlegend: bool (default: True)\nPlots legend with corr_coeff coef.,\nfit coef., and intercept values.\nxlim: array-like (x_min, x_max) or 'auto' (default: 'auto')\nX-axis limits for the linear line fit.\n\n\n\n\n\n\nReturns\n\n\n\n\n\n\nregression_fit\n : tuple\n\n\nintercept, slope, corr_coeff (float, float, float)",
            "title": "Plot linear regression"
        },
        {
            "location": "/user_guide/regression_utils/plot_linear_regression/#linear-regression-plot",
            "text": "A function to plot linear regression fits.    from mlxtend.regression_utils import plot_linear_regression",
            "title": "Linear Regression Plot"
        },
        {
            "location": "/user_guide/regression_utils/plot_linear_regression/#overview",
            "text": "The  plot_linear_regression  is a convenience function that uses scikit-learn's  linear_model.LinearRegression  to fit a linear model and SciPy's  stats.pearsonr  to calculate the correlation coefficient.",
            "title": "Overview"
        },
        {
            "location": "/user_guide/regression_utils/plot_linear_regression/#references",
            "text": "-",
            "title": "References"
        },
        {
            "location": "/user_guide/regression_utils/plot_linear_regression/#examples",
            "text": "",
            "title": "Examples"
        },
        {
            "location": "/user_guide/regression_utils/plot_linear_regression/#example-1-ordinary-least-squares-simple-linear-regression",
            "text": "import matplotlib.pyplot as plt\nfrom mlxtend.regression_utils import plot_linear_regression\nimport numpy as np\n\nX = np.array([4, 8, 13, 26, 31, 10, 8, 30, 18, 12, 20, 5, 28, 18, 6, 31, 12,\n   12, 27, 11, 6, 14, 25, 7, 13,4, 15, 21, 15])\n\ny = np.array([14, 24, 22, 59, 66, 25, 18, 60, 39, 32, 53, 18, 55, 41, 28, 61, 35,\n   36, 52, 23, 19, 25, 73, 16, 32, 14, 31, 43, 34])\n\nintercept, slope, corr_coeff = plot_linear_regression(X, y)\nplt.show()",
            "title": "Example 1 - Ordinary Least Squares Simple Linear Regression"
        },
        {
            "location": "/user_guide/regression_utils/plot_linear_regression/#api",
            "text": "plot_linear_regression(X, y, model=LinearRegression(copy_X=True, fit_intercept=True, n_jobs=1, normalize=False), corr_func='pearsonr', scattercolor='blue', fit_style='k--', legend=True, xlim='auto')  Plot a linear regression line fit.  Parameters    X  : numpy array, shape = [n_samples,]  Samples.    y  : numpy array, shape (n_samples,)  Target values\nmodel: object (default: sklearn.linear_model.LinearRegression)\nEstimator object for regression. Must implement\na .fit() and .predict() method.\ncorr_func: str or function (default: 'pearsonr')\nUses  pearsonr  from scipy.stats if corr_func='pearsonr'.\nto compute the regression slope. If not 'pearsonr', the  corr_func ,\nthe  corr_func  parameter expects a function of the form\nfunc( ,  ) as inputs, which is expected to return\na tuple  (<correlation_coefficient>, <some_unused_value>) .\nscattercolor: string (default: blue)\nColor of scatter plot points.\nfit_style: string (default: k--)\nStyle for the line fit.\nlegend: bool (default: True)\nPlots legend with corr_coeff coef.,\nfit coef., and intercept values.\nxlim: array-like (x_min, x_max) or 'auto' (default: 'auto')\nX-axis limits for the linear line fit.    Returns    regression_fit  : tuple  intercept, slope, corr_coeff (float, float, float)",
            "title": "API"
        },
        {
            "location": "/user_guide/feature_selection/SequentialFeatureSelector/",
            "text": "Sequential Feature Selector\n\n\nImplementation of \nsequential feature algorithms\n (SFAs) -- greedy search algorithms -- that have been developed as a suboptimal solution to the computationally often not feasible exhaustive search.\n\n\n\n\nfrom mlxtend.feature_selection import SequentialFeatureSelector\n\n\n\n\nOverview\n\n\nSequential feature selection algorithms are a family of greedy search algorithms that are used to reduce an initial \nd\n-dimensional feature space to a \nk\n-dimensional feature subspace where \nk < d\n. The motivation behind feature selection algorithms is to automatically select a subset of features that are most relevant to the problem to improve computational efficiency or reduce the generalization error of the model by removing irrelevant features or noise, which can be useful for algorithms that don't support regularization. \n\n\nIn a nutshell, SFAs remove or add one feature at the time based on the classifier performance until a feature subset of the desired size \nk\n is reached. There are 4 different flavors of SFAs available via the \nSequentialFeatureSelector\n:\n\n\n\n\nSequential Forward Selection (SFS)\n\n\nSequential Backward Selection (SBS)\n\n\nSequential Floating Forward Selection (SFFS)\n\n\nSequential Floating Backward Selection (SFBS)\n\n\n\n\nThe \nfloating\n variants, SFFS and SFBS, can be considered as extensions to the simpler SFS and SBS algorithms. The floating algorithms have an additional exclusion or inclusion step to remove features once they were included (or excluded), so that a larger number of feature subset combinations can be sampled. It is important to emphasize that this step is conditional and only occurs if the resulting feature subset is assessed as \"better\" by the criterion function after removal (or addition) of a particular feature. Furthermore, I added an optional check to skip the conditional exclusion steps if the algorithm gets stuck in cycles.  \n\n\n\n\nHow is this different from \nRecursive Feature Elimination\n (RFE)  -- e.g., as implemented in \nsklearn.feature_selection.RFE\n? RFE is computationally less complex using the feature's weight coefficients (e.g., linear models) or feature importances (tree-based algorithms) to eliminate features recursively, whereas SFSs eliminate (or add) features based on a user-defined classifier/regression performance metric.\n\n\n\n\nThe SFAs  are outlined in pseudo code below:\n\n\nSequential Forward Selection (SFS)\n\n\nInput:\n \n$Y = \\{y_1, y_2, ..., y_d\\}$\n  \n\n\n\n\nThe \nSFS\n algorithm takes the whole \n$d$\n-dimensional feature set as input.\n\n\n\n\nOutput:\n \n$X_k = \\{x_j \\; | \\;j = 1, 2, ..., k; \\; x_j \\in Y\\}$\n, where \n$k = (0, 1, 2, ..., d)$\n\n\n\n\nSFS returns a subset of features; the number of selected features \n$k$\n, where \n$k < d$\n, has to be specified \na priori\n.\n\n\n\n\nInitialization:\n \n$X_0 = \\emptyset$\n, \n$k = 0$\n\n\n\n\nWe initialize the algorithm with an empty set \n$\\emptyset$\n (\"null set\") so that \n$k = 0$\n (where \n$k$\n is the size of the subset).\n\n\n\n\nStep 1 (Inclusion):\n  \n\n\n$x^+ = \\text{ arg max } J(x_k + x), \\text{ where }  x \\in Y - X_k$\n\n\n$X_k+1 = X_k + x^+$\n\n\n$k = k + 1$\n  \n\n\nGo to Step 1\n \n\n\n\n\nin this step, we add an additional feature, \n$x^+$\n, to our feature subset \n$X_k$\n.\n\n\n$x^+$\n is the feature that maximizes our criterion function, that is, the feature that is associated with the best classifier performance if it is added to \n$X_k$\n.\n\n\nWe repeat this procedure until the termination criterion is satisfied.\n\n\n\n\nTermination:\n \n$k = p$\n\n\n\n\nWe add features from the feature subset \n$X_k$\n until the feature subset of size \n$k$\n contains the number of desired features \n$p$\n that we specified \na priori\n.\n\n\n\n\nSequential Floating Backward (SBS)\n\n\nInput:\n the set of all features, \n$Y = \\{y_1, y_2, ..., y_d\\}$\n  \n\n\n\n\nThe SBS algorithm takes the whole feature set as input.\n\n\n\n\nOutput:\n \n$X_k = \\{x_j \\; | \\;j = 1, 2, ..., k; \\; x_j \\in Y\\}$\n, where \n$k = (0, 1, 2, ..., d)$\n\n\n\n\nSBS returns a subset of features; the number of selected features \n$k$\n, where \n$k < d$\n, has to be specified \na priori\n.\n\n\n\n\nInitialization:\n \n$X_0 = Y$\n, \n$k = d$\n\n\n\n\nWe initialize the algorithm with the given feature set so that the \n$k = d$\n.\n\n\n\n\nStep 1 (Exclusion):\n  \n\n\n$x^- = \\text{ arg max } J(x_k - x), \\text{  where } x \\in X_k$\n\n\n$X_k-1 = X_k - x^-$\n\n\n$k = k - 1$\n\n\nGo to Step 1\n  \n\n\n\n\nIn this step, we remove a feature, \n$x^-$\n from our feature subset \n$X_k$\n.\n\n\n$x^-$\n is the feature that maximizes our criterion function upon re,oval, that is, the feature that is associated with the best classifier performance if it is removed from \n$X_k$\n.\n\n\nWe repeat this procedure until the termination criterion is satisfied.\n\n\n\n\nTermination:\n \n$k = p$\n\n\n\n\nWe add features from the feature subset \n$X_k$\n until the feature subset of size \n$k$\n contains the number of desired features \n$p$\n that we specified \na priori\n.\n\n\n\n\nSequential Floating Backward Selection (SFBS)\n\n\nInput:\n the set of all features, \n$Y = \\{y_1, y_2, ..., y_d\\}$\n  \n\n\n\n\nThe SFBS algorithm takes the whole feature set as input.\n\n\n\n\nOutput:\n \n$X_k = \\{x_j \\; | \\;j = 1, 2, ..., k; \\; x_j \\in Y\\}$\n, where \n$k = (0, 1, 2, ..., d)$\n\n\n\n\nSFBS returns a subset of features; the number of selected features \n$k$\n, where \n$k < d$\n, has to be specified \na priori\n.\n\n\n\n\nInitialization:\n \n$X_0 = Y$\n, \n$k = d$\n\n\n\n\nWe initialize the algorithm with the given feature set so that the \n$k = d$\n.\n\n\n\n\nStep 1 (Exclusion):\n  \n\n\n$x^- = \\text{ arg max } J(x_k - x), \\text{  where } x \\in X_k$\n\n\n$X_k-1 = X_k - x^-$\n\n\n$k = k - 1$\n\n\nGo to Step 2\n  \n\n\n\n\nIn this step, we remove a feature, \n$x^-$\n from our feature subset \n$X_k$\n.\n\n\n$x^-$\n is the feature that maximizes our criterion function upon re,oval, that is, the feature that is associated with the best classifier performance if it is removed from \n$X_k$\n.\n\n\n\n\nStep 2 (Conditional Inclusion):\n\n\n\n\n$x^+ = \\text{ arg max } J(x_k + x), \\text{ where } x \\in Y - X_k$\n\n\nif J(x_k + x) > J(x_k + x)\n:  \n\n\u00a0\u00a0\u00a0\u00a0 \n$X_k+1 = X_k + x^+$\n\n\u00a0\u00a0\u00a0\u00a0 \n$k = k + 1$\n\n\nGo to Step 1\n  \n\n\n\n\nIn Step 2, we search for features that improve the classifier performance if they are added back to the feature subset. If such features exist, we add the feature \n$x^+$\n for which the perfomance improvement is max.\n\n\nSteps 1 and 2 are reapeated until the \nTermination\n criterion is reached.\n\n\n\n\nTermination:\n \n$k = p$\n\n\n\n\nWe add features from the feature subset \n$X_k$\n until the feature subset of size \n$k$\n contains the number of desired features \n$p$\n that we specified \na priori\n.\n\n\n\n\nSequential Floating Forward Selection (SFFS)\n\n\nInput:\n the set of all features, \n$Y = \\{y_1, y_2, ..., y_d\\}$\n  \n\n\n\n\nThe \nSFFS\n algorithm takes the whole feature set as input, if our feature space consists of, e.g. 10, if our feature space consists of 10 dimensions (\nd = 10\n).\n\n\n\n\n\nOutput:\n a subset of features, \n$X_k = \\{x_j \\; | \\;j = 1, 2, ..., k; \\; x_j \\in Y\\}$\n, where \n$k = (0, 1, 2, ..., d)$\n\n\n\n\nThe returned output of the algorithm is a subset of the feature space of a specified size. E.g., a subset of 5 features from a 10-dimensional feature space (\nk = 5, d = 10\n).\n\n\n\n\n\nInitialization:\n \n$X_0 = Y$\n, \n$k = d$\n\n\n\n\nWe initialize the algorithm with an empty set (\"null set\") so that the \nk = 0\n (where \nk\n is the size of the subset)\n\n\n\n\n\nStep 1 (Inclusion):\n\n\n\n\u00a0\u00a0\u00a0\u00a0 \n$x^+ = \\text{ arg max } J(x_k + x), \\text{ where }  x \\in Y - X_k$\n\n\u00a0\u00a0\u00a0\u00a0 \n$X_k+1 = X_k + x^+$\n\n\u00a0\u00a0\u00a0\u00a0 \n$k = k + 1$\n  \n\n\u00a0\u00a0\u00a0\u00a0\nGo to Step 2\n\n\n \n\n\nStep 2 (Conditional Exclusion):\n\n\n\n\u00a0\u00a0\u00a0\u00a0 \n$x^- = \\text{ arg max } J(x_k - x), \\text{ where } x \\in X_k$\n\n\u00a0\u00a0\u00a0\u00a0\n$if \\; J(x_k - x) > J(x_k - x)$\n:  \n\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 \n$X_k-1 = X_k - x^- $\n\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 \n$k = k - 1$\n  \n\n\u00a0\u00a0\u00a0\u00a0\nGo to Step 1\n  \n\n\n\n\nIn step 1, we include the feature from the \nfeature space\n that leads to the best performance increase for our \nfeature subset\n (assessed by the \ncriterion function\n). Then, we go over to step 2\n\n\nIn step 2, we only remove a feature if the resulting subset would gain an increase in performance. We go back to step 1.  \n\n\nSteps 1 and 2 are reapeated until the \nTermination\n criterion is reached.\n\n\n\n\n\nTermination:\n stop when \nk\n equals the number of desired features\n\n\nReferences\n\n\n\n\nFerri, F., et al. \n\"Comparative study of techniques for large-scale feature selection.\"\n Pattern Recognition in Practice IV (1994): 403-413.\n\n\n\n\nExamples\n\n\nExample 1 - A simple Sequential Forward Selection example\n\n\nInitializing a simple classifier from scikit-learn:\n\n\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.datasets import load_iris\niris = load_iris()\nX = iris.data\ny = iris.target\nknn = KNeighborsClassifier(n_neighbors=4)\n\n\n\n\nWe start by selection the \"best\" 3 features from the Iris dataset via Sequential Forward Selection (SFS). Here, we set \nforward=True\n and \nfloating=False\n. By choosing \ncv=0\n, we don't perform any cross-validation, therefore, the performance (here: \n'accuracy'\n) is computed entirely on the training set. \n\n\nfrom mlxtend.feature_selection import SequentialFeatureSelector as SFS\n\nsfs1 = SFS(knn, \n           k_features=3, \n           forward=True, \n           floating=False, \n           scoring='accuracy',\n           cv=0)\nsfs1 = sfs1.fit(X, y)\n\n\n\n\nFeatures: 3/3\n\n\n\nVia the \nsubsets_\n attribute, we can take a look at the selected feature indices at each step:\n\n\nsfs1.subsets_\n\n\n\n\n{1: {'avg_score': 0.95999999999999996,\n  'cv_scores': array([ 0.96]),\n  'feature_idx': (3,)},\n 2: {'avg_score': 0.97333333333333338,\n  'cv_scores': array([ 0.97333333]),\n  'feature_idx': (2, 3)},\n 3: {'avg_score': 0.97333333333333338,\n  'cv_scores': array([ 0.97333333]),\n  'feature_idx': (1, 2, 3)}}\n\n\n\nFurthermore, we can access the indices of the 3 best features directly via the \nk_feature_idx_\n attribute:\n\n\nsfs1.k_feature_idx_\n\n\n\n\n(1, 2, 3)\n\n\n\nFinally, the prediction score for these 3 features can be accesses via \nk_score_\n:\n\n\nsfs1.k_score_\n\n\n\n\n0.97333333333333338\n\n\n\nExample 2 - Toggling between SFS, SBS, SFFS, and SFBS\n\n\nUsing the \nforward\n and \nfloating\n parameters, we can toggle between SFS, SBS, SFFS, and SFBS as shown below. Note that we are performing (stratified) 4-fold cross-validation for more robust estimates in contrast to Example 1. Via \nn_jobs=-1\n, we choose to run the cross-validation on all our available CPU cores.\n\n\n# Sequential Forward Selection\nsfs = SFS(knn, \n          k_features=3, \n          forward=True, \n          floating=False, \n          scoring='accuracy',\n          print_progress=False,\n          cv=4,\n          n_jobs=-1)\nsfs = sfs.fit(X, y)\n\nprint('\\nSequential Forward Selection (k=3):')\nprint(sfs.k_feature_idx_)\nprint('CV Score:')\nprint(sfs.k_score_)\n\n###################################################\n\n# Sequential Backward Selection\nsbs = SFS(knn, \n          k_features=3, \n          forward=False, \n          floating=False, \n          scoring='accuracy',\n          print_progress=False,\n          cv=4,\n          n_jobs=-1)\nsbs = sbs.fit(X, y)\n\nprint('\\nSequential Backward Selection (k=3):')\nprint(sbs.k_feature_idx_)\nprint('CV Score:')\nprint(sbs.k_score_)\n\n###################################################\n\n# Sequential Floating Forward Selection\nsffs = SFS(knn, \n           k_features=3, \n           forward=True, \n           floating=True, \n           scoring='accuracy',\n           print_progress=False,\n           cv=4,\n           n_jobs=-1)\nsffs = sffs.fit(X, y)\n\nprint('\\nSequential Floating Forward Selection (k=3):')\nprint(sffs.k_feature_idx_)\nprint('CV Score:')\nprint(sffs.k_score_)\n\n###################################################\n\n# Sequential Floating Backward Selection\nsfbs = SFS(knn, \n           k_features=3, \n           forward=False, \n           floating=True, \n           scoring='accuracy',\n           print_progress=False,\n           cv=4,\n           n_jobs=-1)\nsfbs = sfbs.fit(X, y)\n\nprint('\\nSequential Floating Backward Selection (k=3):')\nprint(sfbs.k_feature_idx_)\nprint('CV Score:')\nprint(sfbs.k_score_)\n\n\n\n\nSequential Forward Selection (k=3):\n(1, 2, 3)\nCV Score:\n0.972756410256\n\nSequential Backward Selection (k=3):\n(1, 2, 3)\nCV Score:\n0.972756410256\n\nSequential Floating Forward Selection (k=3):\n(1, 2, 3)\nCV Score:\n0.972756410256\n\nSequential Floating Backward Selection (k=3):\n(1, 2, 3)\nCV Score:\n0.972756410256\n\n\n\nIn this simple scenario, selecting the best 3 features out of the 4 available features in the Iris set, we end up with similar results regardless of which sequential selection algorithms we used.\n\n\nExample 3 - Visualizing the results in DataFrames\n\n\nFor our convenience, we can visualize the output from the feature selection in a pandas DataFrame format using the \nget_metric_dict\n method of the SequentialFeatureSelector object. The columns \nstd_dev\n and \nstd_err\n represent the standard deviation and standard errors of the cross-validation scores, respectively.\n\n\nBelow, we see the DataFrame of the Sequential Forward Selector from Example 2:\n\n\nimport pandas as pd\npd.DataFrame.from_dict(sfs.get_metric_dict()).T\n\n\n\n\n\n\n\n  \n\n    \n\n      \n\n      \navg_score\n\n      \nci_bound\n\n      \ncv_scores\n\n      \nfeature_idx\n\n      \nstd_dev\n\n      \nstd_err\n\n    \n\n  \n\n  \n\n    \n\n      \n1\n\n      \n0.952991\n\n      \n0.0660624\n\n      \n[0.974358974359, 0.948717948718, 0.88888888888...\n\n      \n(3,)\n\n      \n0.0412122\n\n      \n0.0237939\n\n    \n\n    \n\n      \n2\n\n      \n0.959936\n\n      \n0.0494801\n\n      \n[0.974358974359, 0.948717948718, 0.91666666666...\n\n      \n(2, 3)\n\n      \n0.0308676\n\n      \n0.0178214\n\n    \n\n    \n\n      \n3\n\n      \n0.972756\n\n      \n0.0315204\n\n      \n[0.974358974359, 1.0, 0.944444444444, 0.972222...\n\n      \n(1, 2, 3)\n\n      \n0.0196636\n\n      \n0.0113528\n\n    \n\n  \n\n\n\n\n\n\n\nNow, let's compare it to the Sequential Backward Selector:\n\n\npd.DataFrame.from_dict(sbs.get_metric_dict()).T\n\n\n\n\n\n\n\n  \n\n    \n\n      \n\n      \navg_score\n\n      \nci_bound\n\n      \ncv_scores\n\n      \nfeature_idx\n\n      \nstd_dev\n\n      \nstd_err\n\n    \n\n  \n\n  \n\n    \n\n      \n3\n\n      \n0.972756\n\n      \n0.0315204\n\n      \n[0.974358974359, 1.0, 0.944444444444, 0.972222...\n\n      \n(1, 2, 3)\n\n      \n0.0196636\n\n      \n0.0113528\n\n    \n\n    \n\n      \n4\n\n      \n0.952991\n\n      \n0.0372857\n\n      \n[0.974358974359, 0.948717948718, 0.91666666666...\n\n      \n(0, 1, 2, 3)\n\n      \n0.0232602\n\n      \n0.0134293\n\n    \n\n  \n\n\n\n\n\n\n\nWe can see that both SFS and SFBS found the same \"best\" 3 features, however, the intermediate steps where obviously different.\n\n\nThe \nci_bound\n column in the DataFrames above represents the confidence interval around the computed cross-validation scores. By default, a confidence interval of 95% is used, but we can use different confidence bounds via the \nconfidence_interval\n parameter. E.g., the confidence bounds for a 90% confidence interval can be obtained as follows:\n\n\npd.DataFrame.from_dict(sbs.get_metric_dict(confidence_interval=0.90)).T\n\n\n\n\n\n\n\n  \n\n    \n\n      \n\n      \navg_score\n\n      \nci_bound\n\n      \ncv_scores\n\n      \nfeature_idx\n\n      \nstd_dev\n\n      \nstd_err\n\n    \n\n  \n\n  \n\n    \n\n      \n3\n\n      \n0.972756\n\n      \n0.0242024\n\n      \n[0.974358974359, 1.0, 0.944444444444, 0.972222...\n\n      \n(1, 2, 3)\n\n      \n0.0196636\n\n      \n0.0113528\n\n    \n\n    \n\n      \n4\n\n      \n0.952991\n\n      \n0.0286292\n\n      \n[0.974358974359, 0.948717948718, 0.91666666666...\n\n      \n(0, 1, 2, 3)\n\n      \n0.0232602\n\n      \n0.0134293\n\n    \n\n  \n\n\n\n\n\n\n\nExample 4 - Plotting the results\n\n\nAfter importing the little helper function \nplot_sequential_feature_selection\n, we can also visualize the results using matplotlib figures.\n\n\nfrom mlxtend.feature_selection import plot_sequential_feature_selection as plot_sfs\nimport matplotlib.pyplot as plt\n\nsfs = SFS(knn, \n          k_features=4, \n          forward=True, \n          floating=False, \n          scoring='accuracy',\n          cv=5)\n\nsfs = sfs.fit(X, y)\n\nfig1 = plot_sfs(sfs.get_metric_dict(), kind='std_dev')\n\nplt.ylim([0.8, 1])\nplt.title('Sequential Forward Selection (w. StdDev)')\nplt.grid()\nplt.show()\n\n\n\n\nFeatures: 4/4\n\n\n\n\n\nExample 5 - Sequential Feature Selection for Regression\n\n\nSimilar to the classification examples above, the \nSequentialFeatureSelector\n also supports scikit-learn's estimators\nfor regression.\n\n\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.datasets import load_boston\n\nboston = load_boston()\nX, y = boston.data, boston.target\n\nlr = LinearRegression()\n\nsfs = SFS(lr, \n          k_features=13, \n          forward=True, \n          floating=False, \n          scoring='mean_squared_error',\n          cv=10)\n\nsfs = sfs.fit(X, y)\nfig = plot_sfs(sfs.get_metric_dict(), kind='std_err')\n\nplt.title('Sequential Forward Selection (w. StdDev)')\nplt.grid()\nplt.show()\n\n\n\n\nFeatures: 13/13\n\n\n\n\n\nExample 6 -- Using the Selected Feature Subset For Making New Predictions\n\n\n# Initialize the dataset\n\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.datasets import load_iris\nfrom sklearn.cross_validation import train_test_split\n\niris = load_iris()\nX, y = iris.data, iris.target\nX_train, X_test, y_train, y_test = train_test_split(\n         X, y, test_size=0.33, random_state=1)\n\nknn = KNeighborsClassifier(n_neighbors=4)\n\n\n\n\n# Select the \"best\" three features via\n# 5-fold cross-validation on the training set.\n\nfrom mlxtend.feature_selection import SequentialFeatureSelector as SFS\n\nsfs1 = SFS(knn, \n           k_features=3, \n           forward=True, \n           floating=False, \n           scoring='accuracy',\n           cv=5)\nsfs1 = sfs1.fit(X_train, y_train)\n\n\n\n\nFeatures: 3/3\n\n\n\nprint('Selected features:', sfs1.k_feature_idx_)\n\n\n\n\nSelected features: (1, 2, 3)\n\n\n\n# Generate the new subsets based on the selected features\n# Note that the transform call is equivalent to\n# X_train[:, sfs1.k_feature_idx_]\n\nX_train_sfs = sfs1.transform(X_train)\nX_test_sfs = sfs1.transform(X_test)\n\n# Fit the estimator using the new feature subset\n# and make a prediction on the test data\nknn.fit(X_train_sfs, y_train)\ny_pred = knn.predict(X_test_sfs)\n\n# Compute the accuracy of the prediction\nacc = float((y_test == y_pred).sum()) / y_pred.shape[0]\nprint('Test set accuracy: %.2f %%' % (acc*100))\n\n\n\n\nTest set accuracy: 96.00 %\n\n\n\nAPI\n\n\nSequentialFeatureSelector(estimator, k_features, forward=True, floating=False, print_progress=True, scoring='accuracy', cv=5, skip_if_stuck=True, n_jobs=1, pre_dispatch='2\nn_jobs')*\n\n\nSequential Feature Selection for Classification and Regression.\n\n\nParameters\n\n\n\n\n\n\nestimator\n : scikit-learn classifier or regressor\n\n\n\n\n\n\nk_features\n : int\n\n\nNumber of features to select,\nwhere k_features < the full feature set.\n\n\n\n\n\n\nforward\n : bool (default: True)\n\n\nForward selection if True,\nbackward selection otherwise\n\n\n\n\n\n\nfloating\n : bool (default: False)\n\n\nAdds a conditional exclusion/inclusion if True.\n\n\n\n\n\n\nprint_progress\n : bool (default: True)\n\n\nPrints progress as the number of epochs\nto stderr.\n\n\n\n\n\n\nscoring\n : str, (default='accuracy')\n\n\nScoring metric in {accuracy, f1, precision, recall, roc_auc}\nfor classifiers,\n{'mean_absolute_error', 'mean_squared_error',\n'median_absolute_error', 'r2'} for regressors,\nor a callable object or function with\nsignature \nscorer(estimator, X, y)\n.\n\n\n\n\n\n\ncv\n : int (default: 5)\n\n\nScikit-learn cross-validation generator or \nint\n.\nIf estimator is a classifier (or y consists of integer class labels),\nstratified k-fold is performed, and regular k-fold cross-validation\notherwise.\nNo cross-validation if cv is None, False, or 0.\nskip_if_stuck: bool (default: True)\nSet to True to skip conditional\nexlusion/inclusion if floating=True and\nalgorithm gets stuck in cycles.\n\n\n\n\n\n\nn_jobs\n : int (default: 1)\n\n\nThe number of CPUs to use for cross validation. -1 means 'all CPUs'.\n\n\n\n\n\n\npre_dispatch\n : int, or string (default: '2*n_jobs')\n\n\nControls the number of jobs that get dispatched\nduring parallel execution in cross_val_score.\nReducing this number can be useful to avoid an explosion of\nmemory consumption when more jobs get dispatched than CPUs can process.\nThis parameter can be:\nNone, in which case all the jobs are immediately created and spawned.\nUse this for lightweight and fast-running jobs,\nto avoid delays due to on-demand spawning of the jobs\nAn int, giving the exact number of total jobs that are spawned\nA string, giving an expression as a function\nof n_jobs, as in \n'2*n_jobs'\n\n\n\n\n\n\nAttributes\n\n\n\n\n\n\nk_feature_idx_\n : array-like, shape = [n_predictions]\n\n\nFeature Indices of the selected feature subsets.\n\n\n\n\n\n\nk_score_\n : float\n\n\nCross validation average score of the selected subset.\n\n\n\n\n\n\nsubsets_\n : dict\n\n\nA dictionary of selected feature subsets during the\nsequential selection, where the dictionary keys are\nthe lenghts k of these feature subsets. The dictionary\nvalues are dictionaries themselves with the following\nkeys: 'feature_idx' (tuple of indices of the feature subset)\n'cv_scores' (list individual cross-validation scores)\n'avg_score' (average cross-validation score)\n\n\n\n\n\n\nMethods\n\n\n\n\n\nfit(X, y)\n\n\nNone\n\n\n\n\n\nfit_transform(X, y)\n\n\nNone\n\n\n\n\n\nget_metric_dict(confidence_interval=0.95)\n\n\nNone\n\n\n\n\n\nget_params(deep=True)\n\n\nGet parameters for this estimator.\n\n\nParameters\n\n\ndeep: boolean, optional\n    If True, will return the parameters for this estimator and\n    contained subobjects that are estimators.\n\n\nReturns\n\n\n\n\n\n\nparams\n : mapping of string to any\n\n\nParameter names mapped to their values.\n\n\n\n\n\n\n\n\n\nset_params(\nparams)\n\n\nSet the parameters of this estimator.\n\n\nThe method works on simple estimators as well as on nested objects\n(such as pipelines). The former have parameters of the form\n``<component>__<parameter>`` so that it's possible to update each\ncomponent of a nested object.\n\n\n\nReturns\n\n\nself\n\n\n\n\n\ntransform(X)\n\n\nNone\n\n\n\n\nplot_sequential_feature_selection(metric_dict, kind='std_dev', color='blue', bcolor='steelblue', marker='o', alpha=0.2, ylabel='Performance', confidence_interval=0.95)\n\n\nPlot sequential feature selection.\n\n\nParameters\n\n\n\n\n\n\nmetric_dict\n : mlxtend.SequentialFeatureSelector.get_metric_dict() object\n\n\n\n\n\n\nkind\n : str (default: \"std_dev\")\n\n\nThe kind of error bar or confidence interval in\n{'std_dev', 'std_err', 'ci', None}.\n\n\n\n\n\n\ncolor\n : str (default: \"blue\")\n\n\nColor of the lineplot (accepts any matplotlib color name)\n\n\n\n\n\n\nbcolor\n : str (default: \"steelblue\").\n\n\nColor of the error bars / confidence intervals\n(accepts any matplotlib color name).\n\n\n\n\n\n\nmarker\n : str (default: \"o\")\n\n\nMarker of the line plot\n(accepts any matplotlib marker name).\n\n\n\n\n\n\nalpha\n : float in [0, 1] (default: 0.2)\n\n\nTransparency of the error bars / confidence intervals.\n\n\n\n\n\n\nylabel\n : str (default: \"Performance\")\n\n\nY-axis label.\n\n\n\n\n\n\nconfidence_interval\n : float (default: 0.95)\n\n\nConfidence level if \nkind='ci'\n.\n\n\n\n\n\n\nReturns\n\n\n\n\nfig\n : matplotlib.pyplot.figure() object",
            "title": "SequentialFeatureSelector"
        },
        {
            "location": "/user_guide/feature_selection/SequentialFeatureSelector/#sequential-feature-selector",
            "text": "Implementation of  sequential feature algorithms  (SFAs) -- greedy search algorithms -- that have been developed as a suboptimal solution to the computationally often not feasible exhaustive search.   from mlxtend.feature_selection import SequentialFeatureSelector",
            "title": "Sequential Feature Selector"
        },
        {
            "location": "/user_guide/feature_selection/SequentialFeatureSelector/#overview",
            "text": "Sequential feature selection algorithms are a family of greedy search algorithms that are used to reduce an initial  d -dimensional feature space to a  k -dimensional feature subspace where  k < d . The motivation behind feature selection algorithms is to automatically select a subset of features that are most relevant to the problem to improve computational efficiency or reduce the generalization error of the model by removing irrelevant features or noise, which can be useful for algorithms that don't support regularization.   In a nutshell, SFAs remove or add one feature at the time based on the classifier performance until a feature subset of the desired size  k  is reached. There are 4 different flavors of SFAs available via the  SequentialFeatureSelector :   Sequential Forward Selection (SFS)  Sequential Backward Selection (SBS)  Sequential Floating Forward Selection (SFFS)  Sequential Floating Backward Selection (SFBS)   The  floating  variants, SFFS and SFBS, can be considered as extensions to the simpler SFS and SBS algorithms. The floating algorithms have an additional exclusion or inclusion step to remove features once they were included (or excluded), so that a larger number of feature subset combinations can be sampled. It is important to emphasize that this step is conditional and only occurs if the resulting feature subset is assessed as \"better\" by the criterion function after removal (or addition) of a particular feature. Furthermore, I added an optional check to skip the conditional exclusion steps if the algorithm gets stuck in cycles.     How is this different from  Recursive Feature Elimination  (RFE)  -- e.g., as implemented in  sklearn.feature_selection.RFE ? RFE is computationally less complex using the feature's weight coefficients (e.g., linear models) or feature importances (tree-based algorithms) to eliminate features recursively, whereas SFSs eliminate (or add) features based on a user-defined classifier/regression performance metric.   The SFAs  are outlined in pseudo code below:",
            "title": "Overview"
        },
        {
            "location": "/user_guide/feature_selection/SequentialFeatureSelector/#sequential-forward-selection-sfs",
            "text": "Input:   $Y = \\{y_1, y_2, ..., y_d\\}$      The  SFS  algorithm takes the whole  $d$ -dimensional feature set as input.   Output:   $X_k = \\{x_j \\; | \\;j = 1, 2, ..., k; \\; x_j \\in Y\\}$ , where  $k = (0, 1, 2, ..., d)$   SFS returns a subset of features; the number of selected features  $k$ , where  $k < d$ , has to be specified  a priori .   Initialization:   $X_0 = \\emptyset$ ,  $k = 0$   We initialize the algorithm with an empty set  $\\emptyset$  (\"null set\") so that  $k = 0$  (where  $k$  is the size of the subset).   Step 1 (Inclusion):     $x^+ = \\text{ arg max } J(x_k + x), \\text{ where }  x \\in Y - X_k$  $X_k+1 = X_k + x^+$  $k = k + 1$     Go to Step 1     in this step, we add an additional feature,  $x^+$ , to our feature subset  $X_k$ .  $x^+$  is the feature that maximizes our criterion function, that is, the feature that is associated with the best classifier performance if it is added to  $X_k$ .  We repeat this procedure until the termination criterion is satisfied.   Termination:   $k = p$   We add features from the feature subset  $X_k$  until the feature subset of size  $k$  contains the number of desired features  $p$  that we specified  a priori .",
            "title": "Sequential Forward Selection (SFS)"
        },
        {
            "location": "/user_guide/feature_selection/SequentialFeatureSelector/#sequential-floating-backward-sbs",
            "text": "Input:  the set of all features,  $Y = \\{y_1, y_2, ..., y_d\\}$      The SBS algorithm takes the whole feature set as input.   Output:   $X_k = \\{x_j \\; | \\;j = 1, 2, ..., k; \\; x_j \\in Y\\}$ , where  $k = (0, 1, 2, ..., d)$   SBS returns a subset of features; the number of selected features  $k$ , where  $k < d$ , has to be specified  a priori .   Initialization:   $X_0 = Y$ ,  $k = d$   We initialize the algorithm with the given feature set so that the  $k = d$ .   Step 1 (Exclusion):     $x^- = \\text{ arg max } J(x_k - x), \\text{  where } x \\in X_k$  $X_k-1 = X_k - x^-$  $k = k - 1$  Go to Step 1      In this step, we remove a feature,  $x^-$  from our feature subset  $X_k$ .  $x^-$  is the feature that maximizes our criterion function upon re,oval, that is, the feature that is associated with the best classifier performance if it is removed from  $X_k$ .  We repeat this procedure until the termination criterion is satisfied.   Termination:   $k = p$   We add features from the feature subset  $X_k$  until the feature subset of size  $k$  contains the number of desired features  $p$  that we specified  a priori .",
            "title": "Sequential Floating Backward (SBS)"
        },
        {
            "location": "/user_guide/feature_selection/SequentialFeatureSelector/#sequential-floating-backward-selection-sfbs",
            "text": "Input:  the set of all features,  $Y = \\{y_1, y_2, ..., y_d\\}$      The SFBS algorithm takes the whole feature set as input.   Output:   $X_k = \\{x_j \\; | \\;j = 1, 2, ..., k; \\; x_j \\in Y\\}$ , where  $k = (0, 1, 2, ..., d)$   SFBS returns a subset of features; the number of selected features  $k$ , where  $k < d$ , has to be specified  a priori .   Initialization:   $X_0 = Y$ ,  $k = d$   We initialize the algorithm with the given feature set so that the  $k = d$ .   Step 1 (Exclusion):     $x^- = \\text{ arg max } J(x_k - x), \\text{  where } x \\in X_k$  $X_k-1 = X_k - x^-$  $k = k - 1$  Go to Step 2      In this step, we remove a feature,  $x^-$  from our feature subset  $X_k$ .  $x^-$  is the feature that maximizes our criterion function upon re,oval, that is, the feature that is associated with the best classifier performance if it is removed from  $X_k$ .   Step 2 (Conditional Inclusion):   $x^+ = \\text{ arg max } J(x_k + x), \\text{ where } x \\in Y - X_k$  if J(x_k + x) > J(x_k + x) :   \n\u00a0\u00a0\u00a0\u00a0  $X_k+1 = X_k + x^+$ \n\u00a0\u00a0\u00a0\u00a0  $k = k + 1$  Go to Step 1      In Step 2, we search for features that improve the classifier performance if they are added back to the feature subset. If such features exist, we add the feature  $x^+$  for which the perfomance improvement is max.  Steps 1 and 2 are reapeated until the  Termination  criterion is reached.   Termination:   $k = p$   We add features from the feature subset  $X_k$  until the feature subset of size  $k$  contains the number of desired features  $p$  that we specified  a priori .",
            "title": "Sequential Floating Backward Selection (SFBS)"
        },
        {
            "location": "/user_guide/feature_selection/SequentialFeatureSelector/#sequential-floating-forward-selection-sffs",
            "text": "Input:  the set of all features,  $Y = \\{y_1, y_2, ..., y_d\\}$      The  SFFS  algorithm takes the whole feature set as input, if our feature space consists of, e.g. 10, if our feature space consists of 10 dimensions ( d = 10 ).   Output:  a subset of features,  $X_k = \\{x_j \\; | \\;j = 1, 2, ..., k; \\; x_j \\in Y\\}$ , where  $k = (0, 1, 2, ..., d)$   The returned output of the algorithm is a subset of the feature space of a specified size. E.g., a subset of 5 features from a 10-dimensional feature space ( k = 5, d = 10 ).   Initialization:   $X_0 = Y$ ,  $k = d$   We initialize the algorithm with an empty set (\"null set\") so that the  k = 0  (where  k  is the size of the subset)   Step 1 (Inclusion):  \n\u00a0\u00a0\u00a0\u00a0  $x^+ = \\text{ arg max } J(x_k + x), \\text{ where }  x \\in Y - X_k$ \n\u00a0\u00a0\u00a0\u00a0  $X_k+1 = X_k + x^+$ \n\u00a0\u00a0\u00a0\u00a0  $k = k + 1$    \n\u00a0\u00a0\u00a0\u00a0 Go to Step 2     Step 2 (Conditional Exclusion):  \n\u00a0\u00a0\u00a0\u00a0  $x^- = \\text{ arg max } J(x_k - x), \\text{ where } x \\in X_k$ \n\u00a0\u00a0\u00a0\u00a0 $if \\; J(x_k - x) > J(x_k - x)$ :   \n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0  $X_k-1 = X_k - x^- $ \n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0  $k = k - 1$    \n\u00a0\u00a0\u00a0\u00a0 Go to Step 1      In step 1, we include the feature from the  feature space  that leads to the best performance increase for our  feature subset  (assessed by the  criterion function ). Then, we go over to step 2  In step 2, we only remove a feature if the resulting subset would gain an increase in performance. We go back to step 1.    Steps 1 and 2 are reapeated until the  Termination  criterion is reached.   Termination:  stop when  k  equals the number of desired features",
            "title": "Sequential Floating Forward Selection (SFFS)"
        },
        {
            "location": "/user_guide/feature_selection/SequentialFeatureSelector/#references",
            "text": "Ferri, F., et al.  \"Comparative study of techniques for large-scale feature selection.\"  Pattern Recognition in Practice IV (1994): 403-413.",
            "title": "References"
        },
        {
            "location": "/user_guide/feature_selection/SequentialFeatureSelector/#examples",
            "text": "",
            "title": "Examples"
        },
        {
            "location": "/user_guide/feature_selection/SequentialFeatureSelector/#example-1-a-simple-sequential-forward-selection-example",
            "text": "Initializing a simple classifier from scikit-learn:  from sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.datasets import load_iris\niris = load_iris()\nX = iris.data\ny = iris.target\nknn = KNeighborsClassifier(n_neighbors=4)  We start by selection the \"best\" 3 features from the Iris dataset via Sequential Forward Selection (SFS). Here, we set  forward=True  and  floating=False . By choosing  cv=0 , we don't perform any cross-validation, therefore, the performance (here:  'accuracy' ) is computed entirely on the training set.   from mlxtend.feature_selection import SequentialFeatureSelector as SFS\n\nsfs1 = SFS(knn, \n           k_features=3, \n           forward=True, \n           floating=False, \n           scoring='accuracy',\n           cv=0)\nsfs1 = sfs1.fit(X, y)  Features: 3/3  Via the  subsets_  attribute, we can take a look at the selected feature indices at each step:  sfs1.subsets_  {1: {'avg_score': 0.95999999999999996,\n  'cv_scores': array([ 0.96]),\n  'feature_idx': (3,)},\n 2: {'avg_score': 0.97333333333333338,\n  'cv_scores': array([ 0.97333333]),\n  'feature_idx': (2, 3)},\n 3: {'avg_score': 0.97333333333333338,\n  'cv_scores': array([ 0.97333333]),\n  'feature_idx': (1, 2, 3)}}  Furthermore, we can access the indices of the 3 best features directly via the  k_feature_idx_  attribute:  sfs1.k_feature_idx_  (1, 2, 3)  Finally, the prediction score for these 3 features can be accesses via  k_score_ :  sfs1.k_score_  0.97333333333333338",
            "title": "Example 1 - A simple Sequential Forward Selection example"
        },
        {
            "location": "/user_guide/feature_selection/SequentialFeatureSelector/#example-2-toggling-between-sfs-sbs-sffs-and-sfbs",
            "text": "Using the  forward  and  floating  parameters, we can toggle between SFS, SBS, SFFS, and SFBS as shown below. Note that we are performing (stratified) 4-fold cross-validation for more robust estimates in contrast to Example 1. Via  n_jobs=-1 , we choose to run the cross-validation on all our available CPU cores.  # Sequential Forward Selection\nsfs = SFS(knn, \n          k_features=3, \n          forward=True, \n          floating=False, \n          scoring='accuracy',\n          print_progress=False,\n          cv=4,\n          n_jobs=-1)\nsfs = sfs.fit(X, y)\n\nprint('\\nSequential Forward Selection (k=3):')\nprint(sfs.k_feature_idx_)\nprint('CV Score:')\nprint(sfs.k_score_)\n\n###################################################\n\n# Sequential Backward Selection\nsbs = SFS(knn, \n          k_features=3, \n          forward=False, \n          floating=False, \n          scoring='accuracy',\n          print_progress=False,\n          cv=4,\n          n_jobs=-1)\nsbs = sbs.fit(X, y)\n\nprint('\\nSequential Backward Selection (k=3):')\nprint(sbs.k_feature_idx_)\nprint('CV Score:')\nprint(sbs.k_score_)\n\n###################################################\n\n# Sequential Floating Forward Selection\nsffs = SFS(knn, \n           k_features=3, \n           forward=True, \n           floating=True, \n           scoring='accuracy',\n           print_progress=False,\n           cv=4,\n           n_jobs=-1)\nsffs = sffs.fit(X, y)\n\nprint('\\nSequential Floating Forward Selection (k=3):')\nprint(sffs.k_feature_idx_)\nprint('CV Score:')\nprint(sffs.k_score_)\n\n###################################################\n\n# Sequential Floating Backward Selection\nsfbs = SFS(knn, \n           k_features=3, \n           forward=False, \n           floating=True, \n           scoring='accuracy',\n           print_progress=False,\n           cv=4,\n           n_jobs=-1)\nsfbs = sfbs.fit(X, y)\n\nprint('\\nSequential Floating Backward Selection (k=3):')\nprint(sfbs.k_feature_idx_)\nprint('CV Score:')\nprint(sfbs.k_score_)  Sequential Forward Selection (k=3):\n(1, 2, 3)\nCV Score:\n0.972756410256\n\nSequential Backward Selection (k=3):\n(1, 2, 3)\nCV Score:\n0.972756410256\n\nSequential Floating Forward Selection (k=3):\n(1, 2, 3)\nCV Score:\n0.972756410256\n\nSequential Floating Backward Selection (k=3):\n(1, 2, 3)\nCV Score:\n0.972756410256  In this simple scenario, selecting the best 3 features out of the 4 available features in the Iris set, we end up with similar results regardless of which sequential selection algorithms we used.",
            "title": "Example 2 - Toggling between SFS, SBS, SFFS, and SFBS"
        },
        {
            "location": "/user_guide/feature_selection/SequentialFeatureSelector/#example-3-visualizing-the-results-in-dataframes",
            "text": "For our convenience, we can visualize the output from the feature selection in a pandas DataFrame format using the  get_metric_dict  method of the SequentialFeatureSelector object. The columns  std_dev  and  std_err  represent the standard deviation and standard errors of the cross-validation scores, respectively.  Below, we see the DataFrame of the Sequential Forward Selector from Example 2:  import pandas as pd\npd.DataFrame.from_dict(sfs.get_metric_dict()).T   \n   \n     \n       \n       avg_score \n       ci_bound \n       cv_scores \n       feature_idx \n       std_dev \n       std_err \n     \n   \n   \n     \n       1 \n       0.952991 \n       0.0660624 \n       [0.974358974359, 0.948717948718, 0.88888888888... \n       (3,) \n       0.0412122 \n       0.0237939 \n     \n     \n       2 \n       0.959936 \n       0.0494801 \n       [0.974358974359, 0.948717948718, 0.91666666666... \n       (2, 3) \n       0.0308676 \n       0.0178214 \n     \n     \n       3 \n       0.972756 \n       0.0315204 \n       [0.974358974359, 1.0, 0.944444444444, 0.972222... \n       (1, 2, 3) \n       0.0196636 \n       0.0113528 \n     \n      Now, let's compare it to the Sequential Backward Selector:  pd.DataFrame.from_dict(sbs.get_metric_dict()).T   \n   \n     \n       \n       avg_score \n       ci_bound \n       cv_scores \n       feature_idx \n       std_dev \n       std_err \n     \n   \n   \n     \n       3 \n       0.972756 \n       0.0315204 \n       [0.974358974359, 1.0, 0.944444444444, 0.972222... \n       (1, 2, 3) \n       0.0196636 \n       0.0113528 \n     \n     \n       4 \n       0.952991 \n       0.0372857 \n       [0.974358974359, 0.948717948718, 0.91666666666... \n       (0, 1, 2, 3) \n       0.0232602 \n       0.0134293 \n     \n      We can see that both SFS and SFBS found the same \"best\" 3 features, however, the intermediate steps where obviously different.  The  ci_bound  column in the DataFrames above represents the confidence interval around the computed cross-validation scores. By default, a confidence interval of 95% is used, but we can use different confidence bounds via the  confidence_interval  parameter. E.g., the confidence bounds for a 90% confidence interval can be obtained as follows:  pd.DataFrame.from_dict(sbs.get_metric_dict(confidence_interval=0.90)).T   \n   \n     \n       \n       avg_score \n       ci_bound \n       cv_scores \n       feature_idx \n       std_dev \n       std_err \n     \n   \n   \n     \n       3 \n       0.972756 \n       0.0242024 \n       [0.974358974359, 1.0, 0.944444444444, 0.972222... \n       (1, 2, 3) \n       0.0196636 \n       0.0113528 \n     \n     \n       4 \n       0.952991 \n       0.0286292 \n       [0.974358974359, 0.948717948718, 0.91666666666... \n       (0, 1, 2, 3) \n       0.0232602 \n       0.0134293",
            "title": "Example 3 - Visualizing the results in DataFrames"
        },
        {
            "location": "/user_guide/feature_selection/SequentialFeatureSelector/#example-4-plotting-the-results",
            "text": "After importing the little helper function  plot_sequential_feature_selection , we can also visualize the results using matplotlib figures.  from mlxtend.feature_selection import plot_sequential_feature_selection as plot_sfs\nimport matplotlib.pyplot as plt\n\nsfs = SFS(knn, \n          k_features=4, \n          forward=True, \n          floating=False, \n          scoring='accuracy',\n          cv=5)\n\nsfs = sfs.fit(X, y)\n\nfig1 = plot_sfs(sfs.get_metric_dict(), kind='std_dev')\n\nplt.ylim([0.8, 1])\nplt.title('Sequential Forward Selection (w. StdDev)')\nplt.grid()\nplt.show()  Features: 4/4",
            "title": "Example 4 - Plotting the results"
        },
        {
            "location": "/user_guide/feature_selection/SequentialFeatureSelector/#example-5-sequential-feature-selection-for-regression",
            "text": "Similar to the classification examples above, the  SequentialFeatureSelector  also supports scikit-learn's estimators\nfor regression.  from sklearn.linear_model import LinearRegression\nfrom sklearn.datasets import load_boston\n\nboston = load_boston()\nX, y = boston.data, boston.target\n\nlr = LinearRegression()\n\nsfs = SFS(lr, \n          k_features=13, \n          forward=True, \n          floating=False, \n          scoring='mean_squared_error',\n          cv=10)\n\nsfs = sfs.fit(X, y)\nfig = plot_sfs(sfs.get_metric_dict(), kind='std_err')\n\nplt.title('Sequential Forward Selection (w. StdDev)')\nplt.grid()\nplt.show()  Features: 13/13",
            "title": "Example 5 - Sequential Feature Selection for Regression"
        },
        {
            "location": "/user_guide/feature_selection/SequentialFeatureSelector/#example-6-using-the-selected-feature-subset-for-making-new-predictions",
            "text": "# Initialize the dataset\n\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.datasets import load_iris\nfrom sklearn.cross_validation import train_test_split\n\niris = load_iris()\nX, y = iris.data, iris.target\nX_train, X_test, y_train, y_test = train_test_split(\n         X, y, test_size=0.33, random_state=1)\n\nknn = KNeighborsClassifier(n_neighbors=4)  # Select the \"best\" three features via\n# 5-fold cross-validation on the training set.\n\nfrom mlxtend.feature_selection import SequentialFeatureSelector as SFS\n\nsfs1 = SFS(knn, \n           k_features=3, \n           forward=True, \n           floating=False, \n           scoring='accuracy',\n           cv=5)\nsfs1 = sfs1.fit(X_train, y_train)  Features: 3/3  print('Selected features:', sfs1.k_feature_idx_)  Selected features: (1, 2, 3)  # Generate the new subsets based on the selected features\n# Note that the transform call is equivalent to\n# X_train[:, sfs1.k_feature_idx_]\n\nX_train_sfs = sfs1.transform(X_train)\nX_test_sfs = sfs1.transform(X_test)\n\n# Fit the estimator using the new feature subset\n# and make a prediction on the test data\nknn.fit(X_train_sfs, y_train)\ny_pred = knn.predict(X_test_sfs)\n\n# Compute the accuracy of the prediction\nacc = float((y_test == y_pred).sum()) / y_pred.shape[0]\nprint('Test set accuracy: %.2f %%' % (acc*100))  Test set accuracy: 96.00 %",
            "title": "Example 6 -- Using the Selected Feature Subset For Making New Predictions"
        },
        {
            "location": "/user_guide/feature_selection/SequentialFeatureSelector/#api",
            "text": "SequentialFeatureSelector(estimator, k_features, forward=True, floating=False, print_progress=True, scoring='accuracy', cv=5, skip_if_stuck=True, n_jobs=1, pre_dispatch='2 n_jobs')*  Sequential Feature Selection for Classification and Regression.  Parameters    estimator  : scikit-learn classifier or regressor    k_features  : int  Number of features to select,\nwhere k_features < the full feature set.    forward  : bool (default: True)  Forward selection if True,\nbackward selection otherwise    floating  : bool (default: False)  Adds a conditional exclusion/inclusion if True.    print_progress  : bool (default: True)  Prints progress as the number of epochs\nto stderr.    scoring  : str, (default='accuracy')  Scoring metric in {accuracy, f1, precision, recall, roc_auc}\nfor classifiers,\n{'mean_absolute_error', 'mean_squared_error',\n'median_absolute_error', 'r2'} for regressors,\nor a callable object or function with\nsignature  scorer(estimator, X, y) .    cv  : int (default: 5)  Scikit-learn cross-validation generator or  int .\nIf estimator is a classifier (or y consists of integer class labels),\nstratified k-fold is performed, and regular k-fold cross-validation\notherwise.\nNo cross-validation if cv is None, False, or 0.\nskip_if_stuck: bool (default: True)\nSet to True to skip conditional\nexlusion/inclusion if floating=True and\nalgorithm gets stuck in cycles.    n_jobs  : int (default: 1)  The number of CPUs to use for cross validation. -1 means 'all CPUs'.    pre_dispatch  : int, or string (default: '2*n_jobs')  Controls the number of jobs that get dispatched\nduring parallel execution in cross_val_score.\nReducing this number can be useful to avoid an explosion of\nmemory consumption when more jobs get dispatched than CPUs can process.\nThis parameter can be:\nNone, in which case all the jobs are immediately created and spawned.\nUse this for lightweight and fast-running jobs,\nto avoid delays due to on-demand spawning of the jobs\nAn int, giving the exact number of total jobs that are spawned\nA string, giving an expression as a function\nof n_jobs, as in  '2*n_jobs'    Attributes    k_feature_idx_  : array-like, shape = [n_predictions]  Feature Indices of the selected feature subsets.    k_score_  : float  Cross validation average score of the selected subset.    subsets_  : dict  A dictionary of selected feature subsets during the\nsequential selection, where the dictionary keys are\nthe lenghts k of these feature subsets. The dictionary\nvalues are dictionaries themselves with the following\nkeys: 'feature_idx' (tuple of indices of the feature subset)\n'cv_scores' (list individual cross-validation scores)\n'avg_score' (average cross-validation score)",
            "title": "API"
        },
        {
            "location": "/user_guide/feature_selection/SequentialFeatureSelector/#methods",
            "text": "fit(X, y)  None   fit_transform(X, y)  None   get_metric_dict(confidence_interval=0.95)  None   get_params(deep=True)  Get parameters for this estimator.  Parameters  deep: boolean, optional\n    If True, will return the parameters for this estimator and\n    contained subobjects that are estimators.  Returns    params  : mapping of string to any  Parameter names mapped to their values.     set_params( params)  Set the parameters of this estimator.  The method works on simple estimators as well as on nested objects\n(such as pipelines). The former have parameters of the form\n``<component>__<parameter>`` so that it's possible to update each\ncomponent of a nested object.  Returns  self   transform(X)  None   plot_sequential_feature_selection(metric_dict, kind='std_dev', color='blue', bcolor='steelblue', marker='o', alpha=0.2, ylabel='Performance', confidence_interval=0.95)  Plot sequential feature selection.  Parameters    metric_dict  : mlxtend.SequentialFeatureSelector.get_metric_dict() object    kind  : str (default: \"std_dev\")  The kind of error bar or confidence interval in\n{'std_dev', 'std_err', 'ci', None}.    color  : str (default: \"blue\")  Color of the lineplot (accepts any matplotlib color name)    bcolor  : str (default: \"steelblue\").  Color of the error bars / confidence intervals\n(accepts any matplotlib color name).    marker  : str (default: \"o\")  Marker of the line plot\n(accepts any matplotlib marker name).    alpha  : float in [0, 1] (default: 0.2)  Transparency of the error bars / confidence intervals.    ylabel  : str (default: \"Performance\")  Y-axis label.    confidence_interval  : float (default: 0.95)  Confidence level if  kind='ci' .    Returns   fig  : matplotlib.pyplot.figure() object",
            "title": "Methods"
        },
        {
            "location": "/user_guide/evaluate/confusion_matrix/",
            "text": "Confusion Matrix\n\n\nFunctions for generating confusion matrices.\n\n\n\n\nfrom mlxtend.evaluate import confusion_matrix\n  \n\n\nfrom mlxtend.evaluate import plot_confusion_matrix\n\n\n\n\nOverview\n\n\nConfusion Matrix\n\n\nThe \nconfusion matrix\n (or \nerror matrix\n) is one way to summarize the performance of a classifier for binary classification tasks. This square matrix consists of columns and rows that list the number of instances as absolute or relative \"actual class\" vs. \"predicted class\" ratios.\n\n\nLet \n$P$\n be the label of class 1 and \n$N$\n be the label of a second class or the label of all classes that are \nnot class 1\n in a multi-class setting.\n\n\n\n\nReferences\n\n\n\n\n-\n\n\n\n\nExamples\n\n\nExample 1 - Binary classification\n\n\nfrom mlxtend.evaluate import confusion_matrix\n\ny_target =    [0, 0, 1, 0, 0, 1, 1, 1]\ny_predicted = [1, 0, 1, 0, 0, 0, 0, 1]\n\ncm = confusion_matrix(y_target=y_target, y_predicted=y_predicted)\ncm\n\n\n\n\narray([[3, 1],\n       [2, 2]])\n\n\n\nimport matplotlib.pyplot as plt\nfrom mlxtend.evaluate import plot_confusion_matrix\n\nfig, ax = plot_confusion_matrix(conf_mat=cm)\nplt.show()\n\n\n\n\n\n\nExample 2 - Multi-class classification\n\n\nfrom mlxtend.evaluate import confusion_matrix\n\ny_target =    [1, 1, 1, 0, 0, 2, 0, 3]\ny_predicted = [1, 0, 1, 0, 0, 2, 1, 3]\n\ncm = confusion_matrix(y_target=y_target, y_predicted=y_predicted, binary=False)\ncm\n\n\n\n\narray([[2, 1, 0, 0],\n       [1, 2, 0, 0],\n       [0, 0, 1, 0],\n       [0, 0, 0, 1]])\n\n\n\nimport matplotlib.pyplot as plt\nfrom mlxtend.evaluate import confusion_matrix\n\nfig, ax = plot_confusion_matrix(conf_mat=cm)\nplt.show()\n\n\n\n\n\n\nExample 3 - Multi-class to binary\n\n\nBy setting \nbinary=True\n, all class labels that are not the positive class label are being summarized to class 0. The positive class label becomes class 1.\n\n\nimport matplotlib.pyplot as plt\nfrom mlxtend.evaluate import confusion_matrix\n\ny_target =    [1, 1, 1, 0, 0, 2, 0, 3]\ny_predicted = [1, 0, 1, 0, 0, 2, 1, 3]\n\ncm = confusion_matrix(y_target=y_target, y_predicted=y_predicted, binary=True, positive_label=1)\ncm\n\n\n\n\narray([[4, 1],\n       [1, 2]])\n\n\n\nfrom mlxtend.evaluate import plot_confusion_matrix\n\nfig, ax = plot_confusion_matrix(conf_mat=cm)\nplt.show()\n\n\n\n\n\n\nAPI\n\n\nconfusion_matrix(y_target, y_predicted, binary=False, positive_label=1)\n\n\nCompute a confusion matrix/contingency table.\n\n\nParameters\n\n\n\n\n\n\ny_target\n : array-like, shape=[n_samples]\n\n\nTrue class labels.\n\n\n\n\n\n\ny_predicted\n : array-like, shape=[n_samples]\n\n\nPredicted class labels.\n\n\n\n\n\n\nbinary\n : bool (default: False)\n\n\nMaps a multi-class problem onto a\nbinary confusion matrix, where\nthe positive class is 1 and\nall other classes are 0.\n\n\n\n\n\n\npositive_label\n : int (default: 1)\n\n\nClass label of the positive class.\n\n\n\n\n\n\nReturns\n\n\n\n\nmat\n : array-like, shape=[n_classes, n_classes]\n\n\n\n\n\n\nplot_confusion_matrix(conf_mat, hide_spines=False, hide_ticks=False, figsize=(2.5, 2.5), cmap=None, alpha=0.3)\n\n\nPlot a confusion matrix via matplotlib.\n\n\nParameters\n\n\n\n\n\n\nconf_mat\n : array-like, shape = [n_classes, n_classes]\n\n\nConfusion matrix from evaluate.confusion matrix.\n\n\n\n\n\n\nhide_spines\n : bool (default: False)\n\n\nHides axis spines if True.\n\n\n\n\n\n\nhide_ticks\n : bool (default: False)\n\n\nHides axis ticks if True\n\n\n\n\n\n\nfigsize\n : tuple (default: (2.5, 2.5))\n\n\nHeight and width of the figure\n\n\n\n\n\n\ncmap\n : matplotlib colormap (default: \nNone\n)\n\n\nUses matplotlib.pyplot.cm.Blues if \nNone\n\n\n\n\n\n\nReturns\n\n\n\n\n\n\nfig, ax\n : matplotlib.pyplot subplot objects\n\n\nFigure and axis elements of the subplot.",
            "title": "Confusion matrix"
        },
        {
            "location": "/user_guide/evaluate/confusion_matrix/#confusion-matrix",
            "text": "Functions for generating confusion matrices.   from mlxtend.evaluate import confusion_matrix     from mlxtend.evaluate import plot_confusion_matrix",
            "title": "Confusion Matrix"
        },
        {
            "location": "/user_guide/evaluate/confusion_matrix/#overview",
            "text": "",
            "title": "Overview"
        },
        {
            "location": "/user_guide/evaluate/confusion_matrix/#confusion-matrix_1",
            "text": "The  confusion matrix  (or  error matrix ) is one way to summarize the performance of a classifier for binary classification tasks. This square matrix consists of columns and rows that list the number of instances as absolute or relative \"actual class\" vs. \"predicted class\" ratios.  Let  $P$  be the label of class 1 and  $N$  be the label of a second class or the label of all classes that are  not class 1  in a multi-class setting.",
            "title": "Confusion Matrix"
        },
        {
            "location": "/user_guide/evaluate/confusion_matrix/#references",
            "text": "-",
            "title": "References"
        },
        {
            "location": "/user_guide/evaluate/confusion_matrix/#examples",
            "text": "",
            "title": "Examples"
        },
        {
            "location": "/user_guide/evaluate/confusion_matrix/#example-1-binary-classification",
            "text": "from mlxtend.evaluate import confusion_matrix\n\ny_target =    [0, 0, 1, 0, 0, 1, 1, 1]\ny_predicted = [1, 0, 1, 0, 0, 0, 0, 1]\n\ncm = confusion_matrix(y_target=y_target, y_predicted=y_predicted)\ncm  array([[3, 1],\n       [2, 2]])  import matplotlib.pyplot as plt\nfrom mlxtend.evaluate import plot_confusion_matrix\n\nfig, ax = plot_confusion_matrix(conf_mat=cm)\nplt.show()",
            "title": "Example 1 - Binary classification"
        },
        {
            "location": "/user_guide/evaluate/confusion_matrix/#example-2-multi-class-classification",
            "text": "from mlxtend.evaluate import confusion_matrix\n\ny_target =    [1, 1, 1, 0, 0, 2, 0, 3]\ny_predicted = [1, 0, 1, 0, 0, 2, 1, 3]\n\ncm = confusion_matrix(y_target=y_target, y_predicted=y_predicted, binary=False)\ncm  array([[2, 1, 0, 0],\n       [1, 2, 0, 0],\n       [0, 0, 1, 0],\n       [0, 0, 0, 1]])  import matplotlib.pyplot as plt\nfrom mlxtend.evaluate import confusion_matrix\n\nfig, ax = plot_confusion_matrix(conf_mat=cm)\nplt.show()",
            "title": "Example 2 - Multi-class classification"
        },
        {
            "location": "/user_guide/evaluate/confusion_matrix/#example-3-multi-class-to-binary",
            "text": "By setting  binary=True , all class labels that are not the positive class label are being summarized to class 0. The positive class label becomes class 1.  import matplotlib.pyplot as plt\nfrom mlxtend.evaluate import confusion_matrix\n\ny_target =    [1, 1, 1, 0, 0, 2, 0, 3]\ny_predicted = [1, 0, 1, 0, 0, 2, 1, 3]\n\ncm = confusion_matrix(y_target=y_target, y_predicted=y_predicted, binary=True, positive_label=1)\ncm  array([[4, 1],\n       [1, 2]])  from mlxtend.evaluate import plot_confusion_matrix\n\nfig, ax = plot_confusion_matrix(conf_mat=cm)\nplt.show()",
            "title": "Example 3 - Multi-class to binary"
        },
        {
            "location": "/user_guide/evaluate/confusion_matrix/#api",
            "text": "confusion_matrix(y_target, y_predicted, binary=False, positive_label=1)  Compute a confusion matrix/contingency table.  Parameters    y_target  : array-like, shape=[n_samples]  True class labels.    y_predicted  : array-like, shape=[n_samples]  Predicted class labels.    binary  : bool (default: False)  Maps a multi-class problem onto a\nbinary confusion matrix, where\nthe positive class is 1 and\nall other classes are 0.    positive_label  : int (default: 1)  Class label of the positive class.    Returns   mat  : array-like, shape=[n_classes, n_classes]    plot_confusion_matrix(conf_mat, hide_spines=False, hide_ticks=False, figsize=(2.5, 2.5), cmap=None, alpha=0.3)  Plot a confusion matrix via matplotlib.  Parameters    conf_mat  : array-like, shape = [n_classes, n_classes]  Confusion matrix from evaluate.confusion matrix.    hide_spines  : bool (default: False)  Hides axis spines if True.    hide_ticks  : bool (default: False)  Hides axis ticks if True    figsize  : tuple (default: (2.5, 2.5))  Height and width of the figure    cmap  : matplotlib colormap (default:  None )  Uses matplotlib.pyplot.cm.Blues if  None    Returns    fig, ax  : matplotlib.pyplot subplot objects  Figure and axis elements of the subplot.",
            "title": "API"
        },
        {
            "location": "/user_guide/evaluate/plot_decision_regions/",
            "text": "Plotting Decision Regions\n\n\nA function for plotting decision regions of classifiers in 1 or 2 dimensions.\n\n\n\n\nfrom mlxtend.evaluate import plot_decision_regions\n\n\n\n\nReferences\n\n\n\n\n\n\n\n\nExamples\n\n\nExample 1 - Decision regions in 2D\n\n\nfrom mlxtend.evaluate import plot_decision_regions\nimport matplotlib.pyplot as plt\nfrom sklearn import datasets\nfrom sklearn.svm import SVC\n\n# Loading some example data\niris = datasets.load_iris()\nX = iris.data[:, [0,2]]\ny = iris.target\n\n# Training a classifier\nsvm = SVC(C=0.5, kernel='linear')\nsvm.fit(X,y)\n\n# Plotting decision regions\nplot_decision_regions(X, y, clf=svm, res=0.02, legend=2)\n\n# Adding axes annotations\nplt.xlabel('sepal length [cm]')\nplt.ylabel('petal length [cm]')\nplt.title('SVM on Iris')\nplt.show()\n\n\n\n\n\n\nExample 2 - Decision regions in 1D\n\n\nfrom mlxtend.evaluate import plot_decision_regions\nimport matplotlib.pyplot as plt\nfrom sklearn import datasets\nfrom sklearn.svm import SVC\n\n# Loading some example data\niris = datasets.load_iris()\nX = iris.data[:, 2]\nX = X[:, None]\ny = iris.target\n\n# Training a classifier\nsvm = SVC(C=0.5, kernel='linear')\nsvm.fit(X,y)\n\n# Plotting decision regions\nplot_decision_regions(X, y, clf=svm, res=0.02, legend=2)\n\n# Adding axes annotations\nplt.xlabel('sepal length [cm]')\nplt.title('SVM on Iris')\n\nplt.show()\n\n\n\n\n\n\nExample 3 - Decision Region Grids\n\n\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.naive_bayes import GaussianNB \nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.svm import SVC\nfrom sklearn import datasets\nimport numpy as np\n\n# Initializing Classifiers\nclf1 = LogisticRegression(random_state=1)\nclf2 = RandomForestClassifier(random_state=1)\nclf3 = GaussianNB()\nclf4 = SVC()\n\n# Loading some example data\niris = datasets.load_iris()\nX = iris.data[:, [0,2]]\ny = iris.target\n\n\n\n\nimport matplotlib.pyplot as plt\nfrom mlxtend.evaluate import plot_decision_regions\nimport matplotlib.gridspec as gridspec\nimport itertools\ngs = gridspec.GridSpec(2, 2)\n\nfig = plt.figure(figsize=(10,8))\n\nfor clf, lab, grd in zip([clf1, clf2, clf3, clf4],\n                         ['Logistic Regression', 'Random Forest', 'Naive Bayes', 'SVM'],\n                         itertools.product([0, 1], repeat=2)):\n\n    clf.fit(X, y)\n    ax = plt.subplot(gs[grd[0], grd[1]])\n    fig = plot_decision_regions(X=X, y=y, clf=clf, legend=2)\n    plt.title(lab)\n\nplt.show()\n\n\n\n\n\n\nExample 4 - Highlighting Test Data Points\n\n\nfrom mlxtend.evaluate import plot_decision_regions\nfrom mlxtend.preprocessing import shuffle_arrays_unison\nimport matplotlib.pyplot as plt\nfrom sklearn import datasets\nfrom sklearn.svm import SVC\n\n\n# Loading some example data\niris = datasets.load_iris()\nX, y = iris.data[:, [0,2]], iris.target\nX, y = shuffle_arrays_unison(arrays=[X, y], random_seed=3)\n\nX_train, y_train = X[:100], y[:100]\nX_test, y_test = X[100:], y[100:]\n\n# Training a classifier\nsvm = SVC(C=0.5, kernel='linear')\nsvm.fit(X_train, y_train)\n\n# Plotting decision regions\nplot_decision_regions(X, y, clf=svm, res=0.02, legend=2, X_highlight=X_test)\n\n# Adding axes annotations\nplt.xlabel('sepal length [cm]')\nplt.ylabel('petal length [cm]')\nplt.title('SVM on Iris')\nplt.show()\n\n\n\n\n\n\nExample 5 - Evaluating Classifier Behavior on Non-Linear Problems\n\n\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.naive_bayes import GaussianNB \nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.svm import SVC\n\n# Initializing Classifiers\nclf1 = LogisticRegression(random_state=1)\nclf2 = RandomForestClassifier(n_estimators=100, random_state=1)\nclf3 = GaussianNB()\nclf4 = SVC()\n\n\n\n\n# Loading Plotting Utilities\nimport matplotlib.pyplot as plt\nimport matplotlib.gridspec as gridspec\nimport itertools\n#from mlxtend.evaluate import plot_decision_regions\nimport numpy as np\n\n\n\n\nXOR\n\n\nxx, yy = np.meshgrid(np.linspace(-3, 3, 50),\n                     np.linspace(-3, 3, 50))\nrng = np.random.RandomState(0)\nX = rng.randn(300, 2)\ny = np.array(np.logical_xor(X[:, 0] > 0, X[:, 1] > 0), dtype=int)\n\n\n\n\ngs = gridspec.GridSpec(2, 2)\n\nfig = plt.figure(figsize=(10,8))\n\nfor clf, lab, grd in zip([clf1, clf2, clf3, clf4],\n                         ['Logistic Regression', 'Random Forest', 'Naive Bayes', 'SVM'],\n                         itertools.product([0, 1], repeat=2)):\n\n    clf.fit(X, y)\n    ax = plt.subplot(gs[grd[0], grd[1]])\n    fig = plot_decision_regions(X=X, y=y, clf=clf, legend=2)\n    plt.title(lab)\n\nplt.show()\n\n\n\n\n\n\nHalf-Moons\n\n\nfrom sklearn.datasets import make_moons\nX, y = make_moons(n_samples=100, random_state=123)\n\ngs = gridspec.GridSpec(2, 2)\n\nfig = plt.figure(figsize=(10,8))\n\nfor clf, lab, grd in zip([clf1, clf2, clf3, clf4],\n                         ['Logistic Regression', 'Random Forest', 'Naive Bayes', 'SVM'],\n                         itertools.product([0, 1], repeat=2)):\n\n    clf.fit(X, y)\n    ax = plt.subplot(gs[grd[0], grd[1]])\n    fig = plot_decision_regions(X=X, y=y, clf=clf, legend=2)\n    plt.title(lab)\n\nplt.show()\n\n\n\n\n\n\nConcentric Circles\n\n\nfrom sklearn.datasets import make_circles\nX, y = make_circles(n_samples=1000, random_state=123, noise=0.1, factor=0.2)\n\ngs = gridspec.GridSpec(2, 2)\n\nfig = plt.figure(figsize=(10,8))\n\nfor clf, lab, grd in zip([clf1, clf2, clf3, clf4],\n                         ['Logistic Regression', 'Random Forest', 'Naive Bayes', 'SVM'],\n                         itertools.product([0, 1], repeat=2)):\n\n    clf.fit(X, y)\n    ax = plt.subplot(gs[grd[0], grd[1]])\n    fig = plot_decision_regions(X=X, y=y, clf=clf, legend=2)\n    plt.title(lab)\n\nplt.show()\n\n\n\n\n\n\nAPI\n\n\nplot_decision_regions(X, y, clf, ax=None, X_highlight=None, res=0.02, legend=1, hide_spines=True, markers='s^oxv<>', colors='red,blue,limegreen,gray,cyan')\n\n\nPlot decision regions of a classifier.\n\n\nParameters\n\n\n\n\n\n\nX\n : array-like, shape = [n_samples, n_features]\n\n\nFeature Matrix.\n\n\n\n\n\n\ny\n : array-like, shape = [n_samples]\n\n\nTrue class labels.\n\n\n\n\n\n\nclf\n : Classifier object.\n\n\nMust have a .predict method.\n\n\n\n\n\n\nax\n : matplotlib.axes.Axes (default: None)\n\n\nAn existing matplotlib Axes. Creates\none if ax=None.\n\n\n\n\n\n\nX_highlight\n : array-like, shape = [n_samples, n_features] (default: None)\n\n\nAn array with data points that are used to highlight samples in \nX\n.\n\n\n\n\n\n\nres\n : float (default: 0.02)\n\n\nGrid width. Lower values increase the resolution but\nslow down the plotting.\n\n\n\n\n\n\nhide_spines\n : bool (default: True)\n\n\nHide axis spines if True.\n\n\n\n\n\n\nlegend\n : int (default: 1)\n\n\nInteger to specify the legend location.\nNo legend if legend is 0.\n\n\n\n\n\n\nmarkers\n : list\n\n\nScatterplot markers.\n\n\n\n\n\n\ncolors\n : str (default 'red,blue,limegreen,gray,cyan')\n\n\nComma separated list of colors.\n\n\n\n\n\n\nReturns\n\n\n\n\nax\n : matplotlib.axes.Axes object",
            "title": "Plot decision regions"
        },
        {
            "location": "/user_guide/evaluate/plot_decision_regions/#plotting-decision-regions",
            "text": "A function for plotting decision regions of classifiers in 1 or 2 dimensions.   from mlxtend.evaluate import plot_decision_regions",
            "title": "Plotting Decision Regions"
        },
        {
            "location": "/user_guide/evaluate/plot_decision_regions/#references",
            "text": "",
            "title": "References"
        },
        {
            "location": "/user_guide/evaluate/plot_decision_regions/#examples",
            "text": "",
            "title": "Examples"
        },
        {
            "location": "/user_guide/evaluate/plot_decision_regions/#example-1-decision-regions-in-2d",
            "text": "from mlxtend.evaluate import plot_decision_regions\nimport matplotlib.pyplot as plt\nfrom sklearn import datasets\nfrom sklearn.svm import SVC\n\n# Loading some example data\niris = datasets.load_iris()\nX = iris.data[:, [0,2]]\ny = iris.target\n\n# Training a classifier\nsvm = SVC(C=0.5, kernel='linear')\nsvm.fit(X,y)\n\n# Plotting decision regions\nplot_decision_regions(X, y, clf=svm, res=0.02, legend=2)\n\n# Adding axes annotations\nplt.xlabel('sepal length [cm]')\nplt.ylabel('petal length [cm]')\nplt.title('SVM on Iris')\nplt.show()",
            "title": "Example 1 - Decision regions in 2D"
        },
        {
            "location": "/user_guide/evaluate/plot_decision_regions/#example-2-decision-regions-in-1d",
            "text": "from mlxtend.evaluate import plot_decision_regions\nimport matplotlib.pyplot as plt\nfrom sklearn import datasets\nfrom sklearn.svm import SVC\n\n# Loading some example data\niris = datasets.load_iris()\nX = iris.data[:, 2]\nX = X[:, None]\ny = iris.target\n\n# Training a classifier\nsvm = SVC(C=0.5, kernel='linear')\nsvm.fit(X,y)\n\n# Plotting decision regions\nplot_decision_regions(X, y, clf=svm, res=0.02, legend=2)\n\n# Adding axes annotations\nplt.xlabel('sepal length [cm]')\nplt.title('SVM on Iris')\n\nplt.show()",
            "title": "Example 2 - Decision regions in 1D"
        },
        {
            "location": "/user_guide/evaluate/plot_decision_regions/#example-3-decision-region-grids",
            "text": "from sklearn.linear_model import LogisticRegression\nfrom sklearn.naive_bayes import GaussianNB \nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.svm import SVC\nfrom sklearn import datasets\nimport numpy as np\n\n# Initializing Classifiers\nclf1 = LogisticRegression(random_state=1)\nclf2 = RandomForestClassifier(random_state=1)\nclf3 = GaussianNB()\nclf4 = SVC()\n\n# Loading some example data\niris = datasets.load_iris()\nX = iris.data[:, [0,2]]\ny = iris.target  import matplotlib.pyplot as plt\nfrom mlxtend.evaluate import plot_decision_regions\nimport matplotlib.gridspec as gridspec\nimport itertools\ngs = gridspec.GridSpec(2, 2)\n\nfig = plt.figure(figsize=(10,8))\n\nfor clf, lab, grd in zip([clf1, clf2, clf3, clf4],\n                         ['Logistic Regression', 'Random Forest', 'Naive Bayes', 'SVM'],\n                         itertools.product([0, 1], repeat=2)):\n\n    clf.fit(X, y)\n    ax = plt.subplot(gs[grd[0], grd[1]])\n    fig = plot_decision_regions(X=X, y=y, clf=clf, legend=2)\n    plt.title(lab)\n\nplt.show()",
            "title": "Example 3 - Decision Region Grids"
        },
        {
            "location": "/user_guide/evaluate/plot_decision_regions/#example-4-highlighting-test-data-points",
            "text": "from mlxtend.evaluate import plot_decision_regions\nfrom mlxtend.preprocessing import shuffle_arrays_unison\nimport matplotlib.pyplot as plt\nfrom sklearn import datasets\nfrom sklearn.svm import SVC\n\n\n# Loading some example data\niris = datasets.load_iris()\nX, y = iris.data[:, [0,2]], iris.target\nX, y = shuffle_arrays_unison(arrays=[X, y], random_seed=3)\n\nX_train, y_train = X[:100], y[:100]\nX_test, y_test = X[100:], y[100:]\n\n# Training a classifier\nsvm = SVC(C=0.5, kernel='linear')\nsvm.fit(X_train, y_train)\n\n# Plotting decision regions\nplot_decision_regions(X, y, clf=svm, res=0.02, legend=2, X_highlight=X_test)\n\n# Adding axes annotations\nplt.xlabel('sepal length [cm]')\nplt.ylabel('petal length [cm]')\nplt.title('SVM on Iris')\nplt.show()",
            "title": "Example 4 - Highlighting Test Data Points"
        },
        {
            "location": "/user_guide/evaluate/plot_decision_regions/#example-5-evaluating-classifier-behavior-on-non-linear-problems",
            "text": "from sklearn.linear_model import LogisticRegression\nfrom sklearn.naive_bayes import GaussianNB \nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.svm import SVC\n\n# Initializing Classifiers\nclf1 = LogisticRegression(random_state=1)\nclf2 = RandomForestClassifier(n_estimators=100, random_state=1)\nclf3 = GaussianNB()\nclf4 = SVC()  # Loading Plotting Utilities\nimport matplotlib.pyplot as plt\nimport matplotlib.gridspec as gridspec\nimport itertools\n#from mlxtend.evaluate import plot_decision_regions\nimport numpy as np",
            "title": "Example 5 - Evaluating Classifier Behavior on Non-Linear Problems"
        },
        {
            "location": "/user_guide/evaluate/plot_decision_regions/#xor",
            "text": "xx, yy = np.meshgrid(np.linspace(-3, 3, 50),\n                     np.linspace(-3, 3, 50))\nrng = np.random.RandomState(0)\nX = rng.randn(300, 2)\ny = np.array(np.logical_xor(X[:, 0] > 0, X[:, 1] > 0), dtype=int)  gs = gridspec.GridSpec(2, 2)\n\nfig = plt.figure(figsize=(10,8))\n\nfor clf, lab, grd in zip([clf1, clf2, clf3, clf4],\n                         ['Logistic Regression', 'Random Forest', 'Naive Bayes', 'SVM'],\n                         itertools.product([0, 1], repeat=2)):\n\n    clf.fit(X, y)\n    ax = plt.subplot(gs[grd[0], grd[1]])\n    fig = plot_decision_regions(X=X, y=y, clf=clf, legend=2)\n    plt.title(lab)\n\nplt.show()",
            "title": "XOR"
        },
        {
            "location": "/user_guide/evaluate/plot_decision_regions/#half-moons",
            "text": "from sklearn.datasets import make_moons\nX, y = make_moons(n_samples=100, random_state=123)\n\ngs = gridspec.GridSpec(2, 2)\n\nfig = plt.figure(figsize=(10,8))\n\nfor clf, lab, grd in zip([clf1, clf2, clf3, clf4],\n                         ['Logistic Regression', 'Random Forest', 'Naive Bayes', 'SVM'],\n                         itertools.product([0, 1], repeat=2)):\n\n    clf.fit(X, y)\n    ax = plt.subplot(gs[grd[0], grd[1]])\n    fig = plot_decision_regions(X=X, y=y, clf=clf, legend=2)\n    plt.title(lab)\n\nplt.show()",
            "title": "Half-Moons"
        },
        {
            "location": "/user_guide/evaluate/plot_decision_regions/#concentric-circles",
            "text": "from sklearn.datasets import make_circles\nX, y = make_circles(n_samples=1000, random_state=123, noise=0.1, factor=0.2)\n\ngs = gridspec.GridSpec(2, 2)\n\nfig = plt.figure(figsize=(10,8))\n\nfor clf, lab, grd in zip([clf1, clf2, clf3, clf4],\n                         ['Logistic Regression', 'Random Forest', 'Naive Bayes', 'SVM'],\n                         itertools.product([0, 1], repeat=2)):\n\n    clf.fit(X, y)\n    ax = plt.subplot(gs[grd[0], grd[1]])\n    fig = plot_decision_regions(X=X, y=y, clf=clf, legend=2)\n    plt.title(lab)\n\nplt.show()",
            "title": "Concentric Circles"
        },
        {
            "location": "/user_guide/evaluate/plot_decision_regions/#api",
            "text": "plot_decision_regions(X, y, clf, ax=None, X_highlight=None, res=0.02, legend=1, hide_spines=True, markers='s^oxv<>', colors='red,blue,limegreen,gray,cyan')  Plot decision regions of a classifier.  Parameters    X  : array-like, shape = [n_samples, n_features]  Feature Matrix.    y  : array-like, shape = [n_samples]  True class labels.    clf  : Classifier object.  Must have a .predict method.    ax  : matplotlib.axes.Axes (default: None)  An existing matplotlib Axes. Creates\none if ax=None.    X_highlight  : array-like, shape = [n_samples, n_features] (default: None)  An array with data points that are used to highlight samples in  X .    res  : float (default: 0.02)  Grid width. Lower values increase the resolution but\nslow down the plotting.    hide_spines  : bool (default: True)  Hide axis spines if True.    legend  : int (default: 1)  Integer to specify the legend location.\nNo legend if legend is 0.    markers  : list  Scatterplot markers.    colors  : str (default 'red,blue,limegreen,gray,cyan')  Comma separated list of colors.    Returns   ax  : matplotlib.axes.Axes object",
            "title": "API"
        },
        {
            "location": "/user_guide/evaluate/plot_learning_curves/",
            "text": "Plotting Learning Curves\n\n\nA function to plot learning curves for classifiers. Learning curves are extremely useful to analyze if a model is suffering from over- or under-fitting (high variance or high bias). The function can be imported via\n\n\n\n\nfrom mlxtend.evaluate import plot_learning_curves\n\n\n\n\nReferences\n\n\n-\n\n\nExamples\n\n\nExample 1\n\n\nfrom mlxtend.evaluate import plot_learning_curves\nimport matplotlib.pyplot as plt\nfrom mlxtend.data import iris_data\nfrom mlxtend.preprocessing import shuffle_arrays_unison\nfrom sklearn.neighbors import KNeighborsClassifier\nimport numpy as np\n\n\n# Loading some example data\nX, y = iris_data()\nX, y = shuffle_arrays_unison(arrays=[X, y], random_state=123)\nX_train, X_test = X[:100], X[100:]\ny_train, y_test = y[:100], y[100:]\n\nclf = KNeighborsClassifier(n_neighbors=5)\n\nplot_learning_curves(X_train, y_train, X_test, y_test, clf)\nplt.show()\n\n\n\n\n\n\nAPI\n\n\nplot_learning_curves(X_train, y_train, X_test, y_test, clf, train_marker='o', test_marker='^', scoring='misclassification error', suppress_plot=False, print_model=True, style='fivethirtyeight', legend_loc='best')\n\n\nPlots learning curves of a classifier.\n\n\nParameters\n\n\n\n\n\n\nX_train\n : array-like, shape = [n_samples, n_features]\n\n\nFeature matrix of the training dataset.\n\n\n\n\n\n\ny_train\n : array-like, shape = [n_samples]\n\n\nTrue class labels of the training dataset.\n\n\n\n\n\n\nX_test\n : array-like, shape = [n_samples, n_features]\n\n\nFeature matrix of the test dataset.\n\n\n\n\n\n\ny_test\n : array-like, shape = [n_samples]\n\n\nTrue class labels of the test dataset.\n\n\n\n\n\n\nclf\n : Classifier object. Must have a .predict .fit method.\n\n\n\n\n\n\ntrain_marker\n : str (default: 'o')\n\n\nMarker for the training set line plot.\n\n\n\n\n\n\ntest_marker\n : str (default: '^')\n\n\nMarker for the test set line plot.\n\n\n\n\n\n\nscoring\n : str (default: 'misclassification error')\n\n\nIf not 'misclassification error', accepts the following metrics\n(from scikit-learn):\n\n\n\n\n\n\nsuppress_plot=False\n : bool (default: False)\n\n\nSuppress matplotlib plots if True. Recommended\nfor testing purposes.\n\n\n\n\n\n\nprint_model\n : bool (default: True)\n\n\nPrint model parameters in plot title if True.\n\n\n\n\n\n\nstyle\n : str (default: 'fivethirtyeight')\n\n\nMatplotlib style\n\n\n\n\n\n\nlegend_loc\n : str (default: 'best')\n\n\nWhere to place the plot legend:\n\n\n\n\n\n\nReturns\n\n\n\n\nerrors\n : (training_error, test_error): tuple of lists",
            "title": "Plot learning curves"
        },
        {
            "location": "/user_guide/evaluate/plot_learning_curves/#plotting-learning-curves",
            "text": "A function to plot learning curves for classifiers. Learning curves are extremely useful to analyze if a model is suffering from over- or under-fitting (high variance or high bias). The function can be imported via   from mlxtend.evaluate import plot_learning_curves",
            "title": "Plotting Learning Curves"
        },
        {
            "location": "/user_guide/evaluate/plot_learning_curves/#references",
            "text": "-",
            "title": "References"
        },
        {
            "location": "/user_guide/evaluate/plot_learning_curves/#examples",
            "text": "",
            "title": "Examples"
        },
        {
            "location": "/user_guide/evaluate/plot_learning_curves/#example-1",
            "text": "from mlxtend.evaluate import plot_learning_curves\nimport matplotlib.pyplot as plt\nfrom mlxtend.data import iris_data\nfrom mlxtend.preprocessing import shuffle_arrays_unison\nfrom sklearn.neighbors import KNeighborsClassifier\nimport numpy as np\n\n\n# Loading some example data\nX, y = iris_data()\nX, y = shuffle_arrays_unison(arrays=[X, y], random_state=123)\nX_train, X_test = X[:100], X[100:]\ny_train, y_test = y[:100], y[100:]\n\nclf = KNeighborsClassifier(n_neighbors=5)\n\nplot_learning_curves(X_train, y_train, X_test, y_test, clf)\nplt.show()",
            "title": "Example 1"
        },
        {
            "location": "/user_guide/evaluate/plot_learning_curves/#api",
            "text": "plot_learning_curves(X_train, y_train, X_test, y_test, clf, train_marker='o', test_marker='^', scoring='misclassification error', suppress_plot=False, print_model=True, style='fivethirtyeight', legend_loc='best')  Plots learning curves of a classifier.  Parameters    X_train  : array-like, shape = [n_samples, n_features]  Feature matrix of the training dataset.    y_train  : array-like, shape = [n_samples]  True class labels of the training dataset.    X_test  : array-like, shape = [n_samples, n_features]  Feature matrix of the test dataset.    y_test  : array-like, shape = [n_samples]  True class labels of the test dataset.    clf  : Classifier object. Must have a .predict .fit method.    train_marker  : str (default: 'o')  Marker for the training set line plot.    test_marker  : str (default: '^')  Marker for the test set line plot.    scoring  : str (default: 'misclassification error')  If not 'misclassification error', accepts the following metrics\n(from scikit-learn):    suppress_plot=False  : bool (default: False)  Suppress matplotlib plots if True. Recommended\nfor testing purposes.    print_model  : bool (default: True)  Print model parameters in plot title if True.    style  : str (default: 'fivethirtyeight')  Matplotlib style    legend_loc  : str (default: 'best')  Where to place the plot legend:    Returns   errors  : (training_error, test_error): tuple of lists",
            "title": "API"
        },
        {
            "location": "/user_guide/evaluate/scoring/",
            "text": "Scoring\n\n\nA function for computing various different performance metrics.\n\n\n\n\nfrom mlxtend.evaluate import scoring\n\n\n\n\nOverview\n\n\nConfusion Matrix\n\n\nThe \nconfusion matrix\n (or \nerror matrix\n) is one way to summarize the performance of a classifier for binary classification tasks. This square matrix consists of columns and rows that list the number of instances as absolute or relative \"actual class\" vs. \"predicted class\" ratios.\n\n\nLet \n$P$\n be the label of class 1 and \n$N$\n be the label of a second class or the label of all classes that are \nnot class 1\n in a multi-class setting.\n\n\n\n\nError and Accuracy\n\n\nBoth the prediction \nerror\n (ERR) and \naccuracy\n (ACC) provide general information about how many samples are misclassified. The \nerror\n can be understood as the sum of all false predictions divided by the number of total predications, and the the accuracy is calculated as the sum of correct predictions divided by the total number of predictions, respectively. \n\n\n$$ERR = \\frac{FP + FN}{FP+ FN + TP + TN} = 1-ACC$$\n\n\n$$ACC = \\frac{TP + TN}{FP+ FN + TP + TN} = 1-ERR$$\n\n\nTrue and False Positive Rates\n\n\nThe \nTrue Positive Rate\n (TPR) and \nFalse Positive Rate\n (FPR) are performance metrics that are especially useful for imbalanced class problems. In \nspam classification\n, for example, we are of course primarily interested in the detection and filtering out of \nspam\n. However, it is also important to decrease the number of messages that were incorrectly classified as \nspam\n (\nFalse Positives\n): A situation where a person misses an important message is considered as \"worse\" than a situation where a person ends up with a few \nspam\n messages in his e-mail inbox. In contrast to the \nFPR\n, the \nTrue Positive Rate\n provides useful information about the fraction of \npositive\n (or \nrelevant\n) samples that were correctly identified out of the total pool of \nPositives\n.\n\n\n$$FPR = \\frac{FP}{N} =  \\frac{FP}{FP + TN}$$\n\n\n$$TPR = \\frac{TP}{P} =  \\frac{TP}{FN + TP}$$\n\n\nPrecision, Recall, and the F1-Score\n\n\nPrecision\n (PRE) and \nRecall\n (REC) are metrics that are more commonly used in \nInformation Technology\n and related to the \nFalse\n and \nTrue Prositive Rates\n. In fact, \nRecall\n is synonymous to the \nTrue Positive Rate\n and also sometimes called \nSensitivity\n. The F\n$_1$\n-Score can be understood as a combination of both \nPrecision\n and \nRecall\n.\n\n\n$$PRE = \\frac{TP}{TP + FP}$$\n\n\n$$REC = TPR = \\frac{TP}{P} =  \\frac{TP}{FN + TP}$$\n\n\n$$F_1 = 2 \\cdot \\frac{PRE \\cdot REC}{PRE + REC}$$\n\n\nSensitivity and Specificity\n\n\nSensitivity\n (SEN) is synonymous to \nRecall\n and the \nTrue Positive Rate\n whereas \nSpecificity (SPC)\n is synonymous to the \nTrue Negative Rate\n -- \nSensitivity\n measures the recovery rate of the \nPositives\n and complimentary, the \nSpecificity\n measures the recovery rate of the \nNegatives\n.\n\n\n$$SEN = TPR = REC = \\frac{TP}{P} =  \\frac{TP}{FN + TP}$$\n\n\n$$SPC = TNR =\\frac{TN}{N} =  \\frac{TN}{FP + TN}$$\n\n\nMatthews correlation coefficient\n\n\nMatthews correlation coefficient\n (MCC) was first formulated by Brian W. Matthews [3] in 1975 to assess the performance of protein secondary structure predictions. The MCC can be understood as a specific case of a linear correlation coefficient (\nPearson's R\n) for a binary classification setting and is considered as especially useful in unbalanced class settings.\nThe previous metrics take values in the range between 0 (worst) and 1 (best), whereas the MCC is bounded between the range 1 (perfect correlation between ground truth and predicted outcome) and -1 (inverse or negative correlation) -- a value of 0 denotes a random prediction. \n\n\n$$MCC = \\frac{ TP \\times TN - FP \\times FN } {\\sqrt{ (TP + FP) ( TP + FN ) ( TN + FP ) ( TN + FN ) } }$$\n\n\nReferences\n\n\n\n\n[1] S. Raschka. \nAn overview of general performance metrics of binary classifier systems\n. Computing Research Repository (CoRR), abs/1410.5330, 2014.\n\n\n[2] Cyril Goutte and Eric Gaussier. \nA probabilistic interpretation of precision, recall and f-score, with implication for evaluation\n. In Advances in Information Retrieval, pages 345\u2013359. Springer, 2005.\n\n\n[3] Brian W Matthews. \nComparison of the predicted and observed secondary structure of T4 phage lysozyme\n. Biochimica et Biophysica Acta (BBA)- Protein Structure, 405(2):442\u2013451, 1975.\n\n\n\n\nExamples\n\n\nExample 1 - Classification Error\n\n\nfrom mlxtend.evaluate import scoring\n\ny_targ = [1, 1, 1, 0, 0, 2, 0, 3]\ny_pred = [1, 0, 1, 0, 0, 2, 1, 3]\nres = scoring(y_target=y_targ, y_predicted=y_pred, metric='error')\n\nprint('Error: %s%%' % (res * 100))\n\n\n\n\nError: 25.0%\n\n\n\nAPI\n\n\nscoring(y_target, y_predicted, metric='error', positive_label=1)\n\n\nCompute a scoring metric for supervised learning.\n\n\nParameters\n\n\n\n\n\n\ny_target\n : array-like, shape=[n_values]\n\n\nTrue class labels or target values.\n\n\n\n\n\n\ny_predicted\n : array-like, shape=[n_values]\n\n\nPredicted class labels or target values.\n\n\n\n\n\n\nmetric\n : str (default: 'error')\n\n\nPerformance metric.\n[TP: True positives, TN = True negatives,\n\n\nTN: True negatives, FN = False negatives]\n\n\n'accuracy': (TP + TN)/(FP + FN + TP + TN) = 1-ERR\n\n\n'error': (TP + TN)/(FP+ FN + TP + TN) = 1-ACC\n\n\n'false_positive_rate': FP/N = FP/(FP + TN)\n\n\n'true_positive_rate': TP/P = TP/(FN + TP)\n\n\n'true_negative_rate': TN/N = TN/(FP + TN)\n\n\n'precision': TP/(TP + FP)\n\n\n'recall': equal to 'true_positive_rate'\n\n\n'sensitivity': equal to 'true_positive_rate' or 'recall'\n\n\n'specificity': equal to 'true_negative_rate'\n\n\n'f1': 2 * (PRE * REC)/(PRE + REC)\n\n\n'matthews_corr_coef':  (TP\nTN - FP\nFN)\n/ (sqrt{(TP + FP)( TP + FN )( TN + FP )( TN + FN )})\n\n\n\n\n\n\npositive_label\n : int (default: 1)\n\n\nLabel of the positive class for binary classification\nmetrics.\n\n\n\n\n\n\nReturns\n\n\n\n\nscore\n : float",
            "title": "Scoring"
        },
        {
            "location": "/user_guide/evaluate/scoring/#scoring",
            "text": "A function for computing various different performance metrics.   from mlxtend.evaluate import scoring",
            "title": "Scoring"
        },
        {
            "location": "/user_guide/evaluate/scoring/#overview",
            "text": "",
            "title": "Overview"
        },
        {
            "location": "/user_guide/evaluate/scoring/#confusion-matrix",
            "text": "The  confusion matrix  (or  error matrix ) is one way to summarize the performance of a classifier for binary classification tasks. This square matrix consists of columns and rows that list the number of instances as absolute or relative \"actual class\" vs. \"predicted class\" ratios.  Let  $P$  be the label of class 1 and  $N$  be the label of a second class or the label of all classes that are  not class 1  in a multi-class setting.",
            "title": "Confusion Matrix"
        },
        {
            "location": "/user_guide/evaluate/scoring/#error-and-accuracy",
            "text": "Both the prediction  error  (ERR) and  accuracy  (ACC) provide general information about how many samples are misclassified. The  error  can be understood as the sum of all false predictions divided by the number of total predications, and the the accuracy is calculated as the sum of correct predictions divided by the total number of predictions, respectively.   $$ERR = \\frac{FP + FN}{FP+ FN + TP + TN} = 1-ACC$$  $$ACC = \\frac{TP + TN}{FP+ FN + TP + TN} = 1-ERR$$",
            "title": "Error and Accuracy"
        },
        {
            "location": "/user_guide/evaluate/scoring/#true-and-false-positive-rates",
            "text": "The  True Positive Rate  (TPR) and  False Positive Rate  (FPR) are performance metrics that are especially useful for imbalanced class problems. In  spam classification , for example, we are of course primarily interested in the detection and filtering out of  spam . However, it is also important to decrease the number of messages that were incorrectly classified as  spam  ( False Positives ): A situation where a person misses an important message is considered as \"worse\" than a situation where a person ends up with a few  spam  messages in his e-mail inbox. In contrast to the  FPR , the  True Positive Rate  provides useful information about the fraction of  positive  (or  relevant ) samples that were correctly identified out of the total pool of  Positives .  $$FPR = \\frac{FP}{N} =  \\frac{FP}{FP + TN}$$  $$TPR = \\frac{TP}{P} =  \\frac{TP}{FN + TP}$$",
            "title": "True and False Positive Rates"
        },
        {
            "location": "/user_guide/evaluate/scoring/#precision-recall-and-the-f1-score",
            "text": "Precision  (PRE) and  Recall  (REC) are metrics that are more commonly used in  Information Technology  and related to the  False  and  True Prositive Rates . In fact,  Recall  is synonymous to the  True Positive Rate  and also sometimes called  Sensitivity . The F $_1$ -Score can be understood as a combination of both  Precision  and  Recall .  $$PRE = \\frac{TP}{TP + FP}$$  $$REC = TPR = \\frac{TP}{P} =  \\frac{TP}{FN + TP}$$  $$F_1 = 2 \\cdot \\frac{PRE \\cdot REC}{PRE + REC}$$",
            "title": "Precision, Recall, and the F1-Score"
        },
        {
            "location": "/user_guide/evaluate/scoring/#sensitivity-and-specificity",
            "text": "Sensitivity  (SEN) is synonymous to  Recall  and the  True Positive Rate  whereas  Specificity (SPC)  is synonymous to the  True Negative Rate  --  Sensitivity  measures the recovery rate of the  Positives  and complimentary, the  Specificity  measures the recovery rate of the  Negatives .  $$SEN = TPR = REC = \\frac{TP}{P} =  \\frac{TP}{FN + TP}$$  $$SPC = TNR =\\frac{TN}{N} =  \\frac{TN}{FP + TN}$$",
            "title": "Sensitivity and Specificity"
        },
        {
            "location": "/user_guide/evaluate/scoring/#matthews-correlation-coefficient",
            "text": "Matthews correlation coefficient  (MCC) was first formulated by Brian W. Matthews [3] in 1975 to assess the performance of protein secondary structure predictions. The MCC can be understood as a specific case of a linear correlation coefficient ( Pearson's R ) for a binary classification setting and is considered as especially useful in unbalanced class settings.\nThe previous metrics take values in the range between 0 (worst) and 1 (best), whereas the MCC is bounded between the range 1 (perfect correlation between ground truth and predicted outcome) and -1 (inverse or negative correlation) -- a value of 0 denotes a random prediction.   $$MCC = \\frac{ TP \\times TN - FP \\times FN } {\\sqrt{ (TP + FP) ( TP + FN ) ( TN + FP ) ( TN + FN ) } }$$",
            "title": "Matthews correlation coefficient"
        },
        {
            "location": "/user_guide/evaluate/scoring/#references",
            "text": "[1] S. Raschka.  An overview of general performance metrics of binary classifier systems . Computing Research Repository (CoRR), abs/1410.5330, 2014.  [2] Cyril Goutte and Eric Gaussier.  A probabilistic interpretation of precision, recall and f-score, with implication for evaluation . In Advances in Information Retrieval, pages 345\u2013359. Springer, 2005.  [3] Brian W Matthews.  Comparison of the predicted and observed secondary structure of T4 phage lysozyme . Biochimica et Biophysica Acta (BBA)- Protein Structure, 405(2):442\u2013451, 1975.",
            "title": "References"
        },
        {
            "location": "/user_guide/evaluate/scoring/#examples",
            "text": "",
            "title": "Examples"
        },
        {
            "location": "/user_guide/evaluate/scoring/#example-1-classification-error",
            "text": "from mlxtend.evaluate import scoring\n\ny_targ = [1, 1, 1, 0, 0, 2, 0, 3]\ny_pred = [1, 0, 1, 0, 0, 2, 1, 3]\nres = scoring(y_target=y_targ, y_predicted=y_pred, metric='error')\n\nprint('Error: %s%%' % (res * 100))  Error: 25.0%",
            "title": "Example 1 - Classification Error"
        },
        {
            "location": "/user_guide/evaluate/scoring/#api",
            "text": "scoring(y_target, y_predicted, metric='error', positive_label=1)  Compute a scoring metric for supervised learning.  Parameters    y_target  : array-like, shape=[n_values]  True class labels or target values.    y_predicted  : array-like, shape=[n_values]  Predicted class labels or target values.    metric  : str (default: 'error')  Performance metric.\n[TP: True positives, TN = True negatives,  TN: True negatives, FN = False negatives]  'accuracy': (TP + TN)/(FP + FN + TP + TN) = 1-ERR  'error': (TP + TN)/(FP+ FN + TP + TN) = 1-ACC  'false_positive_rate': FP/N = FP/(FP + TN)  'true_positive_rate': TP/P = TP/(FN + TP)  'true_negative_rate': TN/N = TN/(FP + TN)  'precision': TP/(TP + FP)  'recall': equal to 'true_positive_rate'  'sensitivity': equal to 'true_positive_rate' or 'recall'  'specificity': equal to 'true_negative_rate'  'f1': 2 * (PRE * REC)/(PRE + REC)  'matthews_corr_coef':  (TP TN - FP FN)\n/ (sqrt{(TP + FP)( TP + FN )( TN + FP )( TN + FN )})    positive_label  : int (default: 1)  Label of the positive class for binary classification\nmetrics.    Returns   score  : float",
            "title": "API"
        },
        {
            "location": "/user_guide/preprocessing/DenseTransformer/",
            "text": "DenseTransformer\n\n\nA simple transformer that converts a sparse into a dense numpy array, e.g., required for scikit-learn's \nPipeline\n when, for example, \nCountVectorizers\n are used in combination with \nRandomForest\ns.\n\n\n\n\nfrom mlxtend.preprocessing import DenseTransformer\n\n\n\n\nExamples\n\n\nExample 1\n\n\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.grid_search import GridSearchCV\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom mlxtend.preprocessing import DenseTransformer\nimport re\nimport numpy as np\n\nX_train = np.array(['abc def ghi', 'this is a test',\n                    'this is a test', 'this is a test'])\ny_train = np.array([0, 0, 1, 1])\n\npipe_1 = Pipeline([\n    ('vect', CountVectorizer()),\n    ('to_dense', DenseTransformer()),\n    ('clf', RandomForestClassifier())\n])\n\nparameters_1 = dict(\n    clf__n_estimators=[50, 100, 200],\n    clf__max_features=['sqrt', 'log2', None],)\n\ngrid_search_1 = GridSearchCV(pipe_1, \n                             parameters_1, \n                             n_jobs=1, \n                             verbose=1,\n                             scoring='accuracy',\n                             cv=2)\n\n\nprint(\"Performing grid search...\")\nprint(\"pipeline:\", [name for name, _ in pipe_1.steps])\nprint(\"parameters:\")\ngrid_search_1.fit(X_train, y_train)\nprint(\"Best score: %0.3f\" % grid_search_1.best_score_)\nprint(\"Best parameters set:\")\nbest_parameters_1 = grid_search_1.best_estimator_.get_params()\nfor param_name in sorted(parameters_1.keys()):\n    print(\"\\t%s: %r\" % (param_name, best_parameters_1[param_name]))\n\n\n\n\nPerforming grid search...\npipeline: ['vect', 'to_dense', 'clf']\nparameters:\nFitting 2 folds for each of 9 candidates, totalling 18 fits\nBest score: 0.500\nBest parameters set:\n    clf__max_features: 'sqrt'\n    clf__n_estimators: 50\n\n\n[Parallel(n_jobs=1)]: Done  18 out of  18 | elapsed:    2.4s finished\n\n\n\nAPI\n\n\nDenseTransformer(some_param=True)\n\n\nConvert a sparse matrix into a dense matrix.\n\n\nMethods\n\n\n\n\n\nfit(X, y=None)\n\n\nNone\n\n\n\n\n\nfit_transform(X, y=None)\n\n\nNone\n\n\n\n\n\nget_params(deep=True)\n\n\nNone\n\n\n\n\n\ntransform(X, y=None)\n\n\nNone",
            "title": "DenseTransformer"
        },
        {
            "location": "/user_guide/preprocessing/DenseTransformer/#densetransformer",
            "text": "A simple transformer that converts a sparse into a dense numpy array, e.g., required for scikit-learn's  Pipeline  when, for example,  CountVectorizers  are used in combination with  RandomForest s.   from mlxtend.preprocessing import DenseTransformer",
            "title": "DenseTransformer"
        },
        {
            "location": "/user_guide/preprocessing/DenseTransformer/#examples",
            "text": "",
            "title": "Examples"
        },
        {
            "location": "/user_guide/preprocessing/DenseTransformer/#example-1",
            "text": "from sklearn.pipeline import Pipeline\nfrom sklearn.grid_search import GridSearchCV\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom mlxtend.preprocessing import DenseTransformer\nimport re\nimport numpy as np\n\nX_train = np.array(['abc def ghi', 'this is a test',\n                    'this is a test', 'this is a test'])\ny_train = np.array([0, 0, 1, 1])\n\npipe_1 = Pipeline([\n    ('vect', CountVectorizer()),\n    ('to_dense', DenseTransformer()),\n    ('clf', RandomForestClassifier())\n])\n\nparameters_1 = dict(\n    clf__n_estimators=[50, 100, 200],\n    clf__max_features=['sqrt', 'log2', None],)\n\ngrid_search_1 = GridSearchCV(pipe_1, \n                             parameters_1, \n                             n_jobs=1, \n                             verbose=1,\n                             scoring='accuracy',\n                             cv=2)\n\n\nprint(\"Performing grid search...\")\nprint(\"pipeline:\", [name for name, _ in pipe_1.steps])\nprint(\"parameters:\")\ngrid_search_1.fit(X_train, y_train)\nprint(\"Best score: %0.3f\" % grid_search_1.best_score_)\nprint(\"Best parameters set:\")\nbest_parameters_1 = grid_search_1.best_estimator_.get_params()\nfor param_name in sorted(parameters_1.keys()):\n    print(\"\\t%s: %r\" % (param_name, best_parameters_1[param_name]))  Performing grid search...\npipeline: ['vect', 'to_dense', 'clf']\nparameters:\nFitting 2 folds for each of 9 candidates, totalling 18 fits\nBest score: 0.500\nBest parameters set:\n    clf__max_features: 'sqrt'\n    clf__n_estimators: 50\n\n\n[Parallel(n_jobs=1)]: Done  18 out of  18 | elapsed:    2.4s finished",
            "title": "Example 1"
        },
        {
            "location": "/user_guide/preprocessing/DenseTransformer/#api",
            "text": "DenseTransformer(some_param=True)  Convert a sparse matrix into a dense matrix.",
            "title": "API"
        },
        {
            "location": "/user_guide/preprocessing/DenseTransformer/#methods",
            "text": "fit(X, y=None)  None   fit_transform(X, y=None)  None   get_params(deep=True)  None   transform(X, y=None)  None",
            "title": "Methods"
        },
        {
            "location": "/user_guide/preprocessing/MeanCenterer/",
            "text": "Mean Centerer\n\n\nA transformer object that performs column-based mean centering on a NumPy array.\n\n\n\n\nfrom mlxtend.preprocessing import MeanCenterer\n\n\n\n\nExamples\n\n\nExample 1 - Centering a NumPy Array\n\n\nUse the \nfit\n method to fit the column means of a dataset (e.g., the training dataset) to a new \nMeanCenterer\n object. Then, call the \ntransform\n method on the same dataset to center it at the sample mean.\n\n\nimport numpy as np\nfrom mlxtend.preprocessing import MeanCenterer\nX_train = np.array(\n                   [[1, 2, 3],\n                    [4, 5, 6],\n                    [7, 8, 9]])\nmc = MeanCenterer().fit(X_train)\nmc.transform(X_train)\n\n\n\n\narray([[-3., -3., -3.],\n       [ 0.,  0.,  0.],\n       [ 3.,  3.,  3.]])\n\n\n\nAPI\n\n\nMeanCenterer()\n\n\nColumn centering of vectors and matrices.\n\n\nAttributes\n\n\n\n\n\n\ncol_means\n : numpy.ndarray [n_columns]\n\n\nNumPy array storing the mean values for centering after fitting\nthe MeanCenterer object.\n\n\n\n\n\n\nMethods\n\n\n\n\n\nfit(X)\n\n\nGets the column means for mean centering.\n\n\nParameters\n\n\n\n\n\n\nX\n : {array-like, sparse matrix}, shape = [n_samples, n_features]\n\n\nArray of data vectors, where n_samples is the number of samples and\nn_features is the number of features.\n\n\n\n\n\n\nReturns\n\n    self\n\n\n\n\n\nfit_transform(X)\n\n\nFits and transforms an arry.\n\n\nParameters\n\n\n\n\n\n\nX\n : {array-like, sparse matrix}, shape = [n_samples, n_features]\n\n\nArray of data vectors, where n_samples is the number of samples and\nn_features is the number of features.\n\n\n\n\n\n\nReturns\n\n\n\n\n\n\nX_tr\n : {array-like, sparse matrix}, shape = [n_samples, n_features]\n\n\nA copy of the input array with the columns centered.\n\n\n\n\n\n\n\n\n\ntransform(X)\n\n\nCenters a NumPy array.\n\n\nParameters\n\n\n\n\n\n\nX\n : {array-like, sparse matrix}, shape = [n_samples, n_features]\n\n\nArray of data vectors, where n_samples is the number of samples and\nn_features is the number of features.\n\n\n\n\n\n\nReturns\n\n\n\n\n\n\nX_tr\n : {array-like, sparse matrix}, shape = [n_samples, n_features]\n\n\nA copy of the input array with the columns centered.",
            "title": "MeanCenterer"
        },
        {
            "location": "/user_guide/preprocessing/MeanCenterer/#mean-centerer",
            "text": "A transformer object that performs column-based mean centering on a NumPy array.   from mlxtend.preprocessing import MeanCenterer",
            "title": "Mean Centerer"
        },
        {
            "location": "/user_guide/preprocessing/MeanCenterer/#examples",
            "text": "",
            "title": "Examples"
        },
        {
            "location": "/user_guide/preprocessing/MeanCenterer/#example-1-centering-a-numpy-array",
            "text": "Use the  fit  method to fit the column means of a dataset (e.g., the training dataset) to a new  MeanCenterer  object. Then, call the  transform  method on the same dataset to center it at the sample mean.  import numpy as np\nfrom mlxtend.preprocessing import MeanCenterer\nX_train = np.array(\n                   [[1, 2, 3],\n                    [4, 5, 6],\n                    [7, 8, 9]])\nmc = MeanCenterer().fit(X_train)\nmc.transform(X_train)  array([[-3., -3., -3.],\n       [ 0.,  0.,  0.],\n       [ 3.,  3.,  3.]])",
            "title": "Example 1 - Centering a NumPy Array"
        },
        {
            "location": "/user_guide/preprocessing/MeanCenterer/#api",
            "text": "MeanCenterer()  Column centering of vectors and matrices.  Attributes    col_means  : numpy.ndarray [n_columns]  NumPy array storing the mean values for centering after fitting\nthe MeanCenterer object.",
            "title": "API"
        },
        {
            "location": "/user_guide/preprocessing/MeanCenterer/#methods",
            "text": "fit(X)  Gets the column means for mean centering.  Parameters    X  : {array-like, sparse matrix}, shape = [n_samples, n_features]  Array of data vectors, where n_samples is the number of samples and\nn_features is the number of features.    Returns \n    self   fit_transform(X)  Fits and transforms an arry.  Parameters    X  : {array-like, sparse matrix}, shape = [n_samples, n_features]  Array of data vectors, where n_samples is the number of samples and\nn_features is the number of features.    Returns    X_tr  : {array-like, sparse matrix}, shape = [n_samples, n_features]  A copy of the input array with the columns centered.     transform(X)  Centers a NumPy array.  Parameters    X  : {array-like, sparse matrix}, shape = [n_samples, n_features]  Array of data vectors, where n_samples is the number of samples and\nn_features is the number of features.    Returns    X_tr  : {array-like, sparse matrix}, shape = [n_samples, n_features]  A copy of the input array with the columns centered.",
            "title": "Methods"
        },
        {
            "location": "/user_guide/preprocessing/minmax_scaling/",
            "text": "MinMax Scaling\n\n\nA function for min-max scaling of pandas DataFrames or NumPy arrays.\n\n\n\n\nfrom mlxtend.preprocessing import MinMaxScaling\n\n\n\n\nAn alternative approach to Z-score normalization (or standardization) is the so-called Min-Max scaling (often also simply called \"normalization\" - a common cause for ambiguities).\nIn this approach, the data is scaled to a fixed range - usually 0 to 1.\nThe cost of having this bounded range - in contrast to standardization - is that we will end up with smaller standard deviations, which can suppress the effect of outliers.\n\n\nA Min-Max scaling is typically done via the following equation:\n\n\n$$X_{sc} = \\frac{X - X_{min}}{X_{max} - X_{min}}.$$\n\n\nOne family of algorithms that is scale-invariant encompasses tree-based learning algorithms. Let's take the general CART decision tree algorithm. Without going into much depth regarding information gain and impurity measures, we can think of the decision as \"is feature x_i >= some_val?\" Intuitively, we can see that it really doesn't matter on which scale this feature is (centimeters, Fahrenheit, a standardized scale -- it really doesn't matter).\n\n\nSome examples of algorithms where feature scaling matters are:\n\n\n\n\nk-nearest neighbors with an Euclidean distance measure if want all features to contribute equally\n\n\nk-means (see k-nearest neighbors)\n\n\nlogistic regression, SVMs, perceptrons, neural networks etc. if you are using gradient descent/ascent-based optimization, otherwise some weights will update much faster than others\n\n\nlinear discriminant analysis, principal component analysis, kernel principal component analysis since you want to find directions of maximizing the variance (under the constraints that those directions/eigenvectors/principal components are orthogonal); you want to have features on the same scale since you'd emphasize variables on \"larger measurement scales\" more.\n\n\n\n\nThere are many more cases than I can possibly list here ... I always recommend you to think about the algorithm and what it's doing, and then it typically becomes obvious whether we want to scale your features or not.\n\n\nIn addition, we'd also want to think about whether we want to \"standardize\" or \"normalize\" (here: scaling to [0, 1] range) our data. Some algorithms assume that our data is centered at 0. For example, if we initialize the weights of a small multi-layer perceptron with tanh activation units to 0 or small random values centered around zero, we want to update the model weights \"equally.\"\nAs a rule of thumb I'd say: When in doubt, just standardize the data, it shouldn't hurt.   \n\n\nExamples\n\n\nExample 1 - Scaling a Pandas DataFrame\n\n\nimport pandas as pd\n\ns1 = pd.Series([1, 2, 3, 4, 5, 6], index=(range(6)))\ns2 = pd.Series([10, 9, 8, 7, 6, 5], index=(range(6)))\ndf = pd.DataFrame(s1, columns=['s1'])\ndf['s2'] = s2\ndf\n\n\n\n\n\n\n\n  \n\n    \n\n      \n\n      \ns1\n\n      \ns2\n\n    \n\n  \n\n  \n\n    \n\n      \n0\n\n      \n1\n\n      \n10\n\n    \n\n    \n\n      \n1\n\n      \n2\n\n      \n9\n\n    \n\n    \n\n      \n2\n\n      \n3\n\n      \n8\n\n    \n\n    \n\n      \n3\n\n      \n4\n\n      \n7\n\n    \n\n    \n\n      \n4\n\n      \n5\n\n      \n6\n\n    \n\n    \n\n      \n5\n\n      \n6\n\n      \n5\n\n    \n\n  \n\n\n\n\n\n\n\nfrom mlxtend.preprocessing import minmax_scaling\nminmax_scaling(df, columns=['s1', 's2'])\n\n\n\n\n\n\n\n  \n\n    \n\n      \n\n      \ns1\n\n      \ns2\n\n    \n\n  \n\n  \n\n    \n\n      \n0\n\n      \n0.0\n\n      \n1.0\n\n    \n\n    \n\n      \n1\n\n      \n0.2\n\n      \n0.8\n\n    \n\n    \n\n      \n2\n\n      \n0.4\n\n      \n0.6\n\n    \n\n    \n\n      \n3\n\n      \n0.6\n\n      \n0.4\n\n    \n\n    \n\n      \n4\n\n      \n0.8\n\n      \n0.2\n\n    \n\n    \n\n      \n5\n\n      \n1.0\n\n      \n0.0\n\n    \n\n  \n\n\n\n\n\n\n\nExample 2 - Scaling a NumPy Array\n\n\nimport numpy as np\n\nX = np.array([[1, 10], [2, 9], [3, 8], [4, 7], [5, 6], [6, 5]])\nX\n\n\n\n\narray([[ 1, 10],\n       [ 2,  9],\n       [ 3,  8],\n       [ 4,  7],\n       [ 5,  6],\n       [ 6,  5]])\n\n\n\nfrom mlxtend.preprocessing import minmax_scaling\nminmax_scaling(X, columns=[0, 1])\n\n\n\n\narray([[ 0. ,  1. ],\n       [ 0.2,  0.8],\n       [ 0.4,  0.6],\n       [ 0.6,  0.4],\n       [ 0.8,  0.2],\n       [ 1. ,  0. ]])\n\n\n\nAPI\n\n\nminmax_scaling(array, columns, min_val=0, max_val=1)\n\n\nMin max scaling of pandas' DataFrames.\n\n\nParameters\n\n\n\n\n\n\narray\n : pandas DataFrame or NumPy ndarray, shape = [n_rows, n_columns].\n\n\n\n\n\n\ncolumns\n : array-like, shape = [n_columns]\n\n\nArray-like with column names, e.g., ['col1', 'col2', ...]\nor column indices [0, 2, 4, ...]\n\n\n\n\n\n\nmin_val\n : \nint\n or \nfloat\n, optional (default=\n0\n)\n\n\nminimum value after rescaling.\n\n\n\n\n\n\nmin_val\n : \nint\n or \nfloat\n, optional (default=\n1\n)\n\n\nmaximum value after rescaling.\n\n\n\n\n\n\nReturns\n\n\n\n\n\n\ndf_new\n : pandas DataFrame object.\n\n\nCopy of the array or DataFrame with rescaled columns.",
            "title": "Minmax scaling"
        },
        {
            "location": "/user_guide/preprocessing/minmax_scaling/#minmax-scaling",
            "text": "A function for min-max scaling of pandas DataFrames or NumPy arrays.   from mlxtend.preprocessing import MinMaxScaling   An alternative approach to Z-score normalization (or standardization) is the so-called Min-Max scaling (often also simply called \"normalization\" - a common cause for ambiguities).\nIn this approach, the data is scaled to a fixed range - usually 0 to 1.\nThe cost of having this bounded range - in contrast to standardization - is that we will end up with smaller standard deviations, which can suppress the effect of outliers.  A Min-Max scaling is typically done via the following equation:  $$X_{sc} = \\frac{X - X_{min}}{X_{max} - X_{min}}.$$  One family of algorithms that is scale-invariant encompasses tree-based learning algorithms. Let's take the general CART decision tree algorithm. Without going into much depth regarding information gain and impurity measures, we can think of the decision as \"is feature x_i >= some_val?\" Intuitively, we can see that it really doesn't matter on which scale this feature is (centimeters, Fahrenheit, a standardized scale -- it really doesn't matter).  Some examples of algorithms where feature scaling matters are:   k-nearest neighbors with an Euclidean distance measure if want all features to contribute equally  k-means (see k-nearest neighbors)  logistic regression, SVMs, perceptrons, neural networks etc. if you are using gradient descent/ascent-based optimization, otherwise some weights will update much faster than others  linear discriminant analysis, principal component analysis, kernel principal component analysis since you want to find directions of maximizing the variance (under the constraints that those directions/eigenvectors/principal components are orthogonal); you want to have features on the same scale since you'd emphasize variables on \"larger measurement scales\" more.   There are many more cases than I can possibly list here ... I always recommend you to think about the algorithm and what it's doing, and then it typically becomes obvious whether we want to scale your features or not.  In addition, we'd also want to think about whether we want to \"standardize\" or \"normalize\" (here: scaling to [0, 1] range) our data. Some algorithms assume that our data is centered at 0. For example, if we initialize the weights of a small multi-layer perceptron with tanh activation units to 0 or small random values centered around zero, we want to update the model weights \"equally.\"\nAs a rule of thumb I'd say: When in doubt, just standardize the data, it shouldn't hurt.",
            "title": "MinMax Scaling"
        },
        {
            "location": "/user_guide/preprocessing/minmax_scaling/#examples",
            "text": "",
            "title": "Examples"
        },
        {
            "location": "/user_guide/preprocessing/minmax_scaling/#example-1-scaling-a-pandas-dataframe",
            "text": "import pandas as pd\n\ns1 = pd.Series([1, 2, 3, 4, 5, 6], index=(range(6)))\ns2 = pd.Series([10, 9, 8, 7, 6, 5], index=(range(6)))\ndf = pd.DataFrame(s1, columns=['s1'])\ndf['s2'] = s2\ndf   \n   \n     \n       \n       s1 \n       s2 \n     \n   \n   \n     \n       0 \n       1 \n       10 \n     \n     \n       1 \n       2 \n       9 \n     \n     \n       2 \n       3 \n       8 \n     \n     \n       3 \n       4 \n       7 \n     \n     \n       4 \n       5 \n       6 \n     \n     \n       5 \n       6 \n       5 \n     \n      from mlxtend.preprocessing import minmax_scaling\nminmax_scaling(df, columns=['s1', 's2'])   \n   \n     \n       \n       s1 \n       s2 \n     \n   \n   \n     \n       0 \n       0.0 \n       1.0 \n     \n     \n       1 \n       0.2 \n       0.8 \n     \n     \n       2 \n       0.4 \n       0.6 \n     \n     \n       3 \n       0.6 \n       0.4 \n     \n     \n       4 \n       0.8 \n       0.2 \n     \n     \n       5 \n       1.0 \n       0.0",
            "title": "Example 1 - Scaling a Pandas DataFrame"
        },
        {
            "location": "/user_guide/preprocessing/minmax_scaling/#example-2-scaling-a-numpy-array",
            "text": "import numpy as np\n\nX = np.array([[1, 10], [2, 9], [3, 8], [4, 7], [5, 6], [6, 5]])\nX  array([[ 1, 10],\n       [ 2,  9],\n       [ 3,  8],\n       [ 4,  7],\n       [ 5,  6],\n       [ 6,  5]])  from mlxtend.preprocessing import minmax_scaling\nminmax_scaling(X, columns=[0, 1])  array([[ 0. ,  1. ],\n       [ 0.2,  0.8],\n       [ 0.4,  0.6],\n       [ 0.6,  0.4],\n       [ 0.8,  0.2],\n       [ 1. ,  0. ]])",
            "title": "Example 2 - Scaling a NumPy Array"
        },
        {
            "location": "/user_guide/preprocessing/minmax_scaling/#api",
            "text": "minmax_scaling(array, columns, min_val=0, max_val=1)  Min max scaling of pandas' DataFrames.  Parameters    array  : pandas DataFrame or NumPy ndarray, shape = [n_rows, n_columns].    columns  : array-like, shape = [n_columns]  Array-like with column names, e.g., ['col1', 'col2', ...]\nor column indices [0, 2, 4, ...]    min_val  :  int  or  float , optional (default= 0 )  minimum value after rescaling.    min_val  :  int  or  float , optional (default= 1 )  maximum value after rescaling.    Returns    df_new  : pandas DataFrame object.  Copy of the array or DataFrame with rescaled columns.",
            "title": "API"
        },
        {
            "location": "/user_guide/preprocessing/shuffle_arrays_unison/",
            "text": "Shuffle Arrays in Unison\n\n\nA function for NumPy arrays in unison.\n\n\n\n\nfrom mlxtend.preprocessing import shuffle_arrays_unison\n\n\n\n\nExamples\n\n\nExample 1 - Scaling a Pandas DataFrame\n\n\nimport numpy as np\nfrom mlxtend.preprocessing import shuffle_arrays_unison\nX = np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]])\ny = np.array([1, 2, 3])\nprint('X:\\n%s' % X)\nprint('y:\\n%s' % y)\n\n\n\n\nX:\n[[1 2 3]\n [4 5 6]\n [7 8 9]]\ny:\n[1 2 3]\n\n\n\nX2, y2 = shuffle_arrays_unison(arrays=[X, y], random_seed=3)\nprint('X2:\\n%s' % X2)\nprint('y2:\\n%s' % y2)\n\n\n\n\nX2:\n[[4 5 6]\n [1 2 3]\n [7 8 9]]\ny2:\n[2 1 3]\n\n\n\nAPI\n\n\nshuffle_arrays_unison(arrays, random_seed=None)\n\n\nShuffle NumPy arrays in unison.\n\n\nParameters\n\n\n\n\n\n\narrays\n : array-like, shape = [n_arrays]\n\n\nA list of NumPy arrays.\n\n\n\n\n\n\nrandom_seed\n : int (default: None)\n\n\nSets the random state.\n\n\n\n\n\n\nReturns\n\n\n\n\nshuffled_arrays\n : A list of NumPy arrays after shuffling.\n\n\n\n\nExamples\n\n\n>>> import numpy as np\n>>> from mlxtend.preprocessing import shuffle_arrays_unison\n>>> X1 = np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]])\n>>> y1 = np.array([1, 2, 3])\n>>> X2, y2 = shuffle_arrays_unison(arrays=[X1, y1], random_state=3)\n>>> assert(X2.all() == np.array([[4, 5, 6], [1, 2, 3], [7, 8, 9]]).all())\n>>> assert(y2.all() == np.array([2, 1, 3]).all())\n>>>",
            "title": "Shuffle arrays unison"
        },
        {
            "location": "/user_guide/preprocessing/shuffle_arrays_unison/#shuffle-arrays-in-unison",
            "text": "A function for NumPy arrays in unison.   from mlxtend.preprocessing import shuffle_arrays_unison",
            "title": "Shuffle Arrays in Unison"
        },
        {
            "location": "/user_guide/preprocessing/shuffle_arrays_unison/#examples",
            "text": "",
            "title": "Examples"
        },
        {
            "location": "/user_guide/preprocessing/shuffle_arrays_unison/#example-1-scaling-a-pandas-dataframe",
            "text": "import numpy as np\nfrom mlxtend.preprocessing import shuffle_arrays_unison\nX = np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]])\ny = np.array([1, 2, 3])\nprint('X:\\n%s' % X)\nprint('y:\\n%s' % y)  X:\n[[1 2 3]\n [4 5 6]\n [7 8 9]]\ny:\n[1 2 3]  X2, y2 = shuffle_arrays_unison(arrays=[X, y], random_seed=3)\nprint('X2:\\n%s' % X2)\nprint('y2:\\n%s' % y2)  X2:\n[[4 5 6]\n [1 2 3]\n [7 8 9]]\ny2:\n[2 1 3]",
            "title": "Example 1 - Scaling a Pandas DataFrame"
        },
        {
            "location": "/user_guide/preprocessing/shuffle_arrays_unison/#api",
            "text": "shuffle_arrays_unison(arrays, random_seed=None)  Shuffle NumPy arrays in unison.  Parameters    arrays  : array-like, shape = [n_arrays]  A list of NumPy arrays.    random_seed  : int (default: None)  Sets the random state.    Returns   shuffled_arrays  : A list of NumPy arrays after shuffling.   Examples  >>> import numpy as np\n>>> from mlxtend.preprocessing import shuffle_arrays_unison\n>>> X1 = np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]])\n>>> y1 = np.array([1, 2, 3])\n>>> X2, y2 = shuffle_arrays_unison(arrays=[X1, y1], random_state=3)\n>>> assert(X2.all() == np.array([[4, 5, 6], [1, 2, 3], [7, 8, 9]]).all())\n>>> assert(y2.all() == np.array([2, 1, 3]).all())\n>>>",
            "title": "API"
        },
        {
            "location": "/user_guide/preprocessing/standardize/",
            "text": "Standardize\n\n\nA function that performs column-based standardization on a NumPy array.\n\n\n\n\nfrom mlxtend.preprocessing import standardize\n\n\n\n\nOverview\n\n\nThe result of standardization (or Z-score normalization) is that the features will be rescaled so that they'll have the properties of a standard normal distribution with\n\n\n$\\mu = 0$\n and \n$\\sigma = 1$\n.\n\n\nwhere \n$\\mu$\n is the mean (average) and \n$\\sigma$\n is the standard deviation from the mean; standard scores (also called z scores) of the samples are calculated as\n\n\n$$z=\\frac{x-\\mu}{\\sigma}.$$\n\n\nStandardizing the features so that they are centered around 0 with a standard deviation of 1 is not only important if we are comparing measurements that have different units, but it is also a general requirement for the optimal performance of many machine learning algorithms. \n\n\nOne family of algorithms that is scale-invariant encompasses tree-based learning algorithms. Let's take the general CART decision tree algorithm. Without going into much depth regarding information gain and impurity measures, we can think of the decision as \"is feature x_i >= some_val?\" Intuitively, we can see that it really doesn't matter on which scale this feature is (centimeters, Fahrenheit, a standardized scale -- it really doesn't matter).\n\n\nSome examples of algorithms where feature scaling matters are:\n\n\n\n\nk-nearest neighbors with an Euclidean distance measure if want all features to contribute equally\n\n\nk-means (see k-nearest neighbors)\n\n\nlogistic regression, SVMs, perceptrons, neural networks etc. if you are using gradient descent/ascent-based optimization, otherwise some weights will update much faster than others\n\n\nlinear discriminant analysis, principal component analysis, kernel principal component analysis since you want to find directions of maximizing the variance (under the constraints that those directions/eigenvectors/principal components are orthogonal); you want to have features on the same scale since you'd emphasize variables on \"larger measurement scales\" more.\n\n\n\n\nThere are many more cases than I can possibly list here ... I always recommend you to think about the algorithm and what it's doing, and then it typically becomes obvious whether we want to scale your features or not.\n\n\nIn addition, we'd also want to think about whether we want to \"standardize\" or \"normalize\" (here: scaling to [0, 1] range) our data. Some algorithms assume that our data is centered at 0. For example, if we initialize the weights of a small multi-layer perceptron with tanh activation units to 0 or small random values centered around zero, we want to update the model weights \"equally.\"\nAs a rule of thumb I'd say: When in doubt, just standardize the data, it shouldn't hurt.   \n\n\nExamples\n\n\nExample 1 - Standardize a Pandas DataFrame\n\n\nimport pandas as pd\n\ns1 = pd.Series([1, 2, 3, 4, 5, 6], index=(range(6)))\ns2 = pd.Series([10, 9, 8, 7, 6, 5], index=(range(6)))\ndf = pd.DataFrame(s1, columns=['s1'])\ndf['s2'] = s2\ndf\n\n\n\n\n\n\n\n  \n\n    \n\n      \n\n      \ns1\n\n      \ns2\n\n    \n\n  \n\n  \n\n    \n\n      \n0\n\n      \n1\n\n      \n10\n\n    \n\n    \n\n      \n1\n\n      \n2\n\n      \n9\n\n    \n\n    \n\n      \n2\n\n      \n3\n\n      \n8\n\n    \n\n    \n\n      \n3\n\n      \n4\n\n      \n7\n\n    \n\n    \n\n      \n4\n\n      \n5\n\n      \n6\n\n    \n\n    \n\n      \n5\n\n      \n6\n\n      \n5\n\n    \n\n  \n\n\n\n\n\n\n\nfrom mlxtend.preprocessing import standardize\nstandardize(df, columns=['s1', 's2'])\n\n\n\n\n\n\n\n  \n\n    \n\n      \n\n      \ns1\n\n      \ns2\n\n    \n\n  \n\n  \n\n    \n\n      \n0\n\n      \n-1.46385\n\n      \n1.46385\n\n    \n\n    \n\n      \n1\n\n      \n-0.87831\n\n      \n0.87831\n\n    \n\n    \n\n      \n2\n\n      \n-0.29277\n\n      \n0.29277\n\n    \n\n    \n\n      \n3\n\n      \n0.29277\n\n      \n-0.29277\n\n    \n\n    \n\n      \n4\n\n      \n0.87831\n\n      \n-0.87831\n\n    \n\n    \n\n      \n5\n\n      \n1.46385\n\n      \n-1.46385\n\n    \n\n  \n\n\n\n\n\n\n\nExample 2 - Standardize a NumPy Array\n\n\nimport numpy as np\n\nX = np.array([[1, 10], [2, 9], [3, 8], [4, 7], [5, 6], [6, 5]])\nX\n\n\n\n\narray([[ 1, 10],\n       [ 2,  9],\n       [ 3,  8],\n       [ 4,  7],\n       [ 5,  6],\n       [ 6,  5]])\n\n\n\nfrom mlxtend.preprocessing import standardize\nstandardize(X, columns=[0, 1])\n\n\n\n\narray([[-1.46385011,  1.46385011],\n       [-0.87831007,  0.87831007],\n       [-0.29277002,  0.29277002],\n       [ 0.29277002, -0.29277002],\n       [ 0.87831007, -0.87831007],\n       [ 1.46385011, -1.46385011]])\n\n\n\nExample 3 - Re-using parameters\n\n\nIn machine learning contexts, it is desired to re-use the parameters that have been obtained from a training set to scale new, future data (including the independent test set). By setting \nreturn_params=True\n, the \nstandardize\n function returns a second object, a parameter dictionary containing the column means and standard deviations that can be re-used by feeding it to the \nparams\n parameter upon function call.\n\n\nimport numpy as np\nfrom mlxtend.preprocessing import standardize\n\nX_train = np.array([[1, 10], [4, 7], [3, 8]])\nX_test = np.array([[1, 2], [3, 4], [5, 6]])\n\nX_train_std, params = standardize(X_train, columns=[0, 1], return_params=True)\nX_train_std\n\n\n\n\narray([[-1.33630621,  1.33630621],\n       [ 1.06904497, -1.06904497],\n       [ 0.26726124, -0.26726124]])\n\n\n\nparams\n\n\n\n\n{'avgs': array([ 2.66666667,  8.33333333]),\n 'stds': array([ 1.24721913,  1.24721913])}\n\n\n\nX_test_std = standardize(X_test, columns=[0, 1], params=params)\nX_test_std\n\n\n\n\narray([[-1.33630621, -5.0779636 ],\n       [ 0.26726124, -3.47439614],\n       [ 1.87082869, -1.87082869]])\n\n\n\nAPI\n\n\nstandardize(array, columns, ddof=0, return_params=False, params=None)\n\n\nStandardize columns in pandas DataFrames.\n\n\nParameters\n\n\n\n\n\n\narray\n : pandas DataFrame or NumPy ndarray, shape = [n_rows, n_columns].\n\n\n\n\n\n\ncolumns\n : array-like, shape = [n_columns]\n\n\nArray-like with column names, e.g., ['col1', 'col2', ...]\nor column indices [0, 2, 4, ...]\n\n\n\n\n\n\nddof\n : int (default: 0)\n\n\nDelta Degrees of Freedom. The divisor used in calculations\nis N - ddof, where N represents the number of elements.\n\n\n\n\n\n\nreturn_params\n : dict (default: False)\n\n\nIf set to True, a dictionary is returned in addition to the\nstandardized array. The parameter dictionary contains the\ncolumn means ('avgs') and standard deviations ('stds') of\nthe individual columns.\n\n\n\n\n\n\nparams\n : dict (default: None)\n\n\nA dictionary with column means and standard deviations as\nreturned by the \nstandardize\n function if \nreturn_params\n\nwas set to True. If a \nparams\n dictionary is provided, the\n\nstandardize\n function will use these instead of computing\nthem from the current array.\n\n\n\n\n\n\nNotes\n\n\nIf all values in a given column are the same, these values are all\n    set to \n0.0\n. The standard deviation in the \nparameters\n dictionary\n    is consequently set to \n1.0\n to avoid dividing by zero.\n\n\nReturns\n\n\n\n\n\n\ndf_new\n : pandas DataFrame object.\n\n\nCopy of the array or DataFrame with standardized columns.",
            "title": "Standardize"
        },
        {
            "location": "/user_guide/preprocessing/standardize/#standardize",
            "text": "A function that performs column-based standardization on a NumPy array.   from mlxtend.preprocessing import standardize",
            "title": "Standardize"
        },
        {
            "location": "/user_guide/preprocessing/standardize/#overview",
            "text": "The result of standardization (or Z-score normalization) is that the features will be rescaled so that they'll have the properties of a standard normal distribution with  $\\mu = 0$  and  $\\sigma = 1$ .  where  $\\mu$  is the mean (average) and  $\\sigma$  is the standard deviation from the mean; standard scores (also called z scores) of the samples are calculated as  $$z=\\frac{x-\\mu}{\\sigma}.$$  Standardizing the features so that they are centered around 0 with a standard deviation of 1 is not only important if we are comparing measurements that have different units, but it is also a general requirement for the optimal performance of many machine learning algorithms.   One family of algorithms that is scale-invariant encompasses tree-based learning algorithms. Let's take the general CART decision tree algorithm. Without going into much depth regarding information gain and impurity measures, we can think of the decision as \"is feature x_i >= some_val?\" Intuitively, we can see that it really doesn't matter on which scale this feature is (centimeters, Fahrenheit, a standardized scale -- it really doesn't matter).  Some examples of algorithms where feature scaling matters are:   k-nearest neighbors with an Euclidean distance measure if want all features to contribute equally  k-means (see k-nearest neighbors)  logistic regression, SVMs, perceptrons, neural networks etc. if you are using gradient descent/ascent-based optimization, otherwise some weights will update much faster than others  linear discriminant analysis, principal component analysis, kernel principal component analysis since you want to find directions of maximizing the variance (under the constraints that those directions/eigenvectors/principal components are orthogonal); you want to have features on the same scale since you'd emphasize variables on \"larger measurement scales\" more.   There are many more cases than I can possibly list here ... I always recommend you to think about the algorithm and what it's doing, and then it typically becomes obvious whether we want to scale your features or not.  In addition, we'd also want to think about whether we want to \"standardize\" or \"normalize\" (here: scaling to [0, 1] range) our data. Some algorithms assume that our data is centered at 0. For example, if we initialize the weights of a small multi-layer perceptron with tanh activation units to 0 or small random values centered around zero, we want to update the model weights \"equally.\"\nAs a rule of thumb I'd say: When in doubt, just standardize the data, it shouldn't hurt.",
            "title": "Overview"
        },
        {
            "location": "/user_guide/preprocessing/standardize/#examples",
            "text": "",
            "title": "Examples"
        },
        {
            "location": "/user_guide/preprocessing/standardize/#example-1-standardize-a-pandas-dataframe",
            "text": "import pandas as pd\n\ns1 = pd.Series([1, 2, 3, 4, 5, 6], index=(range(6)))\ns2 = pd.Series([10, 9, 8, 7, 6, 5], index=(range(6)))\ndf = pd.DataFrame(s1, columns=['s1'])\ndf['s2'] = s2\ndf   \n   \n     \n       \n       s1 \n       s2 \n     \n   \n   \n     \n       0 \n       1 \n       10 \n     \n     \n       1 \n       2 \n       9 \n     \n     \n       2 \n       3 \n       8 \n     \n     \n       3 \n       4 \n       7 \n     \n     \n       4 \n       5 \n       6 \n     \n     \n       5 \n       6 \n       5 \n     \n      from mlxtend.preprocessing import standardize\nstandardize(df, columns=['s1', 's2'])   \n   \n     \n       \n       s1 \n       s2 \n     \n   \n   \n     \n       0 \n       -1.46385 \n       1.46385 \n     \n     \n       1 \n       -0.87831 \n       0.87831 \n     \n     \n       2 \n       -0.29277 \n       0.29277 \n     \n     \n       3 \n       0.29277 \n       -0.29277 \n     \n     \n       4 \n       0.87831 \n       -0.87831 \n     \n     \n       5 \n       1.46385 \n       -1.46385",
            "title": "Example 1 - Standardize a Pandas DataFrame"
        },
        {
            "location": "/user_guide/preprocessing/standardize/#example-2-standardize-a-numpy-array",
            "text": "import numpy as np\n\nX = np.array([[1, 10], [2, 9], [3, 8], [4, 7], [5, 6], [6, 5]])\nX  array([[ 1, 10],\n       [ 2,  9],\n       [ 3,  8],\n       [ 4,  7],\n       [ 5,  6],\n       [ 6,  5]])  from mlxtend.preprocessing import standardize\nstandardize(X, columns=[0, 1])  array([[-1.46385011,  1.46385011],\n       [-0.87831007,  0.87831007],\n       [-0.29277002,  0.29277002],\n       [ 0.29277002, -0.29277002],\n       [ 0.87831007, -0.87831007],\n       [ 1.46385011, -1.46385011]])",
            "title": "Example 2 - Standardize a NumPy Array"
        },
        {
            "location": "/user_guide/preprocessing/standardize/#example-3-re-using-parameters",
            "text": "In machine learning contexts, it is desired to re-use the parameters that have been obtained from a training set to scale new, future data (including the independent test set). By setting  return_params=True , the  standardize  function returns a second object, a parameter dictionary containing the column means and standard deviations that can be re-used by feeding it to the  params  parameter upon function call.  import numpy as np\nfrom mlxtend.preprocessing import standardize\n\nX_train = np.array([[1, 10], [4, 7], [3, 8]])\nX_test = np.array([[1, 2], [3, 4], [5, 6]])\n\nX_train_std, params = standardize(X_train, columns=[0, 1], return_params=True)\nX_train_std  array([[-1.33630621,  1.33630621],\n       [ 1.06904497, -1.06904497],\n       [ 0.26726124, -0.26726124]])  params  {'avgs': array([ 2.66666667,  8.33333333]),\n 'stds': array([ 1.24721913,  1.24721913])}  X_test_std = standardize(X_test, columns=[0, 1], params=params)\nX_test_std  array([[-1.33630621, -5.0779636 ],\n       [ 0.26726124, -3.47439614],\n       [ 1.87082869, -1.87082869]])",
            "title": "Example 3 - Re-using parameters"
        },
        {
            "location": "/user_guide/preprocessing/standardize/#api",
            "text": "standardize(array, columns, ddof=0, return_params=False, params=None)  Standardize columns in pandas DataFrames.  Parameters    array  : pandas DataFrame or NumPy ndarray, shape = [n_rows, n_columns].    columns  : array-like, shape = [n_columns]  Array-like with column names, e.g., ['col1', 'col2', ...]\nor column indices [0, 2, 4, ...]    ddof  : int (default: 0)  Delta Degrees of Freedom. The divisor used in calculations\nis N - ddof, where N represents the number of elements.    return_params  : dict (default: False)  If set to True, a dictionary is returned in addition to the\nstandardized array. The parameter dictionary contains the\ncolumn means ('avgs') and standard deviations ('stds') of\nthe individual columns.    params  : dict (default: None)  A dictionary with column means and standard deviations as\nreturned by the  standardize  function if  return_params \nwas set to True. If a  params  dictionary is provided, the standardize  function will use these instead of computing\nthem from the current array.    Notes  If all values in a given column are the same, these values are all\n    set to  0.0 . The standard deviation in the  parameters  dictionary\n    is consequently set to  1.0  to avoid dividing by zero.  Returns    df_new  : pandas DataFrame object.  Copy of the array or DataFrame with standardized columns.",
            "title": "API"
        },
        {
            "location": "/user_guide/data/autompg_data/",
            "text": "Auto MPG\n\n\nA function that loads the \nautompg\n dataset into NumPy arrays.\n\n\n\n\nfrom mlxtend.data import autompg_data\n\n\n\n\nOverview\n\n\nThe Auto-MPG dataset for regression analysis. The target (\ny\n) is defined as the miles per gallon (mpg) for 392 automobiles (6 rows containing \"NaN\"s have been removed. The 8 feature columns are:\n\n\nFeatures\n\n\n\n\ncylinders: multi-valued discrete \n\n\ndisplacement: continuous \n\n\nhorsepower: continuous \n\n\nweight: continuous \n\n\nacceleration: continuous \n\n\nmodel year: multi-valued discrete \n\n\norigin: multi-valued discrete \n\n\n\n\ncar name: string (unique for each instance)\n\n\n\n\n\n\nNumber of samples: 392\n\n\n\n\n\n\nTarget variable (continuous): mpg\n\n\n\n\n\n\nReferences\n\n\n\n\nSource: \nhttps://archive.ics.uci.edu/ml/datasets/Auto+MPG\n\n\nQuinlan,R. (1993). Combining Instance-Based and Model-Based Learning. In Proceedings on the Tenth International Conference of Machine Learning, 236-243, University of Massachusetts, Amherst. Morgan Kaufmann.\n\n\n\n\nExamples\n\n\nExample - Dataset overview\n\n\nfrom mlxtend.data import autompg_data\nX, y = autompg_data()\n\nprint('Dimensions: %s x %s' % (X.shape[0], X.shape[1]))\nprint('\\nHeader: %s' % ['cylinders', 'displacement', \n                           'horsepower weight', 'acceleration',\n                           'model year', 'origin', 'car name'])\nprint('1st row', X[0])\n\n\n\n\nDimensions: 392 x 8\n\nHeader: ['cylinders', 'displacement', 'horsepower weight', 'acceleration', 'model year', 'origin', 'car name']\n1st row ['8' '307.0' '130.0' '3504.0' '12.0' '70' '1' 'chevrolet chevelle malibu']\n\n\n\nNote that the feature array contains a \nstr\n column (\"car name\"), thus it is recommended to pick the features as needed and convert it into a \nfloat\n array for further analysis. The example below shows how to get rid of the \ncar name\n column and cast the NumPy array as a \nfloat\n array.\n\n\nX[:, :-1].astype(float)\n\n\n\n\narray([[   8. ,  307. ,  130. , ...,   12. ,   70. ,    1. ],\n       [   8. ,  350. ,  165. , ...,   11.5,   70. ,    1. ],\n       [   8. ,  318. ,  150. , ...,   11. ,   70. ,    1. ],\n       ..., \n       [   4. ,  135. ,   84. , ...,   11.6,   82. ,    1. ],\n       [   4. ,  120. ,   79. , ...,   18.6,   82. ,    1. ],\n       [   4. ,  119. ,   82. , ...,   19.4,   82. ,    1. ]])\n\n\n\nAPI\n\n\nautompg_data()\n\n\nAuto MPG dataset.\n\n\n\n\n\n\nSource\n : https://archive.ics.uci.edu/ml/datasets/Auto+MPG\n\n\n\n\n\n\nNumber of samples\n : 392\n\n\n\n\n\n\nContinuous target variable\n : mpg\n\n\nDataset Attributes:\n\n\n\n\n1) cylinders:  multi-valued discrete\n\n\n2) displacement: continuous\n\n\n3) horsepower: continuous\n\n\n4) weight: continuous\n\n\n5) acceleration: continuous\n\n\n6) model year: multi-valued discrete\n\n\n7) origin: multi-valued discrete\n\n\n8) car name: string (unique for each instance)\n\n\n\n\n\n\n\n\nReturns\n\n\n\n\n\n\nX, y\n : [n_samples, n_features], [n_targets]\n\n\nX is the feature matrix with 392 auto samples as rows\nand 8 feature columns (6 rows with NaNs removed).\ny is a 1-dimensional array of the target MPG values.",
            "title": "Autompg data"
        },
        {
            "location": "/user_guide/data/autompg_data/#auto-mpg",
            "text": "A function that loads the  autompg  dataset into NumPy arrays.   from mlxtend.data import autompg_data",
            "title": "Auto MPG"
        },
        {
            "location": "/user_guide/data/autompg_data/#overview",
            "text": "The Auto-MPG dataset for regression analysis. The target ( y ) is defined as the miles per gallon (mpg) for 392 automobiles (6 rows containing \"NaN\"s have been removed. The 8 feature columns are:  Features   cylinders: multi-valued discrete   displacement: continuous   horsepower: continuous   weight: continuous   acceleration: continuous   model year: multi-valued discrete   origin: multi-valued discrete    car name: string (unique for each instance)    Number of samples: 392    Target variable (continuous): mpg",
            "title": "Overview"
        },
        {
            "location": "/user_guide/data/autompg_data/#references",
            "text": "Source:  https://archive.ics.uci.edu/ml/datasets/Auto+MPG  Quinlan,R. (1993). Combining Instance-Based and Model-Based Learning. In Proceedings on the Tenth International Conference of Machine Learning, 236-243, University of Massachusetts, Amherst. Morgan Kaufmann.",
            "title": "References"
        },
        {
            "location": "/user_guide/data/autompg_data/#examples",
            "text": "",
            "title": "Examples"
        },
        {
            "location": "/user_guide/data/autompg_data/#example-dataset-overview",
            "text": "from mlxtend.data import autompg_data\nX, y = autompg_data()\n\nprint('Dimensions: %s x %s' % (X.shape[0], X.shape[1]))\nprint('\\nHeader: %s' % ['cylinders', 'displacement', \n                           'horsepower weight', 'acceleration',\n                           'model year', 'origin', 'car name'])\nprint('1st row', X[0])  Dimensions: 392 x 8\n\nHeader: ['cylinders', 'displacement', 'horsepower weight', 'acceleration', 'model year', 'origin', 'car name']\n1st row ['8' '307.0' '130.0' '3504.0' '12.0' '70' '1' 'chevrolet chevelle malibu']  Note that the feature array contains a  str  column (\"car name\"), thus it is recommended to pick the features as needed and convert it into a  float  array for further analysis. The example below shows how to get rid of the  car name  column and cast the NumPy array as a  float  array.  X[:, :-1].astype(float)  array([[   8. ,  307. ,  130. , ...,   12. ,   70. ,    1. ],\n       [   8. ,  350. ,  165. , ...,   11.5,   70. ,    1. ],\n       [   8. ,  318. ,  150. , ...,   11. ,   70. ,    1. ],\n       ..., \n       [   4. ,  135. ,   84. , ...,   11.6,   82. ,    1. ],\n       [   4. ,  120. ,   79. , ...,   18.6,   82. ,    1. ],\n       [   4. ,  119. ,   82. , ...,   19.4,   82. ,    1. ]])",
            "title": "Example - Dataset overview"
        },
        {
            "location": "/user_guide/data/autompg_data/#api",
            "text": "autompg_data()  Auto MPG dataset.    Source  : https://archive.ics.uci.edu/ml/datasets/Auto+MPG    Number of samples  : 392    Continuous target variable  : mpg  Dataset Attributes:   1) cylinders:  multi-valued discrete  2) displacement: continuous  3) horsepower: continuous  4) weight: continuous  5) acceleration: continuous  6) model year: multi-valued discrete  7) origin: multi-valued discrete  8) car name: string (unique for each instance)     Returns    X, y  : [n_samples, n_features], [n_targets]  X is the feature matrix with 392 auto samples as rows\nand 8 feature columns (6 rows with NaNs removed).\ny is a 1-dimensional array of the target MPG values.",
            "title": "API"
        },
        {
            "location": "/user_guide/data/boston_housing_data/",
            "text": "Boston Housing Data\n\n\nA function that loads the \nboston_housing_data\n dataset into NumPy arrays.\n\n\n\n\nfrom mlxtend.data import boston_housing_data\n\n\n\n\nOverview\n\n\nThe Boston Housing dataset for regression analysis.\n\n\nFeatures\n\n\n\n\nCRIM:      per capita crime rate by town\n\n\nZN:        proportion of residential land zoned for lots over 25,000 sq.ft.\n\n\nINDUS:     proportion of non-retail business acres per town\n\n\nCHAS:      Charles River dummy variable (= 1 if tract bounds river; 0 otherwise)\n\n\nNOX:       nitric oxides concentration (parts per 10 million)\n\n\nRM:        average number of rooms per dwelling\n\n\nAGE:       proportion of owner-occupied units built prior to 1940\n\n\nDIS:       weighted distances to five Boston employment centres\n\n\nRAD:       index of accessibility to radial highways\n\n\nTAX:      full-value property-tax rate per $10,000\n\n\nPTRATIO:  pupil-teacher ratio by town\n\n\nB:        1000(Bk - 0.63)^2 where Bk is the proportion of b. by town\n\n\n\n\nLSTAT:    % lower status of the population\n\n\n\n\n\n\nNumber of samples: 506\n\n\n\n\n\n\nTarget variable (continuous): MEDV, Median value of owner-occupied homes in $1000's\n\n\n\n\n\n\nReferences\n\n\n\n\nSource: \nhttps://archive.ics.uci.edu/ml/datasets/Wine\n\n\nHarrison, D. and Rubinfeld, D.L. \n'Hedonic prices and the demand for clean air', J. Environ. Economics & Management, vol.5, 81-102, 1978.\n\n\n\n\nExamples\n\n\nExample - Dataset overview\n\n\nfrom mlxtend.data import boston_housing_data\nX, y = boston_housing_data()\n\nprint('Dimensions: %s x %s' % (X.shape[0], X.shape[1]))\nprint('1st row', X[0])\n\n\n\n\nDimensions: 506 x 13\n1st row [  6.32000000e-03   1.80000000e+01   2.31000000e+00   0.00000000e+00\n   5.38000000e-01   6.57500000e+00   6.52000000e+01   4.09000000e+00\n   1.00000000e+00   2.96000000e+02   1.53000000e+01   3.96900000e+02\n   4.98000000e+00]\n\n\n\nAPI\n\n\nboston_housing_data()\n\n\nBoston Housing dataset.\n\n\n\n\n\n\nSource\n : https://archive.ics.uci.edu/ml/datasets/Housing\n\n\n\n\n\n\nNumber of samples\n : 506\n\n\n\n\n\n\nContinuous target variable\n : MEDV\n\n\nMEDV = Median value of owner-occupied homes in $1000's\n\n\nDataset Attributes:\n\n\n\n\n1) CRIM      per capita crime rate by town\n\n\n2) ZN        proportion of residential land zoned for lots over\n25,000 sq.ft.\n\n\n3) INDUS     proportion of non-retail business acres per town\n\n\n4) CHAS      Charles River dummy variable (= 1 if tract bounds\nriver; 0 otherwise)\n\n\n5) NOX       nitric oxides concentration (parts per 10 million)\n\n\n6) RM        average number of rooms per dwelling\n\n\n7) AGE       proportion of owner-occupied units built prior to 1940\n\n\n8) DIS       weighted distances to five Boston employment centres\n\n\n9) RAD       index of accessibility to radial highways\n\n\n10) TAX      full-value property-tax rate per $10,000\n\n\n11) PTRATIO  pupil-teacher ratio by town\n\n\n12) B        1000(Bk - 0.63)^2 where Bk is the proportion of b. by town\n\n\n13) LSTAT    % lower status of the population\n\n\n\n\n\n\n\n\nReturns\n\n\n\n\n\n\nX, y\n : [n_samples, n_features], [n_class_labels]\n\n\nX is the feature matrix with 506 housing samples as rows\nand 13 feature columns.\ny is a 1-dimensional array of the continuous target variable MEDV",
            "title": "Boston housing data"
        },
        {
            "location": "/user_guide/data/boston_housing_data/#boston-housing-data",
            "text": "A function that loads the  boston_housing_data  dataset into NumPy arrays.   from mlxtend.data import boston_housing_data",
            "title": "Boston Housing Data"
        },
        {
            "location": "/user_guide/data/boston_housing_data/#overview",
            "text": "The Boston Housing dataset for regression analysis.  Features   CRIM:      per capita crime rate by town  ZN:        proportion of residential land zoned for lots over 25,000 sq.ft.  INDUS:     proportion of non-retail business acres per town  CHAS:      Charles River dummy variable (= 1 if tract bounds river; 0 otherwise)  NOX:       nitric oxides concentration (parts per 10 million)  RM:        average number of rooms per dwelling  AGE:       proportion of owner-occupied units built prior to 1940  DIS:       weighted distances to five Boston employment centres  RAD:       index of accessibility to radial highways  TAX:      full-value property-tax rate per $10,000  PTRATIO:  pupil-teacher ratio by town  B:        1000(Bk - 0.63)^2 where Bk is the proportion of b. by town   LSTAT:    % lower status of the population    Number of samples: 506    Target variable (continuous): MEDV, Median value of owner-occupied homes in $1000's",
            "title": "Overview"
        },
        {
            "location": "/user_guide/data/boston_housing_data/#references",
            "text": "Source:  https://archive.ics.uci.edu/ml/datasets/Wine  Harrison, D. and Rubinfeld, D.L. \n'Hedonic prices and the demand for clean air', J. Environ. Economics & Management, vol.5, 81-102, 1978.",
            "title": "References"
        },
        {
            "location": "/user_guide/data/boston_housing_data/#examples",
            "text": "",
            "title": "Examples"
        },
        {
            "location": "/user_guide/data/boston_housing_data/#example-dataset-overview",
            "text": "from mlxtend.data import boston_housing_data\nX, y = boston_housing_data()\n\nprint('Dimensions: %s x %s' % (X.shape[0], X.shape[1]))\nprint('1st row', X[0])  Dimensions: 506 x 13\n1st row [  6.32000000e-03   1.80000000e+01   2.31000000e+00   0.00000000e+00\n   5.38000000e-01   6.57500000e+00   6.52000000e+01   4.09000000e+00\n   1.00000000e+00   2.96000000e+02   1.53000000e+01   3.96900000e+02\n   4.98000000e+00]",
            "title": "Example - Dataset overview"
        },
        {
            "location": "/user_guide/data/boston_housing_data/#api",
            "text": "boston_housing_data()  Boston Housing dataset.    Source  : https://archive.ics.uci.edu/ml/datasets/Housing    Number of samples  : 506    Continuous target variable  : MEDV  MEDV = Median value of owner-occupied homes in $1000's  Dataset Attributes:   1) CRIM      per capita crime rate by town  2) ZN        proportion of residential land zoned for lots over\n25,000 sq.ft.  3) INDUS     proportion of non-retail business acres per town  4) CHAS      Charles River dummy variable (= 1 if tract bounds\nriver; 0 otherwise)  5) NOX       nitric oxides concentration (parts per 10 million)  6) RM        average number of rooms per dwelling  7) AGE       proportion of owner-occupied units built prior to 1940  8) DIS       weighted distances to five Boston employment centres  9) RAD       index of accessibility to radial highways  10) TAX      full-value property-tax rate per $10,000  11) PTRATIO  pupil-teacher ratio by town  12) B        1000(Bk - 0.63)^2 where Bk is the proportion of b. by town  13) LSTAT    % lower status of the population     Returns    X, y  : [n_samples, n_features], [n_class_labels]  X is the feature matrix with 506 housing samples as rows\nand 13 feature columns.\ny is a 1-dimensional array of the continuous target variable MEDV",
            "title": "API"
        },
        {
            "location": "/user_guide/data/iris_data/",
            "text": "Iris Dataset\n\n\nA function that loads the \niris\n dataset into NumPy arrays.\n\n\n\n\nfrom mlxtend.data import iris_data\n\n\n\n\nOverview\n\n\nThe Iris dataset for classification.\n\n\nFeatures\n\n\n\n\nSepal length\n\n\nSepal width\n\n\nPetal length\n\n\n\n\nPetal width\n\n\n\n\n\n\nNumber of samples: 150\n\n\n\n\n\n\nTarget variable (discrete): {50x Setosa, 50x Versicolor, 50x Virginica}\n\n\n\n\n\n\nReferences\n\n\n\n\nSource: \nhttps://archive.ics.uci.edu/ml/datasets/Iris\n \n\n\nBache, K. & Lichman, M. (2013). UCI Machine Learning Repository. Irvine, CA: University of California, School of Information and Computer Science.\n\n\n\n\nExamples\n\n\nExample - Dataset overview\n\n\nfrom mlxtend.data import iris_data\nX, y = iris_data()\n\nprint('Dimensions: %s x %s' % (X.shape[0], X.shape[1]))\nprint('\\nHeader: %s' % ['sepal length', 'sepal width',\n                        'petal length', 'petal width'])\nprint('1st row', X[0])\n\n\n\n\nDimensions: 150 x 4\n\nHeader: ['sepal length', 'sepal width', 'petal length', 'petal width']\n1st row [ 5.1  3.5  1.4  0.2]\n\n\n\nimport numpy as np\nprint('Classes: Setosa, Versicolor, Virginica')\nprint(np.unique(y))\nprint('Class distribution: %s' % np.bincount(y))\n\n\n\n\nClasses: Setosa, Versicolor, Virginica\n[0 1 2]\nClass distribution: [50 50 50]\n\n\n\nAPI\n\n\niris_data()\n\n\nIris flower dataset.\n\n\n\n\n\n\nSource\n : https://archive.ics.uci.edu/ml/datasets/Iris\n\n\n\n\n\n\nNumber of samples\n : 150\n\n\n\n\n\n\nClass labels\n : {0, 1, 2}, distribution: [50, 50, 50]\n\n\n0 = setosa, 1 = versicolor, 2 = virginica.\n\n\nDataset Attributes:\n\n\n\n\n1) sepal length [cm]\n\n\n2) sepal width [cm]\n\n\n3) petal length [cm]\n\n\n4) petal width [cm]\n\n\n\n\n\n\n\n\nReturns\n\n\n\n\n\n\nX, y\n : [n_samples, n_features], [n_class_labels]\n\n\nX is the feature matrix with 150 flower samples as rows,\nand 4 feature columns sepal length, sepal width,\npetal length, and petal width.\ny is a 1-dimensional array of the class labels {0, 1, 2}",
            "title": "Iris data"
        },
        {
            "location": "/user_guide/data/iris_data/#iris-dataset",
            "text": "A function that loads the  iris  dataset into NumPy arrays.   from mlxtend.data import iris_data",
            "title": "Iris Dataset"
        },
        {
            "location": "/user_guide/data/iris_data/#overview",
            "text": "The Iris dataset for classification.  Features   Sepal length  Sepal width  Petal length   Petal width    Number of samples: 150    Target variable (discrete): {50x Setosa, 50x Versicolor, 50x Virginica}",
            "title": "Overview"
        },
        {
            "location": "/user_guide/data/iris_data/#references",
            "text": "Source:  https://archive.ics.uci.edu/ml/datasets/Iris    Bache, K. & Lichman, M. (2013). UCI Machine Learning Repository. Irvine, CA: University of California, School of Information and Computer Science.",
            "title": "References"
        },
        {
            "location": "/user_guide/data/iris_data/#examples",
            "text": "",
            "title": "Examples"
        },
        {
            "location": "/user_guide/data/iris_data/#example-dataset-overview",
            "text": "from mlxtend.data import iris_data\nX, y = iris_data()\n\nprint('Dimensions: %s x %s' % (X.shape[0], X.shape[1]))\nprint('\\nHeader: %s' % ['sepal length', 'sepal width',\n                        'petal length', 'petal width'])\nprint('1st row', X[0])  Dimensions: 150 x 4\n\nHeader: ['sepal length', 'sepal width', 'petal length', 'petal width']\n1st row [ 5.1  3.5  1.4  0.2]  import numpy as np\nprint('Classes: Setosa, Versicolor, Virginica')\nprint(np.unique(y))\nprint('Class distribution: %s' % np.bincount(y))  Classes: Setosa, Versicolor, Virginica\n[0 1 2]\nClass distribution: [50 50 50]",
            "title": "Example - Dataset overview"
        },
        {
            "location": "/user_guide/data/iris_data/#api",
            "text": "iris_data()  Iris flower dataset.    Source  : https://archive.ics.uci.edu/ml/datasets/Iris    Number of samples  : 150    Class labels  : {0, 1, 2}, distribution: [50, 50, 50]  0 = setosa, 1 = versicolor, 2 = virginica.  Dataset Attributes:   1) sepal length [cm]  2) sepal width [cm]  3) petal length [cm]  4) petal width [cm]     Returns    X, y  : [n_samples, n_features], [n_class_labels]  X is the feature matrix with 150 flower samples as rows,\nand 4 feature columns sepal length, sepal width,\npetal length, and petal width.\ny is a 1-dimensional array of the class labels {0, 1, 2}",
            "title": "API"
        },
        {
            "location": "/user_guide/data/mnist_data/",
            "text": "MNIST Dataset\n\n\nA function that loads the \nMNIST\n dataset into NumPy arrays.\n\n\n\n\nfrom mlxtend.data import mnist_data\n\n\n\n\nOverview\n\n\nThe MNIST dataset was constructed from two datasets of the US National Institute of Standards and Technology (NIST). The training set consists of handwritten digits from 250 different people, 50 percent high school students, and 50 percent employees from the Census Bureau. Note that the test set contains handwritten digits from different people following the same split.\n\n\nFeatures\n\n\nEach feature vector (row in the feature matrix) consists of 784 pixels (intensities) -- unrolled from the original 28x28 pixels images.\n\n\n\n\n\n\nNumber of samples: A subset of 5000 images (the first 500 digits of each class)\n\n\n\n\n\n\nTarget variable (discrete): {500x 0, ..., 500x 9}\n\n\n\n\n\n\nReferences\n\n\n\n\nSource: \nhttp://yann.lecun.com/exdb/mnist/\n\n\nY. LeCun and C. Cortes. Mnist handwritten digit database. AT&T Labs [Online]. Available: \nhttp://yann.lecun.com/exdb/mnist\n, 2010.\n\n\n\n\nExamples\n\n\nExample - Dataset overview\n\n\nfrom mlxtend.data import mnist_data\nX, y = mnist_data()\n\nprint('Dimensions: %s x %s' % (X.shape[0], X.shape[1]))\nprint('1st row', X[0])\n\n\n\n\nDimensions: 5000 x 784\n1st row [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n   0  51 159 253 159  50   0   0   0   0   0   0   0   0   0   0   0   0\n   0   0   0   0   0   0   0   0   0   0  48 238 252 252 252 237   0   0\n   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n   0  54 227 253 252 239 233 252  57   6   0   0   0   0   0   0   0   0\n   0   0   0   0   0   0   0   0   0  10  60 224 252 253 252 202  84 252\n 253 122   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n   0 163 252 252 252 253 252 252  96 189 253 167   0   0   0   0   0   0\n   0   0   0   0   0   0   0   0   0   0  51 238 253 253 190 114 253 228\n  47  79 255 168   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n   0  48 238 252 252 179  12  75 121  21   0   0 253 243  50   0   0   0\n   0   0   0   0   0   0   0   0   0   0  38 165 253 233 208  84   0   0\n   0   0   0   0 253 252 165   0   0   0   0   0   0   0   0   0   0   0\n   0   7 178 252 240  71  19  28   0   0   0   0   0   0 253 252 195   0\n   0   0   0   0   0   0   0   0   0   0   0  57 252 252  63   0   0   0\n   0   0   0   0   0   0 253 252 195   0   0   0   0   0   0   0   0   0\n   0   0   0 198 253 190   0   0   0   0   0   0   0   0   0   0 255 253\n 196   0   0   0   0   0   0   0   0   0   0   0  76 246 252 112   0   0\n   0   0   0   0   0   0   0   0 253 252 148   0   0   0   0   0   0   0\n   0   0   0   0  85 252 230  25   0   0   0   0   0   0   0   0   7 135\n 253 186  12   0   0   0   0   0   0   0   0   0   0   0  85 252 223   0\n   0   0   0   0   0   0   0   7 131 252 225  71   0   0   0   0   0   0\n   0   0   0   0   0   0  85 252 145   0   0   0   0   0   0   0  48 165\n 252 173   0   0   0   0   0   0   0   0   0   0   0   0   0   0  86 253\n 225   0   0   0   0   0   0 114 238 253 162   0   0   0   0   0   0   0\n   0   0   0   0   0   0   0   0  85 252 249 146  48  29  85 178 225 253\n 223 167  56   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n  85 252 252 252 229 215 252 252 252 196 130   0   0   0   0   0   0   0\n   0   0   0   0   0   0   0   0   0   0  28 199 252 252 253 252 252 233\n 145   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n   0   0   0  25 128 252 253 252 141  37   0   0   0   0   0   0   0   0\n   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n   0   0   0   0   0   0   0   0   0   0]\n\n\n\nimport numpy as np\nprint('Classes: Setosa, Versicolor, Virginica')\nprint(np.unique(y))\nprint('Class distribution: %s' % np.bincount(y))\n\n\n\n\nClasses: Setosa, Versicolor, Virginica\n[0 1 2 3 4 5 6 7 8 9]\nClass distribution: [500 500 500 500 500 500 500 500 500 500]\n\n\n\nExample - Visualize MNIST\n\n\n%matplotlib inline\nimport matplotlib.pyplot as plt\ndef plot_digit(X, y, idx):\n    img = X[idx].reshape(28,28)\n    plt.imshow(img, cmap='Greys',  interpolation='nearest')\n    plt.title('true label: %d' % y[idx])\n    plt.show()\nplot_digit(X, y, 4)       \n\n\n\n\n\n\nAPI\n\n\nmnist_data()\n\n\n5000 samples from the MNIST handwritten digits dataset.\n\n\n\n\nData Source\n : http://yann.lecun.com/exdb/mnist/\n\n\n\n\nReturns\n\n\n\n\n\n\nX, y\n : [n_samples, n_features], [n_class_labels]\n\n\nX is the feature matrix with 5000 image samples as rows,\neach row consists of 28x28 pixels that were unrolled into\n784 pixel feature vectors.\ny contains the 10 unique class labels 0-9.",
            "title": "Mnist data"
        },
        {
            "location": "/user_guide/data/mnist_data/#mnist-dataset",
            "text": "A function that loads the  MNIST  dataset into NumPy arrays.   from mlxtend.data import mnist_data",
            "title": "MNIST Dataset"
        },
        {
            "location": "/user_guide/data/mnist_data/#overview",
            "text": "The MNIST dataset was constructed from two datasets of the US National Institute of Standards and Technology (NIST). The training set consists of handwritten digits from 250 different people, 50 percent high school students, and 50 percent employees from the Census Bureau. Note that the test set contains handwritten digits from different people following the same split.  Features  Each feature vector (row in the feature matrix) consists of 784 pixels (intensities) -- unrolled from the original 28x28 pixels images.    Number of samples: A subset of 5000 images (the first 500 digits of each class)    Target variable (discrete): {500x 0, ..., 500x 9}",
            "title": "Overview"
        },
        {
            "location": "/user_guide/data/mnist_data/#references",
            "text": "Source:  http://yann.lecun.com/exdb/mnist/  Y. LeCun and C. Cortes. Mnist handwritten digit database. AT&T Labs [Online]. Available:  http://yann.lecun.com/exdb/mnist , 2010.",
            "title": "References"
        },
        {
            "location": "/user_guide/data/mnist_data/#examples",
            "text": "",
            "title": "Examples"
        },
        {
            "location": "/user_guide/data/mnist_data/#example-dataset-overview",
            "text": "from mlxtend.data import mnist_data\nX, y = mnist_data()\n\nprint('Dimensions: %s x %s' % (X.shape[0], X.shape[1]))\nprint('1st row', X[0])  Dimensions: 5000 x 784\n1st row [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n   0  51 159 253 159  50   0   0   0   0   0   0   0   0   0   0   0   0\n   0   0   0   0   0   0   0   0   0   0  48 238 252 252 252 237   0   0\n   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n   0  54 227 253 252 239 233 252  57   6   0   0   0   0   0   0   0   0\n   0   0   0   0   0   0   0   0   0  10  60 224 252 253 252 202  84 252\n 253 122   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n   0 163 252 252 252 253 252 252  96 189 253 167   0   0   0   0   0   0\n   0   0   0   0   0   0   0   0   0   0  51 238 253 253 190 114 253 228\n  47  79 255 168   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n   0  48 238 252 252 179  12  75 121  21   0   0 253 243  50   0   0   0\n   0   0   0   0   0   0   0   0   0   0  38 165 253 233 208  84   0   0\n   0   0   0   0 253 252 165   0   0   0   0   0   0   0   0   0   0   0\n   0   7 178 252 240  71  19  28   0   0   0   0   0   0 253 252 195   0\n   0   0   0   0   0   0   0   0   0   0   0  57 252 252  63   0   0   0\n   0   0   0   0   0   0 253 252 195   0   0   0   0   0   0   0   0   0\n   0   0   0 198 253 190   0   0   0   0   0   0   0   0   0   0 255 253\n 196   0   0   0   0   0   0   0   0   0   0   0  76 246 252 112   0   0\n   0   0   0   0   0   0   0   0 253 252 148   0   0   0   0   0   0   0\n   0   0   0   0  85 252 230  25   0   0   0   0   0   0   0   0   7 135\n 253 186  12   0   0   0   0   0   0   0   0   0   0   0  85 252 223   0\n   0   0   0   0   0   0   0   7 131 252 225  71   0   0   0   0   0   0\n   0   0   0   0   0   0  85 252 145   0   0   0   0   0   0   0  48 165\n 252 173   0   0   0   0   0   0   0   0   0   0   0   0   0   0  86 253\n 225   0   0   0   0   0   0 114 238 253 162   0   0   0   0   0   0   0\n   0   0   0   0   0   0   0   0  85 252 249 146  48  29  85 178 225 253\n 223 167  56   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n  85 252 252 252 229 215 252 252 252 196 130   0   0   0   0   0   0   0\n   0   0   0   0   0   0   0   0   0   0  28 199 252 252 253 252 252 233\n 145   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n   0   0   0  25 128 252 253 252 141  37   0   0   0   0   0   0   0   0\n   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n   0   0   0   0   0   0   0   0   0   0]  import numpy as np\nprint('Classes: Setosa, Versicolor, Virginica')\nprint(np.unique(y))\nprint('Class distribution: %s' % np.bincount(y))  Classes: Setosa, Versicolor, Virginica\n[0 1 2 3 4 5 6 7 8 9]\nClass distribution: [500 500 500 500 500 500 500 500 500 500]",
            "title": "Example - Dataset overview"
        },
        {
            "location": "/user_guide/data/mnist_data/#example-visualize-mnist",
            "text": "%matplotlib inline\nimport matplotlib.pyplot as plt\ndef plot_digit(X, y, idx):\n    img = X[idx].reshape(28,28)\n    plt.imshow(img, cmap='Greys',  interpolation='nearest')\n    plt.title('true label: %d' % y[idx])\n    plt.show()\nplot_digit(X, y, 4)",
            "title": "Example - Visualize MNIST"
        },
        {
            "location": "/user_guide/data/mnist_data/#api",
            "text": "mnist_data()  5000 samples from the MNIST handwritten digits dataset.   Data Source  : http://yann.lecun.com/exdb/mnist/   Returns    X, y  : [n_samples, n_features], [n_class_labels]  X is the feature matrix with 5000 image samples as rows,\neach row consists of 28x28 pixels that were unrolled into\n784 pixel feature vectors.\ny contains the 10 unique class labels 0-9.",
            "title": "API"
        },
        {
            "location": "/user_guide/data/load_mnist/",
            "text": "Load MNIST Dataset\n\n\nA utility function that loads the \nMNIST\n dataset from byte-form into NumPy arrays.\n\n\n\n\nfrom mlxtend.data_utils import load_mnist_data\n\n\n\n\nOverview\n\n\nThe MNIST dataset was constructed from two datasets of the US National Institute of Standards and Technology (NIST). The training set consists of handwritten digits from 250 different people, 50 percent high school students, and 50 percent employees from the Census Bureau. Note that the test set contains handwritten digits from different people following the same split.\n\n\nThe MNIST dataset is publicly available at http://yann.lecun.com/exdb/mnist/ and consists of the following four parts:\n- Training set images: train-images-idx3-ubyte.gz (9.9 MB, 47 MB unzipped, and 60,000 samples)\n- Training set labels: train-labels-idx1-ubyte.gz (29 KB, 60 KB unzipped, and 60,000 labels)\n- Test set images: t10k-images-idx3-ubyte.gz (1.6 MB, 7.8 MB, unzipped and 10,000 samples)\n- Test set labels: t10k-labels-idx1-ubyte.gz (5 KB, 10 KB unzipped, and 10,000 labels)\n\n\nFeatures\n\n\nEach feature vector (row in the feature matrix) consists of 784 pixels (intensities) -- unrolled from the original 28x28 pixels images.\n\n\n\n\n\n\nNumber of samples: 50000 images\n\n\n\n\n\n\nTarget variable (discrete): {50x Setosa, 50x Versicolor, 50x Virginica}\n\n\n\n\n\n\nReferences\n\n\n\n\nSource: \nhttp://yann.lecun.com/exdb/mnist/\n\n\nY. LeCun and C. Cortes. Mnist handwritten digit database. AT&T Labs [Online]. Available: http://yann. lecun. com/exdb/mnist, 2010.\n\n\n\n\nExamples\n\n\nDownloading the MNIST dataset\n\n\n1) Download the MNIST files from Y. LeCun's website\n\n\n\n\nhttp://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n\n\nhttp://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n\n\nhttp://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n\n\nhttp://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n\n\n\n\nfor example, via\n\n\ncurl -O http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n\n\n\n2) Unzip the downloaded gzip archives\n\n\nfor example, via\n\n\ngunzip t*-ubyte.gz\n\n\n\nExample - Loading MNIST into NumPy Arrays\n\n\nfrom mlxtend.data import loadlocal_mnist\n\n\n\n\nX, y = loadlocal_mnist(\n        images_path='/Users/Sebastian/Desktop/train-images-idx3-ubyte', \n        labels_path='/Users/Sebastian/Desktop/train-labels-idx1-ubyte')\n\n\n\n\n\nprint('Dimensions: %s x %s' % (X.shape[0], X.shape[1]))\nprint('\\n1st row', X[0])\n\n\n\n\nDimensions: 60000 x 784\n\n1st row [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n   0   0   0   0   0   0   0   0   3  18  18  18 126 136 175  26 166 255\n 247 127   0   0   0   0   0   0   0   0   0   0   0   0  30  36  94 154\n 170 253 253 253 253 253 225 172 253 242 195  64   0   0   0   0   0   0\n   0   0   0   0   0  49 238 253 253 253 253 253 253 253 253 251  93  82\n  82  56  39   0   0   0   0   0   0   0   0   0   0   0   0  18 219 253\n 253 253 253 253 198 182 247 241   0   0   0   0   0   0   0   0   0   0\n   0   0   0   0   0   0   0   0  80 156 107 253 253 205  11   0  43 154\n   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n   0  14   1 154 253  90   0   0   0   0   0   0   0   0   0   0   0   0\n   0   0   0   0   0   0   0   0   0   0   0   0   0 139 253 190   2   0\n   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n   0   0   0   0   0  11 190 253  70   0   0   0   0   0   0   0   0   0\n   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0  35 241\n 225 160 108   1   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n   0   0   0   0   0   0   0   0   0  81 240 253 253 119  25   0   0   0\n   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n   0   0  45 186 253 253 150  27   0   0   0   0   0   0   0   0   0   0\n   0   0   0   0   0   0   0   0   0   0   0   0   0  16  93 252 253 187\n   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n   0   0   0   0   0   0   0 249 253 249  64   0   0   0   0   0   0   0\n   0   0   0   0   0   0   0   0   0   0   0   0   0   0  46 130 183 253\n 253 207   2   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n   0   0   0   0  39 148 229 253 253 253 250 182   0   0   0   0   0   0\n   0   0   0   0   0   0   0   0   0   0   0   0  24 114 221 253 253 253\n 253 201  78   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n   0   0  23  66 213 253 253 253 253 198  81   2   0   0   0   0   0   0\n   0   0   0   0   0   0   0   0   0   0  18 171 219 253 253 253 253 195\n  80   9   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n  55 172 226 253 253 253 253 244 133  11   0   0   0   0   0   0   0   0\n   0   0   0   0   0   0   0   0   0   0 136 253 253 253 212 135 132  16\n   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n   0   0   0   0   0   0   0   0   0   0]\n\n\n\nimport numpy as np\nprint('Digits:  0 1 2 3 4 5 6 7 8 9')\nprint('labels: %s' % np.unique(y))\nprint('Class distribution: %s' % np.bincount(y))\n\n\n\n\nDigits:  0 1 2 3 4 5 6 7 8 9\nlabels: [0 1 2 3 4 5 6 7 8 9]\nClass distribution: [5923 6742 5958 6131 5842 5421 5918 6265 5851 5949]\n\n\n\nExamples - Store as CSV Files\n\n\nnp.savetxt(fname='/Users/Sebastian/Desktop/images.csv', X=X, delimiter=',', fmt='%d')\nnp.savetxt(fname='/Users/Sebastian/Desktop/labels.csv', X=y, delimiter=',', fmt='%d')\n\n\n\n\nAPI\n\n\nloadlocal_mnist(images_path, labels_path)\n\n\nRead MNIST from ubyte files.\n\n\nParameters\n\n\n\n\n\n\nimages_path\n : str\n\n\npath to the test or train MNIST ubyte file\n\n\n\n\n\n\nlabels_path\n : str\n\n\npath to the test or train MNIST class labels file\n\n\n\n\n\n\nReturns\n\n\n\n\n\n\nimages\n : [n_samples, n_pixels] numpy.array\n\n\nPixel values of the images.\n\n\n\n\n\n\nlabels\n : [n_samples] numpy array\n\n\nTarget class labels",
            "title": "Load mnist"
        },
        {
            "location": "/user_guide/data/load_mnist/#load-mnist-dataset",
            "text": "A utility function that loads the  MNIST  dataset from byte-form into NumPy arrays.   from mlxtend.data_utils import load_mnist_data",
            "title": "Load MNIST Dataset"
        },
        {
            "location": "/user_guide/data/load_mnist/#overview",
            "text": "The MNIST dataset was constructed from two datasets of the US National Institute of Standards and Technology (NIST). The training set consists of handwritten digits from 250 different people, 50 percent high school students, and 50 percent employees from the Census Bureau. Note that the test set contains handwritten digits from different people following the same split.  The MNIST dataset is publicly available at http://yann.lecun.com/exdb/mnist/ and consists of the following four parts:\n- Training set images: train-images-idx3-ubyte.gz (9.9 MB, 47 MB unzipped, and 60,000 samples)\n- Training set labels: train-labels-idx1-ubyte.gz (29 KB, 60 KB unzipped, and 60,000 labels)\n- Test set images: t10k-images-idx3-ubyte.gz (1.6 MB, 7.8 MB, unzipped and 10,000 samples)\n- Test set labels: t10k-labels-idx1-ubyte.gz (5 KB, 10 KB unzipped, and 10,000 labels)  Features  Each feature vector (row in the feature matrix) consists of 784 pixels (intensities) -- unrolled from the original 28x28 pixels images.    Number of samples: 50000 images    Target variable (discrete): {50x Setosa, 50x Versicolor, 50x Virginica}",
            "title": "Overview"
        },
        {
            "location": "/user_guide/data/load_mnist/#references",
            "text": "Source:  http://yann.lecun.com/exdb/mnist/  Y. LeCun and C. Cortes. Mnist handwritten digit database. AT&T Labs [Online]. Available: http://yann. lecun. com/exdb/mnist, 2010.",
            "title": "References"
        },
        {
            "location": "/user_guide/data/load_mnist/#examples",
            "text": "",
            "title": "Examples"
        },
        {
            "location": "/user_guide/data/load_mnist/#downloading-the-mnist-dataset",
            "text": "1) Download the MNIST files from Y. LeCun's website   http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz  http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz  http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz  http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz   for example, via  curl -O http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz  2) Unzip the downloaded gzip archives  for example, via  gunzip t*-ubyte.gz",
            "title": "Downloading the MNIST dataset"
        },
        {
            "location": "/user_guide/data/load_mnist/#example-loading-mnist-into-numpy-arrays",
            "text": "from mlxtend.data import loadlocal_mnist  X, y = loadlocal_mnist(\n        images_path='/Users/Sebastian/Desktop/train-images-idx3-ubyte', \n        labels_path='/Users/Sebastian/Desktop/train-labels-idx1-ubyte')  print('Dimensions: %s x %s' % (X.shape[0], X.shape[1]))\nprint('\\n1st row', X[0])  Dimensions: 60000 x 784\n\n1st row [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n   0   0   0   0   0   0   0   0   3  18  18  18 126 136 175  26 166 255\n 247 127   0   0   0   0   0   0   0   0   0   0   0   0  30  36  94 154\n 170 253 253 253 253 253 225 172 253 242 195  64   0   0   0   0   0   0\n   0   0   0   0   0  49 238 253 253 253 253 253 253 253 253 251  93  82\n  82  56  39   0   0   0   0   0   0   0   0   0   0   0   0  18 219 253\n 253 253 253 253 198 182 247 241   0   0   0   0   0   0   0   0   0   0\n   0   0   0   0   0   0   0   0  80 156 107 253 253 205  11   0  43 154\n   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n   0  14   1 154 253  90   0   0   0   0   0   0   0   0   0   0   0   0\n   0   0   0   0   0   0   0   0   0   0   0   0   0 139 253 190   2   0\n   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n   0   0   0   0   0  11 190 253  70   0   0   0   0   0   0   0   0   0\n   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0  35 241\n 225 160 108   1   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n   0   0   0   0   0   0   0   0   0  81 240 253 253 119  25   0   0   0\n   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n   0   0  45 186 253 253 150  27   0   0   0   0   0   0   0   0   0   0\n   0   0   0   0   0   0   0   0   0   0   0   0   0  16  93 252 253 187\n   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n   0   0   0   0   0   0   0 249 253 249  64   0   0   0   0   0   0   0\n   0   0   0   0   0   0   0   0   0   0   0   0   0   0  46 130 183 253\n 253 207   2   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n   0   0   0   0  39 148 229 253 253 253 250 182   0   0   0   0   0   0\n   0   0   0   0   0   0   0   0   0   0   0   0  24 114 221 253 253 253\n 253 201  78   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n   0   0  23  66 213 253 253 253 253 198  81   2   0   0   0   0   0   0\n   0   0   0   0   0   0   0   0   0   0  18 171 219 253 253 253 253 195\n  80   9   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n  55 172 226 253 253 253 253 244 133  11   0   0   0   0   0   0   0   0\n   0   0   0   0   0   0   0   0   0   0 136 253 253 253 212 135 132  16\n   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n   0   0   0   0   0   0   0   0   0   0]  import numpy as np\nprint('Digits:  0 1 2 3 4 5 6 7 8 9')\nprint('labels: %s' % np.unique(y))\nprint('Class distribution: %s' % np.bincount(y))  Digits:  0 1 2 3 4 5 6 7 8 9\nlabels: [0 1 2 3 4 5 6 7 8 9]\nClass distribution: [5923 6742 5958 6131 5842 5421 5918 6265 5851 5949]",
            "title": "Example - Loading MNIST into NumPy Arrays"
        },
        {
            "location": "/user_guide/data/load_mnist/#examples-store-as-csv-files",
            "text": "np.savetxt(fname='/Users/Sebastian/Desktop/images.csv', X=X, delimiter=',', fmt='%d')\nnp.savetxt(fname='/Users/Sebastian/Desktop/labels.csv', X=y, delimiter=',', fmt='%d')",
            "title": "Examples - Store as CSV Files"
        },
        {
            "location": "/user_guide/data/load_mnist/#api",
            "text": "loadlocal_mnist(images_path, labels_path)  Read MNIST from ubyte files.  Parameters    images_path  : str  path to the test or train MNIST ubyte file    labels_path  : str  path to the test or train MNIST class labels file    Returns    images  : [n_samples, n_pixels] numpy.array  Pixel values of the images.    labels  : [n_samples] numpy array  Target class labels",
            "title": "API"
        },
        {
            "location": "/user_guide/data/wine_data/",
            "text": "Wine Dataset\n\n\nA function that loads the \nWine\n dataset into NumPy arrays.\n\n\n\n\nfrom mlxtend.data import wine_data\n\n\n\n\nOverview\n\n\nThe Wine dataset for classification.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSamples\n\n\n178\n\n\n\n\n\n\nFeatures\n\n\n13\n\n\n\n\n\n\nClasses\n\n\n3\n\n\n\n\n\n\nData Set Characteristics:\n\n\nMultivariate\n\n\n\n\n\n\nAttribute Characteristics:\n\n\nInteger, Real\n\n\n\n\n\n\nAssociated Tasks:\n\n\nClassification\n\n\n\n\n\n\nMissing Values\n\n\nNone\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ncolumn\n\n\nattribute\n\n\n\n\n\n\n\n\n\n\n1)\n\n\nClass Label\n\n\n\n\n\n\n2)\n\n\nAlcohol\n\n\n\n\n\n\n3)\n\n\nMalic acid\n\n\n\n\n\n\n4)\n\n\nAsh\n\n\n\n\n\n\n5)\n\n\nAlcalinity of ash\n\n\n\n\n\n\n6)\n\n\nMagnesium\n\n\n\n\n\n\n7)\n\n\nTotal phenols\n\n\n\n\n\n\n8)\n\n\nFlavanoids\n\n\n\n\n\n\n9)\n\n\nNonflavanoid phenols\n\n\n\n\n\n\n10)\n\n\nProanthocyanins\n\n\n\n\n\n\n11)\n\n\nintensity\n\n\n\n\n\n\n12)\n\n\nHue\n\n\n\n\n\n\n13)\n\n\nOD280/OD315 of diluted wines\n\n\n\n\n\n\n14)\n\n\nProline\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nclass\n\n\nsamples\n\n\n\n\n\n\n\n\n\n\n0\n\n\n59\n\n\n\n\n\n\n1\n\n\n71\n\n\n\n\n\n\n2\n\n\n48\n\n\n\n\n\n\n\n\nReferences\n\n\n\n\nForina, M. et al, PARVUS - \nAn Extendible Package for Data Exploration, Classification and Correlation. \nInstitute of Pharmaceutical and Food Analysis and Technologies, Via Brigata Salerno, \n16147 Genoa, Italy. \n\n\nSource: \nhttps://archive.ics.uci.edu/ml/datasets/Wine\n\n\nBache, K. & Lichman, M. (2013). UCI Machine Learning Repository. Irvine, CA: University of California, School of Information and Computer Science.\n\n\n\n\nExamples\n\n\nExample - Dataset overview\n\n\nfrom mlxtend.data import wine_data\nX, y = wine_data()\n\nprint('Dimensions: %s x %s' % (X.shape[0], X.shape[1]))\nprint('\\nHeader: %s' % ['sepal length', 'sepal width',\n                        'petal length', 'petal width'])\nprint('1st row', X[0])\n\n\n\n\nDimensions: 178 x 13\n\nHeader: ['sepal length', 'sepal width', 'petal length', 'petal width']\n1st row [  1.42300000e+01   1.71000000e+00   2.43000000e+00   1.56000000e+01\n   1.27000000e+02   2.80000000e+00   3.06000000e+00   2.80000000e-01\n   2.29000000e+00   5.64000000e+00   1.04000000e+00   3.92000000e+00\n   1.06500000e+03]\n\n\n\nimport numpy as np\nprint('Classes: %s' % np.unique(y))\nprint('Class distribution: %s' % np.bincount(y))\n\n\n\n\nClasses: [0 1 2]\nClass distribution: [59 71 48]\n\n\n\nAPI\n\n\nwine_data()\n\n\nWine dataset.\n\n\n\n\n\n\nSource\n : https://archive.ics.uci.edu/ml/datasets/Wine\n\n\n\n\n\n\nNumber of samples\n : 178\n\n\n\n\n\n\nClass labels\n : {0, 1, 2}, distribution: [59, 71, 48]\n\n\nDataset Attributes:\n\n\n\n\n1) Alcohol\n\n\n2) Malic acid\n\n\n3) Ash\n\n\n4) Alcalinity of ash\n\n\n5) Magnesium\n\n\n6) Total phenols\n\n\n7) Flavanoids\n\n\n8) Nonflavanoid phenols\n\n\n9) Proanthocyanins\n\n\n10) Color intensity\n\n\n11) Hue\n\n\n12) OD280/OD315 of diluted wines\n\n\n13) Proline\n\n\n\n\n\n\n\n\nReturns\n\n\n\n\n\n\nX, y\n : [n_samples, n_features], [n_class_labels]\n\n\nX is the feature matrix with 178 wine samples as rows\nand 13 feature columns.\ny is a 1-dimensional array of the 3 class labels 0, 1, 2",
            "title": "Wine data"
        },
        {
            "location": "/user_guide/data/wine_data/#wine-dataset",
            "text": "A function that loads the  Wine  dataset into NumPy arrays.   from mlxtend.data import wine_data",
            "title": "Wine Dataset"
        },
        {
            "location": "/user_guide/data/wine_data/#overview",
            "text": "The Wine dataset for classification.           Samples  178    Features  13    Classes  3    Data Set Characteristics:  Multivariate    Attribute Characteristics:  Integer, Real    Associated Tasks:  Classification    Missing Values  None        column  attribute      1)  Class Label    2)  Alcohol    3)  Malic acid    4)  Ash    5)  Alcalinity of ash    6)  Magnesium    7)  Total phenols    8)  Flavanoids    9)  Nonflavanoid phenols    10)  Proanthocyanins    11)  intensity    12)  Hue    13)  OD280/OD315 of diluted wines    14)  Proline        class  samples      0  59    1  71    2  48",
            "title": "Overview"
        },
        {
            "location": "/user_guide/data/wine_data/#references",
            "text": "Forina, M. et al, PARVUS - \nAn Extendible Package for Data Exploration, Classification and Correlation. \nInstitute of Pharmaceutical and Food Analysis and Technologies, Via Brigata Salerno, \n16147 Genoa, Italy.   Source:  https://archive.ics.uci.edu/ml/datasets/Wine  Bache, K. & Lichman, M. (2013). UCI Machine Learning Repository. Irvine, CA: University of California, School of Information and Computer Science.",
            "title": "References"
        },
        {
            "location": "/user_guide/data/wine_data/#examples",
            "text": "",
            "title": "Examples"
        },
        {
            "location": "/user_guide/data/wine_data/#example-dataset-overview",
            "text": "from mlxtend.data import wine_data\nX, y = wine_data()\n\nprint('Dimensions: %s x %s' % (X.shape[0], X.shape[1]))\nprint('\\nHeader: %s' % ['sepal length', 'sepal width',\n                        'petal length', 'petal width'])\nprint('1st row', X[0])  Dimensions: 178 x 13\n\nHeader: ['sepal length', 'sepal width', 'petal length', 'petal width']\n1st row [  1.42300000e+01   1.71000000e+00   2.43000000e+00   1.56000000e+01\n   1.27000000e+02   2.80000000e+00   3.06000000e+00   2.80000000e-01\n   2.29000000e+00   5.64000000e+00   1.04000000e+00   3.92000000e+00\n   1.06500000e+03]  import numpy as np\nprint('Classes: %s' % np.unique(y))\nprint('Class distribution: %s' % np.bincount(y))  Classes: [0 1 2]\nClass distribution: [59 71 48]",
            "title": "Example - Dataset overview"
        },
        {
            "location": "/user_guide/data/wine_data/#api",
            "text": "wine_data()  Wine dataset.    Source  : https://archive.ics.uci.edu/ml/datasets/Wine    Number of samples  : 178    Class labels  : {0, 1, 2}, distribution: [59, 71, 48]  Dataset Attributes:   1) Alcohol  2) Malic acid  3) Ash  4) Alcalinity of ash  5) Magnesium  6) Total phenols  7) Flavanoids  8) Nonflavanoid phenols  9) Proanthocyanins  10) Color intensity  11) Hue  12) OD280/OD315 of diluted wines  13) Proline     Returns    X, y  : [n_samples, n_features], [n_class_labels]  X is the feature matrix with 178 wine samples as rows\nand 13 feature columns.\ny is a 1-dimensional array of the 3 class labels 0, 1, 2",
            "title": "API"
        },
        {
            "location": "/user_guide/file_io/find_filegroups/",
            "text": "Find Filegroups\n\n\nA function that finds files that belong together (i.e., differ only by file extension) in different directories and collects them in a Python dictionary for further processing tasks. \n\n\n\n\nfrom mlxtend.file_io import find_filegroups\n\n\n\n\nOverview\n\n\nThis function finds files that are related to each other based on their file names. This can be useful for parsing collections files that have been stored in different subdirectories, for examples:\n\n\ninput_dir/\n    task01.txt\n    task02.txt\n    ...\nlog_dir/\n    task01.log\n    task02.log\n    ...\noutput_dir/\n    task01.dat\n    task02.dat\n    ...\n\n\n\nReferences\n\n\n\n\n-\n\n\n\n\nExamples\n\n\nExample 1 - Grouping related files in a dictionary\n\n\nGiven the following directory and file structure\n\n\ndir_1/\n    file_1.log\n    file_2.log\n    file_3.log\ndir_2/\n    file_1.csv\n    file_2.csv\n    file_3.csv\ndir_3/\n    file_1.txt\n    file_2.txt\n    file_3.txt\n\n\n\nwe can use \nfind_filegroups\n to group related files as items of a dictionary as shown below:\n\n\nfrom mlxtend.file_io import find_filegroups\n\nfind_filegroups(paths=['./data_find_filegroups/dir_1', \n                       './data_find_filegroups/dir_2', \n                       './data_find_filegroups/dir_3'], \n                substring='file_')\n\n\n\n\n{'file_1': ['./data_find_filegroups/dir_1/file_1.log',\n  './data_find_filegroups/dir_2/file_1.csv',\n  './data_find_filegroups/dir_3/file_1.txt'],\n 'file_2': ['./data_find_filegroups/dir_1/file_2.log',\n  './data_find_filegroups/dir_2/file_2.csv',\n  './data_find_filegroups/dir_3/file_2.txt'],\n 'file_3': ['./data_find_filegroups/dir_1/file_3.log',\n  './data_find_filegroups/dir_2/file_3.csv',\n  './data_find_filegroups/dir_3/file_3.txt']}\n\n\n\nAPI\n\n\nfind_filegroups(paths, substring='', extensions=None, validity_check=True, ignore_invisible=True, rstrip='', ignore_substring=None)\n\n\nFind and collect files from different directories in a python dictionary.\n\n\nParameters\n\n\n\n\n\n\npaths\n : \nlist\n\n\nPaths of the directories to be searched. Dictionary keys are build from\nthe first directory.\n\n\n\n\n\n\nsubstring\n : \nstr\n (default: '')\n\n\nSubstring that all files have to contain to be considered.\n\n\n\n\n\n\nextensions\n : \nlist\n (default: None)\n\n\nNone\n or \nlist\n of allowed file extensions for each path.\nIf provided, the number of extensions must match the number of \npaths\n.\n\n\n\n\n\n\nvalidity_check\n : \nbool\n (default: None)\n\n\nIf \nTrue\n, checks if all dictionary values\nhave the same number of file paths. Prints\na warning and returns an empty dictionary if the validity check failed.\n\n\n\n\n\n\nignore_invisible\n : \nbool\n (default: True)\n\n\nIf \nTrue\n, ignores invisible files\n(i.e., files starting with a period).\n\n\n\n\n\n\nrstrip\n : \nstr\n (default: '')\n\n\nIf provided, strips characters from right side of the file\nbase names after splitting the extension.\nUseful to trim different filenames to a common stem.\nE.g,. \"abc_d.txt\" and \"abc_d_.csv\" would share\nthe stem \"abc_d\" if rstrip is set to \"_\".\n\n\n\n\n\n\nignore_substring\n : \nstr\n (default: None)\n\n\nIgnores files that contain the specified substring.\n\n\n\n\n\n\nReturns\n\n\n\n\n\n\ngroups\n : \ndict\n\n\nDictionary of files paths. Keys are the file names\nfound in the first directory listed\nin \npaths\n (without file extension).",
            "title": "Find filegroups"
        },
        {
            "location": "/user_guide/file_io/find_filegroups/#find-filegroups",
            "text": "A function that finds files that belong together (i.e., differ only by file extension) in different directories and collects them in a Python dictionary for further processing tasks.    from mlxtend.file_io import find_filegroups",
            "title": "Find Filegroups"
        },
        {
            "location": "/user_guide/file_io/find_filegroups/#overview",
            "text": "This function finds files that are related to each other based on their file names. This can be useful for parsing collections files that have been stored in different subdirectories, for examples:  input_dir/\n    task01.txt\n    task02.txt\n    ...\nlog_dir/\n    task01.log\n    task02.log\n    ...\noutput_dir/\n    task01.dat\n    task02.dat\n    ...",
            "title": "Overview"
        },
        {
            "location": "/user_guide/file_io/find_filegroups/#references",
            "text": "-",
            "title": "References"
        },
        {
            "location": "/user_guide/file_io/find_filegroups/#examples",
            "text": "",
            "title": "Examples"
        },
        {
            "location": "/user_guide/file_io/find_filegroups/#example-1-grouping-related-files-in-a-dictionary",
            "text": "Given the following directory and file structure  dir_1/\n    file_1.log\n    file_2.log\n    file_3.log\ndir_2/\n    file_1.csv\n    file_2.csv\n    file_3.csv\ndir_3/\n    file_1.txt\n    file_2.txt\n    file_3.txt  we can use  find_filegroups  to group related files as items of a dictionary as shown below:  from mlxtend.file_io import find_filegroups\n\nfind_filegroups(paths=['./data_find_filegroups/dir_1', \n                       './data_find_filegroups/dir_2', \n                       './data_find_filegroups/dir_3'], \n                substring='file_')  {'file_1': ['./data_find_filegroups/dir_1/file_1.log',\n  './data_find_filegroups/dir_2/file_1.csv',\n  './data_find_filegroups/dir_3/file_1.txt'],\n 'file_2': ['./data_find_filegroups/dir_1/file_2.log',\n  './data_find_filegroups/dir_2/file_2.csv',\n  './data_find_filegroups/dir_3/file_2.txt'],\n 'file_3': ['./data_find_filegroups/dir_1/file_3.log',\n  './data_find_filegroups/dir_2/file_3.csv',\n  './data_find_filegroups/dir_3/file_3.txt']}",
            "title": "Example 1 - Grouping related files in a dictionary"
        },
        {
            "location": "/user_guide/file_io/find_filegroups/#api",
            "text": "find_filegroups(paths, substring='', extensions=None, validity_check=True, ignore_invisible=True, rstrip='', ignore_substring=None)  Find and collect files from different directories in a python dictionary.  Parameters    paths  :  list  Paths of the directories to be searched. Dictionary keys are build from\nthe first directory.    substring  :  str  (default: '')  Substring that all files have to contain to be considered.    extensions  :  list  (default: None)  None  or  list  of allowed file extensions for each path.\nIf provided, the number of extensions must match the number of  paths .    validity_check  :  bool  (default: None)  If  True , checks if all dictionary values\nhave the same number of file paths. Prints\na warning and returns an empty dictionary if the validity check failed.    ignore_invisible  :  bool  (default: True)  If  True , ignores invisible files\n(i.e., files starting with a period).    rstrip  :  str  (default: '')  If provided, strips characters from right side of the file\nbase names after splitting the extension.\nUseful to trim different filenames to a common stem.\nE.g,. \"abc_d.txt\" and \"abc_d_.csv\" would share\nthe stem \"abc_d\" if rstrip is set to \"_\".    ignore_substring  :  str  (default: None)  Ignores files that contain the specified substring.    Returns    groups  :  dict  Dictionary of files paths. Keys are the file names\nfound in the first directory listed\nin  paths  (without file extension).",
            "title": "API"
        },
        {
            "location": "/user_guide/file_io/find_files/",
            "text": "Find Filegroups\n\n\nA function that finds files in a given directory based on substring matches and returns a list of the file names found.\n\n\n\n\nfrom mlxtend.file_io import find_files\n\n\n\n\nOverview\n\n\nThis function finds files based on substring search. This is especially useful if we want to find specific files in a directory tree and return their absolute paths for further processing in Python.\n\n\nReferences\n\n\n\n\n-\n\n\n\n\nExamples\n\n\nExample 1 - Grouping related files in a dictionary\n\n\nGiven the following directory and file structure\n\n\ndir_1/\n    file_1.log\n    file_2.log\n    file_3.log\ndir_2/\n    file_1.csv\n    file_2.csv\n    file_3.csv\ndir_3/\n    file_1.txt\n    file_2.txt\n    file_3.txt\n\n\n\nwe can use \nfind_files\n to return the paths to all files that contain the substring \n_2\n as follows: \n\n\nfrom mlxtend.file_io import find_files\n\nfind_files(substring='_2', path='./data_find_filegroups/', recursive=True)\n\n\n\n\n['./data_find_filegroups/dir_1/file_2.log',\n './data_find_filegroups/dir_2/file_2.csv',\n './data_find_filegroups/dir_3/file_2.txt']\n\n\n\nAPI\n\n\nfind_files(substring, path, recursive=False, check_ext=None, ignore_invisible=True, ignore_substring=None)\n\n\nFind files in a directory based on substring matching.\n\n\nParameters\n\n\n\n\n\n\nsubstring\n : \nstr\n\n\nSubstring of the file to be matched.\n\n\n\n\n\n\npath\n : \nstr\n\n\nPath where to look.\nrecursive: \nbool\n\nIf true, searches subdirectories recursively.\ncheck_ext: \nstr\n\nIf string (e.g., '.txt'), only returns files that\nmatch the specified file extension.\n\n\n\n\n\n\nignore_invisible\n : \nbool\n\n\nIf \nTrue\n, ignores invisible files\n(i.e., files starting with a period).\n\n\n\n\n\n\nignore_substring\n : \nstr\n\n\nIgnores files that contain the specified substring.\n\n\n\n\n\n\nReturns\n\n\n\n\n\n\nresults\n : \nlist\n\n\nList of the matched files.",
            "title": "Find files"
        },
        {
            "location": "/user_guide/file_io/find_files/#find-filegroups",
            "text": "A function that finds files in a given directory based on substring matches and returns a list of the file names found.   from mlxtend.file_io import find_files",
            "title": "Find Filegroups"
        },
        {
            "location": "/user_guide/file_io/find_files/#overview",
            "text": "This function finds files based on substring search. This is especially useful if we want to find specific files in a directory tree and return their absolute paths for further processing in Python.",
            "title": "Overview"
        },
        {
            "location": "/user_guide/file_io/find_files/#references",
            "text": "-",
            "title": "References"
        },
        {
            "location": "/user_guide/file_io/find_files/#examples",
            "text": "",
            "title": "Examples"
        },
        {
            "location": "/user_guide/file_io/find_files/#example-1-grouping-related-files-in-a-dictionary",
            "text": "Given the following directory and file structure  dir_1/\n    file_1.log\n    file_2.log\n    file_3.log\ndir_2/\n    file_1.csv\n    file_2.csv\n    file_3.csv\ndir_3/\n    file_1.txt\n    file_2.txt\n    file_3.txt  we can use  find_files  to return the paths to all files that contain the substring  _2  as follows:   from mlxtend.file_io import find_files\n\nfind_files(substring='_2', path='./data_find_filegroups/', recursive=True)  ['./data_find_filegroups/dir_1/file_2.log',\n './data_find_filegroups/dir_2/file_2.csv',\n './data_find_filegroups/dir_3/file_2.txt']",
            "title": "Example 1 - Grouping related files in a dictionary"
        },
        {
            "location": "/user_guide/file_io/find_files/#api",
            "text": "find_files(substring, path, recursive=False, check_ext=None, ignore_invisible=True, ignore_substring=None)  Find files in a directory based on substring matching.  Parameters    substring  :  str  Substring of the file to be matched.    path  :  str  Path where to look.\nrecursive:  bool \nIf true, searches subdirectories recursively.\ncheck_ext:  str \nIf string (e.g., '.txt'), only returns files that\nmatch the specified file extension.    ignore_invisible  :  bool  If  True , ignores invisible files\n(i.e., files starting with a period).    ignore_substring  :  str  Ignores files that contain the specified substring.    Returns    results  :  list  List of the matched files.",
            "title": "API"
        },
        {
            "location": "/user_guide/general_plotting/category_scatter/",
            "text": "Scatterplot with Categories\n\n\nA function to quickly produce a scatter plot colored by categories from a pandas \nDataFrame\n or NumPy \nndarray\n object.\n\n\n\n\nfrom mlxtend.general_plotting import category_scatter\n\n\n\n\nOverview\n\n\nReferences\n\n\n\n\n-\n\n\n\n\nExamples\n\n\nExample 1 - Category Scatter from Pandas DataFrames\n\n\nimport pandas as pd\nfrom io import StringIO\n\ncsvfile = \"\"\"label,x,y\nclass1,10.0,8.04\nclass1,10.5,7.30\nclass2,8.3,5.5\nclass2,8.1,5.9\nclass3,3.5,3.5\nclass3,3.8,5.1\"\"\"\n\ndf = pd.read_csv(StringIO(csvfile))\ndf\n\n\n\n\n\n\n\n  \n\n    \n\n      \n\n      \nlabel\n\n      \nx\n\n      \ny\n\n    \n\n  \n\n  \n\n    \n\n      \n0\n\n      \nclass1\n\n      \n10.0\n\n      \n8.04\n\n    \n\n    \n\n      \n1\n\n      \nclass1\n\n      \n10.5\n\n      \n7.30\n\n    \n\n    \n\n      \n2\n\n      \nclass2\n\n      \n8.3\n\n      \n5.50\n\n    \n\n    \n\n      \n3\n\n      \nclass2\n\n      \n8.1\n\n      \n5.90\n\n    \n\n    \n\n      \n4\n\n      \nclass3\n\n      \n3.5\n\n      \n3.50\n\n    \n\n    \n\n      \n5\n\n      \nclass3\n\n      \n3.8\n\n      \n5.10\n\n    \n\n  \n\n\n\n\n\n\n\nPlotting the data where the categories are determined by the unique values in the label column \nlabel_col\n. The \nx\n and \ny\n values are simply the column names of the DataFrame that we want to plot.\n\n\n%matplotlib inline\nimport matplotlib.pyplot as plt\nfrom mlxtend.general_plotting import category_scatter\n\nfig = category_scatter(x='x', y='y', label_col='label', data=df, legend_loc='upper left')\n\n\n\n\n\n\nExample 2 - Category Scatter from NumPy Arrays\n\n\nimport numpy as np\nfrom io import BytesIO\n\ncsvfile = \"\"\"1,10.0,8.04\n1,10.5,7.30\n2,8.3,5.5\n2,8.1,5.9\n3,3.5,3.5\n3,3.8,5.1\"\"\"\n\nary = np.genfromtxt(BytesIO(csvfile.encode()), delimiter=',')\nary\n\n\n\n\narray([[  1.  ,  10.  ,   8.04],\n       [  1.  ,  10.5 ,   7.3 ],\n       [  2.  ,   8.3 ,   5.5 ],\n       [  2.  ,   8.1 ,   5.9 ],\n       [  3.  ,   3.5 ,   3.5 ],\n       [  3.  ,   3.8 ,   5.1 ]])\n\n\n\nNow, pretending that the first column represents the labels, and the second and third column represent the \nx\n and \ny\n values, respectively.\n\n\n%matplotlib inline\nimport matplotlib.pyplot as plt\nfrom mlxtend.general_plotting import category_scatter\n\nfix = category_scatter(x=1, y=2, label_col=0, data=ary, legend_loc='upper left')\n\n\n\n\n\n\nAPI\n\n\ncategory_scatter(x, y, label_col, data, markers='sxo^v', colors=('blue', 'green', 'red', 'purple', 'gray', 'cyan'), alpha=0.7, markersize=20.0, legend_loc='best')\n\n\nScatter plot to plot categories in different colors/markerstyles.\n\n\nParameters\n\n\n\n\n\n\nx\n : str or int\n\n\nDataFrame column name of the x-axis values or\ninteger for the numpy ndarray column index.\n\n\n\n\n\n\ny\n : str\n\n\nDataFrame column name of the y-axis values or\ninteger for the numpy ndarray column index\n\n\n\n\n\n\ndata\n : Pandas DataFrame object or NumPy ndarray.\n\n\n\n\n\n\nmarkers\n : str\n\n\nMarkers that are cycled through the label category.\n\n\n\n\n\n\ncolors\n : tuple\n\n\nColors that are cycled through the label category.\n\n\n\n\n\n\nalpha\n : float (default: 0.7)\n\n\nParameter to control the transparency.\n\n\n\n\n\n\nmarkersize\n : float (default` : 20.0)\n\n\nParameter to control the marker size.\n\n\n\n\n\n\nlegend_loc\n : str (default: 'best')\n\n\nLocation of the plot legend\n{best, upper left, upper right, lower left, lower right}\nNo legend if legend_loc=False\n\n\n\n\n\n\nReturns\n\n\n\n\nfig\n : matplotlig.pyplot figure object",
            "title": "Category scatter"
        },
        {
            "location": "/user_guide/general_plotting/category_scatter/#scatterplot-with-categories",
            "text": "A function to quickly produce a scatter plot colored by categories from a pandas  DataFrame  or NumPy  ndarray  object.   from mlxtend.general_plotting import category_scatter",
            "title": "Scatterplot with Categories"
        },
        {
            "location": "/user_guide/general_plotting/category_scatter/#overview",
            "text": "",
            "title": "Overview"
        },
        {
            "location": "/user_guide/general_plotting/category_scatter/#references",
            "text": "-",
            "title": "References"
        },
        {
            "location": "/user_guide/general_plotting/category_scatter/#examples",
            "text": "",
            "title": "Examples"
        },
        {
            "location": "/user_guide/general_plotting/category_scatter/#example-1-category-scatter-from-pandas-dataframes",
            "text": "import pandas as pd\nfrom io import StringIO\n\ncsvfile = \"\"\"label,x,y\nclass1,10.0,8.04\nclass1,10.5,7.30\nclass2,8.3,5.5\nclass2,8.1,5.9\nclass3,3.5,3.5\nclass3,3.8,5.1\"\"\"\n\ndf = pd.read_csv(StringIO(csvfile))\ndf   \n   \n     \n       \n       label \n       x \n       y \n     \n   \n   \n     \n       0 \n       class1 \n       10.0 \n       8.04 \n     \n     \n       1 \n       class1 \n       10.5 \n       7.30 \n     \n     \n       2 \n       class2 \n       8.3 \n       5.50 \n     \n     \n       3 \n       class2 \n       8.1 \n       5.90 \n     \n     \n       4 \n       class3 \n       3.5 \n       3.50 \n     \n     \n       5 \n       class3 \n       3.8 \n       5.10 \n     \n      Plotting the data where the categories are determined by the unique values in the label column  label_col . The  x  and  y  values are simply the column names of the DataFrame that we want to plot.  %matplotlib inline\nimport matplotlib.pyplot as plt\nfrom mlxtend.general_plotting import category_scatter\n\nfig = category_scatter(x='x', y='y', label_col='label', data=df, legend_loc='upper left')",
            "title": "Example 1 - Category Scatter from Pandas DataFrames"
        },
        {
            "location": "/user_guide/general_plotting/category_scatter/#example-2-category-scatter-from-numpy-arrays",
            "text": "import numpy as np\nfrom io import BytesIO\n\ncsvfile = \"\"\"1,10.0,8.04\n1,10.5,7.30\n2,8.3,5.5\n2,8.1,5.9\n3,3.5,3.5\n3,3.8,5.1\"\"\"\n\nary = np.genfromtxt(BytesIO(csvfile.encode()), delimiter=',')\nary  array([[  1.  ,  10.  ,   8.04],\n       [  1.  ,  10.5 ,   7.3 ],\n       [  2.  ,   8.3 ,   5.5 ],\n       [  2.  ,   8.1 ,   5.9 ],\n       [  3.  ,   3.5 ,   3.5 ],\n       [  3.  ,   3.8 ,   5.1 ]])  Now, pretending that the first column represents the labels, and the second and third column represent the  x  and  y  values, respectively.  %matplotlib inline\nimport matplotlib.pyplot as plt\nfrom mlxtend.general_plotting import category_scatter\n\nfix = category_scatter(x=1, y=2, label_col=0, data=ary, legend_loc='upper left')",
            "title": "Example 2 - Category Scatter from NumPy Arrays"
        },
        {
            "location": "/user_guide/general_plotting/category_scatter/#api",
            "text": "category_scatter(x, y, label_col, data, markers='sxo^v', colors=('blue', 'green', 'red', 'purple', 'gray', 'cyan'), alpha=0.7, markersize=20.0, legend_loc='best')  Scatter plot to plot categories in different colors/markerstyles.  Parameters    x  : str or int  DataFrame column name of the x-axis values or\ninteger for the numpy ndarray column index.    y  : str  DataFrame column name of the y-axis values or\ninteger for the numpy ndarray column index    data  : Pandas DataFrame object or NumPy ndarray.    markers  : str  Markers that are cycled through the label category.    colors  : tuple  Colors that are cycled through the label category.    alpha  : float (default: 0.7)  Parameter to control the transparency.    markersize  : float (default` : 20.0)  Parameter to control the marker size.    legend_loc  : str (default: 'best')  Location of the plot legend\n{best, upper left, upper right, lower left, lower right}\nNo legend if legend_loc=False    Returns   fig  : matplotlig.pyplot figure object",
            "title": "API"
        },
        {
            "location": "/user_guide/general_plotting/enrichment_plot/",
            "text": "Enrichment Plot\n\n\nA function to plot step plots of cumulative counts.\n\n\n\n\nfrom mlxtend.general_plotting import category_scatter\n\n\n\n\nOverview\n\n\nIn enrichment plots, the y-axis can be interpreted as \"how many samples are less or equal to the corresponding x-axis label.\"\n\n\nReferences\n\n\n\n\n-\n\n\n\n\nExamples\n\n\nExample 1 - Enrichment Plots from Pandas DataFrames\n\n\nimport pandas as pd\ns1 = [1.1, 1.5]\ns2 = [2.1, 1.8]\ns3 = [3.1, 2.1]\ns4 = [3.9, 2.5]\ndata = [s1, s2, s3, s4]\ndf = pd.DataFrame(data, columns=['X1', 'X2'])\ndf\n\n\n\n\n\n\n\n  \n\n    \n\n      \n\n      \nX1\n\n      \nX2\n\n    \n\n  \n\n  \n\n    \n\n      \n0\n\n      \n1.1\n\n      \n1.5\n\n    \n\n    \n\n      \n1\n\n      \n2.1\n\n      \n1.8\n\n    \n\n    \n\n      \n2\n\n      \n3.1\n\n      \n2.1\n\n    \n\n    \n\n      \n3\n\n      \n3.9\n\n      \n2.5\n\n    \n\n  \n\n\n\n\n\n\n\nPlotting the data where the categories are determined by the unique values in the label column \nlabel_col\n. The \nx\n and \ny\n values are simply the column names of the DataFrame that we want to plot.\n\n\n%matplotlib inline\nimport matplotlib.pyplot as plt\nfrom mlxtend.general_plotting import enrichment_plot\n\nax = enrichment_plot(df, legend_loc='upper left')\n\n\n\n\n\n\nAPI\n\n\nenrichment_plot(df, colors='bgrkcy', markers=' ', linestyles='-', alpha=0.5, lw=2, legend=True, where='post', grid=True, count_label='Count', xlim='auto', ylim='auto', invert_axes=False, legend_loc='best', ax=None)\n\n\nPlot stacked barplots\n\n\nParameters\n\n\n\n\n\n\ndf\n : pandas.DataFrame\n\n\nA pandas DataFrame where columns represent the different categories.\ncolors: str (default: 'bgrcky')\nThe colors of the bars.\n\n\n\n\n\n\nmarkers\n : str (default: ' ')\n\n\nMatplotlib markerstyles, e.g,\n'sov' for square,circle, and triangle markers.\n\n\n\n\n\n\nlinestyles\n : str (default: '-')\n\n\nMatplotlib linestyles, e.g.,\n'-,--' to cycle normal and dashed lines. Note\nthat the different linestyles need to be separated by commas.\n\n\n\n\n\n\nalpha\n : float (default: 0.5)\n\n\nTransparency level from 0.0 to 1.0.\n\n\n\n\n\n\nlw\n : int or float (default: 2)\n\n\nLinewidth parameter.\n\n\n\n\n\n\nlegend\n : bool (default: True)\n\n\nPlots legend if True.\n\n\n\n\n\n\nwhere\n : {'post', 'pre', 'mid'} (default: 'post')\n\n\nStarting location of the steps.\n\n\n\n\n\n\ngrid\n : bool (default: \nTrue\n)\n\n\nPlots a grid if True.\n\n\n\n\n\n\ncount_label\n : str (default: 'Count')\n\n\nLabel for the \"Count\"-axis.\n\n\n\n\n\n\nxlim\n : 'auto' or array-like [min, max] (default: 'auto')\n\n\nMin and maximum position of the x-axis range.\n\n\n\n\n\n\nylim\n : 'auto' or array-like [min, max] (default: 'auto')\n\n\nMin and maximum position of the y-axis range.\n\n\n\n\n\n\ninvert_axes\n : bool (default: False)\n\n\nPlots count on the x-axis if True.\n\n\n\n\n\n\nlegend_loc\n : str (default: 'best')\n\n\nLocation of the plot legend\n{best, upper left, upper right, lower left, lower right}\nNo legend if legend_loc=False\n\n\n\n\n\n\nax\n : matplotlib axis, optional (default: None)\n\n\nUse this axis for plotting or make a new one otherwise\n\n\n\n\n\n\nReturns\n\n\n\n\nax\n : matplotlib axis",
            "title": "Enrichment plot"
        },
        {
            "location": "/user_guide/general_plotting/enrichment_plot/#enrichment-plot",
            "text": "A function to plot step plots of cumulative counts.   from mlxtend.general_plotting import category_scatter",
            "title": "Enrichment Plot"
        },
        {
            "location": "/user_guide/general_plotting/enrichment_plot/#overview",
            "text": "In enrichment plots, the y-axis can be interpreted as \"how many samples are less or equal to the corresponding x-axis label.\"",
            "title": "Overview"
        },
        {
            "location": "/user_guide/general_plotting/enrichment_plot/#references",
            "text": "-",
            "title": "References"
        },
        {
            "location": "/user_guide/general_plotting/enrichment_plot/#examples",
            "text": "",
            "title": "Examples"
        },
        {
            "location": "/user_guide/general_plotting/enrichment_plot/#example-1-enrichment-plots-from-pandas-dataframes",
            "text": "import pandas as pd\ns1 = [1.1, 1.5]\ns2 = [2.1, 1.8]\ns3 = [3.1, 2.1]\ns4 = [3.9, 2.5]\ndata = [s1, s2, s3, s4]\ndf = pd.DataFrame(data, columns=['X1', 'X2'])\ndf   \n   \n     \n       \n       X1 \n       X2 \n     \n   \n   \n     \n       0 \n       1.1 \n       1.5 \n     \n     \n       1 \n       2.1 \n       1.8 \n     \n     \n       2 \n       3.1 \n       2.1 \n     \n     \n       3 \n       3.9 \n       2.5 \n     \n      Plotting the data where the categories are determined by the unique values in the label column  label_col . The  x  and  y  values are simply the column names of the DataFrame that we want to plot.  %matplotlib inline\nimport matplotlib.pyplot as plt\nfrom mlxtend.general_plotting import enrichment_plot\n\nax = enrichment_plot(df, legend_loc='upper left')",
            "title": "Example 1 - Enrichment Plots from Pandas DataFrames"
        },
        {
            "location": "/user_guide/general_plotting/enrichment_plot/#api",
            "text": "enrichment_plot(df, colors='bgrkcy', markers=' ', linestyles='-', alpha=0.5, lw=2, legend=True, where='post', grid=True, count_label='Count', xlim='auto', ylim='auto', invert_axes=False, legend_loc='best', ax=None)  Plot stacked barplots  Parameters    df  : pandas.DataFrame  A pandas DataFrame where columns represent the different categories.\ncolors: str (default: 'bgrcky')\nThe colors of the bars.    markers  : str (default: ' ')  Matplotlib markerstyles, e.g,\n'sov' for square,circle, and triangle markers.    linestyles  : str (default: '-')  Matplotlib linestyles, e.g.,\n'-,--' to cycle normal and dashed lines. Note\nthat the different linestyles need to be separated by commas.    alpha  : float (default: 0.5)  Transparency level from 0.0 to 1.0.    lw  : int or float (default: 2)  Linewidth parameter.    legend  : bool (default: True)  Plots legend if True.    where  : {'post', 'pre', 'mid'} (default: 'post')  Starting location of the steps.    grid  : bool (default:  True )  Plots a grid if True.    count_label  : str (default: 'Count')  Label for the \"Count\"-axis.    xlim  : 'auto' or array-like [min, max] (default: 'auto')  Min and maximum position of the x-axis range.    ylim  : 'auto' or array-like [min, max] (default: 'auto')  Min and maximum position of the y-axis range.    invert_axes  : bool (default: False)  Plots count on the x-axis if True.    legend_loc  : str (default: 'best')  Location of the plot legend\n{best, upper left, upper right, lower left, lower right}\nNo legend if legend_loc=False    ax  : matplotlib axis, optional (default: None)  Use this axis for plotting or make a new one otherwise    Returns   ax  : matplotlib axis",
            "title": "API"
        },
        {
            "location": "/user_guide/general_plotting/stacked_barplot/",
            "text": "Stacked Barplot\n\n\nA function to conveniently plot stacked bar plots in matplotlib using pandas \nDataFrame\ns. \n\n\n\n\nfrom mlxtend.general_plotting import category_scatter\n\n\n\n\nOverview\n\n\nA matplotlib convenience function for creating barplots from DataFrames where each sample is associated with several categories.\n\n\nReferences\n\n\n\n\n-\n\n\n\n\nExamples\n\n\nExample 1 - Stacked Barplot from Pandas DataFrames\n\n\nimport pandas as pd\n\ns1 = [1.0, 2.0, 3.0, 4.0]\ns2 = [1.4, 2.1, 2.9, 5.1]\ns3 = [1.9, 2.2, 3.5, 4.1]\ns4 = [1.4, 2.5, 3.5, 4.2]\ndata = [s1, s2, s3, s4]\n\ndf = pd.DataFrame(data, columns=['X1', 'X2', 'X3', 'X4'])\ndf.columns = ['X1', 'X2', 'X3', 'X4']\ndf.index = ['Sample1', 'Sample2', 'Sample3', 'Sample4']\ndf\n\n\n\n\n\n\n\n  \n\n    \n\n      \n\n      \nX1\n\n      \nX2\n\n      \nX3\n\n      \nX4\n\n    \n\n  \n\n  \n\n    \n\n      \nSample1\n\n      \n1.0\n\n      \n2.0\n\n      \n3.0\n\n      \n4.0\n\n    \n\n    \n\n      \nSample2\n\n      \n1.4\n\n      \n2.1\n\n      \n2.9\n\n      \n5.1\n\n    \n\n    \n\n      \nSample3\n\n      \n1.9\n\n      \n2.2\n\n      \n3.5\n\n      \n4.1\n\n    \n\n    \n\n      \nSample4\n\n      \n1.4\n\n      \n2.5\n\n      \n3.5\n\n      \n4.2\n\n    \n\n  \n\n\n\n\n\n\n\nBy default, the index of the \nDataFrame\n is used as column labels, and the \nDataFrame\n columns are used for the plot legend.\n\n\nimport matplotlib.pyplot as plt\nfrom mlxtend.general_plotting import stacked_barplot\n\nfig = stacked_barplot(df, rotation=45, legend_loc='best')\n\n\n\n\n\n\nAPI\n\n\nstacked_barplot(df, bar_width='auto', colors='bgrcky', labels='index', rotation=90, legend_loc='best')\n\n\nFunction to plot stacked barplots\n\n\nParameters\n\n\n\n\n\n\ndf\n : pandas.DataFrame\n\n\nA pandas DataFrame where the index denotes the\nx-axis labels, and the columns contain the different\nmeasurements for each row.\nbar_width: 'auto' or float (default: 'auto')\nParameter to set the widths of the bars. if\n'auto', the width is automatically determined by\nthe number of columns in the dataset.\ncolors: str (default: 'bgrcky')\nThe colors of the bars.\nlabels: 'index' or iterable (default: 'index')\nIf 'index', the DataFrame index will be used as\nx-tick labels.\nrotation: int (default: 90)\nParameter to rotate the x-axis labels.\n\n\n\n\n\n\nlegend_loc\n : str (default: 'best')\n\n\nLocation of the plot legend\n{best, upper left, upper right, lower left, lower right}\nNo legend if legend_loc=False\n\n\n\n\n\n\nReturns\n\n\n\n\nfig\n : matplotlib.pyplot figure object",
            "title": "Stacked barplot"
        },
        {
            "location": "/user_guide/general_plotting/stacked_barplot/#stacked-barplot",
            "text": "A function to conveniently plot stacked bar plots in matplotlib using pandas  DataFrame s.    from mlxtend.general_plotting import category_scatter",
            "title": "Stacked Barplot"
        },
        {
            "location": "/user_guide/general_plotting/stacked_barplot/#overview",
            "text": "A matplotlib convenience function for creating barplots from DataFrames where each sample is associated with several categories.",
            "title": "Overview"
        },
        {
            "location": "/user_guide/general_plotting/stacked_barplot/#references",
            "text": "-",
            "title": "References"
        },
        {
            "location": "/user_guide/general_plotting/stacked_barplot/#examples",
            "text": "",
            "title": "Examples"
        },
        {
            "location": "/user_guide/general_plotting/stacked_barplot/#example-1-stacked-barplot-from-pandas-dataframes",
            "text": "import pandas as pd\n\ns1 = [1.0, 2.0, 3.0, 4.0]\ns2 = [1.4, 2.1, 2.9, 5.1]\ns3 = [1.9, 2.2, 3.5, 4.1]\ns4 = [1.4, 2.5, 3.5, 4.2]\ndata = [s1, s2, s3, s4]\n\ndf = pd.DataFrame(data, columns=['X1', 'X2', 'X3', 'X4'])\ndf.columns = ['X1', 'X2', 'X3', 'X4']\ndf.index = ['Sample1', 'Sample2', 'Sample3', 'Sample4']\ndf   \n   \n     \n       \n       X1 \n       X2 \n       X3 \n       X4 \n     \n   \n   \n     \n       Sample1 \n       1.0 \n       2.0 \n       3.0 \n       4.0 \n     \n     \n       Sample2 \n       1.4 \n       2.1 \n       2.9 \n       5.1 \n     \n     \n       Sample3 \n       1.9 \n       2.2 \n       3.5 \n       4.1 \n     \n     \n       Sample4 \n       1.4 \n       2.5 \n       3.5 \n       4.2 \n     \n      By default, the index of the  DataFrame  is used as column labels, and the  DataFrame  columns are used for the plot legend.  import matplotlib.pyplot as plt\nfrom mlxtend.general_plotting import stacked_barplot\n\nfig = stacked_barplot(df, rotation=45, legend_loc='best')",
            "title": "Example 1 - Stacked Barplot from Pandas DataFrames"
        },
        {
            "location": "/user_guide/general_plotting/stacked_barplot/#api",
            "text": "stacked_barplot(df, bar_width='auto', colors='bgrcky', labels='index', rotation=90, legend_loc='best')  Function to plot stacked barplots  Parameters    df  : pandas.DataFrame  A pandas DataFrame where the index denotes the\nx-axis labels, and the columns contain the different\nmeasurements for each row.\nbar_width: 'auto' or float (default: 'auto')\nParameter to set the widths of the bars. if\n'auto', the width is automatically determined by\nthe number of columns in the dataset.\ncolors: str (default: 'bgrcky')\nThe colors of the bars.\nlabels: 'index' or iterable (default: 'index')\nIf 'index', the DataFrame index will be used as\nx-tick labels.\nrotation: int (default: 90)\nParameter to rotate the x-axis labels.    legend_loc  : str (default: 'best')  Location of the plot legend\n{best, upper left, upper right, lower left, lower right}\nNo legend if legend_loc=False    Returns   fig  : matplotlib.pyplot figure object",
            "title": "API"
        },
        {
            "location": "/user_guide/math/num_combinations/",
            "text": "Compute the Number of Combinations\n\n\nA function to calculate the number of combinations for creating subsequences of \nk\n elements out of a sequence with \nn\n elements.\n\n\n\n\nfrom mlxtend.math import num_combinations\n\n\n\n\nOverview\n\n\nCombinations are selections of items from a collection regardless of the order in which they appear (in contrast to permutations). For example, let's consider a combination of 3 elements (k=3) from a collection of 5 elements (n=5): \n\n\n\n\ncollection: {1, 2, 3, 4, 5}\n\n\ncombination 1a: {1, 3, 5} \n\n\ncombination 1b: {1, 5, 3}\n\n\ncombination 1c: {3, 5, 1}\n\n\n...\n\n\ncombination 2: {1, 3, 4}\n\n\n\n\nIn the example above the combinations 1a, 1b, and 1c, are the \"same combination\" and counted as \"1 possible way to combine items 1, 3, and 5\" -- in combinations, the order does not matter.\n\n\nThe number of ways to combine elements (\nwithout replacement\n)  from a collection with size \nn\n into subsets of size \nk\n is computed via the binomial coefficient (\"\nn\n choose \nk\n\"):\n\n\n\\[ \\begin{pmatrix} \nn  \\\nk \n\\end{pmatrix} = \\frac{n(n-1)\\ldots(n-k+1)}{k(k-1)\\dots1} = \\frac{n!}{k!(n-k)!}  \\]\n\n\nTo compute the number of combinations \nwith replacement\n, the following, alternative equation \nis used (\"\nn\n multichoose \nk\n\"):\n\n\n\\[ \\bigg(\\begin{pmatrix} \nn  \\\nk \n\\end{pmatrix}\\bigg) = \\begin{pmatrix} \nn + k -1  \\\nk \n\\end{pmatrix}  \\]\n\n\nReferences\n\n\n\n\nhttps://en.wikipedia.org/wiki/Combination\n\n\n\n\nExamples\n\n\nExample 1 - Compute the number of combinations\n\n\nfrom mlxtend.math import num_combinations\n\nc = num_combinations(n=20, k=8, with_replacement=False)\nprint('Number of ways to combine 20 elements into 8 subelements: %d' % c)\n\n\n\n\nNumber of ways to combine 20 elements into 8 subelements: 125970\n\n\n\nfrom mlxtend.math import num_combinations\n\nc = num_combinations(n=20, k=8, with_replacement=True)\nprint('Number of ways to combine 20 elements into 8 subelements (with replacement): %d' % c)\n\n\n\n\nNumber of ways to combine 20 elements into 8 subelements (with replacement): 2220075\n\n\n\nExample 2 - A progress tracking use-case\n\n\nIt is often quite useful to track the progress of a computational expensive tasks to estimate its runtime. Here, the \nnum_combination\n function can be used to compute the maximum number of loops of a \ncombinations\n iterable from itertools:\n\n\nimport itertools\nimport sys\nimport time\nfrom mlxtend.math import num_combinations\n\nitems = {1, 2, 3, 4, 5, 6, 7, 8}\nmax_iter = num_combinations(n=len(items), k=3, \n                            with_replacement=False)\n\nfor idx, i in enumerate(itertools.combinations(items, r=3)):\n    # do some computation with itemset i\n    time.sleep(0.1)\n    sys.stdout.write('\\rProgress: %d/%d' % (idx + 1, max_iter))\n    sys.stdout.flush()\n\n\n\n\nProgress: 56/56\n\n\n\nAPI\n\n\nnum_combinations(n, k, with_replacement=False)\n\n\nFunction to calculate the number of possible combinations.\n\n\nParameters\n\n\n\n\n\n\nn\n : \nint\n\n\nTotal number of items.\n\n\n\n\n\n\nk\n : \nint\n\n\nNumber of elements of the target itemset.\n\n\n\n\n\n\nwith_replacement\n : \nbool\n (default: False)\n\n\nAllows repeated elements if True.\n\n\n\n\n\n\nReturns\n\n\n\n\n\n\ncomb\n : \nint\n\n\nNumber of possible combinations.",
            "title": "Num combinations"
        },
        {
            "location": "/user_guide/math/num_combinations/#compute-the-number-of-combinations",
            "text": "A function to calculate the number of combinations for creating subsequences of  k  elements out of a sequence with  n  elements.   from mlxtend.math import num_combinations",
            "title": "Compute the Number of Combinations"
        },
        {
            "location": "/user_guide/math/num_combinations/#overview",
            "text": "Combinations are selections of items from a collection regardless of the order in which they appear (in contrast to permutations). For example, let's consider a combination of 3 elements (k=3) from a collection of 5 elements (n=5):    collection: {1, 2, 3, 4, 5}  combination 1a: {1, 3, 5}   combination 1b: {1, 5, 3}  combination 1c: {3, 5, 1}  ...  combination 2: {1, 3, 4}   In the example above the combinations 1a, 1b, and 1c, are the \"same combination\" and counted as \"1 possible way to combine items 1, 3, and 5\" -- in combinations, the order does not matter.  The number of ways to combine elements ( without replacement )  from a collection with size  n  into subsets of size  k  is computed via the binomial coefficient (\" n  choose  k \"):  \\[ \\begin{pmatrix} \nn  \\\nk \n\\end{pmatrix} = \\frac{n(n-1)\\ldots(n-k+1)}{k(k-1)\\dots1} = \\frac{n!}{k!(n-k)!}  \\]  To compute the number of combinations  with replacement , the following, alternative equation \nis used (\" n  multichoose  k \"):  \\[ \\bigg(\\begin{pmatrix} \nn  \\\nk \n\\end{pmatrix}\\bigg) = \\begin{pmatrix} \nn + k -1  \\\nk \n\\end{pmatrix}  \\]",
            "title": "Overview"
        },
        {
            "location": "/user_guide/math/num_combinations/#references",
            "text": "https://en.wikipedia.org/wiki/Combination",
            "title": "References"
        },
        {
            "location": "/user_guide/math/num_combinations/#examples",
            "text": "",
            "title": "Examples"
        },
        {
            "location": "/user_guide/math/num_combinations/#example-1-compute-the-number-of-combinations",
            "text": "from mlxtend.math import num_combinations\n\nc = num_combinations(n=20, k=8, with_replacement=False)\nprint('Number of ways to combine 20 elements into 8 subelements: %d' % c)  Number of ways to combine 20 elements into 8 subelements: 125970  from mlxtend.math import num_combinations\n\nc = num_combinations(n=20, k=8, with_replacement=True)\nprint('Number of ways to combine 20 elements into 8 subelements (with replacement): %d' % c)  Number of ways to combine 20 elements into 8 subelements (with replacement): 2220075",
            "title": "Example 1 - Compute the number of combinations"
        },
        {
            "location": "/user_guide/math/num_combinations/#example-2-a-progress-tracking-use-case",
            "text": "It is often quite useful to track the progress of a computational expensive tasks to estimate its runtime. Here, the  num_combination  function can be used to compute the maximum number of loops of a  combinations  iterable from itertools:  import itertools\nimport sys\nimport time\nfrom mlxtend.math import num_combinations\n\nitems = {1, 2, 3, 4, 5, 6, 7, 8}\nmax_iter = num_combinations(n=len(items), k=3, \n                            with_replacement=False)\n\nfor idx, i in enumerate(itertools.combinations(items, r=3)):\n    # do some computation with itemset i\n    time.sleep(0.1)\n    sys.stdout.write('\\rProgress: %d/%d' % (idx + 1, max_iter))\n    sys.stdout.flush()  Progress: 56/56",
            "title": "Example 2 - A progress tracking use-case"
        },
        {
            "location": "/user_guide/math/num_combinations/#api",
            "text": "num_combinations(n, k, with_replacement=False)  Function to calculate the number of possible combinations.  Parameters    n  :  int  Total number of items.    k  :  int  Number of elements of the target itemset.    with_replacement  :  bool  (default: False)  Allows repeated elements if True.    Returns    comb  :  int  Number of possible combinations.",
            "title": "API"
        },
        {
            "location": "/user_guide/math/num_permutations/",
            "text": "Compute the Number of Permutations\n\n\nA function to calculate the number of permutations for creating subsequences of \nk\n elements out of a sequence with \nn\n elements.\n\n\n\n\nfrom mlxtend.math import num_permutations\n\n\n\n\nOverview\n\n\nPermutations are selections of items from a collection with regard to the order in which they appear (in contrast to combinations). For example, let's consider a permutation of 3 elements (k=3) from a collection of 5 elements (n=5): \n\n\n\n\ncollection: {1, 2, 3, 4, 5}\n\n\ncombination 1a: {1, 3, 5} \n\n\ncombination 1b: {1, 5, 3}\n\n\ncombination 1c: {3, 5, 1}\n\n\n...\n\n\ncombination 2: {1, 3, 4}\n\n\n\n\nIn the example above the permutations 1a, 1b, and 1c, are the \"same combination\" but distinct permutations -- in combinations, the order does not matter, but in permutation it does matter.\n\n\nThe number of ways to combine elements (\nwithout replacement\n) from a collection with size \nn\n into subsets of size \nk\n is computed via the binomial coefficient (\"\nn\n choose \nk\n\"):\n\n\n\\[ k!\\begin{pmatrix} \nn  \\\nk \n\\end{pmatrix} = k! \\cdot \\frac{n!}{k!(n-k)!} = \\frac{n!}{(n-k)!} \\]\n\n\nTo compute the number of permutations \nwith replacement\n, we simply need to compute \n$n^k$\n.\n\n\nReferences\n\n\n\n\nhttps://en.wikipedia.org/wiki/Permutation\n\n\n\n\nExamples\n\n\nExample 1 - Compute the number of permutations\n\n\nfrom mlxtend.math import num_permutations\n\nc = num_permutations(n=20, k=8, with_replacement=False)\nprint('Number of ways to permute 20 elements into 8 subelements: %d' % c)\n\n\n\n\nNumber of ways to permute 20 elements into 8 subelements: 5079110400\n\n\n\nfrom mlxtend.math import num_permutations\n\nc = num_permutations(n=20, k=8, with_replacement=True)\nprint('Number of ways to combine 20 elements into 8 subelements (with replacement): %d' % c)\n\n\n\n\nNumber of ways to combine 20 elements into 8 subelements (with replacement): 25600000000\n\n\n\nExample 2 - A progress tracking use-case\n\n\nIt is often quite useful to track the progress of a computational expensive tasks to estimate its runtime. Here, the \nnum_combination\n function can be used to compute the maximum number of loops of a \npermutations\n iterable from itertools:\n\n\nimport itertools\nimport sys\nimport time\nfrom mlxtend.math import num_permutations\n\nitems = {1, 2, 3, 4, 5, 6, 7, 8}\nmax_iter = num_permutations(n=len(items), k=3, \n                            with_replacement=False)\n\nfor idx, i in enumerate(itertools.permutations(items, r=3)):\n    # do some computation with itemset i\n    time.sleep(0.01)\n    sys.stdout.write('\\rProgress: %d/%d' % (idx + 1, max_iter))\n    sys.stdout.flush()\n\n\n\n\nProgress: 336/336\n\n\n\nAPI\n\n\nnum_permutations(n, k, with_replacement=False)\n\n\nFunction to calculate the number of possible permutations.\n\n\nParameters\n\n\n\n\n\n\nn\n : \nint\n\n\nTotal number of items.\n\n\n\n\n\n\nk\n : \nint\n\n\nNumber of elements of the target itemset.\n\n\n\n\n\n\nwith_replacement\n : \nbool\n\n\nAllows repeated elements if True.\n\n\n\n\n\n\nReturns\n\n\n\n\n\n\npermut\n : \nint\n\n\nNumber of possible permutations.",
            "title": "Num permutations"
        },
        {
            "location": "/user_guide/math/num_permutations/#compute-the-number-of-permutations",
            "text": "A function to calculate the number of permutations for creating subsequences of  k  elements out of a sequence with  n  elements.   from mlxtend.math import num_permutations",
            "title": "Compute the Number of Permutations"
        },
        {
            "location": "/user_guide/math/num_permutations/#overview",
            "text": "Permutations are selections of items from a collection with regard to the order in which they appear (in contrast to combinations). For example, let's consider a permutation of 3 elements (k=3) from a collection of 5 elements (n=5):    collection: {1, 2, 3, 4, 5}  combination 1a: {1, 3, 5}   combination 1b: {1, 5, 3}  combination 1c: {3, 5, 1}  ...  combination 2: {1, 3, 4}   In the example above the permutations 1a, 1b, and 1c, are the \"same combination\" but distinct permutations -- in combinations, the order does not matter, but in permutation it does matter.  The number of ways to combine elements ( without replacement ) from a collection with size  n  into subsets of size  k  is computed via the binomial coefficient (\" n  choose  k \"):  \\[ k!\\begin{pmatrix} \nn  \\\nk \n\\end{pmatrix} = k! \\cdot \\frac{n!}{k!(n-k)!} = \\frac{n!}{(n-k)!} \\]  To compute the number of permutations  with replacement , we simply need to compute  $n^k$ .",
            "title": "Overview"
        },
        {
            "location": "/user_guide/math/num_permutations/#references",
            "text": "https://en.wikipedia.org/wiki/Permutation",
            "title": "References"
        },
        {
            "location": "/user_guide/math/num_permutations/#examples",
            "text": "",
            "title": "Examples"
        },
        {
            "location": "/user_guide/math/num_permutations/#example-1-compute-the-number-of-permutations",
            "text": "from mlxtend.math import num_permutations\n\nc = num_permutations(n=20, k=8, with_replacement=False)\nprint('Number of ways to permute 20 elements into 8 subelements: %d' % c)  Number of ways to permute 20 elements into 8 subelements: 5079110400  from mlxtend.math import num_permutations\n\nc = num_permutations(n=20, k=8, with_replacement=True)\nprint('Number of ways to combine 20 elements into 8 subelements (with replacement): %d' % c)  Number of ways to combine 20 elements into 8 subelements (with replacement): 25600000000",
            "title": "Example 1 - Compute the number of permutations"
        },
        {
            "location": "/user_guide/math/num_permutations/#example-2-a-progress-tracking-use-case",
            "text": "It is often quite useful to track the progress of a computational expensive tasks to estimate its runtime. Here, the  num_combination  function can be used to compute the maximum number of loops of a  permutations  iterable from itertools:  import itertools\nimport sys\nimport time\nfrom mlxtend.math import num_permutations\n\nitems = {1, 2, 3, 4, 5, 6, 7, 8}\nmax_iter = num_permutations(n=len(items), k=3, \n                            with_replacement=False)\n\nfor idx, i in enumerate(itertools.permutations(items, r=3)):\n    # do some computation with itemset i\n    time.sleep(0.01)\n    sys.stdout.write('\\rProgress: %d/%d' % (idx + 1, max_iter))\n    sys.stdout.flush()  Progress: 336/336",
            "title": "Example 2 - A progress tracking use-case"
        },
        {
            "location": "/user_guide/math/num_permutations/#api",
            "text": "num_permutations(n, k, with_replacement=False)  Function to calculate the number of possible permutations.  Parameters    n  :  int  Total number of items.    k  :  int  Number of elements of the target itemset.    with_replacement  :  bool  Allows repeated elements if True.    Returns    permut  :  int  Number of possible permutations.",
            "title": "API"
        },
        {
            "location": "/user_guide/text/generalize_names/",
            "text": "Generalize Names\n\n\nA function that converts a name into a general format \n<last_name><separator><firstname letter(s)> (all lowercase)\n.\n\n\n\n\nfrom mlxtend.text import generalize_names\n\n\n\n\nOverview\n\n\nA function that converts a name into a general format \n<last_name><separator><firstname letter(s)> (all lowercase)\n, which is useful if data is collected from different sources and is supposed to be compared or merged based on name identifiers. E.g., if names are stored in a pandas \nDataFrame\n column, the apply function can be used to generalize names: \ndf['name'] = df['name'].apply(generalize_names)\n\n\nReferences\n\n\n\n\n-\n\n\n\n\nExamples\n\n\nExample 1 - Defaults\n\n\nfrom mlxtend.text import generalize_names\n\n\n\n\ngeneralize_names('Pozo, Jos\u00e9 \u00c1ngel')\n\n\n\n\n'pozo j'\n\n\n\ngeneralize_names('Jos\u00e9 Pozo')\n\n\n\n\n'pozo j'\n\n\n\ngeneralize_names('Jos\u00e9 \u00c1ngel Pozo')\n\n\n\n\n'pozo j'\n\n\n\nExample 1 - Optional Parameters\n\n\nfrom mlxtend.text import generalize_names\n\n\n\n\ngeneralize_names(\"Eto'o, Samuel\", firstname_output_letters=2)\n\n\n\n\n'etoo sa'\n\n\n\ngeneralize_names(\"Eto'o, Samuel\", firstname_output_letters=0)\n\n\n\n\n'etoo'\n\n\n\ngeneralize_names(\"Eto'o, Samuel\", output_sep=', ')\n\n\n\n\n'etoo, s'\n\n\n\nAPI",
            "title": "Generalize names"
        },
        {
            "location": "/user_guide/text/generalize_names/#generalize-names",
            "text": "A function that converts a name into a general format  <last_name><separator><firstname letter(s)> (all lowercase) .   from mlxtend.text import generalize_names",
            "title": "Generalize Names"
        },
        {
            "location": "/user_guide/text/generalize_names/#overview",
            "text": "A function that converts a name into a general format  <last_name><separator><firstname letter(s)> (all lowercase) , which is useful if data is collected from different sources and is supposed to be compared or merged based on name identifiers. E.g., if names are stored in a pandas  DataFrame  column, the apply function can be used to generalize names:  df['name'] = df['name'].apply(generalize_names)",
            "title": "Overview"
        },
        {
            "location": "/user_guide/text/generalize_names/#references",
            "text": "-",
            "title": "References"
        },
        {
            "location": "/user_guide/text/generalize_names/#examples",
            "text": "",
            "title": "Examples"
        },
        {
            "location": "/user_guide/text/generalize_names/#example-1-defaults",
            "text": "from mlxtend.text import generalize_names  generalize_names('Pozo, Jos\u00e9 \u00c1ngel')  'pozo j'  generalize_names('Jos\u00e9 Pozo')  'pozo j'  generalize_names('Jos\u00e9 \u00c1ngel Pozo')  'pozo j'",
            "title": "Example 1 - Defaults"
        },
        {
            "location": "/user_guide/text/generalize_names/#example-1-optional-parameters",
            "text": "from mlxtend.text import generalize_names  generalize_names(\"Eto'o, Samuel\", firstname_output_letters=2)  'etoo sa'  generalize_names(\"Eto'o, Samuel\", firstname_output_letters=0)  'etoo'  generalize_names(\"Eto'o, Samuel\", output_sep=', ')  'etoo, s'",
            "title": "Example 1 - Optional Parameters"
        },
        {
            "location": "/user_guide/text/generalize_names/#api",
            "text": "",
            "title": "API"
        },
        {
            "location": "/user_guide/text/generalize_names_duplcheck/",
            "text": "Generalize Names & Duplicate Checking\n\n\nA function that converts a name into a general format \n<last_name><separator><firstname letter(s)> (all lowercase)\n in a \npandas DataFrame\n while avoiding duplicate entries.\n\n\n\n\nfrom mlxtend.text import generalize_names_duplcheck\n\n\n\n\nOverview\n\n\nNote\n that using \nmlxtend.text.generalize_names\n with few \nfirstname_output_letters\n can result in duplicate entries. E.g., if your dataset contains the names \"Adam Johnson\" and \"Andrew Johnson\", the default setting (i.e., 1 first name letter) will produce the generalized name \"johnson a\" in both cases.\n\n\nOne solution is to increase the number of first name letters in the output by setting the parameter \nfirstname_output_letters\n to a value larger than 1. \n\n\nAn alternative solution is to use the \ngeneralize_names_duplcheck\n function if you are working with pandas DataFrames. \n\n\nBy default,  \ngeneralize_names_duplcheck\n will apply  \ngeneralize_names\n to a pandas DataFrame column with the minimum number of first name letters and append as many first name letters as necessary until no duplicates are present in the given DataFrame column. An example dataset column that contains the names  \n\n\nReferences\n\n\n\n\n-\n\n\n\n\nExamples\n\n\nExample 1 - Defaults\n\n\nReading in a CSV file that has column \nName\n for which we want to generalize the names:\n\n\n\n\nSamuel Eto'o\n\n\nAdam Johnson\n\n\nAndrew Johnson\n\n\n\n\nimport pandas as pd\nfrom io import StringIO\n\nsimulated_csv = \"name,some_value\\n\"\\\n                \"Samuel Eto'o,1\\n\"\\\n                \"Adam Johnson,1\\n\"\\\n                \"Andrew Johnson,1\\n\"\n\ndf = pd.read_csv(StringIO(simulated_csv))\ndf\n\n\n\n\n\n\n\n  \n\n    \n\n      \n\n      \nname\n\n      \nsome_value\n\n    \n\n  \n\n  \n\n    \n\n      \n0\n\n      \nSamuel Eto'o\n\n      \n1\n\n    \n\n    \n\n      \n1\n\n      \nAdam Johnson\n\n      \n1\n\n    \n\n    \n\n      \n2\n\n      \nAndrew Johnson\n\n      \n1\n\n    \n\n  \n\n\n\n\n\n\n\nApplying \ngeneralize_names_duplcheck\n to generate a new DataFrame with the generalized names without duplicates: \n\n\nfrom mlxtend.text import generalize_names_duplcheck\ndf_new = generalize_names_duplcheck(df=df, col_name='name')\ndf_new\n\n\n\n\n\n\n\n  \n\n    \n\n      \n\n      \nname\n\n      \nsome_value\n\n    \n\n  \n\n  \n\n    \n\n      \n0\n\n      \netoo s\n\n      \n1\n\n    \n\n    \n\n      \n1\n\n      \njohnson ad\n\n      \n1\n\n    \n\n    \n\n      \n2\n\n      \njohnson an\n\n      \n1\n\n    \n\n  \n\n\n\n\n\n\n\nAPI\n\n\ngeneralize_names_duplcheck(df, col_name)\n\n\nGeneralizes names and removes duplicates.\n\n\nApplies mlxtend.text.generalize_names to a DataFrame with 1 first name letter\n    by default and uses more first name letters if duplicates are detected.\n\n\nParameters\n\n\n\n\n\n\ndf\n : \npandas.DataFrame\n\n\nDataFrame that contains a column where generalize_names should be applied.\n\n\n\n\n\n\ncol_name\n : \nstr\n\n\nName of the DataFrame column where \ngeneralize_names\n function should be applied to.\n\n\n\n\n\n\nReturns\n\n\n\n\n\n\ndf_new\n : \nstr\n\n\nNew DataFrame object where generalize_names function has been applied without duplicates.",
            "title": "Generalize names duplcheck"
        },
        {
            "location": "/user_guide/text/generalize_names_duplcheck/#generalize-names-duplicate-checking",
            "text": "A function that converts a name into a general format  <last_name><separator><firstname letter(s)> (all lowercase)  in a  pandas DataFrame  while avoiding duplicate entries.   from mlxtend.text import generalize_names_duplcheck",
            "title": "Generalize Names &amp; Duplicate Checking"
        },
        {
            "location": "/user_guide/text/generalize_names_duplcheck/#overview",
            "text": "Note  that using  mlxtend.text.generalize_names  with few  firstname_output_letters  can result in duplicate entries. E.g., if your dataset contains the names \"Adam Johnson\" and \"Andrew Johnson\", the default setting (i.e., 1 first name letter) will produce the generalized name \"johnson a\" in both cases.  One solution is to increase the number of first name letters in the output by setting the parameter  firstname_output_letters  to a value larger than 1.   An alternative solution is to use the  generalize_names_duplcheck  function if you are working with pandas DataFrames.   By default,   generalize_names_duplcheck  will apply   generalize_names  to a pandas DataFrame column with the minimum number of first name letters and append as many first name letters as necessary until no duplicates are present in the given DataFrame column. An example dataset column that contains the names",
            "title": "Overview"
        },
        {
            "location": "/user_guide/text/generalize_names_duplcheck/#references",
            "text": "-",
            "title": "References"
        },
        {
            "location": "/user_guide/text/generalize_names_duplcheck/#examples",
            "text": "",
            "title": "Examples"
        },
        {
            "location": "/user_guide/text/generalize_names_duplcheck/#example-1-defaults",
            "text": "Reading in a CSV file that has column  Name  for which we want to generalize the names:   Samuel Eto'o  Adam Johnson  Andrew Johnson   import pandas as pd\nfrom io import StringIO\n\nsimulated_csv = \"name,some_value\\n\"\\\n                \"Samuel Eto'o,1\\n\"\\\n                \"Adam Johnson,1\\n\"\\\n                \"Andrew Johnson,1\\n\"\n\ndf = pd.read_csv(StringIO(simulated_csv))\ndf   \n   \n     \n       \n       name \n       some_value \n     \n   \n   \n     \n       0 \n       Samuel Eto'o \n       1 \n     \n     \n       1 \n       Adam Johnson \n       1 \n     \n     \n       2 \n       Andrew Johnson \n       1 \n     \n      Applying  generalize_names_duplcheck  to generate a new DataFrame with the generalized names without duplicates:   from mlxtend.text import generalize_names_duplcheck\ndf_new = generalize_names_duplcheck(df=df, col_name='name')\ndf_new   \n   \n     \n       \n       name \n       some_value \n     \n   \n   \n     \n       0 \n       etoo s \n       1 \n     \n     \n       1 \n       johnson ad \n       1 \n     \n     \n       2 \n       johnson an \n       1",
            "title": "Example 1 - Defaults"
        },
        {
            "location": "/user_guide/text/generalize_names_duplcheck/#api",
            "text": "generalize_names_duplcheck(df, col_name)  Generalizes names and removes duplicates.  Applies mlxtend.text.generalize_names to a DataFrame with 1 first name letter\n    by default and uses more first name letters if duplicates are detected.  Parameters    df  :  pandas.DataFrame  DataFrame that contains a column where generalize_names should be applied.    col_name  :  str  Name of the DataFrame column where  generalize_names  function should be applied to.    Returns    df_new  :  str  New DataFrame object where generalize_names function has been applied without duplicates.",
            "title": "API"
        },
        {
            "location": "/user_guide/text/tokenizer/",
            "text": "Tokenizer\n\n\nDifferent functions to tokenize text.\n\n\n\n\nfrom mlxtend.text import tokenizer_[type]\n\n\n\n\nOverview\n\n\nDifferent functions to tokenize text for natural language processing tasks, for example such as building a bag-of-words model for text classification.\n\n\nReferences\n\n\n\n\n-\n\n\n\n\nExamples\n\n\nExample 1 - Extract Emoticons\n\n\nfrom mlxtend.text import tokenizer_emoticons\n\n\n\n\ntokenizer_emoticons('</a>This :) is :( a test :-)!')\n\n\n\n\n[':)', ':(', ':-)']\n\n\n\nExample 2 - Extract Words and Emoticons\n\n\nfrom mlxtend.text import tokenizer_words_and_emoticons\n\n\n\n\ntokenizer_words_and_emoticons('</a>This :) is :( a test :-)!')\n\n\n\n\n['this', 'is', 'a', 'test', ':)', ':(', ':-)']\n\n\n\nAPI\n\n\ntokenizer_emoticons(text)\n\n\nReturn emoticons from text\n\n\nExample:\n>>> tokenizer_emoticons('</a>This :) is :( a test :-)!')\n[':)', ':(', ':-)']\n\n\n\n\n\ntokenizer_words_and_emoticons(text)\n\n\nConvert text to lowercase words and emoticons.\n\n\nExample:\n>>> tokenizer_words_and_emoticons('</a>This :) is :( a test :-)!')\n['this', 'is', 'a', 'test', ':)', ':(', ':-)']",
            "title": "Tokenizer"
        },
        {
            "location": "/user_guide/text/tokenizer/#tokenizer",
            "text": "Different functions to tokenize text.   from mlxtend.text import tokenizer_[type]",
            "title": "Tokenizer"
        },
        {
            "location": "/user_guide/text/tokenizer/#overview",
            "text": "Different functions to tokenize text for natural language processing tasks, for example such as building a bag-of-words model for text classification.",
            "title": "Overview"
        },
        {
            "location": "/user_guide/text/tokenizer/#references",
            "text": "-",
            "title": "References"
        },
        {
            "location": "/user_guide/text/tokenizer/#examples",
            "text": "",
            "title": "Examples"
        },
        {
            "location": "/user_guide/text/tokenizer/#example-1-extract-emoticons",
            "text": "from mlxtend.text import tokenizer_emoticons  tokenizer_emoticons('</a>This :) is :( a test :-)!')  [':)', ':(', ':-)']",
            "title": "Example 1 - Extract Emoticons"
        },
        {
            "location": "/user_guide/text/tokenizer/#example-2-extract-words-and-emoticons",
            "text": "from mlxtend.text import tokenizer_words_and_emoticons  tokenizer_words_and_emoticons('</a>This :) is :( a test :-)!')  ['this', 'is', 'a', 'test', ':)', ':(', ':-)']",
            "title": "Example 2 - Extract Words and Emoticons"
        },
        {
            "location": "/user_guide/text/tokenizer/#api",
            "text": "tokenizer_emoticons(text)  Return emoticons from text  Example:\n>>> tokenizer_emoticons('</a>This :) is :( a test :-)!')\n[':)', ':(', ':-)']   tokenizer_words_and_emoticons(text)  Convert text to lowercase words and emoticons.  Example:\n>>> tokenizer_words_and_emoticons('</a>This :) is :( a test :-)!')\n['this', 'is', 'a', 'test', ':)', ':(', ':-)']",
            "title": "API"
        },
        {
            "location": "/user_guide/utils/Counter/",
            "text": "Counter\n\n\nA simple progress counter to print the number of iterations and time elapsed in a for-loop execution.\n\n\n\n\nfrom mlxtend.utils import Counter\n\n\n\n\nOverview\n\n\nThe \nCounter\n class implements an object for displaying the number of iterations and time elapsed in a for-loop. Please note that the \nCounter\n was implemented for efficiency; thus, the \nCounter\n offers only very basic functionality in order to avoid relatively expensive evaluations (of if-else statements).\n\n\nReferences\n\n\n\n\n-\n\n\n\n\nExamples\n\n\nExample 1 - Counting the iterations in a for-loop\n\n\nfrom mlxtend.utils import Counter\n\n\n\n\nimport time\n\ncnt = Counter()\nfor i in range(20):\n    # do some computation\n    time.sleep(0.1)\n    cnt.update()\n\n\n\n\n20 iter | 2 sec\n\n\n\nNote that the first number displays the current iteration, and the second number shows the time elapsed after initializing the \nCounter\n.\n\n\nAPI\n\n\nCounter(stderr=False, start_newline=True)\n\n\nClass to display the progress of for-loop iterators.\n\n\nParameters\n\n\n\n\n\n\nstderr\n : bool (default: True)\n\n\nPrints output to sys.stderr if True; uses sys.stdout otherwise.\n\n\n\n\n\n\nstart_newline\n : bool (default: True)\n\n\nPrepends a new line to the counter, which prevents overwriting counters\nif multiple counters are printed in succession.\n\n\n\n\n\n\nAttributes\n\n\n\n\n\n\ncurr_iter\n : int\n\n\nThe current iteration.\n\n\n\n\n\n\nstart_time\n : int\n\n\nThe system's time in seconds when the Counter was initialized.\n\n\n\n\n\n\nMethods\n\n\n\n\n\nupdate()\n\n\nPrint current iteration and time elapsed.",
            "title": "Counter"
        },
        {
            "location": "/user_guide/utils/Counter/#counter",
            "text": "A simple progress counter to print the number of iterations and time elapsed in a for-loop execution.   from mlxtend.utils import Counter",
            "title": "Counter"
        },
        {
            "location": "/user_guide/utils/Counter/#overview",
            "text": "The  Counter  class implements an object for displaying the number of iterations and time elapsed in a for-loop. Please note that the  Counter  was implemented for efficiency; thus, the  Counter  offers only very basic functionality in order to avoid relatively expensive evaluations (of if-else statements).",
            "title": "Overview"
        },
        {
            "location": "/user_guide/utils/Counter/#references",
            "text": "-",
            "title": "References"
        },
        {
            "location": "/user_guide/utils/Counter/#examples",
            "text": "",
            "title": "Examples"
        },
        {
            "location": "/user_guide/utils/Counter/#example-1-counting-the-iterations-in-a-for-loop",
            "text": "from mlxtend.utils import Counter  import time\n\ncnt = Counter()\nfor i in range(20):\n    # do some computation\n    time.sleep(0.1)\n    cnt.update()  20 iter | 2 sec  Note that the first number displays the current iteration, and the second number shows the time elapsed after initializing the  Counter .",
            "title": "Example 1 - Counting the iterations in a for-loop"
        },
        {
            "location": "/user_guide/utils/Counter/#api",
            "text": "Counter(stderr=False, start_newline=True)  Class to display the progress of for-loop iterators.  Parameters    stderr  : bool (default: True)  Prints output to sys.stderr if True; uses sys.stdout otherwise.    start_newline  : bool (default: True)  Prepends a new line to the counter, which prevents overwriting counters\nif multiple counters are printed in succession.    Attributes    curr_iter  : int  The current iteration.    start_time  : int  The system's time in seconds when the Counter was initialized.",
            "title": "API"
        },
        {
            "location": "/user_guide/utils/Counter/#methods",
            "text": "update()  Print current iteration and time elapsed.",
            "title": "Methods"
        },
        {
            "location": "/user_guide/general_concepts/activation-functions/",
            "text": "Activation Functions for Artificial Neural Networks",
            "title": "Activation functions"
        },
        {
            "location": "/user_guide/general_concepts/activation-functions/#activation-functions-for-artificial-neural-networks",
            "text": "",
            "title": "Activation Functions for Artificial Neural Networks"
        },
        {
            "location": "/user_guide/general_concepts/gradient-optimization/",
            "text": "Gradient Descent and Stochastic Gradient Descent\n\n\nGradient Descent (GD) Optimization\n\n\nUsing the Gradient Decent optimization algorithm, the weights are updated incrementally after each epoch (= pass over the training dataset).\n\n\nCompatible cost functions \n$J(\\cdot)$\n\n\n\n\n\n\nSum of squared errors (SSE) [ \nmlxtend.regressor.LinearRegression\n, \nmlxtend.classfier.Adaline\n ]:\n\n$$J(\\mathbf{w}) = \\frac{1}{2} \\sum_i (\\text{target}^{(i)} - \\text{output}^{(i)})^2$$\n\n\n\n\n\n\nLogistic Cost (cross-entropy) [ \nmlxtend.classfier.LogisticRegression\n ]:\n...\n\n\n\n\n\n\nThe magnitude and direction of the weight update is computed by taking a step in the opposite direction of the cost gradient\n\n\n$$\\Delta w_j = -\\eta \\frac{\\partial J}{\\partial w_j},$$\n\n\nwhere \n$\\eta$\n is the learning rate. The weights are then updated after each epoch via the following update rule:\n\n\n$$\\mathbf{w} := \\mathbf{w} + \\Delta\\mathbf{w},$$\n\n\nwhere \n$\\Delta\\mathbf{w}$\n is a vector that contains the weight updates of each weight coefficient \n${w}$\n, which are computed as follows:\n\n\n$$\\Delta w_j = -\\eta \\frac{\\partial J}{\\partial w_j}\\\\\n= -\\eta \\sum_i (\\text{target}^{(i)} - \\text{output}^{(i)})(-x_{j}^{(i)})\\\\\n= \\eta \\sum_i (\\text{target}^{(i)} - \\text{output}^{(i)})x_{j}^{(i)}.$$\n\n\nEssentially, we can picture Gradient Descent optimization as a hiker (the weight coefficient) who wants to climb down a mountain (cost function) into valley (cost minimum), and each step is determined by the steepness of the slope (gradient) and the leg length of the hiker (learning rate). Considering a cost function with only a single weight coefficient, we can illustrate this concept as follows:\n\n\n\n\nStochastic Gradient Descent (SGD)\n\n\nIn Gradient Descent optimization, we compute the cost gradient based on the complete training set; hence, we sometimes also call it \nbatch gradient descent\n. In case of very large datasets, using Gradient Descent can be quite costly since we are only taking a single step for one pass over the training set -- thus, the larger the training set, the slower our algorithm updates the weights and the longer it may take until it converges to the global cost minimum (note that the SSE cost function is convex).\n\n\nIn Stochastic Gradient Descent (sometimes also referred to as \niterative\n or \non-line\n gradient descent), we \ndon't\n accumulate the weight updates as we've seen above for Gradient Descent:\n\n\n\n\nfor one or more epochs:\n\n\nfor each weight \n$j$\n\n\n$w_j := w + \\Delta w_j$\n,   where:   \n$\\Delta w_j= \\eta \\sum_i (\\text{target}^{(i)} - \\text{output}^{(i)})x_{j}^{(i)}$\n\n\n\n\n\n\n\n\n\n\n\n\nInstead, we update the weights after each training sample:\n\n\n\n\nfor one or more epochs, or until approx. cost minimum is reached:\n\n\nfor training sample \n$i$\n:\n\n\nfor each weight \n$j$\n\n\n$w_j := w + \\Delta w_j$\n,   where:   \n$\\Delta w_j= \\eta (\\text{target}^{(i)} - \\text{output}^{(i)})x_{j}^{(i)}$\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nHere, the term \"stochastic\" comes from the fact that the gradient based on a single training sample is a \"stochastic approximation\" of the \"true\" cost gradient. Due to its stochastic nature, the path towards the global cost minimum is not \"direct\" as in Gradient Descent, but may go \"zig-zag\" if we are visuallizing the cost surface in a 2D space. However, it has been shown that Stochastic Gradient Descent almost surely converges to the global cost minimum if the cost function is convex (or pseudo-convex)[1].\n\n\nStochastic Gradient Descent Shuffling\n\n\nThere are several different flavors of stochastic gradient descent, which can be all seen throughout the literature. Let's take a look at the three most common variants:\n\n\nA)\n\n\n\n\nrandomly shuffle samples in the training set\n\n\nfor one or more epochs, or until approx. cost minimum is reached\n\n\nfor training sample \ni\n\n\ncompute gradients and perform weight updates\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nB)\n\n\n\n\nfor one or more epochs, or until approx. cost minimum is reached\n\n\nrandomly shuffle samples in the training set\n\n\nfor training sample \ni\n\n\ncompute gradients and perform weight updates\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nC)\n\n\n\n\nfor iterations \nt\n, or until approx. cost minimum is reached:\n\n\ndraw random sample from the training set\n\n\ncompute gradients and perform weight updates\n\n\n\n\n\n\n\n\n\n\n\n\nIn scenario A [3], we shuffle the training set only one time in the beginning; whereas in scenario B, we shuffle the training set after each epoch to prevent repeating update cycles. In both scenario A and scenario B, each training sample is only used once per epoch to update the model weights.\n\n\nIn scenario C, we draw the training samples randomly with replacement from the training set [2]. If the number of iterations \nt\n is equal to the number of training samples, we learn the model based on a \nbootstrap sample\n of the training set.\n\n\nMini-Batch Gradient Descent (MB-GD)\n\n\nMini-Batch Gradient Descent (MB-GD) a compromise between batch GD and SGD. In MB-GD, we update the model based on smaller groups of training samples; instead of computing the gradient from 1 sample (SGD) or all \nn\n training samples (GD), we compute the gradient from \n$1 < k < n$\n training samples (a common mini-batch size is \n$k=50$\n).\n\n\nMB-GD converges in fewer iterations than GD because we update the weights more frequently; however, MB-GD let's us utilize vectorized operation, which typically results in a computational performance gain over SGD.\n\n\nLearning Rates\n\n\n\n\n\n\nAn adaptive learning rate \n$\\eta$\n: Choosing a decrease constant \nd\n that shrinks the learning rate over time:  \n$\\eta(t+1) := \\eta(t) / (1 + t \\times d)$\n\n\n\n\n\n\nMomentum learning by adding a factor of the previous gradient to the weight update for faster updates: \n$\\Delta \\mathbf{w}_{t+1} := \\eta \\nabla J(\\mathbf{w}_{t+1}) + \\alpha \\Delta {w}_{t}$\n\n\n\n\n\n\nReferences\n\n\n\n\n[1] Bottou, L\u00e9on (1998). \n\"Online Algorithms and Stochastic Approximations\"\n. Online Learning and Neural Networks. Cambridge University Press. ISBN 978-0-521-65263-6\n\n\n[2] Bottou, L\u00e9on. \n\"Large-scale machine learning with stochastic gradient descent.\"\n Proceedings of COMPSTAT'2010. Physica-Verlag HD, 2010. 177-186.\n\n\n[3] Bottou, L\u00e9on. \n\"Stochastic gradient descent tricks.\"\n Neural Networks: Tricks of the Trade. Springer Berlin Heidelberg, 2012. 421-436.",
            "title": "Gradient optimization"
        },
        {
            "location": "/user_guide/general_concepts/gradient-optimization/#gradient-descent-and-stochastic-gradient-descent",
            "text": "",
            "title": "Gradient Descent and Stochastic Gradient Descent"
        },
        {
            "location": "/user_guide/general_concepts/gradient-optimization/#gradient-descent-gd-optimization",
            "text": "Using the Gradient Decent optimization algorithm, the weights are updated incrementally after each epoch (= pass over the training dataset).  Compatible cost functions  $J(\\cdot)$    Sum of squared errors (SSE) [  mlxtend.regressor.LinearRegression ,  mlxtend.classfier.Adaline  ]: $$J(\\mathbf{w}) = \\frac{1}{2} \\sum_i (\\text{target}^{(i)} - \\text{output}^{(i)})^2$$    Logistic Cost (cross-entropy) [  mlxtend.classfier.LogisticRegression  ]:\n...    The magnitude and direction of the weight update is computed by taking a step in the opposite direction of the cost gradient  $$\\Delta w_j = -\\eta \\frac{\\partial J}{\\partial w_j},$$  where  $\\eta$  is the learning rate. The weights are then updated after each epoch via the following update rule:  $$\\mathbf{w} := \\mathbf{w} + \\Delta\\mathbf{w},$$  where  $\\Delta\\mathbf{w}$  is a vector that contains the weight updates of each weight coefficient  ${w}$ , which are computed as follows:  $$\\Delta w_j = -\\eta \\frac{\\partial J}{\\partial w_j}\\\\\n= -\\eta \\sum_i (\\text{target}^{(i)} - \\text{output}^{(i)})(-x_{j}^{(i)})\\\\\n= \\eta \\sum_i (\\text{target}^{(i)} - \\text{output}^{(i)})x_{j}^{(i)}.$$  Essentially, we can picture Gradient Descent optimization as a hiker (the weight coefficient) who wants to climb down a mountain (cost function) into valley (cost minimum), and each step is determined by the steepness of the slope (gradient) and the leg length of the hiker (learning rate). Considering a cost function with only a single weight coefficient, we can illustrate this concept as follows:",
            "title": "Gradient Descent (GD) Optimization"
        },
        {
            "location": "/user_guide/general_concepts/gradient-optimization/#stochastic-gradient-descent-sgd",
            "text": "In Gradient Descent optimization, we compute the cost gradient based on the complete training set; hence, we sometimes also call it  batch gradient descent . In case of very large datasets, using Gradient Descent can be quite costly since we are only taking a single step for one pass over the training set -- thus, the larger the training set, the slower our algorithm updates the weights and the longer it may take until it converges to the global cost minimum (note that the SSE cost function is convex).  In Stochastic Gradient Descent (sometimes also referred to as  iterative  or  on-line  gradient descent), we  don't  accumulate the weight updates as we've seen above for Gradient Descent:   for one or more epochs:  for each weight  $j$  $w_j := w + \\Delta w_j$ ,   where:    $\\Delta w_j= \\eta \\sum_i (\\text{target}^{(i)} - \\text{output}^{(i)})x_{j}^{(i)}$       Instead, we update the weights after each training sample:   for one or more epochs, or until approx. cost minimum is reached:  for training sample  $i$ :  for each weight  $j$  $w_j := w + \\Delta w_j$ ,   where:    $\\Delta w_j= \\eta (\\text{target}^{(i)} - \\text{output}^{(i)})x_{j}^{(i)}$         Here, the term \"stochastic\" comes from the fact that the gradient based on a single training sample is a \"stochastic approximation\" of the \"true\" cost gradient. Due to its stochastic nature, the path towards the global cost minimum is not \"direct\" as in Gradient Descent, but may go \"zig-zag\" if we are visuallizing the cost surface in a 2D space. However, it has been shown that Stochastic Gradient Descent almost surely converges to the global cost minimum if the cost function is convex (or pseudo-convex)[1].",
            "title": "Stochastic Gradient Descent (SGD)"
        },
        {
            "location": "/user_guide/general_concepts/gradient-optimization/#stochastic-gradient-descent-shuffling",
            "text": "There are several different flavors of stochastic gradient descent, which can be all seen throughout the literature. Let's take a look at the three most common variants:",
            "title": "Stochastic Gradient Descent Shuffling"
        },
        {
            "location": "/user_guide/general_concepts/gradient-optimization/#a",
            "text": "randomly shuffle samples in the training set  for one or more epochs, or until approx. cost minimum is reached  for training sample  i  compute gradients and perform weight updates",
            "title": "A)"
        },
        {
            "location": "/user_guide/general_concepts/gradient-optimization/#b",
            "text": "for one or more epochs, or until approx. cost minimum is reached  randomly shuffle samples in the training set  for training sample  i  compute gradients and perform weight updates",
            "title": "B)"
        },
        {
            "location": "/user_guide/general_concepts/gradient-optimization/#c",
            "text": "for iterations  t , or until approx. cost minimum is reached:  draw random sample from the training set  compute gradients and perform weight updates       In scenario A [3], we shuffle the training set only one time in the beginning; whereas in scenario B, we shuffle the training set after each epoch to prevent repeating update cycles. In both scenario A and scenario B, each training sample is only used once per epoch to update the model weights.  In scenario C, we draw the training samples randomly with replacement from the training set [2]. If the number of iterations  t  is equal to the number of training samples, we learn the model based on a  bootstrap sample  of the training set.",
            "title": "C)"
        },
        {
            "location": "/user_guide/general_concepts/gradient-optimization/#mini-batch-gradient-descent-mb-gd",
            "text": "Mini-Batch Gradient Descent (MB-GD) a compromise between batch GD and SGD. In MB-GD, we update the model based on smaller groups of training samples; instead of computing the gradient from 1 sample (SGD) or all  n  training samples (GD), we compute the gradient from  $1 < k < n$  training samples (a common mini-batch size is  $k=50$ ).  MB-GD converges in fewer iterations than GD because we update the weights more frequently; however, MB-GD let's us utilize vectorized operation, which typically results in a computational performance gain over SGD.",
            "title": "Mini-Batch Gradient Descent (MB-GD)"
        },
        {
            "location": "/user_guide/general_concepts/gradient-optimization/#learning-rates",
            "text": "An adaptive learning rate  $\\eta$ : Choosing a decrease constant  d  that shrinks the learning rate over time:   $\\eta(t+1) := \\eta(t) / (1 + t \\times d)$    Momentum learning by adding a factor of the previous gradient to the weight update for faster updates:  $\\Delta \\mathbf{w}_{t+1} := \\eta \\nabla J(\\mathbf{w}_{t+1}) + \\alpha \\Delta {w}_{t}$",
            "title": "Learning Rates"
        },
        {
            "location": "/user_guide/general_concepts/gradient-optimization/#references",
            "text": "[1] Bottou, L\u00e9on (1998).  \"Online Algorithms and Stochastic Approximations\" . Online Learning and Neural Networks. Cambridge University Press. ISBN 978-0-521-65263-6  [2] Bottou, L\u00e9on.  \"Large-scale machine learning with stochastic gradient descent.\"  Proceedings of COMPSTAT'2010. Physica-Verlag HD, 2010. 177-186.  [3] Bottou, L\u00e9on.  \"Stochastic gradient descent tricks.\"  Neural Networks: Tricks of the Trade. Springer Berlin Heidelberg, 2012. 421-436.",
            "title": "References"
        },
        {
            "location": "/user_guide/general_concepts/linear-gradient-derivative/",
            "text": "Deriving the Gradient Descent Rule for Linear Regression and Adaline\n\n\nLinear Regression and Adaptive Linear Neurons (Adalines) are closely related to each other. In fact, the Adaline algorithm is a identical to linear regression except for a threshold function \n$\\phi(\\cdot)_T$\n that converts the continuous output into a categorical class label\n\n\n$$\\phi(z)_T = \\begin{cases} \n      1 & if \\; z \\geq 0 \\\\\n      0 & if \\; z < 0 \n   \\end{cases},$$\n\n\nwhere \n$z$\n is the net input, which is computed as the sum of the input features \n$\\mathbf{x}$\n multiplied by the model weights \n$\\mathbf{w}$\n:\n\n\n$$z = w_0x_0 + w_1x_1 \\dots w_mx_m = \\sum_{j=0}^{m} x_j w_j = \\mathbf{w}^T \\mathbf{x}$$\n\n\n(Note that \n$x_0$\n refers to the bias unit so that \n$x_0=1$\n.)\n\n\nIn the case of linear regression and Adaline, the activation function \n$\\phi(\\cdot)_A$\n is simply the identity function so that \n$\\phi(z)_A = z$\n.\n\n\n\n\nNow, in order to learn the optimal model weights \n$\\mathbf{w}$\n, we need to define a cost function that we can optimize. Here, our cost function \n$J({\\cdot})$\n is the sum of squared errors (SSE), which we multiply by \n$\\frac{1}{2}$\n to make the derivation easier:\n\n\n$$J({\\mathbf{w}}) = \\frac{1}{2} \\sum_i \\big(y^{(i)} - \\phi(z)_{A}^{(i)}\\big)^2,$$\n\n\nwhere \n$y^{(i)}$\n is the label or target label of the \n$i$\nth training point \n$x^{(i)}$\n.\n\n\n(Note that the SSE cost function is convex and therefore differentiable.)\n\n\nIn simple words, we can summarize the gradient descent learning as follows:\n\n\n\n\nInitialize the weights to 0 or small random numbers.\n\n\nFor \n$k$\n epochs (passes over the training set)\n\n\nFor each training sample \n$x^{(i)}$\n\n\nCompute the predicted output value \n$\\hat{y}^{(i)}$\n\n\nCompare \n$\\hat{y}^{(i)}$\n to the actual output \n$y^{(i)}$\n and Compute the \"weight update\" value\n\n\nUpdate the \"weight update\" value\n\n\n\n\n\n\nUpdate the weight coefficients by the accumulated \"weight update\" values\n\n\n\n\n\n\n\n\nWhich we can translate into a more mathematical notation:\n\n\n\n\nInitialize the weights to 0 or small random numbers.\n\n\n\n\nFor \n$k$\n epochs\n\n\n\n\n\n\nFor each training sample \n$x^{(i)}$\n\n\n\n\n$\\phi(z^{(i)})_A = \\hat{y}^{(i)}$\n\n\n$\\Delta w_{(t+1), \\; j} = \\eta (y^{(i)} - \\hat{y}^{(i)}) x_{j}^{(i)}\\;$\n  (where \n$\\eta$\n is the learning rate); \n\n\n$\\Delta w_{j} :=  \\Delta w_j\\; + \\Delta w_{(t+1), \\;j}$\n \n\n\n\n\n\n\n\n\n$\\mathbf{w} := \\mathbf{w} + \\Delta \\mathbf{w}$\n\n\n\n\n\n\n\n\n\n\nPerforming this global weight update\n\n\n$$\\mathbf{w} := \\mathbf{w} + \\Delta \\mathbf{w},$$\n\n\ncan be understood as \"updating the model weights by taking an opposite step towards the cost gradient scaled by the learning rate \n$\\eta$\n\" \n\n\n$$\\Delta \\mathbf{w} = - \\eta \\nabla J(\\mathbf{w}),$$\n\n\nwhere the partial derivative with respect to each \n$w_j$\n can be written as\n\n\n$$\\frac{\\partial J}{\\partial w_j} = - \\sum_i \\big(y^{(i)} - \\phi(z)_{A}^{(i)}\\big) x_{j}^{(i)}.$$\n\n\nTo summarize: in order to use gradient descent to learn the model coefficients, we simply update the weights \n$\\mathbf{w}$\n by taking a step into the opposite direction of the gradient for each pass over the training set -- that's basically it. But how do we get to the equation\n\n\n$$\\frac{\\partial J}{\\partial w_j} = - \\sum_i \\big(y^{(i)} - \\phi(z)_{A}^{(i)}\\big) x_{j}^{(i)}?$$\n\n\nLet's walk through the derivation step by step.\n\n\n$$\\begin{aligned}\n& \\frac{\\partial J}{\\partial w_j} \\\\\n& = \\frac{\\partial}{\\partial w_j} \\frac{1}{2} \\sum_i \\big(y^{(i)} - \\phi(z)_{A}^{(i)}\\big)^2 \\\\\n& = \\frac{1}{2} \\frac{\\partial}{\\partial w_j} \\sum_i \\big(y^{(i)} - \\phi(z)_{A}^{(i)}\\big)^2 \\\\\n& = \\frac{1}{2} \\sum_i  \\big(y^{(i)} - \\phi(z)_{A}^{(i)}\\big) \\frac{\\partial}{\\partial w_j}  \\big(y^{(i)} - \\phi(z)_{A}^{(i)}\\big) \\\\\n& = \\sum_i  \\big(y^{(i)} - \\phi(z)_{A}^{(i)}\\big) \\frac{\\partial}{\\partial w_j} \\bigg(y^{(i)} - \\sum_i \\big(w_{j}^{(i)} x_{j}^{(i)} \\big) \\bigg) \\\\\n& = \\sum_i \\big(y^{(i)} - \\phi(z)_{A}^{(i)}\\big)(-x_{j}^{(i)}) \\\\\n& = - \\sum_i \\big(y^{(i)} - \\phi(z)_{A}^{(i)}\\big)x_{j}^{(i)} \n\\end{aligned}$$",
            "title": "Linear gradient derivative"
        },
        {
            "location": "/user_guide/general_concepts/linear-gradient-derivative/#deriving-the-gradient-descent-rule-for-linear-regression-and-adaline",
            "text": "Linear Regression and Adaptive Linear Neurons (Adalines) are closely related to each other. In fact, the Adaline algorithm is a identical to linear regression except for a threshold function  $\\phi(\\cdot)_T$  that converts the continuous output into a categorical class label  $$\\phi(z)_T = \\begin{cases} \n      1 & if \\; z \\geq 0 \\\\\n      0 & if \\; z < 0 \n   \\end{cases},$$  where  $z$  is the net input, which is computed as the sum of the input features  $\\mathbf{x}$  multiplied by the model weights  $\\mathbf{w}$ :  $$z = w_0x_0 + w_1x_1 \\dots w_mx_m = \\sum_{j=0}^{m} x_j w_j = \\mathbf{w}^T \\mathbf{x}$$  (Note that  $x_0$  refers to the bias unit so that  $x_0=1$ .)  In the case of linear regression and Adaline, the activation function  $\\phi(\\cdot)_A$  is simply the identity function so that  $\\phi(z)_A = z$ .   Now, in order to learn the optimal model weights  $\\mathbf{w}$ , we need to define a cost function that we can optimize. Here, our cost function  $J({\\cdot})$  is the sum of squared errors (SSE), which we multiply by  $\\frac{1}{2}$  to make the derivation easier:  $$J({\\mathbf{w}}) = \\frac{1}{2} \\sum_i \\big(y^{(i)} - \\phi(z)_{A}^{(i)}\\big)^2,$$  where  $y^{(i)}$  is the label or target label of the  $i$ th training point  $x^{(i)}$ .  (Note that the SSE cost function is convex and therefore differentiable.)  In simple words, we can summarize the gradient descent learning as follows:   Initialize the weights to 0 or small random numbers.  For  $k$  epochs (passes over the training set)  For each training sample  $x^{(i)}$  Compute the predicted output value  $\\hat{y}^{(i)}$  Compare  $\\hat{y}^{(i)}$  to the actual output  $y^{(i)}$  and Compute the \"weight update\" value  Update the \"weight update\" value    Update the weight coefficients by the accumulated \"weight update\" values     Which we can translate into a more mathematical notation:   Initialize the weights to 0 or small random numbers.   For  $k$  epochs    For each training sample  $x^{(i)}$   $\\phi(z^{(i)})_A = \\hat{y}^{(i)}$  $\\Delta w_{(t+1), \\; j} = \\eta (y^{(i)} - \\hat{y}^{(i)}) x_{j}^{(i)}\\;$   (where  $\\eta$  is the learning rate);   $\\Delta w_{j} :=  \\Delta w_j\\; + \\Delta w_{(t+1), \\;j}$       $\\mathbf{w} := \\mathbf{w} + \\Delta \\mathbf{w}$      Performing this global weight update  $$\\mathbf{w} := \\mathbf{w} + \\Delta \\mathbf{w},$$  can be understood as \"updating the model weights by taking an opposite step towards the cost gradient scaled by the learning rate  $\\eta$ \"   $$\\Delta \\mathbf{w} = - \\eta \\nabla J(\\mathbf{w}),$$  where the partial derivative with respect to each  $w_j$  can be written as  $$\\frac{\\partial J}{\\partial w_j} = - \\sum_i \\big(y^{(i)} - \\phi(z)_{A}^{(i)}\\big) x_{j}^{(i)}.$$  To summarize: in order to use gradient descent to learn the model coefficients, we simply update the weights  $\\mathbf{w}$  by taking a step into the opposite direction of the gradient for each pass over the training set -- that's basically it. But how do we get to the equation  $$\\frac{\\partial J}{\\partial w_j} = - \\sum_i \\big(y^{(i)} - \\phi(z)_{A}^{(i)}\\big) x_{j}^{(i)}?$$  Let's walk through the derivation step by step.  $$\\begin{aligned}\n& \\frac{\\partial J}{\\partial w_j} \\\\\n& = \\frac{\\partial}{\\partial w_j} \\frac{1}{2} \\sum_i \\big(y^{(i)} - \\phi(z)_{A}^{(i)}\\big)^2 \\\\\n& = \\frac{1}{2} \\frac{\\partial}{\\partial w_j} \\sum_i \\big(y^{(i)} - \\phi(z)_{A}^{(i)}\\big)^2 \\\\\n& = \\frac{1}{2} \\sum_i  \\big(y^{(i)} - \\phi(z)_{A}^{(i)}\\big) \\frac{\\partial}{\\partial w_j}  \\big(y^{(i)} - \\phi(z)_{A}^{(i)}\\big) \\\\\n& = \\sum_i  \\big(y^{(i)} - \\phi(z)_{A}^{(i)}\\big) \\frac{\\partial}{\\partial w_j} \\bigg(y^{(i)} - \\sum_i \\big(w_{j}^{(i)} x_{j}^{(i)} \\big) \\bigg) \\\\\n& = \\sum_i \\big(y^{(i)} - \\phi(z)_{A}^{(i)}\\big)(-x_{j}^{(i)}) \\\\\n& = - \\sum_i \\big(y^{(i)} - \\phi(z)_{A}^{(i)}\\big)x_{j}^{(i)} \n\\end{aligned}$$",
            "title": "Deriving the Gradient Descent Rule for Linear Regression and Adaline"
        },
        {
            "location": "/user_guide/general_concepts/regularization-linear/",
            "text": "Regularization of Generalized Linear Models\n\n\nOverview\n\n\nWe can understand regularization as an approach of adding an additional bias to a model to reduce the degree of overfitting in models that suffer from high variance. By adding regularization terms to the cost function, we penalize large model coefficients (weights); effectively, we are reducing the complexity of the model.\n\n\nL2 regularization\n\n\nIn L2 regularization, we shrink the weights by computing the Euclidean norm of the weight coefficients (the weight vector \n$\\mathbf{w}$\n); \n$\\lambda$\n is the regularization parameter to be optimized.\n\n\n$$L2: \\lambda\\; \\lVert \\mathbf{w} \\lVert_2 = \\lambda \\sum_{j=1}^{m} w_j^2$$\n\n\nFor example, we can regularize the sum of squared errors cost function (SSE) as follows:\n\n$$SSE =  \\sum^{n}_{i=1} \\big(\\text{target}^{(i)} - \\text{output}^{(i)}\\big)^2 + L2$$\n\n\nIntuitively, we can think of regression as an additional penalty term or constraint as shown in the figure below. Without regularization, our objective is to find the global cost minimum. By adding a regularization penalty, our objective becomes to minimize the cost function under the constraint that we have to stay within our \"budget\" (the gray-shaded ball).\n\n\n\n\nIn addition, we can control the regularization strength via the regularization\nparameter \n$\\lambda$\n. The larger the value of \n$\\lambda$\n, the stronger the regularization of the model. The weight coefficients approach 0 when \n$\\lambda$\n goes towards infinity.\n\n\nL1 regularization\n\n\nIn L1 regularization, we shrink the weights using the absolute values of the weight coefficients (the weight vector \n$\\mathbf{w}$\n); \n$\\lambda$\n is the regularization parameter to be optimized.\n\n\n$$L1: \\lambda \\; \\lVert\\mathbf{w}\\rVert_1 = \\lambda \\sum_{j=1}^{m} |w_j|$$\n\n\nFor example, we can regularize the sum of squared errors cost function (SSE) as follows:\n\n$$SSE =  \\sum^{n}_{i=1} \\big(\\text{target}^{(i)} - \\text{output}^{(i)}\\big)^2 + L1$$\n\n\nAt its core, L1-regularization is very similar to L2 regularization. However, instead of a quadratic penalty term as in L2, we penalize the model by the absolute weight coefficients. As we can see in the figure below, our \"budget\" has \"sharp edges,\" which is the geometric interpretation of why the L1 model induces sparsity.\n\n\n\n\nReferences\n\n\n\n\n[1] M. Y. Park and T. Hastie. \n\"L1-regularization path algorithm for generalized linear models\"\n. Journal of the Royal Statistical Society: Series B (Statistical Methodology), 69(4):659\u2013677, 2007.\n\n\n[2] A. Y. Ng. \n\"Feature selection, L1 vs. L2 regularization, and rotational invariance\"\n. In Proceedings of the twenty-first international conference on Machine learning, page 78. ACM, 2004.",
            "title": "Regularization linear"
        },
        {
            "location": "/user_guide/general_concepts/regularization-linear/#regularization-of-generalized-linear-models",
            "text": "",
            "title": "Regularization of Generalized Linear Models"
        },
        {
            "location": "/user_guide/general_concepts/regularization-linear/#overview",
            "text": "We can understand regularization as an approach of adding an additional bias to a model to reduce the degree of overfitting in models that suffer from high variance. By adding regularization terms to the cost function, we penalize large model coefficients (weights); effectively, we are reducing the complexity of the model.",
            "title": "Overview"
        },
        {
            "location": "/user_guide/general_concepts/regularization-linear/#l2-regularization",
            "text": "In L2 regularization, we shrink the weights by computing the Euclidean norm of the weight coefficients (the weight vector  $\\mathbf{w}$ );  $\\lambda$  is the regularization parameter to be optimized.  $$L2: \\lambda\\; \\lVert \\mathbf{w} \\lVert_2 = \\lambda \\sum_{j=1}^{m} w_j^2$$  For example, we can regularize the sum of squared errors cost function (SSE) as follows: $$SSE =  \\sum^{n}_{i=1} \\big(\\text{target}^{(i)} - \\text{output}^{(i)}\\big)^2 + L2$$  Intuitively, we can think of regression as an additional penalty term or constraint as shown in the figure below. Without regularization, our objective is to find the global cost minimum. By adding a regularization penalty, our objective becomes to minimize the cost function under the constraint that we have to stay within our \"budget\" (the gray-shaded ball).   In addition, we can control the regularization strength via the regularization\nparameter  $\\lambda$ . The larger the value of  $\\lambda$ , the stronger the regularization of the model. The weight coefficients approach 0 when  $\\lambda$  goes towards infinity.",
            "title": "L2 regularization"
        },
        {
            "location": "/user_guide/general_concepts/regularization-linear/#l1-regularization",
            "text": "In L1 regularization, we shrink the weights using the absolute values of the weight coefficients (the weight vector  $\\mathbf{w}$ );  $\\lambda$  is the regularization parameter to be optimized.  $$L1: \\lambda \\; \\lVert\\mathbf{w}\\rVert_1 = \\lambda \\sum_{j=1}^{m} |w_j|$$  For example, we can regularize the sum of squared errors cost function (SSE) as follows: $$SSE =  \\sum^{n}_{i=1} \\big(\\text{target}^{(i)} - \\text{output}^{(i)}\\big)^2 + L1$$  At its core, L1-regularization is very similar to L2 regularization. However, instead of a quadratic penalty term as in L2, we penalize the model by the absolute weight coefficients. As we can see in the figure below, our \"budget\" has \"sharp edges,\" which is the geometric interpretation of why the L1 model induces sparsity.",
            "title": "L1 regularization"
        },
        {
            "location": "/user_guide/general_concepts/regularization-linear/#references",
            "text": "[1] M. Y. Park and T. Hastie.  \"L1-regularization path algorithm for generalized linear models\" . Journal of the Royal Statistical Society: Series B (Statistical Methodology), 69(4):659\u2013677, 2007.  [2] A. Y. Ng.  \"Feature selection, L1 vs. L2 regularization, and rotational invariance\" . In Proceedings of the twenty-first international conference on Machine learning, page 78. ACM, 2004.",
            "title": "References"
        },
        {
            "location": "/user_guide/regressor/StackingRegressor/",
            "text": "StackingRegressor\n\n\nAn ensemble-learning meta-regressor for stacking regression\n\n\n\n\nfrom mlxtend.regressor import StackingRegressor\n\n\n\n\nOverview\n\n\nStacking regression is an ensemble learning technique to combine multiple regression models via a meta-regressor. The individual regression models are trained based on the complete training set; then, the meta-regressor is fitted based on the outputs -- meta-features -- of the individual regression models in the ensemble.\n\n\n\n\nReferences\n\n\n\n\nBreiman, Leo. \"\nStacked regressions.\n\" Machine learning 24.1 (1996): 49-64.\n\n\n\n\nExamples\n\n\nExample 1 - Simple Stacked Regression\n\n\nfrom mlxtend.regressor import StackingRegressor\nfrom mlxtend.data import boston_housing_data\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.linear_model import Ridge\nfrom sklearn.svm import SVR\nimport matplotlib.pyplot as plt\nimport numpy as np\n\n# Generating a sample dataset\nnp.random.seed(1)\nX = np.sort(5 * np.random.rand(40, 1), axis=0)\ny = np.sin(X).ravel()\ny[::5] += 3 * (0.5 - np.random.rand(8))\n\n\n\n\n# Initializing models\n\nlr = LinearRegression()\nsvr_lin = SVR(kernel='linear')\nridge = Ridge(random_state=1)\nsvr_rbf = SVR(kernel='rbf')\n\nstregr = StackingRegressor(regressors=[svr_lin, lr, ridge], \n                           meta_regressor=svr_rbf)\n\n# Training the stacking classifier\n\nstregr.fit(X, y)\nstregr.predict(X)\n\n# Evaluate and visualize the fit\n\nprint(\"Mean Squared Error: %.2f\"\n      % np.mean((stregr.predict(X) - y) ** 2))\nprint('Variance Score: %.2f' % stregr.score(X, y))\n\nwith plt.style.context(('seaborn-whitegrid')):\n    plt.scatter(X, y, c='lightgray')\n    plt.plot(X, stregr.predict(X), c='darkgreen', lw=2)\n\nplt.show()\n\n\n\n\nMean Squared Error: 0.20\nVariance Score: 0.70\n\n\n\n\n\nExample 2 - Stacked Regression and GridSearch\n\n\nTo set up a parameter grid for scikit-learn's \nGridSearch\n, we simply provide the estimator's names in the parameter grid -- in the special case of the meta-regressor, we append the \n'meta-'\n prefix.\n\n\nfrom sklearn.grid_search import GridSearchCV\n\n# Initializing models\n\nlr = LinearRegression()\nsvr_lin = SVR(kernel='linear')\nridge = Ridge(random_state=1)\nsvr_rbf = SVR(kernel='rbf')\nstregr = StackingRegressor(regressors=[svr_lin, lr, ridge], \n                           meta_regressor=svr_rbf)\n\nparams = {'ridge__alpha': [0.01, 1.0],\n          'svr__C': [0.01, 1.0],\n          'meta-svr__C': [0.01, 1.0]}\n\ngrid = GridSearchCV(estimator=stregr, \n                    param_grid=params, \n                    cv=5,\n                    refit=True)\ngrid.fit(X, y)\n\nfor params, mean_score, scores in grid.grid_scores_:\n    print(\"%0.3f (+/-%0.03f) for %r\"\n        % (mean_score, scores.std() / 2, params))\n\n\n\n\n-10.771 (+/-7.199) for {'ridge__alpha': 0.01, 'svr__C': 0.01, 'meta-svr__C': 0.01}\n-10.549 (+/-7.116) for {'ridge__alpha': 0.01, 'svr__C': 1.0, 'meta-svr__C': 0.01}\n-10.775 (+/-7.201) for {'ridge__alpha': 1.0, 'svr__C': 0.01, 'meta-svr__C': 0.01}\n-10.552 (+/-7.118) for {'ridge__alpha': 1.0, 'svr__C': 1.0, 'meta-svr__C': 0.01}\n-3.727 (+/-2.972) for {'ridge__alpha': 0.01, 'svr__C': 0.01, 'meta-svr__C': 1.0}\n-1.023 (+/-0.667) for {'ridge__alpha': 0.01, 'svr__C': 1.0, 'meta-svr__C': 1.0}\n-3.784 (+/-3.016) for {'ridge__alpha': 1.0, 'svr__C': 0.01, 'meta-svr__C': 1.0}\n-1.042 (+/-0.685) for {'ridge__alpha': 1.0, 'svr__C': 1.0, 'meta-svr__C': 1.0}\n\n\n\n# Evaluate and visualize the fit\nprint(\"Mean Squared Error: %.2f\"\n      % np.mean((grid.predict(X) - y) ** 2))\nprint('Variance Score: %.2f' % grid.score(X, y))\n\nwith plt.style.context(('seaborn-whitegrid')):\n    plt.scatter(X, y, c='lightgray')\n    plt.plot(X, grid.predict(X), c='darkgreen', lw=2)\n\nplt.show()\n\n\n\n\nMean Squared Error: 0.20\nVariance Score: 0.70\n\n\n\n\n\nIn case we are planning to use a regression algorithm multiple times, all we need to do is to add an additional number suffix in the parameter grid as shown below:\n\n\nstregr = StackingRegressor(regressors=[ridge, ridge, svr_lin], \n                           meta_regressor=svr_rbf)\n\nparams = {'ridge-1__alpha': [0.01, 1.0],\n          'ridge-2__alpha': [0.01, 1.0],\n          'svr__C': [0.01, 1.0],\n          'meta-svr__C': [0.01, 1.0]}\n\ngrid = GridSearchCV(estimator=stregr, \n                    param_grid=params, \n                    cv=5,\n                    refit=True)\ngrid.fit(X, y)\n\nfor params, mean_score, scores in grid.grid_scores_:\n    print(\"%0.3f (+/-%0.03f) for %r\"\n        % (mean_score, scores.std() / 2, params))\n\n\n\n\n-10.771 (+/-7.199) for {'ridge-1__alpha': 0.01, 'ridge-2__alpha': 0.01, 'svr__C': 0.01, 'meta-svr__C': 0.01}\n-10.549 (+/-7.116) for {'ridge-1__alpha': 0.01, 'ridge-2__alpha': 0.01, 'svr__C': 1.0, 'meta-svr__C': 0.01}\n-10.775 (+/-7.201) for {'ridge-1__alpha': 0.01, 'ridge-2__alpha': 1.0, 'svr__C': 0.01, 'meta-svr__C': 0.01}\n-10.552 (+/-7.118) for {'ridge-1__alpha': 0.01, 'ridge-2__alpha': 1.0, 'svr__C': 1.0, 'meta-svr__C': 0.01}\n-10.775 (+/-7.201) for {'ridge-1__alpha': 1.0, 'ridge-2__alpha': 0.01, 'svr__C': 0.01, 'meta-svr__C': 0.01}\n-10.552 (+/-7.118) for {'ridge-1__alpha': 1.0, 'ridge-2__alpha': 0.01, 'svr__C': 1.0, 'meta-svr__C': 0.01}\n-10.780 (+/-7.203) for {'ridge-1__alpha': 1.0, 'ridge-2__alpha': 1.0, 'svr__C': 0.01, 'meta-svr__C': 0.01}\n-10.556 (+/-7.120) for {'ridge-1__alpha': 1.0, 'ridge-2__alpha': 1.0, 'svr__C': 1.0, 'meta-svr__C': 0.01}\n-3.728 (+/-2.972) for {'ridge-1__alpha': 0.01, 'ridge-2__alpha': 0.01, 'svr__C': 0.01, 'meta-svr__C': 1.0}\n-1.023 (+/-0.667) for {'ridge-1__alpha': 0.01, 'ridge-2__alpha': 0.01, 'svr__C': 1.0, 'meta-svr__C': 1.0}\n-3.784 (+/-3.017) for {'ridge-1__alpha': 0.01, 'ridge-2__alpha': 1.0, 'svr__C': 0.01, 'meta-svr__C': 1.0}\n-1.042 (+/-0.685) for {'ridge-1__alpha': 0.01, 'ridge-2__alpha': 1.0, 'svr__C': 1.0, 'meta-svr__C': 1.0}\n-3.784 (+/-3.017) for {'ridge-1__alpha': 1.0, 'ridge-2__alpha': 0.01, 'svr__C': 0.01, 'meta-svr__C': 1.0}\n-1.042 (+/-0.685) for {'ridge-1__alpha': 1.0, 'ridge-2__alpha': 0.01, 'svr__C': 1.0, 'meta-svr__C': 1.0}\n-3.842 (+/-3.060) for {'ridge-1__alpha': 1.0, 'ridge-2__alpha': 1.0, 'svr__C': 0.01, 'meta-svr__C': 1.0}\n-1.062 (+/-0.704) for {'ridge-1__alpha': 1.0, 'ridge-2__alpha': 1.0, 'svr__C': 1.0, 'meta-svr__C': 1.0}\n\n\n\nAPI\n\n\nStackingRegressor(regressors, meta_regressor, verbose=0)\n\n\nA Stacking regressor for scikit-learn estimators for regression.\n\n\nParameters\n\n\n\n\n\n\nregressors\n : array-like, shape = [n_regressors]\n\n\nA list of regressors.\nInvoking the \nfit\n method on the \nStackingRegressor\n will fit clones\nof those original regressors that will\nbe stored in the class attribute\n\nself.regr_\n.\n\n\n\n\n\n\nverbose\n : int, optional (default=0)\n\n\nControls the verbosity of the building process.\n- \nverbose=0\n (default): Prints nothing\n- \nverbose=1\n: Prints the number & name of the regressor being fitted\n- \nverbose=2\n: Prints info about the parameters of the\nregressor being fitted\n- \nverbose>2\n: Changes \nverbose\n param of the underlying regressor to\nself.verbose - 2\n\n\n\n\n\n\nAttributes\n\n\n\n\n\n\nregr_\n : list, shape=[n_regressors]\n\n\nFitted regressors (clones of the original regressors)\n\n\n\n\n\n\nmeta_regr_\n : estimator\n\n\nFitted meta-regressor (clone of the original meta-estimator)\n\n\n\n\n\n\ncoef_\n : array-like, shape = [n_features]\n\n\nModel coefficients of the fitted meta-estimator\n\n\n\n\n\n\nintercept_\n : float\n\n\nIntercept of the fitted meta-estimator\n\n\n\n\n\n\nMethods\n\n\n\n\n\nfit(X, y)\n\n\nLearn weight coefficients from training data for each regressor.\n\n\nParameters\n\n\n\n\n\n\nX\n : {array-like, sparse matrix}, shape = [n_samples, n_features]\n\n\nTraining vectors, where n_samples is the number of samples and\nn_features is the number of features.\n\n\n\n\n\n\ny\n : array-like, shape = [n_samples]\n\n\nTarget values.\n\n\n\n\n\n\nReturns\n\n\n\n\nself\n : object\n\n\n\n\n\n\n\nfit_transform(X, y=None, \nfit_params)\n\n\nFit to data, then transform it.\n\n\nFits transformer to X and y with optional parameters fit_params\nand returns a transformed version of X.\n\n\n\nParameters\n\n\n\n\n\n\nX\n : numpy array of shape [n_samples, n_features]\n\n\nTraining set.\n\n\n\n\n\n\ny\n : numpy array of shape [n_samples]\n\n\nTarget values.\n\n\n\n\n\n\nReturns\n\n\n\n\n\n\nX_new\n : numpy array of shape [n_samples, n_features_new]\n\n\nTransformed array.\n\n\n\n\n\n\n\n\n\nget_params(deep=True)\n\n\nReturn estimator parameter names for GridSearch support.\n\n\n\n\n\npredict(X)\n\n\nPredict target values for X.\n\n\nParameters\n\n\n\n\n\n\nX\n : {array-like, sparse matrix}, shape = [n_samples, n_features]\n\n\nTraining vectors, where n_samples is the number of samples and\nn_features is the number of features.\n\n\n\n\n\n\nReturns\n\n\n\n\n\n\nmaj\n : array-like, shape = [n_samples]\n\n\nPredicted target values.\n\n\n\n\n\n\n\n\n\nscore(X, y, sample_weight=None)\n\n\nReturns the coefficient of determination R^2 of the prediction.\n\n\nThe coefficient R^2 is defined as (1 - u/v), where u is the regression\n\n\n\nsum of squares ((y_true - y_pred) \n 2).sum() and v is the residual\nsum of squares ((y_true - y_true.mean()) \n 2).sum().\n\n\nBest possible score is 1.0 and it can be negative (because the\n\n\nmodel can be arbitrarily worse). A constant model that always\n    predicts the expected value of y, disregarding the input features,\n    would get a R^2 score of 0.0.\n\n\nParameters\n\n\n\n\n\n\nX\n : array-like, shape = (n_samples, n_features)\n\n\nTest samples.\n\n\n\n\n\n\ny\n : array-like, shape = (n_samples) or (n_samples, n_outputs)\n\n\nTrue values for X.\n\n\n\n\n\n\nsample_weight\n : array-like, shape = [n_samples], optional\n\n\nSample weights.\n\n\n\n\n\n\nReturns\n\n\n\n\n\n\nscore\n : float\n\n\nR^2 of self.predict(X) wrt. y.\n\n\n\n\n\n\n\n\n\nset_params(\nparams)\n\n\nSet the parameters of this estimator.\n\n\nThe method works on simple estimators as well as on nested objects\n(such as pipelines). The former have parameters of the form\n``<component>__<parameter>`` so that it's possible to update each\ncomponent of a nested object.\n\n\n\nReturns\n\n\nself\n\n\nProperties\n\n\n\n\n\ncoef_\n\n\nNone\n\n\n\n\n\nintercept_\n\n\nNone",
            "title": "StackingRegressor"
        },
        {
            "location": "/user_guide/regressor/StackingRegressor/#stackingregressor",
            "text": "An ensemble-learning meta-regressor for stacking regression   from mlxtend.regressor import StackingRegressor",
            "title": "StackingRegressor"
        },
        {
            "location": "/user_guide/regressor/StackingRegressor/#overview",
            "text": "Stacking regression is an ensemble learning technique to combine multiple regression models via a meta-regressor. The individual regression models are trained based on the complete training set; then, the meta-regressor is fitted based on the outputs -- meta-features -- of the individual regression models in the ensemble.",
            "title": "Overview"
        },
        {
            "location": "/user_guide/regressor/StackingRegressor/#references",
            "text": "Breiman, Leo. \" Stacked regressions. \" Machine learning 24.1 (1996): 49-64.",
            "title": "References"
        },
        {
            "location": "/user_guide/regressor/StackingRegressor/#examples",
            "text": "",
            "title": "Examples"
        },
        {
            "location": "/user_guide/regressor/StackingRegressor/#example-1-simple-stacked-regression",
            "text": "from mlxtend.regressor import StackingRegressor\nfrom mlxtend.data import boston_housing_data\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.linear_model import Ridge\nfrom sklearn.svm import SVR\nimport matplotlib.pyplot as plt\nimport numpy as np\n\n# Generating a sample dataset\nnp.random.seed(1)\nX = np.sort(5 * np.random.rand(40, 1), axis=0)\ny = np.sin(X).ravel()\ny[::5] += 3 * (0.5 - np.random.rand(8))  # Initializing models\n\nlr = LinearRegression()\nsvr_lin = SVR(kernel='linear')\nridge = Ridge(random_state=1)\nsvr_rbf = SVR(kernel='rbf')\n\nstregr = StackingRegressor(regressors=[svr_lin, lr, ridge], \n                           meta_regressor=svr_rbf)\n\n# Training the stacking classifier\n\nstregr.fit(X, y)\nstregr.predict(X)\n\n# Evaluate and visualize the fit\n\nprint(\"Mean Squared Error: %.2f\"\n      % np.mean((stregr.predict(X) - y) ** 2))\nprint('Variance Score: %.2f' % stregr.score(X, y))\n\nwith plt.style.context(('seaborn-whitegrid')):\n    plt.scatter(X, y, c='lightgray')\n    plt.plot(X, stregr.predict(X), c='darkgreen', lw=2)\n\nplt.show()  Mean Squared Error: 0.20\nVariance Score: 0.70",
            "title": "Example 1 - Simple Stacked Regression"
        },
        {
            "location": "/user_guide/regressor/StackingRegressor/#example-2-stacked-regression-and-gridsearch",
            "text": "To set up a parameter grid for scikit-learn's  GridSearch , we simply provide the estimator's names in the parameter grid -- in the special case of the meta-regressor, we append the  'meta-'  prefix.  from sklearn.grid_search import GridSearchCV\n\n# Initializing models\n\nlr = LinearRegression()\nsvr_lin = SVR(kernel='linear')\nridge = Ridge(random_state=1)\nsvr_rbf = SVR(kernel='rbf')\nstregr = StackingRegressor(regressors=[svr_lin, lr, ridge], \n                           meta_regressor=svr_rbf)\n\nparams = {'ridge__alpha': [0.01, 1.0],\n          'svr__C': [0.01, 1.0],\n          'meta-svr__C': [0.01, 1.0]}\n\ngrid = GridSearchCV(estimator=stregr, \n                    param_grid=params, \n                    cv=5,\n                    refit=True)\ngrid.fit(X, y)\n\nfor params, mean_score, scores in grid.grid_scores_:\n    print(\"%0.3f (+/-%0.03f) for %r\"\n        % (mean_score, scores.std() / 2, params))  -10.771 (+/-7.199) for {'ridge__alpha': 0.01, 'svr__C': 0.01, 'meta-svr__C': 0.01}\n-10.549 (+/-7.116) for {'ridge__alpha': 0.01, 'svr__C': 1.0, 'meta-svr__C': 0.01}\n-10.775 (+/-7.201) for {'ridge__alpha': 1.0, 'svr__C': 0.01, 'meta-svr__C': 0.01}\n-10.552 (+/-7.118) for {'ridge__alpha': 1.0, 'svr__C': 1.0, 'meta-svr__C': 0.01}\n-3.727 (+/-2.972) for {'ridge__alpha': 0.01, 'svr__C': 0.01, 'meta-svr__C': 1.0}\n-1.023 (+/-0.667) for {'ridge__alpha': 0.01, 'svr__C': 1.0, 'meta-svr__C': 1.0}\n-3.784 (+/-3.016) for {'ridge__alpha': 1.0, 'svr__C': 0.01, 'meta-svr__C': 1.0}\n-1.042 (+/-0.685) for {'ridge__alpha': 1.0, 'svr__C': 1.0, 'meta-svr__C': 1.0}  # Evaluate and visualize the fit\nprint(\"Mean Squared Error: %.2f\"\n      % np.mean((grid.predict(X) - y) ** 2))\nprint('Variance Score: %.2f' % grid.score(X, y))\n\nwith plt.style.context(('seaborn-whitegrid')):\n    plt.scatter(X, y, c='lightgray')\n    plt.plot(X, grid.predict(X), c='darkgreen', lw=2)\n\nplt.show()  Mean Squared Error: 0.20\nVariance Score: 0.70   In case we are planning to use a regression algorithm multiple times, all we need to do is to add an additional number suffix in the parameter grid as shown below:  stregr = StackingRegressor(regressors=[ridge, ridge, svr_lin], \n                           meta_regressor=svr_rbf)\n\nparams = {'ridge-1__alpha': [0.01, 1.0],\n          'ridge-2__alpha': [0.01, 1.0],\n          'svr__C': [0.01, 1.0],\n          'meta-svr__C': [0.01, 1.0]}\n\ngrid = GridSearchCV(estimator=stregr, \n                    param_grid=params, \n                    cv=5,\n                    refit=True)\ngrid.fit(X, y)\n\nfor params, mean_score, scores in grid.grid_scores_:\n    print(\"%0.3f (+/-%0.03f) for %r\"\n        % (mean_score, scores.std() / 2, params))  -10.771 (+/-7.199) for {'ridge-1__alpha': 0.01, 'ridge-2__alpha': 0.01, 'svr__C': 0.01, 'meta-svr__C': 0.01}\n-10.549 (+/-7.116) for {'ridge-1__alpha': 0.01, 'ridge-2__alpha': 0.01, 'svr__C': 1.0, 'meta-svr__C': 0.01}\n-10.775 (+/-7.201) for {'ridge-1__alpha': 0.01, 'ridge-2__alpha': 1.0, 'svr__C': 0.01, 'meta-svr__C': 0.01}\n-10.552 (+/-7.118) for {'ridge-1__alpha': 0.01, 'ridge-2__alpha': 1.0, 'svr__C': 1.0, 'meta-svr__C': 0.01}\n-10.775 (+/-7.201) for {'ridge-1__alpha': 1.0, 'ridge-2__alpha': 0.01, 'svr__C': 0.01, 'meta-svr__C': 0.01}\n-10.552 (+/-7.118) for {'ridge-1__alpha': 1.0, 'ridge-2__alpha': 0.01, 'svr__C': 1.0, 'meta-svr__C': 0.01}\n-10.780 (+/-7.203) for {'ridge-1__alpha': 1.0, 'ridge-2__alpha': 1.0, 'svr__C': 0.01, 'meta-svr__C': 0.01}\n-10.556 (+/-7.120) for {'ridge-1__alpha': 1.0, 'ridge-2__alpha': 1.0, 'svr__C': 1.0, 'meta-svr__C': 0.01}\n-3.728 (+/-2.972) for {'ridge-1__alpha': 0.01, 'ridge-2__alpha': 0.01, 'svr__C': 0.01, 'meta-svr__C': 1.0}\n-1.023 (+/-0.667) for {'ridge-1__alpha': 0.01, 'ridge-2__alpha': 0.01, 'svr__C': 1.0, 'meta-svr__C': 1.0}\n-3.784 (+/-3.017) for {'ridge-1__alpha': 0.01, 'ridge-2__alpha': 1.0, 'svr__C': 0.01, 'meta-svr__C': 1.0}\n-1.042 (+/-0.685) for {'ridge-1__alpha': 0.01, 'ridge-2__alpha': 1.0, 'svr__C': 1.0, 'meta-svr__C': 1.0}\n-3.784 (+/-3.017) for {'ridge-1__alpha': 1.0, 'ridge-2__alpha': 0.01, 'svr__C': 0.01, 'meta-svr__C': 1.0}\n-1.042 (+/-0.685) for {'ridge-1__alpha': 1.0, 'ridge-2__alpha': 0.01, 'svr__C': 1.0, 'meta-svr__C': 1.0}\n-3.842 (+/-3.060) for {'ridge-1__alpha': 1.0, 'ridge-2__alpha': 1.0, 'svr__C': 0.01, 'meta-svr__C': 1.0}\n-1.062 (+/-0.704) for {'ridge-1__alpha': 1.0, 'ridge-2__alpha': 1.0, 'svr__C': 1.0, 'meta-svr__C': 1.0}",
            "title": "Example 2 - Stacked Regression and GridSearch"
        },
        {
            "location": "/user_guide/regressor/StackingRegressor/#api",
            "text": "StackingRegressor(regressors, meta_regressor, verbose=0)  A Stacking regressor for scikit-learn estimators for regression.  Parameters    regressors  : array-like, shape = [n_regressors]  A list of regressors.\nInvoking the  fit  method on the  StackingRegressor  will fit clones\nof those original regressors that will\nbe stored in the class attribute self.regr_ .    verbose  : int, optional (default=0)  Controls the verbosity of the building process.\n-  verbose=0  (default): Prints nothing\n-  verbose=1 : Prints the number & name of the regressor being fitted\n-  verbose=2 : Prints info about the parameters of the\nregressor being fitted\n-  verbose>2 : Changes  verbose  param of the underlying regressor to\nself.verbose - 2    Attributes    regr_  : list, shape=[n_regressors]  Fitted regressors (clones of the original regressors)    meta_regr_  : estimator  Fitted meta-regressor (clone of the original meta-estimator)    coef_  : array-like, shape = [n_features]  Model coefficients of the fitted meta-estimator    intercept_  : float  Intercept of the fitted meta-estimator",
            "title": "API"
        },
        {
            "location": "/user_guide/regressor/StackingRegressor/#methods",
            "text": "fit(X, y)  Learn weight coefficients from training data for each regressor.  Parameters    X  : {array-like, sparse matrix}, shape = [n_samples, n_features]  Training vectors, where n_samples is the number of samples and\nn_features is the number of features.    y  : array-like, shape = [n_samples]  Target values.    Returns   self  : object    fit_transform(X, y=None,  fit_params)  Fit to data, then transform it.  Fits transformer to X and y with optional parameters fit_params\nand returns a transformed version of X.  Parameters    X  : numpy array of shape [n_samples, n_features]  Training set.    y  : numpy array of shape [n_samples]  Target values.    Returns    X_new  : numpy array of shape [n_samples, n_features_new]  Transformed array.     get_params(deep=True)  Return estimator parameter names for GridSearch support.   predict(X)  Predict target values for X.  Parameters    X  : {array-like, sparse matrix}, shape = [n_samples, n_features]  Training vectors, where n_samples is the number of samples and\nn_features is the number of features.    Returns    maj  : array-like, shape = [n_samples]  Predicted target values.     score(X, y, sample_weight=None)  Returns the coefficient of determination R^2 of the prediction.  The coefficient R^2 is defined as (1 - u/v), where u is the regression  sum of squares ((y_true - y_pred)   2).sum() and v is the residual\nsum of squares ((y_true - y_true.mean())   2).sum().  Best possible score is 1.0 and it can be negative (because the  model can be arbitrarily worse). A constant model that always\n    predicts the expected value of y, disregarding the input features,\n    would get a R^2 score of 0.0.  Parameters    X  : array-like, shape = (n_samples, n_features)  Test samples.    y  : array-like, shape = (n_samples) or (n_samples, n_outputs)  True values for X.    sample_weight  : array-like, shape = [n_samples], optional  Sample weights.    Returns    score  : float  R^2 of self.predict(X) wrt. y.     set_params( params)  Set the parameters of this estimator.  The method works on simple estimators as well as on nested objects\n(such as pipelines). The former have parameters of the form\n``<component>__<parameter>`` so that it's possible to update each\ncomponent of a nested object.  Returns  self",
            "title": "Methods"
        },
        {
            "location": "/user_guide/regressor/StackingRegressor/#properties",
            "text": "coef_  None   intercept_  None",
            "title": "Properties"
        },
        {
            "location": "/user_guide/preprocessing/one-hot_encoding/",
            "text": "One-Hot Encoding\n\n\nA function that performs one-hot encoding for class labels.\n\n\n\n\nfrom mlxtend.preprocessing import one_hot\n\n\n\n\nOverview\n\n\nTypical supervised machine learning algorithms for classifications assume that the class labels are \nnominal\n (a special case of \ncategorical\n where no order is implied). A typical example of an nominal feature would be \"color\" since we can't say (in most applications) that \"orange > blue > red\".\n\n\nThe \none_hot\n function provides a simple interface to convert class label integers into a so-called one-hot array, where each unique label is represented as a column in the new array.\n\n\nFor example, let's assume we have 5 data points from 3 different classes: 0, 1, and 2.\n\n\ny = [0, # sample 1, class 0 \n     1, # sample 2, class 1 \n     0, # sample 3, class 0\n     2, # sample 4, class 2\n     2] # sample 5, class 2\n\n\n\nAfter one-hot encoding, we then obtain the following array (note that the index position of the \"1\" in each row denotes the class label of this sample):\n\n\ny = [[1,  0,  0], # sample 1, class 0 \n     [0,  1,  0], # sample 2, class 1  \n     [1,  0,  0], # sample 3, class 0\n     [0,  0,  1], # sample 4, class 2\n     [0,  0,  1]  # sample 5, class 2\n     ])\n\n\n\nExamples\n\n\nExample 1 - Defaults\n\n\nfrom mlxtend.preprocessing import one_hot\nimport numpy as np\n\ny = np.array([0, 1, 2, 1, 2])\none_hot(y)\n\n\n\n\narray([[ 1.,  0.,  0.],\n       [ 0.,  1.,  0.],\n       [ 0.,  0.,  1.],\n       [ 0.,  1.,  0.],\n       [ 0.,  0.,  1.]])\n\n\n\nExample 2 - Python Lists\n\n\nfrom mlxtend.preprocessing import one_hot\n\ny = [0, 1, 2, 1, 2]\none_hot(y)\n\n\n\n\narray([[ 1.,  0.,  0.],\n       [ 0.,  1.,  0.],\n       [ 0.,  0.,  1.],\n       [ 0.,  1.,  0.],\n       [ 0.,  0.,  1.]])\n\n\n\nExample 3 - Integer Arrays\n\n\nfrom mlxtend.preprocessing import one_hot\n\ny = [0, 1, 2, 1, 2]\none_hot(y, dtype='int')\n\n\n\n\narray([[1, 0, 0],\n       [0, 1, 0],\n       [0, 0, 1],\n       [0, 1, 0],\n       [0, 0, 1]])\n\n\n\nExample 4 - Arbitrary Numbers of Class Labels\n\n\nfrom mlxtend.preprocessing import one_hot\n\ny = [0, 1, 2, 1, 2]\none_hot(y, num_labels=10)\n\n\n\n\narray([[ 1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n       [ 0.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n       [ 0.,  0.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n       [ 0.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n       [ 0.,  0.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.]])\n\n\n\nAPI\n\n\none_hot(y, num_labels='auto', dtype='float')\n\n\nOne-hot encoding of class labels\n\n\nParameters\n\n\n\n\n\n\ny\n : array-like, shape = [n_classlabels]\n\n\nPython list or numpy array consisting of class labels.\n\n\n\n\n\n\nnum_labels\n : int or 'auto'\n\n\nNumber of unique labels in the class label array. Infers the number\nof unique labels from the input array if set to 'auto'.\n\n\n\n\n\n\ndtype\n : str\n\n\nNumPy array type (float, float32, float64) of the output array.\n\n\n\n\n\n\nReturns\n\n\n\n\n\n\nonehot\n : numpy.ndarray, shape = [n_classlabels]\n\n\nOne-hot encoded array, where each sample is represented as\na row vector in the returned array.",
            "title": "One hot encoding"
        },
        {
            "location": "/user_guide/preprocessing/one-hot_encoding/#one-hot-encoding",
            "text": "A function that performs one-hot encoding for class labels.   from mlxtend.preprocessing import one_hot",
            "title": "One-Hot Encoding"
        },
        {
            "location": "/user_guide/preprocessing/one-hot_encoding/#overview",
            "text": "Typical supervised machine learning algorithms for classifications assume that the class labels are  nominal  (a special case of  categorical  where no order is implied). A typical example of an nominal feature would be \"color\" since we can't say (in most applications) that \"orange > blue > red\".  The  one_hot  function provides a simple interface to convert class label integers into a so-called one-hot array, where each unique label is represented as a column in the new array.  For example, let's assume we have 5 data points from 3 different classes: 0, 1, and 2.  y = [0, # sample 1, class 0 \n     1, # sample 2, class 1 \n     0, # sample 3, class 0\n     2, # sample 4, class 2\n     2] # sample 5, class 2  After one-hot encoding, we then obtain the following array (note that the index position of the \"1\" in each row denotes the class label of this sample):  y = [[1,  0,  0], # sample 1, class 0 \n     [0,  1,  0], # sample 2, class 1  \n     [1,  0,  0], # sample 3, class 0\n     [0,  0,  1], # sample 4, class 2\n     [0,  0,  1]  # sample 5, class 2\n     ])",
            "title": "Overview"
        },
        {
            "location": "/user_guide/preprocessing/one-hot_encoding/#examples",
            "text": "",
            "title": "Examples"
        },
        {
            "location": "/user_guide/preprocessing/one-hot_encoding/#example-1-defaults",
            "text": "from mlxtend.preprocessing import one_hot\nimport numpy as np\n\ny = np.array([0, 1, 2, 1, 2])\none_hot(y)  array([[ 1.,  0.,  0.],\n       [ 0.,  1.,  0.],\n       [ 0.,  0.,  1.],\n       [ 0.,  1.,  0.],\n       [ 0.,  0.,  1.]])",
            "title": "Example 1 - Defaults"
        },
        {
            "location": "/user_guide/preprocessing/one-hot_encoding/#example-2-python-lists",
            "text": "from mlxtend.preprocessing import one_hot\n\ny = [0, 1, 2, 1, 2]\none_hot(y)  array([[ 1.,  0.,  0.],\n       [ 0.,  1.,  0.],\n       [ 0.,  0.,  1.],\n       [ 0.,  1.,  0.],\n       [ 0.,  0.,  1.]])",
            "title": "Example 2 - Python Lists"
        },
        {
            "location": "/user_guide/preprocessing/one-hot_encoding/#example-3-integer-arrays",
            "text": "from mlxtend.preprocessing import one_hot\n\ny = [0, 1, 2, 1, 2]\none_hot(y, dtype='int')  array([[1, 0, 0],\n       [0, 1, 0],\n       [0, 0, 1],\n       [0, 1, 0],\n       [0, 0, 1]])",
            "title": "Example 3 - Integer Arrays"
        },
        {
            "location": "/user_guide/preprocessing/one-hot_encoding/#example-4-arbitrary-numbers-of-class-labels",
            "text": "from mlxtend.preprocessing import one_hot\n\ny = [0, 1, 2, 1, 2]\none_hot(y, num_labels=10)  array([[ 1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n       [ 0.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n       [ 0.,  0.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n       [ 0.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n       [ 0.,  0.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.]])",
            "title": "Example 4 - Arbitrary Numbers of Class Labels"
        },
        {
            "location": "/user_guide/preprocessing/one-hot_encoding/#api",
            "text": "one_hot(y, num_labels='auto', dtype='float')  One-hot encoding of class labels  Parameters    y  : array-like, shape = [n_classlabels]  Python list or numpy array consisting of class labels.    num_labels  : int or 'auto'  Number of unique labels in the class label array. Infers the number\nof unique labels from the input array if set to 'auto'.    dtype  : str  NumPy array type (float, float32, float64) of the output array.    Returns    onehot  : numpy.ndarray, shape = [n_classlabels]  One-hot encoded array, where each sample is represented as\na row vector in the returned array.",
            "title": "API"
        },
        {
            "location": "/api_subpackages/mlxtend.classifier/",
            "text": "mlxtend version: 0.3.1dev\n\n\nAdaline\n\n\nAdaline(eta=0.01, epochs=50, solver='sgd', random_seed=None, shuffle=False, zero_init_weight=False, print_progress=0)\n\n\nADAptive LInear NEuron classifier.\n\n\nParameters\n\n\n\n\n\n\neta\n : float (default: 0.01)\n\n\nsolver rate (between 0.0 and 1.0)\n\n\n\n\n\n\nepochs\n : int (default: 50)\n\n\nPasses over the training dataset.\n\n\n\n\n\n\nsolver\n : {'gd', 'sgd', 'normal equation'} (default: 'sgd')\n\n\nMethod for solving the cost function. 'gd' for gradient descent,\n'sgd' for stochastic gradient descent, or 'normal equation' (default)\nto solve the cost function analytically.\n\n\n\n\n\n\nshuffle\n : bool (default: False)\n\n\nShuffles training data every epoch if True to prevent circles.\n\n\n\n\n\n\nrandom_seed\n : int (default: None)\n\n\nSet random state for shuffling and initializing the weights.\n\n\n\n\n\n\nzero_init_weight\n : bool (default: False)\n\n\nIf True, weights are initialized to zero instead of small random\nnumbers in the interval [-0.1, 0.1];\nignored if solver='normal equation'\n\n\n\n\n\n\nprint_progress\n : int (default: 0)\n\n\nPrints progress in fitting to stderr if not solver='normal equation'\n0: No output\n1: Epochs elapsed and cost\n2: 1 plus time elapsed\n3: 2 plus estimated time until completion\n\n\n\n\n\n\nAttributes\n\n\n\n\n\n\nw_\n : 1d-array\n\n\nWeights after fitting.\n\n\n\n\n\n\ncost_\n : list\n\n\nSum of squared errors after each epoch.\n\n\n\n\n\n\nMethods\n\n\n\n\n\nactivation(X)\n\n\nCompute the linear activation from the net input.\n\n\n\n\n\nfit(X, y, init_weights=True)\n\n\nLearn weight coefficients from training data.\n\n\nParameters\n\n\n\n\n\n\nX\n : {array-like, sparse matrix}, shape = [n_samples, n_features]\n\n\nTraining vectors, where n_samples is the number of samples and\nn_features is the number of features.\n\n\n\n\n\n\ny\n : array-like, shape = [n_samples]\n\n\nTarget values.\n\n\n\n\n\n\ninit_weights\n : bool (default: True)\n\n\nRe-initializes weights prior to fitting. Set False to continue\ntraining with weights from a previous fitting.\n\n\n\n\n\n\nReturns\n\n\n\n\nself\n : object\n\n\n\n\n\n\n\nnet_input(X)\n\n\nCompute the linear net input.\n\n\n\n\n\npredict(X)\n\n\nPredict class labels of X.\n\n\nParameters\n\n\n\n\n\n\nX\n : {array-like, sparse matrix}, shape = [n_samples, n_features]\n\n\nTraining vectors, where n_samples is the number of samples and\nn_features is the number of features.\n\n\n\n\n\n\nReturns\n\n\n\n\n\n\nclass_labels\n : array-like, shape = [n_samples]\n\n\nPredicted class labels.\n\n\n\n\n\n\nEnsembleVoteClassifier\n\n\nEnsembleVoteClassifier(clfs, voting='hard', weights=None, verbose=0)\n\n\nSoft Voting/Majority Rule classifier for scikit-learn estimators.\n\n\nParameters\n\n\n\n\n\n\nclfs\n : array-like, shape = [n_classifiers]\n\n\nA list of classifiers.\nInvoking the \nfit\n method on the \nVotingClassifier\n will fit clones\nof those original classifiers that will\nbe stored in the class attribute\n\nself.clfs_\n.\n\n\n\n\n\n\nvoting\n : str, {'hard', 'soft'} (default='hard')\n\n\nIf 'hard', uses predicted class labels for majority rule voting.\nElse if 'soft', predicts the class label based on the argmax of\nthe sums of the predicted probalities, which is recommended for\nan ensemble of well-calibrated classifiers.\n\n\n\n\n\n\nweights\n : array-like, shape = [n_classifiers], optional (default=\nNone\n)\n\n\nSequence of weights (\nfloat\n or \nint\n) to weight the occurances of\npredicted class labels (\nhard\n voting) or class probabilities\nbefore averaging (\nsoft\n voting). Uses uniform weights if \nNone\n.\n\n\n\n\n\n\nverbose\n : int, optional (default=0)\n\n\nControls the verbosity of the building process.\n- \nverbose=0\n (default): Prints nothing\n- \nverbose=1\n: Prints the number & name of the clf being fitted\n- \nverbose=2\n: Prints info about the parameters of the clf being fitted\n- \nverbose>2\n: Changes \nverbose\n param of the underlying clf to\nself.verbose - 2\n\n\n\n\n\n\nAttributes\n\n\n\n\n\n\nclasses_\n : array-like, shape = [n_predictions]\n\n\n\n\n\n\nclf\n : array-like, shape = [n_predictions]\n\n\nThe unmodified input classifiers\n\n\n\n\n\n\nclf_\n : array-like, shape = [n_predictions]\n\n\nFitted clones of the input classifiers\n\n\n\n\n\n\nExamples\n\n\n>>> import numpy as np\n>>> from sklearn.linear_model import LogisticRegression\n>>> from sklearn.naive_bayes import GaussianNB\n>>> from sklearn.ensemble import RandomForestClassifier\n>>> from mlxtend.sklearn import EnsembleVoteClassifier\n>>> clf1 = LogisticRegression(random_seed=1)\n>>> clf2 = RandomForestClassifier(random_seed=1)\n>>> clf3 = GaussianNB()\n>>> X = np.array([[-1, -1], [-2, -1], [-3, -2], [1, 1], [2, 1], [3, 2]])\n>>> y = np.array([1, 1, 1, 2, 2, 2])\n>>> eclf1 = EnsembleVoteClassifier(clfs=[clf1, clf2, clf3],\n... voting='hard', verbose=1)\n>>> eclf1 = eclf1.fit(X, y)\n>>> print(eclf1.predict(X))\n[1 1 1 2 2 2]\n>>> eclf2 = EnsembleVoteClassifier(clfs=[clf1, clf2, clf3], voting='soft')\n>>> eclf2 = eclf2.fit(X, y)\n>>> print(eclf2.predict(X))\n[1 1 1 2 2 2]\n>>> eclf3 = EnsembleVoteClassifier(clfs=[clf1, clf2, clf3],\n...                          voting='soft', weights=[2,1,1])\n>>> eclf3 = eclf3.fit(X, y)\n>>> print(eclf3.predict(X))\n[1 1 1 2 2 2]\n>>>\n\n\n\nMethods\n\n\n\n\n\nfit(X, y)\n\n\nLearn weight coefficients from training data for each classifier.\n\n\nParameters\n\n\n\n\n\n\nX\n : {array-like, sparse matrix}, shape = [n_samples, n_features]\n\n\nTraining vectors, where n_samples is the number of samples and\nn_features is the number of features.\n\n\n\n\n\n\ny\n : array-like, shape = [n_samples]\n\n\nTarget values.\n\n\n\n\n\n\nReturns\n\n\n\n\nself\n : object\n\n\n\n\n\n\n\nfit_transform(X, y=None, \nfit_params)\n\n\nFit to data, then transform it.\n\n\nFits transformer to X and y with optional parameters fit_params\nand returns a transformed version of X.\n\n\n\nParameters\n\n\n\n\n\n\nX\n : numpy array of shape [n_samples, n_features]\n\n\nTraining set.\n\n\n\n\n\n\ny\n : numpy array of shape [n_samples]\n\n\nTarget values.\n\n\n\n\n\n\nReturns\n\n\n\n\n\n\nX_new\n : numpy array of shape [n_samples, n_features_new]\n\n\nTransformed array.\n\n\n\n\n\n\n\n\n\nget_params(deep=True)\n\n\nReturn estimator parameter names for GridSearch support.\n\n\n\n\n\npredict(X)\n\n\nPredict class labels for X.\n\n\nParameters\n\n\n\n\n\n\nX\n : {array-like, sparse matrix}, shape = [n_samples, n_features]\n\n\nTraining vectors, where n_samples is the number of samples and\nn_features is the number of features.\n\n\n\n\n\n\nReturns\n\n\n\n\n\n\nmaj\n : array-like, shape = [n_samples]\n\n\nPredicted class labels.\n\n\n\n\n\n\n\n\n\npredict_proba(X)\n\n\nPredict class probabilities for X.\n\n\nParameters\n\n\n\n\n\n\nX\n : {array-like, sparse matrix}, shape = [n_samples, n_features]\n\n\nTraining vectors, where n_samples is the number of samples and\nn_features is the number of features.\n\n\n\n\n\n\nReturns\n\n\n\n\n\n\navg\n : array-like, shape = [n_samples, n_classes]\n\n\nWeighted average probability for each class per sample.\n\n\n\n\n\n\n\n\n\nscore(X, y, sample_weight=None)\n\n\nReturns the mean accuracy on the given test data and labels.\n\n\nIn multi-label classification, this is the subset accuracy\nwhich is a harsh metric since you require for each sample that\neach label set be correctly predicted.\n\n\n\nParameters\n\n\n\n\n\n\nX\n : array-like, shape = (n_samples, n_features)\n\n\nTest samples.\n\n\n\n\n\n\ny\n : array-like, shape = (n_samples) or (n_samples, n_outputs)\n\n\nTrue labels for X.\n\n\n\n\n\n\nsample_weight\n : array-like, shape = [n_samples], optional\n\n\nSample weights.\n\n\n\n\n\n\nReturns\n\n\n\n\n\n\nscore\n : float\n\n\nMean accuracy of self.predict(X) wrt. y.\n\n\n\n\n\n\n\n\n\nset_params(\nparams)\n\n\nSet the parameters of this estimator.\n\n\nThe method works on simple estimators as well as on nested objects\n(such as pipelines). The former have parameters of the form\n``<component>__<parameter>`` so that it's possible to update each\ncomponent of a nested object.\n\n\n\nReturns\n\n\nself\n\n\n\n\n\ntransform(X)\n\n\nReturn class labels or probabilities for X for each estimator.\n\n\nParameters\n\n\n\n\n\n\nX\n : {array-like, sparse matrix}, shape = [n_samples, n_features]\n\n\nTraining vectors, where n_samples is the number of samples and\nn_features is the number of features.\n\n\n\n\n\n\nReturns\n\n\n\n\n\n\nIf\nvoting='soft'`` : array-like = [n_classifiers, n_samples, n_classes]\n\n\nClass probabilties calculated by each classifier.\n\n\n\n\n\n\nIf\nvoting='hard'`` : array-like = [n_classifiers, n_samples]\n\n\nClass labels predicted by each classifier.\n\n\n\n\n\n\nLogisticRegression\n\n\nLogisticRegression(eta=0.01, epochs=50, regularization=None, l2_lambda=0.0, learning='sgd', shuffle=False, random_seed=None, zero_init_weight=False, print_progress=0)\n\n\nLogistic regression classifier.\n\n\nParameters\n\n\n\n\n\n\neta\n : float (default: 0.01)\n\n\nLearning rate (between 0.0 and 1.0)\n\n\n\n\n\n\nepochs\n : int (default: 50)\n\n\nPasses over the training dataset.\n\n\n\n\n\n\nlearning\n : str (default: sgd)\n\n\nLearning rule, sgd (stochastic gradient descent)\nor gd (gradient descent).\n\n\n\n\n\n\nregularization\n : {None, 'l2'} (default: None)\n\n\nType of regularization. No regularization if\n\nregularization=None\n.\n\n\n\n\n\n\nl2_lambda\n : float\n\n\nRegularization parameter for L2 regularization.\nNo regularization if l2_lambda=0.0.\n\n\n\n\n\n\nshuffle\n : bool (default: False)\n\n\nShuffles training data every epoch if True to prevent circles.\n\n\n\n\n\n\nrandom_seed\n : int (default: None)\n\n\nSet random state for shuffling and initializing the weights.\n\n\n\n\n\n\nzero_init_weight\n : bool (default: False)\n\n\nIf True, weights are initialized to zero instead of small random\nnumbers in the interval [-0.1, 0.1];\nignored if solver='normal equation'\n\n\n\n\n\n\nprint_progress\n : int (default: 0)\n\n\nPrints progress in fitting to stderr.\n0: No output\n1: Epochs elapsed and cost\n2: 1 plus time elapsed\n3: 2 plus estimated time until completion\n\n\n\n\n\n\nAttributes\n\n\n\n\n\n\nw_\n : 1d-array\n\n\nWeights after fitting.\n\n\n\n\n\n\ncost_\n : list\n\n\nList of floats with sum of squared error cost (sgd or gd) for every\nepoch.\n\n\n\n\n\n\nMethods\n\n\n\n\n\nactivation(X)\n\n\nPredict class probabilities of X from the net input.\n\n\nParameters\n\n\n\n\n\n\nX\n : {array-like, sparse matrix}, shape = [n_samples, n_features]\n\n\nTraining vectors, where n_samples is the number of samples and\nn_features is the number of features.\n\n\n\n\n\n\nReturns\n\n\n\n\nClass 1 probability\n : float\n\n\n\n\n\n\n\nfit(X, y, init_weights=True)\n\n\nLearn weight coefficients from training data.\n\n\nParameters\n\n\n\n\n\n\nX\n : {array-like, sparse matrix}, shape = [n_samples, n_features]\n\n\nTraining vectors, where n_samples is the number of samples and\nn_features is the number of features.\n\n\n\n\n\n\ny\n : array-like, shape = [n_samples]\n\n\nTarget values.\n\n\n\n\n\n\ninit_weights\n : bool (default: True)\n\n\n(Re)initializes weights to small random floats if True.\n\n\n\n\n\n\nReturns\n\n\n\n\nself\n : object\n\n\n\n\n\n\n\nnet_input(X)\n\n\nCompute the linear net input.\n\n\n\n\n\npredict(X)\n\n\nPredict class labels of X.\n\n\nParameters\n\n\n\n\n\n\nX\n : {array-like, sparse matrix}, shape = [n_samples, n_features]\n\n\nTraining vectors, where n_samples is the number of samples and\nn_features is the number of features.\n\n\n\n\n\n\nReturns\n\n\n\n\n\n\nclass_labels\n : array-like, shape = [n_samples]\n\n\nPredicted class labels.\n\n\n\n\n\n\nNeuralNetMLP\n\n\nNeuralNetMLP(n_output, n_features, n_hidden=30, l1=0.0, l2=0.0, epochs=500, eta=0.001, alpha=0.0, decrease_const=0.0, random_weights=[-1.0, 1.0], shuffle_init=True, shuffle_epoch=True, minibatches=1, random_seed=None, print_progress=0)\n\n\nFeedforward neural network / Multi-layer perceptron classifier.\n\n\nParameters\n\n\n\n\n\n\nn_output\n : int\n\n\nNumber of output units, should be equal to the\nnumber of unique class labels.\n\n\n\n\n\n\nn_features\n : int\n\n\nNumber of features (dimensions) in the target dataset.\nShould be equal to the number of columns in the X array.\n\n\n\n\n\n\nn_hidden\n : int (default: 30)\n\n\nNumber of hidden units.\n\n\n\n\n\n\nl1\n : float (default: 0.0)\n\n\nLambda value for L1-regularization.\nNo regularization if l1=0.0 (default)\n\n\n\n\n\n\nl2\n : float (default: 0.0)\n\n\nLambda value for L2-regularization.\nNo regularization if l2=0.0 (default)\n\n\n\n\n\n\nepochs\n : int (default: 500)\n\n\nNumber of passes over the training set.\n\n\n\n\n\n\neta\n : float (default: 0.001)\n\n\nLearning rate.\n\n\n\n\n\n\nalpha\n : float (default: 0.0)\n\n\nMomentum constant. Factor multiplied with the\ngradient of the previous epoch t-1 to improve\nlearning speed\nw(t) := w(t) - (grad(t) + alpha*grad(t-1))\n\n\n\n\n\n\ndecrease_const\n : float (default: 0.0)\n\n\nDecrease constant. Shrinks the learning rate\nafter each epoch via eta / (1 + epoch*decrease_const)\n\n\n\n\n\n\nrandom_weights\n : list (default: [-1.0, 1.0])\n\n\nMin and max values for initializing the random weights.\nInitializes weights to 0 if None or False.\n\n\n\n\n\n\nshuffle_init\n : bool (default: True)\n\n\nShuffles (a copy of the) training data before training.\n\n\n\n\n\n\nshuffle_epoch\n : bool (default: True)\n\n\nShuffles training data before every epoch if True to prevent circles.\n\n\n\n\n\n\nminibatches\n : int (default: 1)\n\n\nDivides training data into k minibatches for efficiency.\nNormal gradient descent learning if k=1 (default).\n\n\n\n\n\n\nrandom_seed\n : int (default: None)\n\n\nSet random seed for shuffling and initializing the weights.\n\n\n\n\n\n\nprint_progress\n : int (default: 0)\n\n\nPrints progress in fitting to stderr.\n0: No output\n1: Epochs elapsed and cost\n2: 1 plus time elapsed\n3: 2 plus estimated time until completion\n\n\n\n\n\n\nAttributes\n\n\n\n\n\n\ncost_\n : list\n\n\nSum of squared errors after each epoch.\n\n\n\n\n\n\nMethods\n\n\n\n\n\nfit(X, y)\n\n\nLearn weight coefficients from training data.\n\n\nParameters\n\n\n\n\n\n\nX\n : array, shape = [n_samples, n_features]\n\n\nInput layer with original features.\n\n\n\n\n\n\ny\n : array, shape = [n_samples]\n\n\nTarget class labels.\n\n\n\n\n\n\nReturns:\n\n\nself\n\n\n\n\n\npredict(X)\n\n\nPredict class labels of X.\n\n\nParameters\n\n\n\n\n\n\nX\n : {array-like, sparse matrix}, shape = [n_samples, n_features]\n\n\nTraining vectors, where n_samples is the number of samples and\nn_features is the number of features.\n\n\n\n\n\n\nReturns\n\n\n\n\n\n\nclass_labels\n : array-like, shape = [n_samples]\n\n\nPredicted class labels.\n\n\n\n\n\n\nPerceptron\n\n\nPerceptron(eta=0.1, epochs=50, shuffle=False, random_seed=None, zero_init_weight=False, print_progress=0)\n\n\nPerceptron classifier.\n\n\nParameters\n\n\n\n\n\n\neta\n : float (default: 0.1)\n\n\nLearning rate (between 0.0 and 1.0)\n\n\n\n\n\n\nepochs\n : int (default: 50)\n\n\nNumber of passes over the training dataset.\n\n\n\n\n\n\nshuffle\n : bool (default: False)\n\n\nShuffles training data every epoch if True to prevent circles.\n\n\n\n\n\n\nrandom_seed\n : int\n\n\nRandom state for initializing random weights.\n\n\n\n\n\n\nzero_init_weight\n : bool (default: False)\n\n\nIf True, weights are initialized to zero instead of small random\nnumbers in the interval [-0.1, 0.1];\nignored if solver='normal equation'\n\n\n\n\n\n\nprint_progress\n : int (default: 0)\n\n\nPrints progress in fitting to stderr.\n0: No output\n1: Epochs elapsed and cost\n2: 1 plus time elapsed\n3: 2 plus estimated time until completion\n\n\n\n\n\n\nAttributes\n\n\n\n\n\n\nw_\n : 1d-array\n\n\nWeights after fitting.\n\n\n\n\n\n\ncost_\n : list\n\n\nNumber of misclassifications in every epoch.\n\n\n\n\n\n\nMethods\n\n\n\n\n\nfit(X, y, init_weights=True)\n\n\nLearn weight coefficients from training data.\n\n\nParameters\n\n\n\n\n\n\nX\n : {array-like, sparse matrix}, shape = [n_samples, n_features]\n\n\nTraining vectors, where n_samples is the number of samples and\nn_features is the number of features.\n\n\n\n\n\n\ny\n : array-like, shape = [n_samples]\n\n\nTarget values.\n\n\n\n\n\n\ninit_weights\n : bool (default: True)\n\n\nRe-initializes weights prior to fitting. Set False to continue\ntraining with weights from a previous fitting.\n\n\n\n\n\n\nReturns\n\n\nself\n\n\n\n\n\nnet_input(X)\n\n\nNet input function\n\n\n\n\n\npredict(X)\n\n\nPredict class labels of X.\n\n\nParameters\n\n\n\n\n\n\nX\n : {array-like, sparse matrix}, shape = [n_samples, n_features]\n\n\nTraining vectors, where n_samples is the number of samples and\nn_features is the number of features.\n\n\n\n\n\n\nReturns\n\n\n\n\n\n\nclass_labels\n : array-like, shape = [n_samples]\n\n\nPredicted class labels.",
            "title": "Mlxtend.classifier"
        },
        {
            "location": "/api_subpackages/mlxtend.classifier/#adaline",
            "text": "Adaline(eta=0.01, epochs=50, solver='sgd', random_seed=None, shuffle=False, zero_init_weight=False, print_progress=0)  ADAptive LInear NEuron classifier.  Parameters    eta  : float (default: 0.01)  solver rate (between 0.0 and 1.0)    epochs  : int (default: 50)  Passes over the training dataset.    solver  : {'gd', 'sgd', 'normal equation'} (default: 'sgd')  Method for solving the cost function. 'gd' for gradient descent,\n'sgd' for stochastic gradient descent, or 'normal equation' (default)\nto solve the cost function analytically.    shuffle  : bool (default: False)  Shuffles training data every epoch if True to prevent circles.    random_seed  : int (default: None)  Set random state for shuffling and initializing the weights.    zero_init_weight  : bool (default: False)  If True, weights are initialized to zero instead of small random\nnumbers in the interval [-0.1, 0.1];\nignored if solver='normal equation'    print_progress  : int (default: 0)  Prints progress in fitting to stderr if not solver='normal equation'\n0: No output\n1: Epochs elapsed and cost\n2: 1 plus time elapsed\n3: 2 plus estimated time until completion    Attributes    w_  : 1d-array  Weights after fitting.    cost_  : list  Sum of squared errors after each epoch.",
            "title": "Adaline"
        },
        {
            "location": "/api_subpackages/mlxtend.classifier/#methods",
            "text": "activation(X)  Compute the linear activation from the net input.   fit(X, y, init_weights=True)  Learn weight coefficients from training data.  Parameters    X  : {array-like, sparse matrix}, shape = [n_samples, n_features]  Training vectors, where n_samples is the number of samples and\nn_features is the number of features.    y  : array-like, shape = [n_samples]  Target values.    init_weights  : bool (default: True)  Re-initializes weights prior to fitting. Set False to continue\ntraining with weights from a previous fitting.    Returns   self  : object    net_input(X)  Compute the linear net input.   predict(X)  Predict class labels of X.  Parameters    X  : {array-like, sparse matrix}, shape = [n_samples, n_features]  Training vectors, where n_samples is the number of samples and\nn_features is the number of features.    Returns    class_labels  : array-like, shape = [n_samples]  Predicted class labels.",
            "title": "Methods"
        },
        {
            "location": "/api_subpackages/mlxtend.classifier/#ensemblevoteclassifier",
            "text": "EnsembleVoteClassifier(clfs, voting='hard', weights=None, verbose=0)  Soft Voting/Majority Rule classifier for scikit-learn estimators.  Parameters    clfs  : array-like, shape = [n_classifiers]  A list of classifiers.\nInvoking the  fit  method on the  VotingClassifier  will fit clones\nof those original classifiers that will\nbe stored in the class attribute self.clfs_ .    voting  : str, {'hard', 'soft'} (default='hard')  If 'hard', uses predicted class labels for majority rule voting.\nElse if 'soft', predicts the class label based on the argmax of\nthe sums of the predicted probalities, which is recommended for\nan ensemble of well-calibrated classifiers.    weights  : array-like, shape = [n_classifiers], optional (default= None )  Sequence of weights ( float  or  int ) to weight the occurances of\npredicted class labels ( hard  voting) or class probabilities\nbefore averaging ( soft  voting). Uses uniform weights if  None .    verbose  : int, optional (default=0)  Controls the verbosity of the building process.\n-  verbose=0  (default): Prints nothing\n-  verbose=1 : Prints the number & name of the clf being fitted\n-  verbose=2 : Prints info about the parameters of the clf being fitted\n-  verbose>2 : Changes  verbose  param of the underlying clf to\nself.verbose - 2    Attributes    classes_  : array-like, shape = [n_predictions]    clf  : array-like, shape = [n_predictions]  The unmodified input classifiers    clf_  : array-like, shape = [n_predictions]  Fitted clones of the input classifiers    Examples  >>> import numpy as np\n>>> from sklearn.linear_model import LogisticRegression\n>>> from sklearn.naive_bayes import GaussianNB\n>>> from sklearn.ensemble import RandomForestClassifier\n>>> from mlxtend.sklearn import EnsembleVoteClassifier\n>>> clf1 = LogisticRegression(random_seed=1)\n>>> clf2 = RandomForestClassifier(random_seed=1)\n>>> clf3 = GaussianNB()\n>>> X = np.array([[-1, -1], [-2, -1], [-3, -2], [1, 1], [2, 1], [3, 2]])\n>>> y = np.array([1, 1, 1, 2, 2, 2])\n>>> eclf1 = EnsembleVoteClassifier(clfs=[clf1, clf2, clf3],\n... voting='hard', verbose=1)\n>>> eclf1 = eclf1.fit(X, y)\n>>> print(eclf1.predict(X))\n[1 1 1 2 2 2]\n>>> eclf2 = EnsembleVoteClassifier(clfs=[clf1, clf2, clf3], voting='soft')\n>>> eclf2 = eclf2.fit(X, y)\n>>> print(eclf2.predict(X))\n[1 1 1 2 2 2]\n>>> eclf3 = EnsembleVoteClassifier(clfs=[clf1, clf2, clf3],\n...                          voting='soft', weights=[2,1,1])\n>>> eclf3 = eclf3.fit(X, y)\n>>> print(eclf3.predict(X))\n[1 1 1 2 2 2]\n>>>",
            "title": "EnsembleVoteClassifier"
        },
        {
            "location": "/api_subpackages/mlxtend.classifier/#methods_1",
            "text": "fit(X, y)  Learn weight coefficients from training data for each classifier.  Parameters    X  : {array-like, sparse matrix}, shape = [n_samples, n_features]  Training vectors, where n_samples is the number of samples and\nn_features is the number of features.    y  : array-like, shape = [n_samples]  Target values.    Returns   self  : object    fit_transform(X, y=None,  fit_params)  Fit to data, then transform it.  Fits transformer to X and y with optional parameters fit_params\nand returns a transformed version of X.  Parameters    X  : numpy array of shape [n_samples, n_features]  Training set.    y  : numpy array of shape [n_samples]  Target values.    Returns    X_new  : numpy array of shape [n_samples, n_features_new]  Transformed array.     get_params(deep=True)  Return estimator parameter names for GridSearch support.   predict(X)  Predict class labels for X.  Parameters    X  : {array-like, sparse matrix}, shape = [n_samples, n_features]  Training vectors, where n_samples is the number of samples and\nn_features is the number of features.    Returns    maj  : array-like, shape = [n_samples]  Predicted class labels.     predict_proba(X)  Predict class probabilities for X.  Parameters    X  : {array-like, sparse matrix}, shape = [n_samples, n_features]  Training vectors, where n_samples is the number of samples and\nn_features is the number of features.    Returns    avg  : array-like, shape = [n_samples, n_classes]  Weighted average probability for each class per sample.     score(X, y, sample_weight=None)  Returns the mean accuracy on the given test data and labels.  In multi-label classification, this is the subset accuracy\nwhich is a harsh metric since you require for each sample that\neach label set be correctly predicted.  Parameters    X  : array-like, shape = (n_samples, n_features)  Test samples.    y  : array-like, shape = (n_samples) or (n_samples, n_outputs)  True labels for X.    sample_weight  : array-like, shape = [n_samples], optional  Sample weights.    Returns    score  : float  Mean accuracy of self.predict(X) wrt. y.     set_params( params)  Set the parameters of this estimator.  The method works on simple estimators as well as on nested objects\n(such as pipelines). The former have parameters of the form\n``<component>__<parameter>`` so that it's possible to update each\ncomponent of a nested object.  Returns  self   transform(X)  Return class labels or probabilities for X for each estimator.  Parameters    X  : {array-like, sparse matrix}, shape = [n_samples, n_features]  Training vectors, where n_samples is the number of samples and\nn_features is the number of features.    Returns    If voting='soft'`` : array-like = [n_classifiers, n_samples, n_classes]  Class probabilties calculated by each classifier.    If voting='hard'`` : array-like = [n_classifiers, n_samples]  Class labels predicted by each classifier.",
            "title": "Methods"
        },
        {
            "location": "/api_subpackages/mlxtend.classifier/#logisticregression",
            "text": "LogisticRegression(eta=0.01, epochs=50, regularization=None, l2_lambda=0.0, learning='sgd', shuffle=False, random_seed=None, zero_init_weight=False, print_progress=0)  Logistic regression classifier.  Parameters    eta  : float (default: 0.01)  Learning rate (between 0.0 and 1.0)    epochs  : int (default: 50)  Passes over the training dataset.    learning  : str (default: sgd)  Learning rule, sgd (stochastic gradient descent)\nor gd (gradient descent).    regularization  : {None, 'l2'} (default: None)  Type of regularization. No regularization if regularization=None .    l2_lambda  : float  Regularization parameter for L2 regularization.\nNo regularization if l2_lambda=0.0.    shuffle  : bool (default: False)  Shuffles training data every epoch if True to prevent circles.    random_seed  : int (default: None)  Set random state for shuffling and initializing the weights.    zero_init_weight  : bool (default: False)  If True, weights are initialized to zero instead of small random\nnumbers in the interval [-0.1, 0.1];\nignored if solver='normal equation'    print_progress  : int (default: 0)  Prints progress in fitting to stderr.\n0: No output\n1: Epochs elapsed and cost\n2: 1 plus time elapsed\n3: 2 plus estimated time until completion    Attributes    w_  : 1d-array  Weights after fitting.    cost_  : list  List of floats with sum of squared error cost (sgd or gd) for every\nepoch.",
            "title": "LogisticRegression"
        },
        {
            "location": "/api_subpackages/mlxtend.classifier/#methods_2",
            "text": "activation(X)  Predict class probabilities of X from the net input.  Parameters    X  : {array-like, sparse matrix}, shape = [n_samples, n_features]  Training vectors, where n_samples is the number of samples and\nn_features is the number of features.    Returns   Class 1 probability  : float    fit(X, y, init_weights=True)  Learn weight coefficients from training data.  Parameters    X  : {array-like, sparse matrix}, shape = [n_samples, n_features]  Training vectors, where n_samples is the number of samples and\nn_features is the number of features.    y  : array-like, shape = [n_samples]  Target values.    init_weights  : bool (default: True)  (Re)initializes weights to small random floats if True.    Returns   self  : object    net_input(X)  Compute the linear net input.   predict(X)  Predict class labels of X.  Parameters    X  : {array-like, sparse matrix}, shape = [n_samples, n_features]  Training vectors, where n_samples is the number of samples and\nn_features is the number of features.    Returns    class_labels  : array-like, shape = [n_samples]  Predicted class labels.",
            "title": "Methods"
        },
        {
            "location": "/api_subpackages/mlxtend.classifier/#neuralnetmlp",
            "text": "NeuralNetMLP(n_output, n_features, n_hidden=30, l1=0.0, l2=0.0, epochs=500, eta=0.001, alpha=0.0, decrease_const=0.0, random_weights=[-1.0, 1.0], shuffle_init=True, shuffle_epoch=True, minibatches=1, random_seed=None, print_progress=0)  Feedforward neural network / Multi-layer perceptron classifier.  Parameters    n_output  : int  Number of output units, should be equal to the\nnumber of unique class labels.    n_features  : int  Number of features (dimensions) in the target dataset.\nShould be equal to the number of columns in the X array.    n_hidden  : int (default: 30)  Number of hidden units.    l1  : float (default: 0.0)  Lambda value for L1-regularization.\nNo regularization if l1=0.0 (default)    l2  : float (default: 0.0)  Lambda value for L2-regularization.\nNo regularization if l2=0.0 (default)    epochs  : int (default: 500)  Number of passes over the training set.    eta  : float (default: 0.001)  Learning rate.    alpha  : float (default: 0.0)  Momentum constant. Factor multiplied with the\ngradient of the previous epoch t-1 to improve\nlearning speed\nw(t) := w(t) - (grad(t) + alpha*grad(t-1))    decrease_const  : float (default: 0.0)  Decrease constant. Shrinks the learning rate\nafter each epoch via eta / (1 + epoch*decrease_const)    random_weights  : list (default: [-1.0, 1.0])  Min and max values for initializing the random weights.\nInitializes weights to 0 if None or False.    shuffle_init  : bool (default: True)  Shuffles (a copy of the) training data before training.    shuffle_epoch  : bool (default: True)  Shuffles training data before every epoch if True to prevent circles.    minibatches  : int (default: 1)  Divides training data into k minibatches for efficiency.\nNormal gradient descent learning if k=1 (default).    random_seed  : int (default: None)  Set random seed for shuffling and initializing the weights.    print_progress  : int (default: 0)  Prints progress in fitting to stderr.\n0: No output\n1: Epochs elapsed and cost\n2: 1 plus time elapsed\n3: 2 plus estimated time until completion    Attributes    cost_  : list  Sum of squared errors after each epoch.",
            "title": "NeuralNetMLP"
        },
        {
            "location": "/api_subpackages/mlxtend.classifier/#methods_3",
            "text": "fit(X, y)  Learn weight coefficients from training data.  Parameters    X  : array, shape = [n_samples, n_features]  Input layer with original features.    y  : array, shape = [n_samples]  Target class labels.    Returns:  self   predict(X)  Predict class labels of X.  Parameters    X  : {array-like, sparse matrix}, shape = [n_samples, n_features]  Training vectors, where n_samples is the number of samples and\nn_features is the number of features.    Returns    class_labels  : array-like, shape = [n_samples]  Predicted class labels.",
            "title": "Methods"
        },
        {
            "location": "/api_subpackages/mlxtend.classifier/#perceptron",
            "text": "Perceptron(eta=0.1, epochs=50, shuffle=False, random_seed=None, zero_init_weight=False, print_progress=0)  Perceptron classifier.  Parameters    eta  : float (default: 0.1)  Learning rate (between 0.0 and 1.0)    epochs  : int (default: 50)  Number of passes over the training dataset.    shuffle  : bool (default: False)  Shuffles training data every epoch if True to prevent circles.    random_seed  : int  Random state for initializing random weights.    zero_init_weight  : bool (default: False)  If True, weights are initialized to zero instead of small random\nnumbers in the interval [-0.1, 0.1];\nignored if solver='normal equation'    print_progress  : int (default: 0)  Prints progress in fitting to stderr.\n0: No output\n1: Epochs elapsed and cost\n2: 1 plus time elapsed\n3: 2 plus estimated time until completion    Attributes    w_  : 1d-array  Weights after fitting.    cost_  : list  Number of misclassifications in every epoch.",
            "title": "Perceptron"
        },
        {
            "location": "/api_subpackages/mlxtend.classifier/#methods_4",
            "text": "fit(X, y, init_weights=True)  Learn weight coefficients from training data.  Parameters    X  : {array-like, sparse matrix}, shape = [n_samples, n_features]  Training vectors, where n_samples is the number of samples and\nn_features is the number of features.    y  : array-like, shape = [n_samples]  Target values.    init_weights  : bool (default: True)  Re-initializes weights prior to fitting. Set False to continue\ntraining with weights from a previous fitting.    Returns  self   net_input(X)  Net input function   predict(X)  Predict class labels of X.  Parameters    X  : {array-like, sparse matrix}, shape = [n_samples, n_features]  Training vectors, where n_samples is the number of samples and\nn_features is the number of features.    Returns    class_labels  : array-like, shape = [n_samples]  Predicted class labels.",
            "title": "Methods"
        },
        {
            "location": "/api_subpackages/mlxtend.data/",
            "text": "mlxtend version: 0.3.1dev\n\n\nautompg_data\n\n\nautompg_data()\n\n\nAuto MPG dataset.\n\n\n\n\n\n\nSource\n : https://archive.ics.uci.edu/ml/datasets/Auto+MPG\n\n\n\n\n\n\nNumber of samples\n : 392\n\n\n\n\n\n\nContinuous target variable\n : mpg\n\n\nDataset Attributes:\n\n\n\n\n1) cylinders:  multi-valued discrete\n\n\n2) displacement: continuous\n\n\n3) horsepower: continuous\n\n\n4) weight: continuous\n\n\n5) acceleration: continuous\n\n\n6) model year: multi-valued discrete\n\n\n7) origin: multi-valued discrete\n\n\n8) car name: string (unique for each instance)\n\n\n\n\n\n\n\n\nReturns\n\n\n\n\n\n\nX, y\n : [n_samples, n_features], [n_targets]\n\n\nX is the feature matrix with 392 auto samples as rows\nand 8 feature columns (6 rows with NaNs removed).\ny is a 1-dimensional array of the target MPG values.\n\n\n\n\n\n\nboston_housing_data\n\n\nboston_housing_data()\n\n\nBoston Housing dataset.\n\n\n\n\n\n\nSource\n : https://archive.ics.uci.edu/ml/datasets/Housing\n\n\n\n\n\n\nNumber of samples\n : 506\n\n\n\n\n\n\nContinuous target variable\n : MEDV\n\n\nMEDV = Median value of owner-occupied homes in $1000's\n\n\nDataset Attributes:\n\n\n\n\n1) CRIM      per capita crime rate by town\n\n\n2) ZN        proportion of residential land zoned for lots over\n25,000 sq.ft.\n\n\n3) INDUS     proportion of non-retail business acres per town\n\n\n4) CHAS      Charles River dummy variable (= 1 if tract bounds\nriver; 0 otherwise)\n\n\n5) NOX       nitric oxides concentration (parts per 10 million)\n\n\n6) RM        average number of rooms per dwelling\n\n\n7) AGE       proportion of owner-occupied units built prior to 1940\n\n\n8) DIS       weighted distances to five Boston employment centres\n\n\n9) RAD       index of accessibility to radial highways\n\n\n10) TAX      full-value property-tax rate per $10,000\n\n\n11) PTRATIO  pupil-teacher ratio by town\n\n\n12) B        1000(Bk - 0.63)^2 where Bk is the proportion of b. by town\n\n\n13) LSTAT    % lower status of the population\n\n\n\n\n\n\n\n\nReturns\n\n\n\n\n\n\nX, y\n : [n_samples, n_features], [n_class_labels]\n\n\nX is the feature matrix with 506 housing samples as rows\nand 13 feature columns.\ny is a 1-dimensional array of the continuous target variable MEDV\n\n\n\n\n\n\niris_data\n\n\niris_data()\n\n\nIris flower dataset.\n\n\n\n\n\n\nSource\n : https://archive.ics.uci.edu/ml/datasets/Iris\n\n\n\n\n\n\nNumber of samples\n : 150\n\n\n\n\n\n\nClass labels\n : {0, 1, 2}, distribution: [50, 50, 50]\n\n\n0 = setosa, 1 = versicolor, 2 = virginica.\n\n\nDataset Attributes:\n\n\n\n\n1) sepal length [cm]\n\n\n2) sepal width [cm]\n\n\n3) petal length [cm]\n\n\n4) petal width [cm]\n\n\n\n\n\n\n\n\nReturns\n\n\n\n\n\n\nX, y\n : [n_samples, n_features], [n_class_labels]\n\n\nX is the feature matrix with 150 flower samples as rows,\nand 4 feature columns sepal length, sepal width,\npetal length, and petal width.\ny is a 1-dimensional array of the class labels {0, 1, 2}\n\n\n\n\n\n\nloadlocal_mnist\n\n\nloadlocal_mnist(images_path, labels_path)\n\n\nRead MNIST from ubyte files.\n\n\nParameters\n\n\n\n\n\n\nimages_path\n : str\n\n\npath to the test or train MNIST ubyte file\n\n\n\n\n\n\nlabels_path\n : str\n\n\npath to the test or train MNIST class labels file\n\n\n\n\n\n\nReturns\n\n\n\n\n\n\nimages\n : [n_samples, n_pixels] numpy.array\n\n\nPixel values of the images.\n\n\n\n\n\n\nlabels\n : [n_samples] numpy array\n\n\nTarget class labels\n\n\n\n\n\n\nmnist_data\n\n\nmnist_data()\n\n\n5000 samples from the MNIST handwritten digits dataset.\n\n\n\n\nData Source\n : http://yann.lecun.com/exdb/mnist/\n\n\n\n\nReturns\n\n\n\n\n\n\nX, y\n : [n_samples, n_features], [n_class_labels]\n\n\nX is the feature matrix with 5000 image samples as rows,\neach row consists of 28x28 pixels that were unrolled into\n784 pixel feature vectors.\ny contains the 10 unique class labels 0-9.\n\n\n\n\n\n\nwine_data\n\n\nwine_data()\n\n\nWine dataset.\n\n\n\n\n\n\nSource\n : https://archive.ics.uci.edu/ml/datasets/Wine\n\n\n\n\n\n\nNumber of samples\n : 178\n\n\n\n\n\n\nClass labels\n : {0, 1, 2}, distribution: [59, 71, 48]\n\n\nDataset Attributes:\n\n\n\n\n1) Alcohol\n\n\n2) Malic acid\n\n\n3) Ash\n\n\n4) Alcalinity of ash\n\n\n5) Magnesium\n\n\n6) Total phenols\n\n\n7) Flavanoids\n\n\n8) Nonflavanoid phenols\n\n\n9) Proanthocyanins\n\n\n10) Color intensity\n\n\n11) Hue\n\n\n12) OD280/OD315 of diluted wines\n\n\n13) Proline\n\n\n\n\n\n\n\n\nReturns\n\n\n\n\n\n\nX, y\n : [n_samples, n_features], [n_class_labels]\n\n\nX is the feature matrix with 178 wine samples as rows\nand 13 feature columns.\ny is a 1-dimensional array of the 3 class labels 0, 1, 2",
            "title": "Mlxtend.data"
        },
        {
            "location": "/api_subpackages/mlxtend.data/#autompg_data",
            "text": "autompg_data()  Auto MPG dataset.    Source  : https://archive.ics.uci.edu/ml/datasets/Auto+MPG    Number of samples  : 392    Continuous target variable  : mpg  Dataset Attributes:   1) cylinders:  multi-valued discrete  2) displacement: continuous  3) horsepower: continuous  4) weight: continuous  5) acceleration: continuous  6) model year: multi-valued discrete  7) origin: multi-valued discrete  8) car name: string (unique for each instance)     Returns    X, y  : [n_samples, n_features], [n_targets]  X is the feature matrix with 392 auto samples as rows\nand 8 feature columns (6 rows with NaNs removed).\ny is a 1-dimensional array of the target MPG values.",
            "title": "autompg_data"
        },
        {
            "location": "/api_subpackages/mlxtend.data/#boston_housing_data",
            "text": "boston_housing_data()  Boston Housing dataset.    Source  : https://archive.ics.uci.edu/ml/datasets/Housing    Number of samples  : 506    Continuous target variable  : MEDV  MEDV = Median value of owner-occupied homes in $1000's  Dataset Attributes:   1) CRIM      per capita crime rate by town  2) ZN        proportion of residential land zoned for lots over\n25,000 sq.ft.  3) INDUS     proportion of non-retail business acres per town  4) CHAS      Charles River dummy variable (= 1 if tract bounds\nriver; 0 otherwise)  5) NOX       nitric oxides concentration (parts per 10 million)  6) RM        average number of rooms per dwelling  7) AGE       proportion of owner-occupied units built prior to 1940  8) DIS       weighted distances to five Boston employment centres  9) RAD       index of accessibility to radial highways  10) TAX      full-value property-tax rate per $10,000  11) PTRATIO  pupil-teacher ratio by town  12) B        1000(Bk - 0.63)^2 where Bk is the proportion of b. by town  13) LSTAT    % lower status of the population     Returns    X, y  : [n_samples, n_features], [n_class_labels]  X is the feature matrix with 506 housing samples as rows\nand 13 feature columns.\ny is a 1-dimensional array of the continuous target variable MEDV",
            "title": "boston_housing_data"
        },
        {
            "location": "/api_subpackages/mlxtend.data/#iris_data",
            "text": "iris_data()  Iris flower dataset.    Source  : https://archive.ics.uci.edu/ml/datasets/Iris    Number of samples  : 150    Class labels  : {0, 1, 2}, distribution: [50, 50, 50]  0 = setosa, 1 = versicolor, 2 = virginica.  Dataset Attributes:   1) sepal length [cm]  2) sepal width [cm]  3) petal length [cm]  4) petal width [cm]     Returns    X, y  : [n_samples, n_features], [n_class_labels]  X is the feature matrix with 150 flower samples as rows,\nand 4 feature columns sepal length, sepal width,\npetal length, and petal width.\ny is a 1-dimensional array of the class labels {0, 1, 2}",
            "title": "iris_data"
        },
        {
            "location": "/api_subpackages/mlxtend.data/#loadlocal_mnist",
            "text": "loadlocal_mnist(images_path, labels_path)  Read MNIST from ubyte files.  Parameters    images_path  : str  path to the test or train MNIST ubyte file    labels_path  : str  path to the test or train MNIST class labels file    Returns    images  : [n_samples, n_pixels] numpy.array  Pixel values of the images.    labels  : [n_samples] numpy array  Target class labels",
            "title": "loadlocal_mnist"
        },
        {
            "location": "/api_subpackages/mlxtend.data/#mnist_data",
            "text": "mnist_data()  5000 samples from the MNIST handwritten digits dataset.   Data Source  : http://yann.lecun.com/exdb/mnist/   Returns    X, y  : [n_samples, n_features], [n_class_labels]  X is the feature matrix with 5000 image samples as rows,\neach row consists of 28x28 pixels that were unrolled into\n784 pixel feature vectors.\ny contains the 10 unique class labels 0-9.",
            "title": "mnist_data"
        },
        {
            "location": "/api_subpackages/mlxtend.data/#wine_data",
            "text": "wine_data()  Wine dataset.    Source  : https://archive.ics.uci.edu/ml/datasets/Wine    Number of samples  : 178    Class labels  : {0, 1, 2}, distribution: [59, 71, 48]  Dataset Attributes:   1) Alcohol  2) Malic acid  3) Ash  4) Alcalinity of ash  5) Magnesium  6) Total phenols  7) Flavanoids  8) Nonflavanoid phenols  9) Proanthocyanins  10) Color intensity  11) Hue  12) OD280/OD315 of diluted wines  13) Proline     Returns    X, y  : [n_samples, n_features], [n_class_labels]  X is the feature matrix with 178 wine samples as rows\nand 13 feature columns.\ny is a 1-dimensional array of the 3 class labels 0, 1, 2",
            "title": "wine_data"
        },
        {
            "location": "/api_subpackages/mlxtend.evaluate/",
            "text": "mlxtend version: 0.3.1dev\n\n\nconfusion_matrix\n\n\nconfusion_matrix(y_target, y_predicted, binary=False, positive_label=1)\n\n\nCompute a confusion matrix/contingency table.\n\n\nParameters\n\n\n\n\n\n\ny_target\n : array-like, shape=[n_samples]\n\n\nTrue class labels.\n\n\n\n\n\n\ny_predicted\n : array-like, shape=[n_samples]\n\n\nPredicted class labels.\n\n\n\n\n\n\nbinary\n : bool (default: False)\n\n\nMaps a multi-class problem onto a\nbinary confusion matrix, where\nthe positive class is 1 and\nall other classes are 0.\n\n\n\n\n\n\npositive_label\n : int (default: 1)\n\n\nClass label of the positive class.\n\n\n\n\n\n\nReturns\n\n\n\n\nmat\n : array-like, shape=[n_classes, n_classes]\n\n\n\n\nplot_confusion_matrix\n\n\nplot_confusion_matrix(conf_mat, hide_spines=False, hide_ticks=False, figsize=(2.5, 2.5), cmap=None, alpha=0.3)\n\n\nPlot a confusion matrix via matplotlib.\n\n\nParameters\n\n\n\n\n\n\nconf_mat\n : array-like, shape = [n_classes, n_classes]\n\n\nConfusion matrix from evaluate.confusion matrix.\n\n\n\n\n\n\nhide_spines\n : bool (default: False)\n\n\nHides axis spines if True.\n\n\n\n\n\n\nhide_ticks\n : bool (default: False)\n\n\nHides axis ticks if True\n\n\n\n\n\n\nfigsize\n : tuple (default: (2.5, 2.5))\n\n\nHeight and width of the figure\n\n\n\n\n\n\ncmap\n : matplotlib colormap (default: \nNone\n)\n\n\nUses matplotlib.pyplot.cm.Blues if \nNone\n\n\n\n\n\n\nReturns\n\n\n\n\n\n\nfig, ax\n : matplotlib.pyplot subplot objects\n\n\nFigure and axis elements of the subplot.\n\n\n\n\n\n\nplot_decision_regions\n\n\nplot_decision_regions(X, y, clf, ax=None, X_highlight=None, res=0.02, legend=1, hide_spines=True, markers='s^oxv<>', colors='red,blue,limegreen,gray,cyan')\n\n\nPlot decision regions of a classifier.\n\n\nParameters\n\n\n\n\n\n\nX\n : array-like, shape = [n_samples, n_features]\n\n\nFeature Matrix.\n\n\n\n\n\n\ny\n : array-like, shape = [n_samples]\n\n\nTrue class labels.\n\n\n\n\n\n\nclf\n : Classifier object.\n\n\nMust have a .predict method.\n\n\n\n\n\n\nax\n : matplotlib.axes.Axes (default: None)\n\n\nAn existing matplotlib Axes. Creates\none if ax=None.\n\n\n\n\n\n\nX_highlight\n : array-like, shape = [n_samples, n_features] (default: None)\n\n\nAn array with data points that are used to highlight samples in \nX\n.\n\n\n\n\n\n\nres\n : float (default: 0.02)\n\n\nGrid width. Lower values increase the resolution but\nslow down the plotting.\n\n\n\n\n\n\nhide_spines\n : bool (default: True)\n\n\nHide axis spines if True.\n\n\n\n\n\n\nlegend\n : int (default: 1)\n\n\nInteger to specify the legend location.\nNo legend if legend is 0.\n\n\n\n\n\n\nmarkers\n : list\n\n\nScatterplot markers.\n\n\n\n\n\n\ncolors\n : str (default 'red,blue,limegreen,gray,cyan')\n\n\nComma separated list of colors.\n\n\n\n\n\n\nReturns\n\n\n\n\nax\n : matplotlib.axes.Axes object\n\n\n\n\nplot_learning_curves\n\n\nplot_learning_curves(X_train, y_train, X_test, y_test, clf, train_marker='o', test_marker='^', scoring='misclassification error', suppress_plot=False, print_model=True, style='fivethirtyeight', legend_loc='best')\n\n\nPlots learning curves of a classifier.\n\n\nParameters\n\n\n\n\n\n\nX_train\n : array-like, shape = [n_samples, n_features]\n\n\nFeature matrix of the training dataset.\n\n\n\n\n\n\ny_train\n : array-like, shape = [n_samples]\n\n\nTrue class labels of the training dataset.\n\n\n\n\n\n\nX_test\n : array-like, shape = [n_samples, n_features]\n\n\nFeature matrix of the test dataset.\n\n\n\n\n\n\ny_test\n : array-like, shape = [n_samples]\n\n\nTrue class labels of the test dataset.\n\n\n\n\n\n\nclf\n : Classifier object. Must have a .predict .fit method.\n\n\n\n\n\n\ntrain_marker\n : str (default: 'o')\n\n\nMarker for the training set line plot.\n\n\n\n\n\n\ntest_marker\n : str (default: '^')\n\n\nMarker for the test set line plot.\n\n\n\n\n\n\nscoring\n : str (default: 'misclassification error')\n\n\nIf not 'misclassification error', accepts the following metrics\n(from scikit-learn):\n\n\n\n\n\n\nsuppress_plot=False\n : bool (default: False)\n\n\nSuppress matplotlib plots if True. Recommended\nfor testing purposes.\n\n\n\n\n\n\nprint_model\n : bool (default: True)\n\n\nPrint model parameters in plot title if True.\n\n\n\n\n\n\nstyle\n : str (default: 'fivethirtyeight')\n\n\nMatplotlib style\n\n\n\n\n\n\nlegend_loc\n : str (default: 'best')\n\n\nWhere to place the plot legend:\n\n\n\n\n\n\nReturns\n\n\n\n\nerrors\n : (training_error, test_error): tuple of lists\n\n\n\n\nscoring\n\n\nscoring(y_target, y_predicted, metric='error', positive_label=1)\n\n\nCompute a scoring metric for supervised learning.\n\n\nParameters\n\n\n\n\n\n\ny_target\n : array-like, shape=[n_values]\n\n\nTrue class labels or target values.\n\n\n\n\n\n\ny_predicted\n : array-like, shape=[n_values]\n\n\nPredicted class labels or target values.\n\n\n\n\n\n\nmetric\n : str (default: 'error')\n\n\nPerformance metric.\n[TP: True positives, TN = True negatives,\n\n\nTN: True negatives, FN = False negatives]\n\n\n'accuracy': (TP + TN)/(FP + FN + TP + TN) = 1-ERR\n\n\n'error': (TP + TN)/(FP+ FN + TP + TN) = 1-ACC\n\n\n'false_positive_rate': FP/N = FP/(FP + TN)\n\n\n'true_positive_rate': TP/P = TP/(FN + TP)\n\n\n'true_negative_rate': TN/N = TN/(FP + TN)\n\n\n'precision': TP/(TP + FP)\n\n\n'recall': equal to 'true_positive_rate'\n\n\n'sensitivity': equal to 'true_positive_rate' or 'recall'\n\n\n'specificity': equal to 'true_negative_rate'\n\n\n'f1': 2 * (PRE * REC)/(PRE + REC)\n\n\n'matthews_corr_coef':  (TP\nTN - FP\nFN)\n/ (sqrt{(TP + FP)( TP + FN )( TN + FP )( TN + FN )})\n\n\n\n\n\n\npositive_label\n : int (default: 1)\n\n\nLabel of the positive class for binary classification\nmetrics.\n\n\n\n\n\n\nReturns\n\n\n\n\nscore\n : float",
            "title": "Mlxtend.evaluate"
        },
        {
            "location": "/api_subpackages/mlxtend.evaluate/#confusion_matrix",
            "text": "confusion_matrix(y_target, y_predicted, binary=False, positive_label=1)  Compute a confusion matrix/contingency table.  Parameters    y_target  : array-like, shape=[n_samples]  True class labels.    y_predicted  : array-like, shape=[n_samples]  Predicted class labels.    binary  : bool (default: False)  Maps a multi-class problem onto a\nbinary confusion matrix, where\nthe positive class is 1 and\nall other classes are 0.    positive_label  : int (default: 1)  Class label of the positive class.    Returns   mat  : array-like, shape=[n_classes, n_classes]",
            "title": "confusion_matrix"
        },
        {
            "location": "/api_subpackages/mlxtend.evaluate/#plot_confusion_matrix",
            "text": "plot_confusion_matrix(conf_mat, hide_spines=False, hide_ticks=False, figsize=(2.5, 2.5), cmap=None, alpha=0.3)  Plot a confusion matrix via matplotlib.  Parameters    conf_mat  : array-like, shape = [n_classes, n_classes]  Confusion matrix from evaluate.confusion matrix.    hide_spines  : bool (default: False)  Hides axis spines if True.    hide_ticks  : bool (default: False)  Hides axis ticks if True    figsize  : tuple (default: (2.5, 2.5))  Height and width of the figure    cmap  : matplotlib colormap (default:  None )  Uses matplotlib.pyplot.cm.Blues if  None    Returns    fig, ax  : matplotlib.pyplot subplot objects  Figure and axis elements of the subplot.",
            "title": "plot_confusion_matrix"
        },
        {
            "location": "/api_subpackages/mlxtend.evaluate/#plot_decision_regions",
            "text": "plot_decision_regions(X, y, clf, ax=None, X_highlight=None, res=0.02, legend=1, hide_spines=True, markers='s^oxv<>', colors='red,blue,limegreen,gray,cyan')  Plot decision regions of a classifier.  Parameters    X  : array-like, shape = [n_samples, n_features]  Feature Matrix.    y  : array-like, shape = [n_samples]  True class labels.    clf  : Classifier object.  Must have a .predict method.    ax  : matplotlib.axes.Axes (default: None)  An existing matplotlib Axes. Creates\none if ax=None.    X_highlight  : array-like, shape = [n_samples, n_features] (default: None)  An array with data points that are used to highlight samples in  X .    res  : float (default: 0.02)  Grid width. Lower values increase the resolution but\nslow down the plotting.    hide_spines  : bool (default: True)  Hide axis spines if True.    legend  : int (default: 1)  Integer to specify the legend location.\nNo legend if legend is 0.    markers  : list  Scatterplot markers.    colors  : str (default 'red,blue,limegreen,gray,cyan')  Comma separated list of colors.    Returns   ax  : matplotlib.axes.Axes object",
            "title": "plot_decision_regions"
        },
        {
            "location": "/api_subpackages/mlxtend.evaluate/#plot_learning_curves",
            "text": "plot_learning_curves(X_train, y_train, X_test, y_test, clf, train_marker='o', test_marker='^', scoring='misclassification error', suppress_plot=False, print_model=True, style='fivethirtyeight', legend_loc='best')  Plots learning curves of a classifier.  Parameters    X_train  : array-like, shape = [n_samples, n_features]  Feature matrix of the training dataset.    y_train  : array-like, shape = [n_samples]  True class labels of the training dataset.    X_test  : array-like, shape = [n_samples, n_features]  Feature matrix of the test dataset.    y_test  : array-like, shape = [n_samples]  True class labels of the test dataset.    clf  : Classifier object. Must have a .predict .fit method.    train_marker  : str (default: 'o')  Marker for the training set line plot.    test_marker  : str (default: '^')  Marker for the test set line plot.    scoring  : str (default: 'misclassification error')  If not 'misclassification error', accepts the following metrics\n(from scikit-learn):    suppress_plot=False  : bool (default: False)  Suppress matplotlib plots if True. Recommended\nfor testing purposes.    print_model  : bool (default: True)  Print model parameters in plot title if True.    style  : str (default: 'fivethirtyeight')  Matplotlib style    legend_loc  : str (default: 'best')  Where to place the plot legend:    Returns   errors  : (training_error, test_error): tuple of lists",
            "title": "plot_learning_curves"
        },
        {
            "location": "/api_subpackages/mlxtend.evaluate/#scoring",
            "text": "scoring(y_target, y_predicted, metric='error', positive_label=1)  Compute a scoring metric for supervised learning.  Parameters    y_target  : array-like, shape=[n_values]  True class labels or target values.    y_predicted  : array-like, shape=[n_values]  Predicted class labels or target values.    metric  : str (default: 'error')  Performance metric.\n[TP: True positives, TN = True negatives,  TN: True negatives, FN = False negatives]  'accuracy': (TP + TN)/(FP + FN + TP + TN) = 1-ERR  'error': (TP + TN)/(FP+ FN + TP + TN) = 1-ACC  'false_positive_rate': FP/N = FP/(FP + TN)  'true_positive_rate': TP/P = TP/(FN + TP)  'true_negative_rate': TN/N = TN/(FP + TN)  'precision': TP/(TP + FP)  'recall': equal to 'true_positive_rate'  'sensitivity': equal to 'true_positive_rate' or 'recall'  'specificity': equal to 'true_negative_rate'  'f1': 2 * (PRE * REC)/(PRE + REC)  'matthews_corr_coef':  (TP TN - FP FN)\n/ (sqrt{(TP + FP)( TP + FN )( TN + FP )( TN + FN )})    positive_label  : int (default: 1)  Label of the positive class for binary classification\nmetrics.    Returns   score  : float",
            "title": "scoring"
        },
        {
            "location": "/api_subpackages/mlxtend.feature_selection/",
            "text": "mlxtend version: 0.3.1dev\n\n\nColumnSelector\n\n\nColumnSelector(cols)\n\n\nA feature selector for scikit-learn's Pipeline class that returns\n    specified columns from a numpy array.\n\n\nMethods\n\n\n\n\n\nfit(X, y=None)\n\n\nNone\n\n\n\n\n\ntransform(X, y=None)\n\n\nNone\n\n\nplot_sequential_feature_selection\n\n\nplot_sequential_feature_selection(metric_dict, kind='std_dev', color='blue', bcolor='steelblue', marker='o', alpha=0.2, ylabel='Performance', confidence_interval=0.95)\n\n\nPlot sequential feature selection.\n\n\nParameters\n\n\n\n\n\n\nmetric_dict\n : mlxtend.SequentialFeatureSelector.get_metric_dict() object\n\n\n\n\n\n\nkind\n : str (default: \"std_dev\")\n\n\nThe kind of error bar or confidence interval in\n{'std_dev', 'std_err', 'ci', None}.\n\n\n\n\n\n\ncolor\n : str (default: \"blue\")\n\n\nColor of the lineplot (accepts any matplotlib color name)\n\n\n\n\n\n\nbcolor\n : str (default: \"steelblue\").\n\n\nColor of the error bars / confidence intervals\n(accepts any matplotlib color name).\n\n\n\n\n\n\nmarker\n : str (default: \"o\")\n\n\nMarker of the line plot\n(accepts any matplotlib marker name).\n\n\n\n\n\n\nalpha\n : float in [0, 1] (default: 0.2)\n\n\nTransparency of the error bars / confidence intervals.\n\n\n\n\n\n\nylabel\n : str (default: \"Performance\")\n\n\nY-axis label.\n\n\n\n\n\n\nconfidence_interval\n : float (default: 0.95)\n\n\nConfidence level if \nkind='ci'\n.\n\n\n\n\n\n\nReturns\n\n\n\n\nfig\n : matplotlib.pyplot.figure() object\n\n\n\n\nSequentialFeatureSelector\n\n\nSequentialFeatureSelector(estimator, k_features, forward=True, floating=False, print_progress=True, scoring='accuracy', cv=5, skip_if_stuck=True, n_jobs=1, pre_dispatch='2\nn_jobs')*\n\n\nSequential Feature Selection for Classification and Regression.\n\n\nParameters\n\n\n\n\n\n\nestimator\n : scikit-learn classifier or regressor\n\n\n\n\n\n\nk_features\n : int\n\n\nNumber of features to select,\nwhere k_features < the full feature set.\n\n\n\n\n\n\nforward\n : bool (default: True)\n\n\nForward selection if True,\nbackward selection otherwise\n\n\n\n\n\n\nfloating\n : bool (default: False)\n\n\nAdds a conditional exclusion/inclusion if True.\n\n\n\n\n\n\nprint_progress\n : bool (default: True)\n\n\nPrints progress as the number of epochs\nto stderr.\n\n\n\n\n\n\nscoring\n : str, (default='accuracy')\n\n\nScoring metric in {accuracy, f1, precision, recall, roc_auc}\nfor classifiers,\n{'mean_absolute_error', 'mean_squared_error',\n'median_absolute_error', 'r2'} for regressors,\nor a callable object or function with\nsignature \nscorer(estimator, X, y)\n.\n\n\n\n\n\n\ncv\n : int (default: 5)\n\n\nScikit-learn cross-validation generator or \nint\n.\nIf estimator is a classifier (or y consists of integer class labels),\nstratified k-fold is performed, and regular k-fold cross-validation\notherwise.\nNo cross-validation if cv is None, False, or 0.\nskip_if_stuck: bool (default: True)\nSet to True to skip conditional\nexlusion/inclusion if floating=True and\nalgorithm gets stuck in cycles.\n\n\n\n\n\n\nn_jobs\n : int (default: 1)\n\n\nThe number of CPUs to use for cross validation. -1 means 'all CPUs'.\n\n\n\n\n\n\npre_dispatch\n : int, or string (default: '2*n_jobs')\n\n\nControls the number of jobs that get dispatched\nduring parallel execution in cross_val_score.\nReducing this number can be useful to avoid an explosion of\nmemory consumption when more jobs get dispatched than CPUs can process.\nThis parameter can be:\nNone, in which case all the jobs are immediately created and spawned.\nUse this for lightweight and fast-running jobs,\nto avoid delays due to on-demand spawning of the jobs\nAn int, giving the exact number of total jobs that are spawned\nA string, giving an expression as a function\nof n_jobs, as in \n'2*n_jobs'\n\n\n\n\n\n\nAttributes\n\n\n\n\n\n\nk_feature_idx_\n : array-like, shape = [n_predictions]\n\n\nFeature Indices of the selected feature subsets.\n\n\n\n\n\n\nk_score_\n : float\n\n\nCross validation average score of the selected subset.\n\n\n\n\n\n\nsubsets_\n : dict\n\n\nA dictionary of selected feature subsets during the\nsequential selection, where the dictionary keys are\nthe lenghts k of these feature subsets. The dictionary\nvalues are dictionaries themselves with the following\nkeys: 'feature_idx' (tuple of indices of the feature subset)\n'cv_scores' (list individual cross-validation scores)\n'avg_score' (average cross-validation score)\n\n\n\n\n\n\nMethods\n\n\n\n\n\nfit(X, y)\n\n\nNone\n\n\n\n\n\nfit_transform(X, y)\n\n\nNone\n\n\n\n\n\nget_metric_dict(confidence_interval=0.95)\n\n\nNone\n\n\n\n\n\nget_params(deep=True)\n\n\nGet parameters for this estimator.\n\n\nParameters\n\n\ndeep: boolean, optional\n    If True, will return the parameters for this estimator and\n    contained subobjects that are estimators.\n\n\nReturns\n\n\n\n\n\n\nparams\n : mapping of string to any\n\n\nParameter names mapped to their values.\n\n\n\n\n\n\n\n\n\nset_params(\nparams)\n\n\nSet the parameters of this estimator.\n\n\nThe method works on simple estimators as well as on nested objects\n(such as pipelines). The former have parameters of the form\n``<component>__<parameter>`` so that it's possible to update each\ncomponent of a nested object.\n\n\n\nReturns\n\n\nself\n\n\n\n\n\ntransform(X)\n\n\nNone",
            "title": "Mlxtend.feature selection"
        },
        {
            "location": "/api_subpackages/mlxtend.feature_selection/#columnselector",
            "text": "ColumnSelector(cols)  A feature selector for scikit-learn's Pipeline class that returns\n    specified columns from a numpy array.",
            "title": "ColumnSelector"
        },
        {
            "location": "/api_subpackages/mlxtend.feature_selection/#methods",
            "text": "fit(X, y=None)  None   transform(X, y=None)  None",
            "title": "Methods"
        },
        {
            "location": "/api_subpackages/mlxtend.feature_selection/#plot_sequential_feature_selection",
            "text": "plot_sequential_feature_selection(metric_dict, kind='std_dev', color='blue', bcolor='steelblue', marker='o', alpha=0.2, ylabel='Performance', confidence_interval=0.95)  Plot sequential feature selection.  Parameters    metric_dict  : mlxtend.SequentialFeatureSelector.get_metric_dict() object    kind  : str (default: \"std_dev\")  The kind of error bar or confidence interval in\n{'std_dev', 'std_err', 'ci', None}.    color  : str (default: \"blue\")  Color of the lineplot (accepts any matplotlib color name)    bcolor  : str (default: \"steelblue\").  Color of the error bars / confidence intervals\n(accepts any matplotlib color name).    marker  : str (default: \"o\")  Marker of the line plot\n(accepts any matplotlib marker name).    alpha  : float in [0, 1] (default: 0.2)  Transparency of the error bars / confidence intervals.    ylabel  : str (default: \"Performance\")  Y-axis label.    confidence_interval  : float (default: 0.95)  Confidence level if  kind='ci' .    Returns   fig  : matplotlib.pyplot.figure() object",
            "title": "plot_sequential_feature_selection"
        },
        {
            "location": "/api_subpackages/mlxtend.feature_selection/#sequentialfeatureselector",
            "text": "SequentialFeatureSelector(estimator, k_features, forward=True, floating=False, print_progress=True, scoring='accuracy', cv=5, skip_if_stuck=True, n_jobs=1, pre_dispatch='2 n_jobs')*  Sequential Feature Selection for Classification and Regression.  Parameters    estimator  : scikit-learn classifier or regressor    k_features  : int  Number of features to select,\nwhere k_features < the full feature set.    forward  : bool (default: True)  Forward selection if True,\nbackward selection otherwise    floating  : bool (default: False)  Adds a conditional exclusion/inclusion if True.    print_progress  : bool (default: True)  Prints progress as the number of epochs\nto stderr.    scoring  : str, (default='accuracy')  Scoring metric in {accuracy, f1, precision, recall, roc_auc}\nfor classifiers,\n{'mean_absolute_error', 'mean_squared_error',\n'median_absolute_error', 'r2'} for regressors,\nor a callable object or function with\nsignature  scorer(estimator, X, y) .    cv  : int (default: 5)  Scikit-learn cross-validation generator or  int .\nIf estimator is a classifier (or y consists of integer class labels),\nstratified k-fold is performed, and regular k-fold cross-validation\notherwise.\nNo cross-validation if cv is None, False, or 0.\nskip_if_stuck: bool (default: True)\nSet to True to skip conditional\nexlusion/inclusion if floating=True and\nalgorithm gets stuck in cycles.    n_jobs  : int (default: 1)  The number of CPUs to use for cross validation. -1 means 'all CPUs'.    pre_dispatch  : int, or string (default: '2*n_jobs')  Controls the number of jobs that get dispatched\nduring parallel execution in cross_val_score.\nReducing this number can be useful to avoid an explosion of\nmemory consumption when more jobs get dispatched than CPUs can process.\nThis parameter can be:\nNone, in which case all the jobs are immediately created and spawned.\nUse this for lightweight and fast-running jobs,\nto avoid delays due to on-demand spawning of the jobs\nAn int, giving the exact number of total jobs that are spawned\nA string, giving an expression as a function\nof n_jobs, as in  '2*n_jobs'    Attributes    k_feature_idx_  : array-like, shape = [n_predictions]  Feature Indices of the selected feature subsets.    k_score_  : float  Cross validation average score of the selected subset.    subsets_  : dict  A dictionary of selected feature subsets during the\nsequential selection, where the dictionary keys are\nthe lenghts k of these feature subsets. The dictionary\nvalues are dictionaries themselves with the following\nkeys: 'feature_idx' (tuple of indices of the feature subset)\n'cv_scores' (list individual cross-validation scores)\n'avg_score' (average cross-validation score)",
            "title": "SequentialFeatureSelector"
        },
        {
            "location": "/api_subpackages/mlxtend.feature_selection/#methods_1",
            "text": "fit(X, y)  None   fit_transform(X, y)  None   get_metric_dict(confidence_interval=0.95)  None   get_params(deep=True)  Get parameters for this estimator.  Parameters  deep: boolean, optional\n    If True, will return the parameters for this estimator and\n    contained subobjects that are estimators.  Returns    params  : mapping of string to any  Parameter names mapped to their values.     set_params( params)  Set the parameters of this estimator.  The method works on simple estimators as well as on nested objects\n(such as pipelines). The former have parameters of the form\n``<component>__<parameter>`` so that it's possible to update each\ncomponent of a nested object.  Returns  self   transform(X)  None",
            "title": "Methods"
        },
        {
            "location": "/api_subpackages/mlxtend.file_io/",
            "text": "mlxtend version: 0.3.1dev\n\n\nfind_filegroups\n\n\nfind_filegroups(paths, substring='', extensions=None, validity_check=True, ignore_invisible=True, rstrip='', ignore_substring=None)\n\n\nFind and collect files from different directories in a python dictionary.\n\n\nParameters\n\n\n\n\n\n\npaths\n : \nlist\n\n\nPaths of the directories to be searched. Dictionary keys are build from\nthe first directory.\n\n\n\n\n\n\nsubstring\n : \nstr\n (default: '')\n\n\nSubstring that all files have to contain to be considered.\n\n\n\n\n\n\nextensions\n : \nlist\n (default: None)\n\n\nNone\n or \nlist\n of allowed file extensions for each path.\nIf provided, the number of extensions must match the number of \npaths\n.\n\n\n\n\n\n\nvalidity_check\n : \nbool\n (default: None)\n\n\nIf \nTrue\n, checks if all dictionary values\nhave the same number of file paths. Prints\na warning and returns an empty dictionary if the validity check failed.\n\n\n\n\n\n\nignore_invisible\n : \nbool\n (default: True)\n\n\nIf \nTrue\n, ignores invisible files\n(i.e., files starting with a period).\n\n\n\n\n\n\nrstrip\n : \nstr\n (default: '')\n\n\nIf provided, strips characters from right side of the file\nbase names after splitting the extension.\nUseful to trim different filenames to a common stem.\nE.g,. \"abc_d.txt\" and \"abc_d_.csv\" would share\nthe stem \"abc_d\" if rstrip is set to \"_\".\n\n\n\n\n\n\nignore_substring\n : \nstr\n (default: None)\n\n\nIgnores files that contain the specified substring.\n\n\n\n\n\n\nReturns\n\n\n\n\n\n\ngroups\n : \ndict\n\n\nDictionary of files paths. Keys are the file names\nfound in the first directory listed\nin \npaths\n (without file extension).\n\n\n\n\n\n\nfind_files\n\n\nfind_files(substring, path, recursive=False, check_ext=None, ignore_invisible=True, ignore_substring=None)\n\n\nFind files in a directory based on substring matching.\n\n\nParameters\n\n\n\n\n\n\nsubstring\n : \nstr\n\n\nSubstring of the file to be matched.\n\n\n\n\n\n\npath\n : \nstr\n\n\nPath where to look.\nrecursive: \nbool\n\nIf true, searches subdirectories recursively.\ncheck_ext: \nstr\n\nIf string (e.g., '.txt'), only returns files that\nmatch the specified file extension.\n\n\n\n\n\n\nignore_invisible\n : \nbool\n\n\nIf \nTrue\n, ignores invisible files\n(i.e., files starting with a period).\n\n\n\n\n\n\nignore_substring\n : \nstr\n\n\nIgnores files that contain the specified substring.\n\n\n\n\n\n\nReturns\n\n\n\n\n\n\nresults\n : \nlist\n\n\nList of the matched files.",
            "title": "Mlxtend.file io"
        },
        {
            "location": "/api_subpackages/mlxtend.file_io/#find_filegroups",
            "text": "find_filegroups(paths, substring='', extensions=None, validity_check=True, ignore_invisible=True, rstrip='', ignore_substring=None)  Find and collect files from different directories in a python dictionary.  Parameters    paths  :  list  Paths of the directories to be searched. Dictionary keys are build from\nthe first directory.    substring  :  str  (default: '')  Substring that all files have to contain to be considered.    extensions  :  list  (default: None)  None  or  list  of allowed file extensions for each path.\nIf provided, the number of extensions must match the number of  paths .    validity_check  :  bool  (default: None)  If  True , checks if all dictionary values\nhave the same number of file paths. Prints\na warning and returns an empty dictionary if the validity check failed.    ignore_invisible  :  bool  (default: True)  If  True , ignores invisible files\n(i.e., files starting with a period).    rstrip  :  str  (default: '')  If provided, strips characters from right side of the file\nbase names after splitting the extension.\nUseful to trim different filenames to a common stem.\nE.g,. \"abc_d.txt\" and \"abc_d_.csv\" would share\nthe stem \"abc_d\" if rstrip is set to \"_\".    ignore_substring  :  str  (default: None)  Ignores files that contain the specified substring.    Returns    groups  :  dict  Dictionary of files paths. Keys are the file names\nfound in the first directory listed\nin  paths  (without file extension).",
            "title": "find_filegroups"
        },
        {
            "location": "/api_subpackages/mlxtend.file_io/#find_files",
            "text": "find_files(substring, path, recursive=False, check_ext=None, ignore_invisible=True, ignore_substring=None)  Find files in a directory based on substring matching.  Parameters    substring  :  str  Substring of the file to be matched.    path  :  str  Path where to look.\nrecursive:  bool \nIf true, searches subdirectories recursively.\ncheck_ext:  str \nIf string (e.g., '.txt'), only returns files that\nmatch the specified file extension.    ignore_invisible  :  bool  If  True , ignores invisible files\n(i.e., files starting with a period).    ignore_substring  :  str  Ignores files that contain the specified substring.    Returns    results  :  list  List of the matched files.",
            "title": "find_files"
        },
        {
            "location": "/api_subpackages/mlxtend.general_plotting/",
            "text": "mlxtend version: 0.3.1dev\n\n\ncategory_scatter\n\n\ncategory_scatter(x, y, label_col, data, markers='sxo^v', colors=('blue', 'green', 'red', 'purple', 'gray', 'cyan'), alpha=0.7, markersize=20.0, legend_loc='best')\n\n\nScatter plot to plot categories in different colors/markerstyles.\n\n\nParameters\n\n\n\n\n\n\nx\n : str or int\n\n\nDataFrame column name of the x-axis values or\ninteger for the numpy ndarray column index.\n\n\n\n\n\n\ny\n : str\n\n\nDataFrame column name of the y-axis values or\ninteger for the numpy ndarray column index\n\n\n\n\n\n\ndata\n : Pandas DataFrame object or NumPy ndarray.\n\n\n\n\n\n\nmarkers\n : str\n\n\nMarkers that are cycled through the label category.\n\n\n\n\n\n\ncolors\n : tuple\n\n\nColors that are cycled through the label category.\n\n\n\n\n\n\nalpha\n : float (default: 0.7)\n\n\nParameter to control the transparency.\n\n\n\n\n\n\nmarkersize\n : float (default` : 20.0)\n\n\nParameter to control the marker size.\n\n\n\n\n\n\nlegend_loc\n : str (default: 'best')\n\n\nLocation of the plot legend\n{best, upper left, upper right, lower left, lower right}\nNo legend if legend_loc=False\n\n\n\n\n\n\nReturns\n\n\n\n\nfig\n : matplotlig.pyplot figure object\n\n\n\n\nenrichment_plot\n\n\nenrichment_plot(df, colors='bgrkcy', markers=' ', linestyles='-', alpha=0.5, lw=2, legend=True, where='post', grid=True, count_label='Count', xlim='auto', ylim='auto', invert_axes=False, legend_loc='best', ax=None)\n\n\nPlot stacked barplots\n\n\nParameters\n\n\n\n\n\n\ndf\n : pandas.DataFrame\n\n\nA pandas DataFrame where columns represent the different categories.\ncolors: str (default: 'bgrcky')\nThe colors of the bars.\n\n\n\n\n\n\nmarkers\n : str (default: ' ')\n\n\nMatplotlib markerstyles, e.g,\n'sov' for square,circle, and triangle markers.\n\n\n\n\n\n\nlinestyles\n : str (default: '-')\n\n\nMatplotlib linestyles, e.g.,\n'-,--' to cycle normal and dashed lines. Note\nthat the different linestyles need to be separated by commas.\n\n\n\n\n\n\nalpha\n : float (default: 0.5)\n\n\nTransparency level from 0.0 to 1.0.\n\n\n\n\n\n\nlw\n : int or float (default: 2)\n\n\nLinewidth parameter.\n\n\n\n\n\n\nlegend\n : bool (default: True)\n\n\nPlots legend if True.\n\n\n\n\n\n\nwhere\n : {'post', 'pre', 'mid'} (default: 'post')\n\n\nStarting location of the steps.\n\n\n\n\n\n\ngrid\n : bool (default: \nTrue\n)\n\n\nPlots a grid if True.\n\n\n\n\n\n\ncount_label\n : str (default: 'Count')\n\n\nLabel for the \"Count\"-axis.\n\n\n\n\n\n\nxlim\n : 'auto' or array-like [min, max] (default: 'auto')\n\n\nMin and maximum position of the x-axis range.\n\n\n\n\n\n\nylim\n : 'auto' or array-like [min, max] (default: 'auto')\n\n\nMin and maximum position of the y-axis range.\n\n\n\n\n\n\ninvert_axes\n : bool (default: False)\n\n\nPlots count on the x-axis if True.\n\n\n\n\n\n\nlegend_loc\n : str (default: 'best')\n\n\nLocation of the plot legend\n{best, upper left, upper right, lower left, lower right}\nNo legend if legend_loc=False\n\n\n\n\n\n\nax\n : matplotlib axis, optional (default: None)\n\n\nUse this axis for plotting or make a new one otherwise\n\n\n\n\n\n\nReturns\n\n\n\n\nax\n : matplotlib axis\n\n\n\n\nremove_borders\n\n\nremove_borders(axes, left=False, bottom=False, right=True, top=True)\n\n\nRemove chart junk from matplotlib plots.\n\n\nParameters\n\n\n\n\n\n\naxes\n : iterable\n\n\nAn iterable containing plt.gca()\nor plt.subplot() objects, e.g. [plt.gca()].\n\n\n\n\n\n\nleft\n : bool (default: \nFalse\n)\n\n\nHide left axis spine if True.\n\n\n\n\n\n\nbottom\n : bool (default: \nFalse\n)\n\n\nHide bottom axis spine if True.\n\n\n\n\n\n\nright\n : bool (default: \nTrue\n)\n\n\nHide right axis spine if True.\n\n\n\n\n\n\ntop\n : bool (default: \nTrue\n)\n\n\nHide top axis spine if True.\n\n\n\n\n\n\nstacked_barplot\n\n\nstacked_barplot(df, bar_width='auto', colors='bgrcky', labels='index', rotation=90, legend_loc='best')\n\n\nFunction to plot stacked barplots\n\n\nParameters\n\n\n\n\n\n\ndf\n : pandas.DataFrame\n\n\nA pandas DataFrame where the index denotes the\nx-axis labels, and the columns contain the different\nmeasurements for each row.\nbar_width: 'auto' or float (default: 'auto')\nParameter to set the widths of the bars. if\n'auto', the width is automatically determined by\nthe number of columns in the dataset.\ncolors: str (default: 'bgrcky')\nThe colors of the bars.\nlabels: 'index' or iterable (default: 'index')\nIf 'index', the DataFrame index will be used as\nx-tick labels.\nrotation: int (default: 90)\nParameter to rotate the x-axis labels.\n\n\n\n\n\n\nlegend_loc\n : str (default: 'best')\n\n\nLocation of the plot legend\n{best, upper left, upper right, lower left, lower right}\nNo legend if legend_loc=False\n\n\n\n\n\n\nReturns\n\n\n\n\nfig\n : matplotlib.pyplot figure object",
            "title": "Mlxtend.general plotting"
        },
        {
            "location": "/api_subpackages/mlxtend.general_plotting/#category_scatter",
            "text": "category_scatter(x, y, label_col, data, markers='sxo^v', colors=('blue', 'green', 'red', 'purple', 'gray', 'cyan'), alpha=0.7, markersize=20.0, legend_loc='best')  Scatter plot to plot categories in different colors/markerstyles.  Parameters    x  : str or int  DataFrame column name of the x-axis values or\ninteger for the numpy ndarray column index.    y  : str  DataFrame column name of the y-axis values or\ninteger for the numpy ndarray column index    data  : Pandas DataFrame object or NumPy ndarray.    markers  : str  Markers that are cycled through the label category.    colors  : tuple  Colors that are cycled through the label category.    alpha  : float (default: 0.7)  Parameter to control the transparency.    markersize  : float (default` : 20.0)  Parameter to control the marker size.    legend_loc  : str (default: 'best')  Location of the plot legend\n{best, upper left, upper right, lower left, lower right}\nNo legend if legend_loc=False    Returns   fig  : matplotlig.pyplot figure object",
            "title": "category_scatter"
        },
        {
            "location": "/api_subpackages/mlxtend.general_plotting/#enrichment_plot",
            "text": "enrichment_plot(df, colors='bgrkcy', markers=' ', linestyles='-', alpha=0.5, lw=2, legend=True, where='post', grid=True, count_label='Count', xlim='auto', ylim='auto', invert_axes=False, legend_loc='best', ax=None)  Plot stacked barplots  Parameters    df  : pandas.DataFrame  A pandas DataFrame where columns represent the different categories.\ncolors: str (default: 'bgrcky')\nThe colors of the bars.    markers  : str (default: ' ')  Matplotlib markerstyles, e.g,\n'sov' for square,circle, and triangle markers.    linestyles  : str (default: '-')  Matplotlib linestyles, e.g.,\n'-,--' to cycle normal and dashed lines. Note\nthat the different linestyles need to be separated by commas.    alpha  : float (default: 0.5)  Transparency level from 0.0 to 1.0.    lw  : int or float (default: 2)  Linewidth parameter.    legend  : bool (default: True)  Plots legend if True.    where  : {'post', 'pre', 'mid'} (default: 'post')  Starting location of the steps.    grid  : bool (default:  True )  Plots a grid if True.    count_label  : str (default: 'Count')  Label for the \"Count\"-axis.    xlim  : 'auto' or array-like [min, max] (default: 'auto')  Min and maximum position of the x-axis range.    ylim  : 'auto' or array-like [min, max] (default: 'auto')  Min and maximum position of the y-axis range.    invert_axes  : bool (default: False)  Plots count on the x-axis if True.    legend_loc  : str (default: 'best')  Location of the plot legend\n{best, upper left, upper right, lower left, lower right}\nNo legend if legend_loc=False    ax  : matplotlib axis, optional (default: None)  Use this axis for plotting or make a new one otherwise    Returns   ax  : matplotlib axis",
            "title": "enrichment_plot"
        },
        {
            "location": "/api_subpackages/mlxtend.general_plotting/#remove_borders",
            "text": "remove_borders(axes, left=False, bottom=False, right=True, top=True)  Remove chart junk from matplotlib plots.  Parameters    axes  : iterable  An iterable containing plt.gca()\nor plt.subplot() objects, e.g. [plt.gca()].    left  : bool (default:  False )  Hide left axis spine if True.    bottom  : bool (default:  False )  Hide bottom axis spine if True.    right  : bool (default:  True )  Hide right axis spine if True.    top  : bool (default:  True )  Hide top axis spine if True.",
            "title": "remove_borders"
        },
        {
            "location": "/api_subpackages/mlxtend.general_plotting/#stacked_barplot",
            "text": "stacked_barplot(df, bar_width='auto', colors='bgrcky', labels='index', rotation=90, legend_loc='best')  Function to plot stacked barplots  Parameters    df  : pandas.DataFrame  A pandas DataFrame where the index denotes the\nx-axis labels, and the columns contain the different\nmeasurements for each row.\nbar_width: 'auto' or float (default: 'auto')\nParameter to set the widths of the bars. if\n'auto', the width is automatically determined by\nthe number of columns in the dataset.\ncolors: str (default: 'bgrcky')\nThe colors of the bars.\nlabels: 'index' or iterable (default: 'index')\nIf 'index', the DataFrame index will be used as\nx-tick labels.\nrotation: int (default: 90)\nParameter to rotate the x-axis labels.    legend_loc  : str (default: 'best')  Location of the plot legend\n{best, upper left, upper right, lower left, lower right}\nNo legend if legend_loc=False    Returns   fig  : matplotlib.pyplot figure object",
            "title": "stacked_barplot"
        },
        {
            "location": "/api_subpackages/mlxtend.preprocessing/",
            "text": "mlxtend version: 0.3.1dev\n\n\nDenseTransformer\n\n\nDenseTransformer(some_param=True)\n\n\nConvert a sparse matrix into a dense matrix.\n\n\nMethods\n\n\n\n\n\nfit(X, y=None)\n\n\nNone\n\n\n\n\n\nfit_transform(X, y=None)\n\n\nNone\n\n\n\n\n\nget_params(deep=True)\n\n\nNone\n\n\n\n\n\ntransform(X, y=None)\n\n\nNone\n\n\nMeanCenterer\n\n\nMeanCenterer()\n\n\nColumn centering of vectors and matrices.\n\n\nAttributes\n\n\n\n\n\n\ncol_means\n : numpy.ndarray [n_columns]\n\n\nNumPy array storing the mean values for centering after fitting\nthe MeanCenterer object.\n\n\n\n\n\n\nMethods\n\n\n\n\n\nfit(X)\n\n\nGets the column means for mean centering.\n\n\nParameters\n\n\n\n\n\n\nX\n : {array-like, sparse matrix}, shape = [n_samples, n_features]\n\n\nArray of data vectors, where n_samples is the number of samples and\nn_features is the number of features.\n\n\n\n\n\n\nReturns\n\n\nself\n\n\n\n\n\nfit_transform(X)\n\n\nFits and transforms an arry.\n\n\nParameters\n\n\n\n\n\n\nX\n : {array-like, sparse matrix}, shape = [n_samples, n_features]\n\n\nArray of data vectors, where n_samples is the number of samples and\nn_features is the number of features.\n\n\n\n\n\n\nReturns\n\n\n\n\n\n\nX_tr\n : {array-like, sparse matrix}, shape = [n_samples, n_features]\n\n\nA copy of the input array with the columns centered.\n\n\n\n\n\n\n\n\n\ntransform(X)\n\n\nCenters a NumPy array.\n\n\nParameters\n\n\n\n\n\n\nX\n : {array-like, sparse matrix}, shape = [n_samples, n_features]\n\n\nArray of data vectors, where n_samples is the number of samples and\nn_features is the number of features.\n\n\n\n\n\n\nReturns\n\n\n\n\n\n\nX_tr\n : {array-like, sparse matrix}, shape = [n_samples, n_features]\n\n\nA copy of the input array with the columns centered.\n\n\n\n\n\n\nminmax_scaling\n\n\nminmax_scaling(array, columns, min_val=0, max_val=1)\n\n\nMin max scaling of pandas' DataFrames.\n\n\nParameters\n\n\n\n\n\n\narray\n : pandas DataFrame or NumPy ndarray, shape = [n_rows, n_columns].\n\n\n\n\n\n\ncolumns\n : array-like, shape = [n_columns]\n\n\nArray-like with column names, e.g., ['col1', 'col2', ...]\nor column indices [0, 2, 4, ...]\n\n\n\n\n\n\nmin_val\n : \nint\n or \nfloat\n, optional (default=\n0\n)\n\n\nminimum value after rescaling.\n\n\n\n\n\n\nmin_val\n : \nint\n or \nfloat\n, optional (default=\n1\n)\n\n\nmaximum value after rescaling.\n\n\n\n\n\n\nReturns\n\n\n\n\n\n\ndf_new\n : pandas DataFrame object.\n\n\nCopy of the array or DataFrame with rescaled columns.\n\n\n\n\n\n\none_hot\n\n\none_hot(y, num_labels='auto', dtype='float')\n\n\nOne-hot encoding of class labels\n\n\nParameters\n\n\n\n\n\n\ny\n : array-like, shape = [n_classlabels]\n\n\nPython list or numpy array consisting of class labels.\n\n\n\n\n\n\nnum_labels\n : int or 'auto'\n\n\nNumber of unique labels in the class label array. Infers the number\nof unique labels from the input array if set to 'auto'.\n\n\n\n\n\n\ndtype\n : str\n\n\nNumPy array type (float, float32, float64) of the output array.\n\n\n\n\n\n\nReturns\n\n\n\n\n\n\nonehot\n : numpy.ndarray, shape = [n_classlabels]\n\n\nOne-hot encoded array, where each sample is represented as\na row vector in the returned array.\n\n\n\n\n\n\nshuffle_arrays_unison\n\n\nshuffle_arrays_unison(arrays, random_seed=None)\n\n\nShuffle NumPy arrays in unison.\n\n\nParameters\n\n\n\n\n\n\narrays\n : array-like, shape = [n_arrays]\n\n\nA list of NumPy arrays.\n\n\n\n\n\n\nrandom_seed\n : int (default: None)\n\n\nSets the random state.\n\n\n\n\n\n\nReturns\n\n\n\n\nshuffled_arrays\n : A list of NumPy arrays after shuffling.\n\n\n\n\nExamples\n\n\n>>> import numpy as np\n>>> from mlxtend.preprocessing import shuffle_arrays_unison\n>>> X1 = np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]])\n>>> y1 = np.array([1, 2, 3])\n>>> X2, y2 = shuffle_arrays_unison(arrays=[X1, y1], random_state=3)\n>>> assert(X2.all() == np.array([[4, 5, 6], [1, 2, 3], [7, 8, 9]]).all())\n>>> assert(y2.all() == np.array([2, 1, 3]).all())\n>>>\n\n\n\nstandardize\n\n\nstandardize(array, columns, ddof=0, return_params=False, params=None)\n\n\nStandardize columns in pandas DataFrames.\n\n\nParameters\n\n\n\n\n\n\narray\n : pandas DataFrame or NumPy ndarray, shape = [n_rows, n_columns].\n\n\n\n\n\n\ncolumns\n : array-like, shape = [n_columns]\n\n\nArray-like with column names, e.g., ['col1', 'col2', ...]\nor column indices [0, 2, 4, ...]\n\n\n\n\n\n\nddof\n : int (default: 0)\n\n\nDelta Degrees of Freedom. The divisor used in calculations\nis N - ddof, where N represents the number of elements.\n\n\n\n\n\n\nreturn_params\n : dict (default: False)\n\n\nIf set to True, a dictionary is returned in addition to the\nstandardized array. The parameter dictionary contains the\ncolumn means ('avgs') and standard deviations ('stds') of\nthe individual columns.\n\n\n\n\n\n\nparams\n : dict (default: None)\n\n\nA dictionary with column means and standard deviations as\nreturned by the \nstandardize\n function if \nreturn_params\n\nwas set to True. If a \nparams\n dictionary is provided, the\n\nstandardize\n function will use these instead of computing\nthem from the current array.\n\n\n\n\n\n\nNotes\n\n\nIf all values in a given column are the same, these values are all\n    set to \n0.0\n. The standard deviation in the \nparameters\n dictionary\n    is consequently set to \n1.0\n to avoid dividing by zero.\n\n\nReturns\n\n\n\n\n\n\ndf_new\n : pandas DataFrame object.\n\n\nCopy of the array or DataFrame with standardized columns.\n\n\n\n\n\n\nTransformerObj\n\n\nTransformerObj()\n\n\nNone\n\n\nMethods",
            "title": "Mlxtend.preprocessing"
        },
        {
            "location": "/api_subpackages/mlxtend.preprocessing/#densetransformer",
            "text": "DenseTransformer(some_param=True)  Convert a sparse matrix into a dense matrix.",
            "title": "DenseTransformer"
        },
        {
            "location": "/api_subpackages/mlxtend.preprocessing/#methods",
            "text": "fit(X, y=None)  None   fit_transform(X, y=None)  None   get_params(deep=True)  None   transform(X, y=None)  None",
            "title": "Methods"
        },
        {
            "location": "/api_subpackages/mlxtend.preprocessing/#meancenterer",
            "text": "MeanCenterer()  Column centering of vectors and matrices.  Attributes    col_means  : numpy.ndarray [n_columns]  NumPy array storing the mean values for centering after fitting\nthe MeanCenterer object.",
            "title": "MeanCenterer"
        },
        {
            "location": "/api_subpackages/mlxtend.preprocessing/#methods_1",
            "text": "fit(X)  Gets the column means for mean centering.  Parameters    X  : {array-like, sparse matrix}, shape = [n_samples, n_features]  Array of data vectors, where n_samples is the number of samples and\nn_features is the number of features.    Returns  self   fit_transform(X)  Fits and transforms an arry.  Parameters    X  : {array-like, sparse matrix}, shape = [n_samples, n_features]  Array of data vectors, where n_samples is the number of samples and\nn_features is the number of features.    Returns    X_tr  : {array-like, sparse matrix}, shape = [n_samples, n_features]  A copy of the input array with the columns centered.     transform(X)  Centers a NumPy array.  Parameters    X  : {array-like, sparse matrix}, shape = [n_samples, n_features]  Array of data vectors, where n_samples is the number of samples and\nn_features is the number of features.    Returns    X_tr  : {array-like, sparse matrix}, shape = [n_samples, n_features]  A copy of the input array with the columns centered.",
            "title": "Methods"
        },
        {
            "location": "/api_subpackages/mlxtend.preprocessing/#minmax_scaling",
            "text": "minmax_scaling(array, columns, min_val=0, max_val=1)  Min max scaling of pandas' DataFrames.  Parameters    array  : pandas DataFrame or NumPy ndarray, shape = [n_rows, n_columns].    columns  : array-like, shape = [n_columns]  Array-like with column names, e.g., ['col1', 'col2', ...]\nor column indices [0, 2, 4, ...]    min_val  :  int  or  float , optional (default= 0 )  minimum value after rescaling.    min_val  :  int  or  float , optional (default= 1 )  maximum value after rescaling.    Returns    df_new  : pandas DataFrame object.  Copy of the array or DataFrame with rescaled columns.",
            "title": "minmax_scaling"
        },
        {
            "location": "/api_subpackages/mlxtend.preprocessing/#one_hot",
            "text": "one_hot(y, num_labels='auto', dtype='float')  One-hot encoding of class labels  Parameters    y  : array-like, shape = [n_classlabels]  Python list or numpy array consisting of class labels.    num_labels  : int or 'auto'  Number of unique labels in the class label array. Infers the number\nof unique labels from the input array if set to 'auto'.    dtype  : str  NumPy array type (float, float32, float64) of the output array.    Returns    onehot  : numpy.ndarray, shape = [n_classlabels]  One-hot encoded array, where each sample is represented as\na row vector in the returned array.",
            "title": "one_hot"
        },
        {
            "location": "/api_subpackages/mlxtend.preprocessing/#shuffle_arrays_unison",
            "text": "shuffle_arrays_unison(arrays, random_seed=None)  Shuffle NumPy arrays in unison.  Parameters    arrays  : array-like, shape = [n_arrays]  A list of NumPy arrays.    random_seed  : int (default: None)  Sets the random state.    Returns   shuffled_arrays  : A list of NumPy arrays after shuffling.   Examples  >>> import numpy as np\n>>> from mlxtend.preprocessing import shuffle_arrays_unison\n>>> X1 = np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]])\n>>> y1 = np.array([1, 2, 3])\n>>> X2, y2 = shuffle_arrays_unison(arrays=[X1, y1], random_state=3)\n>>> assert(X2.all() == np.array([[4, 5, 6], [1, 2, 3], [7, 8, 9]]).all())\n>>> assert(y2.all() == np.array([2, 1, 3]).all())\n>>>",
            "title": "shuffle_arrays_unison"
        },
        {
            "location": "/api_subpackages/mlxtend.preprocessing/#standardize",
            "text": "standardize(array, columns, ddof=0, return_params=False, params=None)  Standardize columns in pandas DataFrames.  Parameters    array  : pandas DataFrame or NumPy ndarray, shape = [n_rows, n_columns].    columns  : array-like, shape = [n_columns]  Array-like with column names, e.g., ['col1', 'col2', ...]\nor column indices [0, 2, 4, ...]    ddof  : int (default: 0)  Delta Degrees of Freedom. The divisor used in calculations\nis N - ddof, where N represents the number of elements.    return_params  : dict (default: False)  If set to True, a dictionary is returned in addition to the\nstandardized array. The parameter dictionary contains the\ncolumn means ('avgs') and standard deviations ('stds') of\nthe individual columns.    params  : dict (default: None)  A dictionary with column means and standard deviations as\nreturned by the  standardize  function if  return_params \nwas set to True. If a  params  dictionary is provided, the standardize  function will use these instead of computing\nthem from the current array.    Notes  If all values in a given column are the same, these values are all\n    set to  0.0 . The standard deviation in the  parameters  dictionary\n    is consequently set to  1.0  to avoid dividing by zero.  Returns    df_new  : pandas DataFrame object.  Copy of the array or DataFrame with standardized columns.",
            "title": "standardize"
        },
        {
            "location": "/api_subpackages/mlxtend.preprocessing/#transformerobj",
            "text": "TransformerObj()  None",
            "title": "TransformerObj"
        },
        {
            "location": "/api_subpackages/mlxtend.preprocessing/#methods_2",
            "text": "",
            "title": "Methods"
        },
        {
            "location": "/api_subpackages/mlxtend.regression_utils/",
            "text": "mlxtend version: 0.3.1dev\n\n\nplot_linear_regression\n\n\nplot_linear_regression(X, y, model=LinearRegression(copy_X=True, fit_intercept=True, n_jobs=1, normalize=False), corr_func='pearsonr', scattercolor='blue', fit_style='k--', legend=True, xlim='auto')\n\n\nPlot a linear regression line fit.\n\n\nParameters\n\n\n\n\n\n\nX\n : numpy array, shape = [n_samples,]\n\n\nSamples.\n\n\n\n\n\n\ny\n : numpy array, shape (n_samples,)\n\n\nTarget values\nmodel: object (default: sklearn.linear_model.LinearRegression)\nEstimator object for regression. Must implement\na .fit() and .predict() method.\ncorr_func: str or function (default: 'pearsonr')\nUses \npearsonr\n from scipy.stats if corr_func='pearsonr'.\nto compute the regression slope. If not 'pearsonr', the \ncorr_func\n,\nthe \ncorr_func\n parameter expects a function of the form\nfunc(\n, \n) as inputs, which is expected to return\na tuple \n(<correlation_coefficient>, <some_unused_value>)\n.\nscattercolor: string (default: blue)\nColor of scatter plot points.\nfit_style: string (default: k--)\nStyle for the line fit.\nlegend: bool (default: True)\nPlots legend with corr_coeff coef.,\nfit coef., and intercept values.\nxlim: array-like (x_min, x_max) or 'auto' (default: 'auto')\nX-axis limits for the linear line fit.\n\n\n\n\n\n\nReturns\n\n\n\n\n\n\nregression_fit\n : tuple\n\n\nintercept, slope, corr_coeff (float, float, float)",
            "title": "Mlxtend.regression utils"
        },
        {
            "location": "/api_subpackages/mlxtend.regression_utils/#plot_linear_regression",
            "text": "plot_linear_regression(X, y, model=LinearRegression(copy_X=True, fit_intercept=True, n_jobs=1, normalize=False), corr_func='pearsonr', scattercolor='blue', fit_style='k--', legend=True, xlim='auto')  Plot a linear regression line fit.  Parameters    X  : numpy array, shape = [n_samples,]  Samples.    y  : numpy array, shape (n_samples,)  Target values\nmodel: object (default: sklearn.linear_model.LinearRegression)\nEstimator object for regression. Must implement\na .fit() and .predict() method.\ncorr_func: str or function (default: 'pearsonr')\nUses  pearsonr  from scipy.stats if corr_func='pearsonr'.\nto compute the regression slope. If not 'pearsonr', the  corr_func ,\nthe  corr_func  parameter expects a function of the form\nfunc( ,  ) as inputs, which is expected to return\na tuple  (<correlation_coefficient>, <some_unused_value>) .\nscattercolor: string (default: blue)\nColor of scatter plot points.\nfit_style: string (default: k--)\nStyle for the line fit.\nlegend: bool (default: True)\nPlots legend with corr_coeff coef.,\nfit coef., and intercept values.\nxlim: array-like (x_min, x_max) or 'auto' (default: 'auto')\nX-axis limits for the linear line fit.    Returns    regression_fit  : tuple  intercept, slope, corr_coeff (float, float, float)",
            "title": "plot_linear_regression"
        },
        {
            "location": "/api_subpackages/mlxtend.regressor/",
            "text": "mlxtend version: 0.3.1dev\n\n\nLinearRegression\n\n\nLinearRegression(solver='normal equation', eta=0.01, epochs=50, random_seed=None, shuffle=False, zero_init_weight=False)\n\n\nOrdinary least squares linear regression.\n\n\nParameters\n\n\n\n\n\n\nsolver\n : {'gd', 'sgd', 'normal equation'} (default: 'normal equation')\n\n\nMethod for solving the cost function. 'gd' for gradient descent,\n'sgd' for stochastic gradient descent, or 'normal equation' (default)\nto solve the cost function analytically.\n\n\n\n\n\n\neta\n : float (default: 0.1)\n\n\nLearning rate (between 0.0 and 1.0);\nignored if solver='normal equation'.\n\n\n\n\n\n\nepochs\n : int (default: 50)\n\n\nPasses over the training dataset;\nignored if solver='normal equation'.\n\n\n\n\n\n\nshuffle\n : bool (default: False)\n\n\nShuffles training data every epoch if True to prevent circles;\nignored if solver='normal equation'.\n\n\n\n\n\n\nrandom_seed\n : int (default: None)\n\n\nSet random state for shuffling and initializing the weights;\nignored if solver='normal equation'.\n\n\n\n\n\n\nzero_init_weight\n : bool (default: False)\n\n\nIf True, weights are initialized to zero instead of small random\nnumbers in the interval [-0.1, 0.1];\nignored if solver='normal equation'\n\n\n\n\n\n\nAttributes\n\n\n\n\n\n\nw_\n : 1d-array\n\n\nWeights after fitting.\n\n\n\n\n\n\ncost_\n : list\n\n\nSum of squared errors after each epoch;\nignored if solver='normal equation'\n\n\n\n\n\n\nMethods\n\n\n\n\n\nfit(X, y, init_weights=True)\n\n\nLearn weight coefficients from training data.\n\n\nParameters\n\n\n\n\n\n\nX\n : {array-like, sparse matrix}, shape = [n_samples, n_features]\n\n\nTraining vectors, where n_samples is the number of samples and\nn_features is the number of features.\n\n\n\n\n\n\ny\n : array-like, shape = [n_samples]\n\n\nTarget values.\n\n\n\n\n\n\ninit_weights\n : bool (default: True)\n\n\nRe-initializes weights prior to fitting. Set False to continue\ntraining with weights from a previous fitting.\n\n\n\n\n\n\nReturns\n\n\n\n\nself\n : object\n\n\n\n\n\n\n\nnet_input(X)\n\n\nCompute the linear net input.\n\n\n\n\n\npredict(X)\n\n\nPredict class labels of X.\n\n\nParameters\n\n\n\n\n\n\nX\n : {array-like, sparse matrix}, shape = [n_samples, n_features]\n\n\nTraining vectors, where n_samples is the number of samples and\nn_features is the number of features.\n\n\n\n\n\n\nReturns\n\n\n\n\nfloat\n : Predicted target value.",
            "title": "Mlxtend.regressor"
        },
        {
            "location": "/api_subpackages/mlxtend.regressor/#linearregression",
            "text": "LinearRegression(solver='normal equation', eta=0.01, epochs=50, random_seed=None, shuffle=False, zero_init_weight=False)  Ordinary least squares linear regression.  Parameters    solver  : {'gd', 'sgd', 'normal equation'} (default: 'normal equation')  Method for solving the cost function. 'gd' for gradient descent,\n'sgd' for stochastic gradient descent, or 'normal equation' (default)\nto solve the cost function analytically.    eta  : float (default: 0.1)  Learning rate (between 0.0 and 1.0);\nignored if solver='normal equation'.    epochs  : int (default: 50)  Passes over the training dataset;\nignored if solver='normal equation'.    shuffle  : bool (default: False)  Shuffles training data every epoch if True to prevent circles;\nignored if solver='normal equation'.    random_seed  : int (default: None)  Set random state for shuffling and initializing the weights;\nignored if solver='normal equation'.    zero_init_weight  : bool (default: False)  If True, weights are initialized to zero instead of small random\nnumbers in the interval [-0.1, 0.1];\nignored if solver='normal equation'    Attributes    w_  : 1d-array  Weights after fitting.    cost_  : list  Sum of squared errors after each epoch;\nignored if solver='normal equation'",
            "title": "LinearRegression"
        },
        {
            "location": "/api_subpackages/mlxtend.regressor/#methods",
            "text": "fit(X, y, init_weights=True)  Learn weight coefficients from training data.  Parameters    X  : {array-like, sparse matrix}, shape = [n_samples, n_features]  Training vectors, where n_samples is the number of samples and\nn_features is the number of features.    y  : array-like, shape = [n_samples]  Target values.    init_weights  : bool (default: True)  Re-initializes weights prior to fitting. Set False to continue\ntraining with weights from a previous fitting.    Returns   self  : object    net_input(X)  Compute the linear net input.   predict(X)  Predict class labels of X.  Parameters    X  : {array-like, sparse matrix}, shape = [n_samples, n_features]  Training vectors, where n_samples is the number of samples and\nn_features is the number of features.    Returns   float  : Predicted target value.",
            "title": "Methods"
        },
        {
            "location": "/api_subpackages/mlxtend.text/",
            "text": "mlxtend version: 0.3.1dev\n\n\ngeneralize_names\n\n\ngeneralize_names(name, output_sep=' ', firstname_output_letters=1)\n\n\nGeneralize a person's first and last name.\n\n\nReturns a person's name in the format\n    \n<last_name><separator><firstname letter(s)> (all lowercase)\n\n\nParameters\n\n\n\n\n\n\nname\n : \nstr\n\n\nName of the player\n\n\n\n\n\n\noutput_sep\n : \nstr\n (default: ' ')\n\n\nString for separating last name and first name in the output.\n\n\n\n\n\n\nfirstname_output_letters\n : \nint\n\n\nNumber of letters in the abbreviated first name.\n\n\n\n\n\n\nReturns\n\n\n\n\n\n\ngen_name\n : \nstr\n\n\nThe generalized name.\n\n\n\n\n\n\ngeneralize_names_duplcheck\n\n\ngeneralize_names_duplcheck(df, col_name)\n\n\nGeneralizes names and removes duplicates.\n\n\nApplies mlxtend.text.generalize_names to a DataFrame with 1 first name letter\n    by default and uses more first name letters if duplicates are detected.\n\n\nParameters\n\n\n\n\n\n\ndf\n : \npandas.DataFrame\n\n\nDataFrame that contains a column where generalize_names should be applied.\n\n\n\n\n\n\ncol_name\n : \nstr\n\n\nName of the DataFrame column where \ngeneralize_names\n function should be applied to.\n\n\n\n\n\n\nReturns\n\n\n\n\n\n\ndf_new\n : \nstr\n\n\nNew DataFrame object where generalize_names function has been applied without duplicates.\n\n\n\n\n\n\ntokenizer_emoticons\n\n\ntokenizer_emoticons(text)\n\n\nReturn emoticons from text\n\n\nExample:\n>>> tokenizer_emoticons('</a>This :) is :( a test :-)!')\n[':)', ':(', ':-)']\n\n\n\ntokenizer_words_and_emoticons\n\n\ntokenizer_words_and_emoticons(text)\n\n\nConvert text to lowercase words and emoticons.\n\n\nExample:\n>>> tokenizer_words_and_emoticons('</a>This :) is :( a test :-)!')\n['this', 'is', 'a', 'test', ':)', ':(', ':-)']",
            "title": "Mlxtend.text"
        },
        {
            "location": "/api_subpackages/mlxtend.text/#generalize_names",
            "text": "generalize_names(name, output_sep=' ', firstname_output_letters=1)  Generalize a person's first and last name.  Returns a person's name in the format\n     <last_name><separator><firstname letter(s)> (all lowercase)  Parameters    name  :  str  Name of the player    output_sep  :  str  (default: ' ')  String for separating last name and first name in the output.    firstname_output_letters  :  int  Number of letters in the abbreviated first name.    Returns    gen_name  :  str  The generalized name.",
            "title": "generalize_names"
        },
        {
            "location": "/api_subpackages/mlxtend.text/#generalize_names_duplcheck",
            "text": "generalize_names_duplcheck(df, col_name)  Generalizes names and removes duplicates.  Applies mlxtend.text.generalize_names to a DataFrame with 1 first name letter\n    by default and uses more first name letters if duplicates are detected.  Parameters    df  :  pandas.DataFrame  DataFrame that contains a column where generalize_names should be applied.    col_name  :  str  Name of the DataFrame column where  generalize_names  function should be applied to.    Returns    df_new  :  str  New DataFrame object where generalize_names function has been applied without duplicates.",
            "title": "generalize_names_duplcheck"
        },
        {
            "location": "/api_subpackages/mlxtend.text/#tokenizer_emoticons",
            "text": "tokenizer_emoticons(text)  Return emoticons from text  Example:\n>>> tokenizer_emoticons('</a>This :) is :( a test :-)!')\n[':)', ':(', ':-)']",
            "title": "tokenizer_emoticons"
        },
        {
            "location": "/api_subpackages/mlxtend.text/#tokenizer_words_and_emoticons",
            "text": "tokenizer_words_and_emoticons(text)  Convert text to lowercase words and emoticons.  Example:\n>>> tokenizer_words_and_emoticons('</a>This :) is :( a test :-)!')\n['this', 'is', 'a', 'test', ':)', ':(', ':-)']",
            "title": "tokenizer_words_and_emoticons"
        },
        {
            "location": "/api_subpackages/mlxtend.utils/",
            "text": "mlxtend version: 0.3.1dev\n\n\nCounter\n\n\nCounter(stderr=False, start_newline=True)\n\n\nClass to display the progress of for-loop iterators.\n\n\nParameters\n\n\n\n\n\n\nstderr\n : bool (default: True)\n\n\nPrints output to sys.stderr if True; uses sys.stdout otherwise.\n\n\n\n\n\n\nstart_newline\n : bool (default: True)\n\n\nPrepends a new line to the counter, which prevents overwriting counters\nif multiple counters are printed in succession.\n\n\n\n\n\n\nAttributes\n\n\n\n\n\n\ncurr_iter\n : int\n\n\nThe current iteration.\n\n\n\n\n\n\nstart_time\n : int\n\n\nThe system's time in seconds when the Counter was initialized.\n\n\n\n\n\n\nMethods\n\n\n\n\n\nupdate()\n\n\nPrint current iteration and time elapsed.",
            "title": "Mlxtend.utils"
        },
        {
            "location": "/api_subpackages/mlxtend.utils/#counter",
            "text": "Counter(stderr=False, start_newline=True)  Class to display the progress of for-loop iterators.  Parameters    stderr  : bool (default: True)  Prints output to sys.stderr if True; uses sys.stdout otherwise.    start_newline  : bool (default: True)  Prepends a new line to the counter, which prevents overwriting counters\nif multiple counters are printed in succession.    Attributes    curr_iter  : int  The current iteration.    start_time  : int  The system's time in seconds when the Counter was initialized.",
            "title": "Counter"
        },
        {
            "location": "/api_subpackages/mlxtend.utils/#methods",
            "text": "update()  Print current iteration and time elapsed.",
            "title": "Methods"
        },
        {
            "location": "/installation/",
            "text": "Installing mlxtend\n\n\n\n\nTo install \nmlxtend\n, just execute  \n\n\npip install mlxtend  \n\n\n\n\nThe \nmlxtend\n version on PyPI may always one step behind; you can install the latest development version from the GitHub repository by executing\n\n\npip install git+git://github.com/rasbt/mlxtend.git#egg=mlxtend\n\n\n\n\nAlternatively, you download the package manually from the Python Package Index \nhttps://pypi.python.org/pypi/mlxtend\n, unzip it, navigate into the package, and use the command:\n\n\npython setup.py install",
            "title": "Installation"
        },
        {
            "location": "/installation/#installing-mlxtend",
            "text": "To install  mlxtend , just execute    pip install mlxtend    The  mlxtend  version on PyPI may always one step behind; you can install the latest development version from the GitHub repository by executing  pip install git+git://github.com/rasbt/mlxtend.git#egg=mlxtend  Alternatively, you download the package manually from the Python Package Index  https://pypi.python.org/pypi/mlxtend , unzip it, navigate into the package, and use the command:  python setup.py install",
            "title": "Installing mlxtend"
        },
        {
            "location": "/changelog/",
            "text": "Release Notes\n\n\n\n\nVersion 0.3.1dev\n\n\n\n\nNew \nStackingRegressor\n (\nregressor.StackingRegressor\n)\n\n\nNew function for one-hot encoding of class labels (\npreprocessing.one_hot\n)\n\n\nevaluate.plot_decision_regions\n improvements:\n\n\nFunction now handles class y-class labels correctly if array is of type \nfloat\n\n\nCorrect handling of input arguments \nmarkers\n and \ncolors\n\n\nAccept an existing \nAxes\n via the \nax\n argument\n\n\nNew \nprint_progress\n parameter for all generalized models and multi-layer neural networks for printing time elapsed, ETA, and the current cost of the current epoch\n\n\nMinibatch learning for \nclassifier.LogisticRegression\n, \nclassifier.Adaline\n, and \nregressor.LinearRegression\n plus streamlined API\n\n\n\n\nVersion 0.3.0 (2016-01-31)\n\n\n\n\nThe \nmlxtend.preprocessing.standardize\n function now optionally returns the parameters, which are estimated from the array, for re-use. A further improvement makes the \nstandardize\n function smarter in order to avoid zero-division errors\n\n\nAdded a progress bar tracker to \nclassifier.NeuralNetMLP\n\n\nAdded a function to score predicted vs. target class labels \nevaluate.scoring\n\n\nAdded confusion matrix functions to create (\nevaluate.confusion_matrix\n) and plot (\nevaluate.plot_confusion_matrix\n) confusion matrices\n\n\nCosmetic improvements to the \nevaluate.plot_decision_regions\n function such as hiding plot axes\n\n\nRenaming of \nclassifier.EnsembleClassfier\n to \nclassifier.EnsembleVoteClassifier\n\n\nImproved random weight initialization in \nPerceptron\n, \nAdaline\n, \nLinearRegression\n, and \nLogisticRegression\n\n\nChanged \nlearning\n parameter of \nmlxtend.classifier.Adaline\n to solver and added \"normal equation\" as closed-form solution solver\n\n\nNew style parameter and improved axis scaling in \nmlxtend.evaluate.plot_learning_curves\n\n\nHide y-axis labels in \nmlxtend.evaluate.plot_decision_regions\n in 1 dimensional evaluations\n\n\nAdded \nloadlocal_mnist\n to \nmlxtend.data\n for streaming MNIST from a local byte files into numpy arrays\n\n\nNew \nNeuralNetMLP\n parameters: \nrandom_weights\n, \nshuffle_init\n, \nshuffle_epoch\n\n\nSequential Feature Selection algorithms were unified into a single \nSequentialFeatureSelector\n class with parameters to enable floating selection and toggle between forward and backward selection.\n\n\nNew \nSFS\n features such as the generation of pandas \nDataFrame\n results tables and plotting functions (with confidence intervals, standard deviation, and standard error bars)\n\n\nAdded support for regression estimators in \nSFS\n\n\nStratified sampling of MNIST (now 500x random samples from each of the 10 digit categories)\n\n\nAdded Boston \nhousing dataset\n\n\nRenaming \nmlxtend.plotting\n to \nmlxtend.general_plotting\n in order to distinguish general plotting function from specialized utility function such as \nevaluate.plot_decision_regions\n\n\nShuffle fix and new shuffle parameter for classifier.NeuralNetMLP\n\n\n\n\nVersion 0.2.9 (2015-07-14)\n\n\n\n\nSequential Feature Selection algorithms: SFS, SFFS, SBS, and SFBS\n\n\nChanged \nregularization\n & \nlambda\n parameters in \nLogisticRegression\n to single parameter \nl2_lambda\n\n\n\n\nVersion 0.2.8 (2015-06-27)\n\n\n\n\nAPI changes:\n\n\nmlxtend.sklearn.EnsembleClassifier\n -> \nmlxtend.classifier.EnsembleClassifier\n\n\nmlxtend.sklearn.ColumnSelector\n -> \nmlxtend.feature_selection.ColumnSelector\n\n\nmlxtend.sklearn.DenseTransformer\n -> \nmlxtend.preprocessing.DenseTransformer\n\n\nmlxtend.pandas.standardizing\n ->  \nmlxtend.preprocessing.standardizing\n\n\nmlxtend.pandas.minmax_scaling\n ->  \nmlxtend.preprocessing.minmax_scaling\n\n\nmlxtend.matplotlib\n -> \nmlxtend.plotting\n\n\n\n\n\n\nAdded momentum learning parameter (alpha coefficient) to \nmlxtend.classifier.NeuralNetMLP\n.\n\n\nAdded adaptive learning rate (decrease constant) to \nmlxtend.classifier.NeuralNetMLP\n.\n\n\nmlxtend.pandas.minmax_scaling\n became \nmlxtend.preprocessing.minmax_scaling\n  and also supports NumPy arrays now\n\n\nmlxtend.pandas.standardizing\n became \nmlxtend.preprocessing.standardizing\n and now supports both NumPy arrays and pandas DataFrames; also, now \nddof\n parameters to set the degrees of freedom when calculating the standard deviation\n\n\n\n\nVersion 0.2.7 (2015-06-20)\n\n\n\n\nAdded multilayer perceptron (feedforward artificial neural network) classifier as \nmlxtend.classifier.NeuralNetMLP\n.\n\n\nAdded 5000 labeled trainingsamples from the MNIST handwritten digits dataset to \nmlxtend.data\n\n\n\n\nVersion 0.2.6 (2015-05-08)\n\n\n\n\nAdded ordinary least square regression using different solvers (gradient and stochastic gradient descent, and the closed form solution (normal equation)\n\n\nAdded option for random weight initialization to logistic regression classifier and updated l2 regularization\n\n\nAdded \nwine\n dataset to \nmlxtend.data\n\n\nAdded \ninvert_axes\n parameter \nmlxtend.matplotlib.enrichtment_plot\n to optionally plot the \"Count\" on the x-axis\n\n\nNew \nverbose\n parameter for \nmlxtend.sklearn.EnsembleClassifier\n by \nAlejandro C. Bahnsen\n\n\nAdded \nmlxtend.pandas.standardizing\n to standardize columns in a Pandas DataFrame\n\n\nAdded parameters \nlinestyles\n and \nmarkers\n to \nmlxtend.matplotlib.enrichment_plot\n\n\nmlxtend.regression.lin_regplot\n automatically adds np.newaxis and works w. python lists\n\n\nAdded tokenizers: \nmlxtend.text.extract_emoticons\n and \nmlxtend.text.extract_words_and_emoticons\n\n\n\n\nVersion 0.2.5 (2015-04-17)\n\n\n\n\nAdded Sequential Backward Selection (mlxtend.sklearn.SBS)\n\n\nAdded \nX_highlight\n parameter to \nmlxtend.evaluate.plot_decision_regions\n for highlighting test data points.\n\n\nAdded mlxtend.regression.lin_regplot to plot the fitted line from linear regression.\n\n\nAdded mlxtend.matplotlib.stacked_barplot to conveniently produce stacked barplots using pandas \nDataFrame\ns.\n\n\nAdded mlxtend.matplotlib.enrichment_plot\n\n\n\n\nVersion 0.2.4 (2015-03-15)\n\n\n\n\nAdded \nscoring\n to \nmlxtend.evaluate.learning_curves\n (by user pfsq)\n\n\nFixed setup.py bug caused by the missing README.html file\n\n\nmatplotlib.category_scatter for pandas DataFrames and Numpy arrays\n\n\n\n\nVersion 0.2.3 (2015-03-11)\n\n\n\n\nAdded Logistic regression\n\n\nGradient descent and stochastic gradient descent perceptron was changed\n  to Adaline (Adaptive Linear Neuron)\n\n\nPerceptron and Adaline for {0, 1} classes\n\n\nAdded \nmlxtend.preprocessing.shuffle_arrays_unison\n function to\n  shuffle one or more NumPy arrays.\n\n\nAdded shuffle and random seed parameter to stochastic gradient descent classifier.\n\n\nAdded \nrstrip\n parameter to \nmlxtend.file_io.find_filegroups\n to allow trimming of base names.\n\n\nAdded \nignore_substring\n parameter to \nmlxtend.file_io.find_filegroups\n and \nfind_files\n.\n\n\nReplaced .rstrip in \nmlxtend.file_io.find_filegroups\n with more robust regex.\n\n\nGridsearch support for \nmlxtend.sklearn.EnsembleClassifier\n\n\n\n\nVersion 0.2.2 (2015-03-01)\n\n\n\n\nImproved robustness of EnsembleClassifier.\n\n\nExtended plot_decision_regions() functionality for plotting 1D decision boundaries.\n\n\nFunction matplotlib.plot_decision_regions was reorganized  to evaluate.plot_decision_regions .\n\n\nevaluate.plot_learning_curves() function added.\n\n\nAdded Rosenblatt, gradient descent, and stochastic gradient descent perceptrons.\n\n\n\n\nVersion 0.2.1 (2015-01-20)\n\n\n\n\nAdded mlxtend.pandas.minmax_scaling - a function to rescale pandas DataFrame columns.\n\n\nSlight update to the EnsembleClassifier interface (additional \nvoting\n parameter)\n\n\nFixed EnsembleClassifier to return correct class labels if class labels are not\n  integers from 0 to n.\n\n\nAdded new matplotlib function to plot decision regions of classifiers.\n\n\n\n\nVersion 0.2.0 (2015-01-13)\n\n\n\n\nImproved mlxtend.text.generalize_duplcheck to remove duplicates and prevent endless looping issue.\n\n\nAdded \nrecursive\n search parameter to mlxtend.file_io.find_files.\n\n\nAdded \ncheck_ext\n parameter mlxtend.file_io.find_files to search based on file extensions.\n\n\nDefault parameter to ignore invisible files for mlxtend.file_io.find.\n\n\nAdded \ntransform\n and \nfit_transform\n to the \nEnsembleClassifier\n.\n\n\nAdded mlxtend.file_io.find_filegroups function.\n\n\n\n\nVersion 0.1.9 (2015-01-10)\n\n\n\n\nImplemented scikit-learn EnsembleClassifier (majority voting rule) class.\n\n\n\n\nVersion 0.1.8 (2015-01-07)\n\n\n\n\nImprovements to mlxtend.text.generalize_names to handle certain Dutch last name prefixes (van, van der, de, etc.).\n\n\nAdded mlxtend.text.generalize_name_duplcheck function to apply mlxtend.text.generalize_names function to a pandas DataFrame without creating duplicates.\n\n\n\n\nVersion 0.1.7 (2015-01-07)\n\n\n\n\nAdded text utilities with name generalization function.\n\n\nAdded  and file_io utilities.\n\n\n\n\nVersion 0.1.6 (2015-01-04)\n\n\n\n\nAdded combinations and permutations estimators.\n\n\n\n\nVersion 0.1.5 (2014-12-11)\n\n\n\n\nAdded \nDenseTransformer\n for pipelines and grid search.\n\n\n\n\nVersion 0.1.4 (2014-08-20)\n\n\n\n\nmean_centering\n function is now a Class that creates \nMeanCenterer\n objects\n  that can be used to fit data via the \nfit\n method, and center data at the column\n  means via the \ntransform\n and \nfit_transform\n method.\n\n\n\n\nVersion 0.1.3 (2014-08-19)\n\n\n\n\nAdded \npreprocessing\n module and \nmean_centering\n function.\n\n\n\n\nVersion 0.1.2 (2014-08-19)\n\n\n\n\nAdded \nmatplotlib\n utilities and \nremove_borders\n function.\n\n\n\n\nVersion 0.1.1 (2014-08-13)\n\n\n\n\nSimplified code for ColumnSelector.",
            "title": "Release Notes"
        },
        {
            "location": "/changelog/#release-notes",
            "text": "",
            "title": "Release Notes"
        },
        {
            "location": "/changelog/#version-031dev",
            "text": "New  StackingRegressor  ( regressor.StackingRegressor )  New function for one-hot encoding of class labels ( preprocessing.one_hot )  evaluate.plot_decision_regions  improvements:  Function now handles class y-class labels correctly if array is of type  float  Correct handling of input arguments  markers  and  colors  Accept an existing  Axes  via the  ax  argument  New  print_progress  parameter for all generalized models and multi-layer neural networks for printing time elapsed, ETA, and the current cost of the current epoch  Minibatch learning for  classifier.LogisticRegression ,  classifier.Adaline , and  regressor.LinearRegression  plus streamlined API",
            "title": "Version 0.3.1dev"
        },
        {
            "location": "/changelog/#version-030-2016-01-31",
            "text": "The  mlxtend.preprocessing.standardize  function now optionally returns the parameters, which are estimated from the array, for re-use. A further improvement makes the  standardize  function smarter in order to avoid zero-division errors  Added a progress bar tracker to  classifier.NeuralNetMLP  Added a function to score predicted vs. target class labels  evaluate.scoring  Added confusion matrix functions to create ( evaluate.confusion_matrix ) and plot ( evaluate.plot_confusion_matrix ) confusion matrices  Cosmetic improvements to the  evaluate.plot_decision_regions  function such as hiding plot axes  Renaming of  classifier.EnsembleClassfier  to  classifier.EnsembleVoteClassifier  Improved random weight initialization in  Perceptron ,  Adaline ,  LinearRegression , and  LogisticRegression  Changed  learning  parameter of  mlxtend.classifier.Adaline  to solver and added \"normal equation\" as closed-form solution solver  New style parameter and improved axis scaling in  mlxtend.evaluate.plot_learning_curves  Hide y-axis labels in  mlxtend.evaluate.plot_decision_regions  in 1 dimensional evaluations  Added  loadlocal_mnist  to  mlxtend.data  for streaming MNIST from a local byte files into numpy arrays  New  NeuralNetMLP  parameters:  random_weights ,  shuffle_init ,  shuffle_epoch  Sequential Feature Selection algorithms were unified into a single  SequentialFeatureSelector  class with parameters to enable floating selection and toggle between forward and backward selection.  New  SFS  features such as the generation of pandas  DataFrame  results tables and plotting functions (with confidence intervals, standard deviation, and standard error bars)  Added support for regression estimators in  SFS  Stratified sampling of MNIST (now 500x random samples from each of the 10 digit categories)  Added Boston  housing dataset  Renaming  mlxtend.plotting  to  mlxtend.general_plotting  in order to distinguish general plotting function from specialized utility function such as  evaluate.plot_decision_regions  Shuffle fix and new shuffle parameter for classifier.NeuralNetMLP",
            "title": "Version 0.3.0 (2016-01-31)"
        },
        {
            "location": "/changelog/#version-029-2015-07-14",
            "text": "Sequential Feature Selection algorithms: SFS, SFFS, SBS, and SFBS  Changed  regularization  &  lambda  parameters in  LogisticRegression  to single parameter  l2_lambda",
            "title": "Version 0.2.9 (2015-07-14)"
        },
        {
            "location": "/changelog/#version-028-2015-06-27",
            "text": "API changes:  mlxtend.sklearn.EnsembleClassifier  ->  mlxtend.classifier.EnsembleClassifier  mlxtend.sklearn.ColumnSelector  ->  mlxtend.feature_selection.ColumnSelector  mlxtend.sklearn.DenseTransformer  ->  mlxtend.preprocessing.DenseTransformer  mlxtend.pandas.standardizing  ->   mlxtend.preprocessing.standardizing  mlxtend.pandas.minmax_scaling  ->   mlxtend.preprocessing.minmax_scaling  mlxtend.matplotlib  ->  mlxtend.plotting    Added momentum learning parameter (alpha coefficient) to  mlxtend.classifier.NeuralNetMLP .  Added adaptive learning rate (decrease constant) to  mlxtend.classifier.NeuralNetMLP .  mlxtend.pandas.minmax_scaling  became  mlxtend.preprocessing.minmax_scaling   and also supports NumPy arrays now  mlxtend.pandas.standardizing  became  mlxtend.preprocessing.standardizing  and now supports both NumPy arrays and pandas DataFrames; also, now  ddof  parameters to set the degrees of freedom when calculating the standard deviation",
            "title": "Version 0.2.8 (2015-06-27)"
        },
        {
            "location": "/changelog/#version-027-2015-06-20",
            "text": "Added multilayer perceptron (feedforward artificial neural network) classifier as  mlxtend.classifier.NeuralNetMLP .  Added 5000 labeled trainingsamples from the MNIST handwritten digits dataset to  mlxtend.data",
            "title": "Version 0.2.7 (2015-06-20)"
        },
        {
            "location": "/changelog/#version-026-2015-05-08",
            "text": "Added ordinary least square regression using different solvers (gradient and stochastic gradient descent, and the closed form solution (normal equation)  Added option for random weight initialization to logistic regression classifier and updated l2 regularization  Added  wine  dataset to  mlxtend.data  Added  invert_axes  parameter  mlxtend.matplotlib.enrichtment_plot  to optionally plot the \"Count\" on the x-axis  New  verbose  parameter for  mlxtend.sklearn.EnsembleClassifier  by  Alejandro C. Bahnsen  Added  mlxtend.pandas.standardizing  to standardize columns in a Pandas DataFrame  Added parameters  linestyles  and  markers  to  mlxtend.matplotlib.enrichment_plot  mlxtend.regression.lin_regplot  automatically adds np.newaxis and works w. python lists  Added tokenizers:  mlxtend.text.extract_emoticons  and  mlxtend.text.extract_words_and_emoticons",
            "title": "Version 0.2.6 (2015-05-08)"
        },
        {
            "location": "/changelog/#version-025-2015-04-17",
            "text": "Added Sequential Backward Selection (mlxtend.sklearn.SBS)  Added  X_highlight  parameter to  mlxtend.evaluate.plot_decision_regions  for highlighting test data points.  Added mlxtend.regression.lin_regplot to plot the fitted line from linear regression.  Added mlxtend.matplotlib.stacked_barplot to conveniently produce stacked barplots using pandas  DataFrame s.  Added mlxtend.matplotlib.enrichment_plot",
            "title": "Version 0.2.5 (2015-04-17)"
        },
        {
            "location": "/changelog/#version-024-2015-03-15",
            "text": "Added  scoring  to  mlxtend.evaluate.learning_curves  (by user pfsq)  Fixed setup.py bug caused by the missing README.html file  matplotlib.category_scatter for pandas DataFrames and Numpy arrays",
            "title": "Version 0.2.4 (2015-03-15)"
        },
        {
            "location": "/changelog/#version-023-2015-03-11",
            "text": "Added Logistic regression  Gradient descent and stochastic gradient descent perceptron was changed\n  to Adaline (Adaptive Linear Neuron)  Perceptron and Adaline for {0, 1} classes  Added  mlxtend.preprocessing.shuffle_arrays_unison  function to\n  shuffle one or more NumPy arrays.  Added shuffle and random seed parameter to stochastic gradient descent classifier.  Added  rstrip  parameter to  mlxtend.file_io.find_filegroups  to allow trimming of base names.  Added  ignore_substring  parameter to  mlxtend.file_io.find_filegroups  and  find_files .  Replaced .rstrip in  mlxtend.file_io.find_filegroups  with more robust regex.  Gridsearch support for  mlxtend.sklearn.EnsembleClassifier",
            "title": "Version 0.2.3 (2015-03-11)"
        },
        {
            "location": "/changelog/#version-022-2015-03-01",
            "text": "Improved robustness of EnsembleClassifier.  Extended plot_decision_regions() functionality for plotting 1D decision boundaries.  Function matplotlib.plot_decision_regions was reorganized  to evaluate.plot_decision_regions .  evaluate.plot_learning_curves() function added.  Added Rosenblatt, gradient descent, and stochastic gradient descent perceptrons.",
            "title": "Version 0.2.2 (2015-03-01)"
        },
        {
            "location": "/changelog/#version-021-2015-01-20",
            "text": "Added mlxtend.pandas.minmax_scaling - a function to rescale pandas DataFrame columns.  Slight update to the EnsembleClassifier interface (additional  voting  parameter)  Fixed EnsembleClassifier to return correct class labels if class labels are not\n  integers from 0 to n.  Added new matplotlib function to plot decision regions of classifiers.",
            "title": "Version 0.2.1 (2015-01-20)"
        },
        {
            "location": "/changelog/#version-020-2015-01-13",
            "text": "Improved mlxtend.text.generalize_duplcheck to remove duplicates and prevent endless looping issue.  Added  recursive  search parameter to mlxtend.file_io.find_files.  Added  check_ext  parameter mlxtend.file_io.find_files to search based on file extensions.  Default parameter to ignore invisible files for mlxtend.file_io.find.  Added  transform  and  fit_transform  to the  EnsembleClassifier .  Added mlxtend.file_io.find_filegroups function.",
            "title": "Version 0.2.0 (2015-01-13)"
        },
        {
            "location": "/changelog/#version-019-2015-01-10",
            "text": "Implemented scikit-learn EnsembleClassifier (majority voting rule) class.",
            "title": "Version 0.1.9 (2015-01-10)"
        },
        {
            "location": "/changelog/#version-018-2015-01-07",
            "text": "Improvements to mlxtend.text.generalize_names to handle certain Dutch last name prefixes (van, van der, de, etc.).  Added mlxtend.text.generalize_name_duplcheck function to apply mlxtend.text.generalize_names function to a pandas DataFrame without creating duplicates.",
            "title": "Version 0.1.8 (2015-01-07)"
        },
        {
            "location": "/changelog/#version-017-2015-01-07",
            "text": "Added text utilities with name generalization function.  Added  and file_io utilities.",
            "title": "Version 0.1.7 (2015-01-07)"
        },
        {
            "location": "/changelog/#version-016-2015-01-04",
            "text": "Added combinations and permutations estimators.",
            "title": "Version 0.1.6 (2015-01-04)"
        },
        {
            "location": "/changelog/#version-015-2014-12-11",
            "text": "Added  DenseTransformer  for pipelines and grid search.",
            "title": "Version 0.1.5 (2014-12-11)"
        },
        {
            "location": "/changelog/#version-014-2014-08-20",
            "text": "mean_centering  function is now a Class that creates  MeanCenterer  objects\n  that can be used to fit data via the  fit  method, and center data at the column\n  means via the  transform  and  fit_transform  method.",
            "title": "Version 0.1.4 (2014-08-20)"
        },
        {
            "location": "/changelog/#version-013-2014-08-19",
            "text": "Added  preprocessing  module and  mean_centering  function.",
            "title": "Version 0.1.3 (2014-08-19)"
        },
        {
            "location": "/changelog/#version-012-2014-08-19",
            "text": "Added  matplotlib  utilities and  remove_borders  function.",
            "title": "Version 0.1.2 (2014-08-19)"
        },
        {
            "location": "/changelog/#version-011-2014-08-13",
            "text": "Simplified code for ColumnSelector.",
            "title": "Version 0.1.1 (2014-08-13)"
        },
        {
            "location": "/contributing/",
            "text": "How to Contribute\n\n\n\n\nI would be very happy about any kind of contributions that help to improve and extend the functionality of mlxtend.\n\n\nQuick Contributor Checklist\n\n\nThis is a quick checklist about the different steps of a typical contribution to mlxtend (and\nother open source projects). Consider copying this list to a local text file (or the issue tracker)\nand checking off items as you go.\n\n\n\n\n[ ]  Open a new \"issue\" on GitHub to discuss the new feature / bug fix  \n\n\n[ ]  Fork the mlxtend repository from GitHub (if not already done earlier)\n\n\n[ ]  Create and check out a new topic branch   \n\n\n[ ]  Implement a new feature or apply the bug-fix  \n\n\n[ ]  Add appropriate unit test functions  \n\n\n[ ]  Run \nnosetests ./mlxtend -sv\n and make sure that all unit tests pass  \n\n\n[ ]  Check/improve the test coverage by running \nnosetests ./mlxtend --with-coverage\n\n\n[ ]  Check for style issues by running \nflake8 ./mlxtend\n (you may want to run \nnosetests\n again after you made modifications to the code)\n\n\n[ ]  Add a note about the modification/contribution to the \n./docs/sources/changelog.md\n file  \n\n\n[ ]  Modify documentation in the appropriate location under \nmlxtend/docs/sources/\n  \n\n\n[ ]  Push the topic branch to the server and create a pull request\n\n\n[ ]  Check the Travis-CI build passed at \nhttps://travis-ci.org/rasbt/mlxtend\n\n\n[ ]  Check/improve the unit test coverage at \nhttps://coveralls.io/github/rasbt/mlxtend\n\n\n[ ]  Check/improve the code health at \nhttps://landscape.io/github/rasbt/mlxtend\n\n\n[ ]  Squash (many small) commits to a larger commit\n\n\n\n\n\n\n\nTips for Contributors\n\n\nGetting Started - Creating a New Issue and Forking the Repository\n\n\n\n\nIf you don't have a \nGitHub\n account, yet, please create one to contribute to this project.\n\n\nPlease submit a ticket for your issue to discuss the fix or new feature before too much time and effort is spent for the implementation.\n\n\n\n\n\n\n\n\nFork the \nmlxtend\n repository from the GitHub web interface.\n\n\n\n\n\n\n\n\nClone the \nmlxtend\n repository to your local machine by executing\n \ngit clone https://github.com/<your_username>/mlxtend.git\n\n\n\n\nSyncing an Existing Fork\n\n\nIf you already forked mlxtend earlier, you can bring you \"Fork\" up to date\nwith the master branch as follows:\n\n\n1. Configuring a remote that points to the upstream repository on GitHub\n\n\nList the current configured remote repository of your fork by executing\n\n\n$ git remote -v\n\n\n\n\nIf you see something like\n\n\norigin  https://github.com/<your username>/mlxtend.git (fetch)\norigin  https://github.com/<your username>/mlxtend.git (push)\n\n\n\n\nyou need to specify a new remote \nupstream\n repository via\n\n\n$ git remote add upstream https://github.com/rasbt/mlxtend.git\n\n\n\n\nNow, verify the new upstream repository you've specified for your fork by executing\n\n\n$ git remote -v\n\n\n\n\nYou should see following output if everything is configured correctly:\n\n\norigin  https://github.com/<your username>/mlxtend.git (fetch)\norigin  https://github.com/<your username>/mlxtend.git (push)\nupstream    https://github.com/rasbt/mlxtend.git (fetch)\nupstream    https://github.com/rasbt/mlxtend.git (push)\n\n\n\n\n2. Syncing your Fork\n\n\nFirst, fetch the updates of the original project's master branch by executing:\n\n\n$ git fetch upstream\n\n\n\n\nYou should see the following output\n\n\nremote: Counting objects: xx, done.\nremote: Compressing objects: 100% (xx/xx), done.\nremote: Total xx (delta xx), reused xx (delta x)\nUnpacking objects: 100% (xx/xx), done.\nFrom https://github.com/rasbt/mlxtend\n * [new branch]      master     -> upstream/master\n\n\n\n\nThis means that the commits to the \nrasbt/mlxtend\n master branch are now\nstored in the local branch \nupstream/master\n.\n\n\nIf you are not already on your local project's master branch, execute\n\n\n$ git checkout master\n\n\n\n\nFinally, merge the changes in upstream/master to your local master branch by\nexecuting\n\n\n$ git merge upstream/master\n\n\n\n\nwhich will give you an output that looks similar to\n\n\nUpdating xxx...xxx\nFast-forward\nSOME FILE1                    |    12 +++++++\nSOME FILE2                    |    10 +++++++\n2 files changed, 22 insertions(+),\n\n\n\n\n*The Main Workflow - Making Changes in a New Topic Branch\n\n\nListed below are the 9 typical steps of a contribution.\n\n\n1. Discussing the Feature or Modification\n\n\nBefore you start coding, please discuss the new feature, bugfix, or other modification to the project\non the project's \nissue tracker\n. Before you open a \"new issue,\" please\ndo a quick search to see if a similar issue has been submitted already.\n\n\n2. Creating a new feature branch\n\n\nPlease avoid working directly on the master branch but create a new feature branch:\n\n\n$ git branch <new_feature>\n\n\n\n\nSwitch to the new feature branch by executing\n\n\n$ git checkout <new_feature>\n\n\n\n\n3. Developing the new feature / bug fix\n\n\nNow it's time to modify existing code or to contribute new code to the project.\n\n\n4. Testing your code\n\n\nAdd the respective unit tests and check if they pass:\n\n\n$ nosetests -sv\n\n\n\n\nUse the \n--with-coverage\n flag to ensure that all code is being covered in the unit tests:\n\n\n$ nosetests --with-coverage\n\n\n\n\n5. Documenting changes\n\n\nPlease add an entry to the \nmlxtend/docs/sources/changelog.md\n file.\nIf it is a new feature, it would also be nice if you could update the documentation in appropriate location in \nmlxtend/sources\n.\n\n\n6. Committing changes\n\n\nWhen you are ready to commit the changes, please provide a meaningful \ncommit\n message:\n\n\n$ git add <modifies_files> # or `git add .`\n$ git commit -m '<meaningful commit message>'\n\n\n\n\n7. Optional: squashing commits\n\n\nIf you made multiple smaller commits, it would be nice if you could group them into a larger, summarizing commit. First, list your recent commit via\n\n\n$ git log\n\n\n\n\nwhich will list the commits from newest to oldest in the following format by default:\n\n\ncommit 046e3af8a9127df8eac879454f029937c8a31c41\nAuthor: rasbt <mail@sebastianraschka.com>\nDate:   Tue Nov 24 03:46:37 2015 -0500\n\n    fixed setup.py\n\ncommit c3c00f6ba0e8f48bbe1c9081b8ae3817e57ecc5c\nAuthor: rasbt <mail@sebastianraschka.com>\nDate:   Tue Nov 24 03:04:39 2015 -0500\n\n        documented feature x\n\ncommit d87934fe8726c46f0b166d6290a3bf38915d6e75\nAuthor: rasbt <mail@sebastianraschka.com>\nDate:   Tue Nov 24 02:44:45 2015 -0500\n\n        added support for feature x\n\n\n\n\nAssuming that it would make sense to group these 3 commits into one, we can execute\n\n\n$ git rebase -i HEAD~3\n\n\n\n\nwhich will bring our default git editor with the following contents:\n\n\npick d87934f added support for feature x\npick c3c00f6 documented feature x\npick 046e3af fixed setup.py\n\n\n\n\nSince \nc3c00f6\n and \n046e3af\n are related to the original commit of \nfeature x\n, let's keep the \nd87934f\n and squash the 2 following commits into this initial one by changes the lines to\n\n\npick d87934f added support for feature x\nsquash c3c00f6 documented feature x\nsquash 046e3af fixed setup.py\n\n\n\n\nNow, save the changes in your editor. Now, quitting the editor will apply the \nrebase\n changes, and the editor will open a second time, prompting you to enter a new commit message. In this case, we could enter \nsupport for feature x\n to summarize the contributions.\n\n\n8. Uploading changes\n\n\nPush your changes to a topic branch to the git server by executing:\n\n\n$ git push origin <feature_branch>\n\n\n\n\n9. Submitting a \npull request\n\n\nGo to your GitHub repository online, select the new feature branch, and submit a new pull request:\n\n\n\n\n\n\n\nNotes for Developers\n\n\nBuilding the documentation\n\n\nThe documentation is built via \nMkDocs\n; to ensure that the documentation is rendered correctly, you can view the documentation locally by executing \nmkdocs serve\n from the \nmlxtend/docs\n directory.\n\n\nFor example,\n\n\n~/github/mlxtend/docs$ mkdocs serve\n\n\n\n\n1. Building the API documentation\n\n\nTo build the API documentation, navigate to \nmlxtend/docs\n and execute the \nmake_api.py\n file from this directory via\n\n\n~/github/mlxtend/docs$ python make_api.py\n\n\n\n\nThis should place the API documentation into the correct directories into the two directories:\n\n\n\n\nmlxtend/docs/sources/api_modules\n\n\nmlxtend/docs/sources/api_subpackes\n\n\n\n\n2. Editing the User Guide\n\n\nThe documents containing code examples for the \"User Guide\" are generated from IPython Notebook files. In order to convert a IPython notebook file to markdown after editing, please follow the following steps:\n\n\n\n\nModify or edit the existing notebook.\n\n\nExecute all cells in the current notebook and make sure that no errors occur.\n\n\nConvert the notebook to markdown using the \nipynb2markdown.py\n converter\n\n\n\n\n~/github/mlxtend/docs$ python ipynb2markdown.py --ipynb_path ./sources/user_guide/subpackage/notebookname.ipynb\n\n\n\n\nNote\n  \n\n\nIf you are adding a new document, please also include it in the pages section in the \nmlxtend/docs/mkdocs.yml\n file.\n\n\n3. Building static HTML files of the documentation\n\n\nFirst, please check the documenation via localhost (http://127.0.0.1:8000/):\n\n\n~/github/mlxtend/docs$ mkdocs serve\n\n\n\n\nNext, build the static HTML files of the mlxtend documentation via\n\n\n~/github/mlxtend/docs$ mkdocs build --clean\n\n\n\n\nTo deploy the documentation, execute\n\n\n~/github/mlxtend/docs$ mkdocs gh-deploy --clean\n\n\n\n\nUploading a new version to PyPI\n\n\n1. Creating a new testing environment\n\n\nAssuming we are using \nconda\n, create a new python environment via\n\n\n$ conda create -n 'mlxtend-testing' python=3 pandas\n\n\n\n\nNext, activate the environment by executing\n\n\n$ source activate mlxtend-testing\n\n\n\n\n2. Installing the package from local files\n\n\nTest the installation by executing\n\n\n$ python setup.py install --record files.txt\n\n\n\n\nthe \n--record files.txt\n flag will create a \nfiles.txt\n file listing the locations where these files will be installed.\n\n\nTry to import the package to see if it works, for example, by executing\n\n\n$ python -c 'import mlxtend; print(mlxtend.__file__)'\n\n\n\n\nIf everything seems to be fine, remove the installation via\n\n\n$ cat files.txt | xargs rm -rf ; rm files.txt\n\n\n\n\nNext, test if \npip\n is able to install the packages. First, navigate to a different directory, and from there, install the package:\n\n\n$ pip install code/mlxtend/\n\n\n\n\nand uninstall it again\n\n\n$ pip uninstall mlxtend\n\n\n\n\n3. Deploying the package\n\n\nConsider deploying the package to the PyPI test server first. The setup instructions can be found \nhere\n.\n\n\n$ python setup.py sdist upload -r https://testpypi.python.org/pypi\n\n\n\n\nTest if it can be installed from there by executing\n\n\n$ pip install -i https://testpypi.python.org/pypi mlxtend\n\n\n\n\nand uninstall it\n\n\n$ pip uninstall mlxtend\n\n\n\n\nAfter this dry-run succeeded, repeat this process using the \"real\" PyPI:\n\n\n$ python setup.py sdist upload\n\n\n\n\n4. Removing the virtual environment\n\n\nFinally, to cleanup our local drive, remove the virtual testing environment via\n\n\n$ conda remove --name 'mlxtend-testing' --all",
            "title": "Contributing"
        },
        {
            "location": "/contributing/#how-to-contribute",
            "text": "I would be very happy about any kind of contributions that help to improve and extend the functionality of mlxtend.",
            "title": "How to Contribute"
        },
        {
            "location": "/contributing/#quick-contributor-checklist",
            "text": "This is a quick checklist about the different steps of a typical contribution to mlxtend (and\nother open source projects). Consider copying this list to a local text file (or the issue tracker)\nand checking off items as you go.   [ ]  Open a new \"issue\" on GitHub to discuss the new feature / bug fix    [ ]  Fork the mlxtend repository from GitHub (if not already done earlier)  [ ]  Create and check out a new topic branch     [ ]  Implement a new feature or apply the bug-fix    [ ]  Add appropriate unit test functions    [ ]  Run  nosetests ./mlxtend -sv  and make sure that all unit tests pass    [ ]  Check/improve the test coverage by running  nosetests ./mlxtend --with-coverage  [ ]  Check for style issues by running  flake8 ./mlxtend  (you may want to run  nosetests  again after you made modifications to the code)  [ ]  Add a note about the modification/contribution to the  ./docs/sources/changelog.md  file    [ ]  Modify documentation in the appropriate location under  mlxtend/docs/sources/     [ ]  Push the topic branch to the server and create a pull request  [ ]  Check the Travis-CI build passed at  https://travis-ci.org/rasbt/mlxtend  [ ]  Check/improve the unit test coverage at  https://coveralls.io/github/rasbt/mlxtend  [ ]  Check/improve the code health at  https://landscape.io/github/rasbt/mlxtend  [ ]  Squash (many small) commits to a larger commit",
            "title": "Quick Contributor Checklist"
        },
        {
            "location": "/contributing/#tips-for-contributors",
            "text": "",
            "title": "Tips for Contributors"
        },
        {
            "location": "/contributing/#getting-started-creating-a-new-issue-and-forking-the-repository",
            "text": "If you don't have a  GitHub  account, yet, please create one to contribute to this project.  Please submit a ticket for your issue to discuss the fix or new feature before too much time and effort is spent for the implementation.     Fork the  mlxtend  repository from the GitHub web interface.     Clone the  mlxtend  repository to your local machine by executing\n  git clone https://github.com/<your_username>/mlxtend.git",
            "title": "Getting Started - Creating a New Issue and Forking the Repository"
        },
        {
            "location": "/contributing/#syncing-an-existing-fork",
            "text": "If you already forked mlxtend earlier, you can bring you \"Fork\" up to date\nwith the master branch as follows:",
            "title": "Syncing an Existing Fork"
        },
        {
            "location": "/contributing/#1-configuring-a-remote-that-points-to-the-upstream-repository-on-github",
            "text": "List the current configured remote repository of your fork by executing  $ git remote -v  If you see something like  origin  https://github.com/<your username>/mlxtend.git (fetch)\norigin  https://github.com/<your username>/mlxtend.git (push)  you need to specify a new remote  upstream  repository via  $ git remote add upstream https://github.com/rasbt/mlxtend.git  Now, verify the new upstream repository you've specified for your fork by executing  $ git remote -v  You should see following output if everything is configured correctly:  origin  https://github.com/<your username>/mlxtend.git (fetch)\norigin  https://github.com/<your username>/mlxtend.git (push)\nupstream    https://github.com/rasbt/mlxtend.git (fetch)\nupstream    https://github.com/rasbt/mlxtend.git (push)",
            "title": "1. Configuring a remote that points to the upstream repository on GitHub"
        },
        {
            "location": "/contributing/#2-syncing-your-fork",
            "text": "First, fetch the updates of the original project's master branch by executing:  $ git fetch upstream  You should see the following output  remote: Counting objects: xx, done.\nremote: Compressing objects: 100% (xx/xx), done.\nremote: Total xx (delta xx), reused xx (delta x)\nUnpacking objects: 100% (xx/xx), done.\nFrom https://github.com/rasbt/mlxtend\n * [new branch]      master     -> upstream/master  This means that the commits to the  rasbt/mlxtend  master branch are now\nstored in the local branch  upstream/master .  If you are not already on your local project's master branch, execute  $ git checkout master  Finally, merge the changes in upstream/master to your local master branch by\nexecuting  $ git merge upstream/master  which will give you an output that looks similar to  Updating xxx...xxx\nFast-forward\nSOME FILE1                    |    12 +++++++\nSOME FILE2                    |    10 +++++++\n2 files changed, 22 insertions(+),",
            "title": "2. Syncing your Fork"
        },
        {
            "location": "/contributing/#the-main-workflow-making-changes-in-a-new-topic-branch",
            "text": "Listed below are the 9 typical steps of a contribution.",
            "title": "*The Main Workflow - Making Changes in a New Topic Branch"
        },
        {
            "location": "/contributing/#1-discussing-the-feature-or-modification",
            "text": "Before you start coding, please discuss the new feature, bugfix, or other modification to the project\non the project's  issue tracker . Before you open a \"new issue,\" please\ndo a quick search to see if a similar issue has been submitted already.",
            "title": "1. Discussing the Feature or Modification"
        },
        {
            "location": "/contributing/#2-creating-a-new-feature-branch",
            "text": "Please avoid working directly on the master branch but create a new feature branch:  $ git branch <new_feature>  Switch to the new feature branch by executing  $ git checkout <new_feature>",
            "title": "2. Creating a new feature branch"
        },
        {
            "location": "/contributing/#3-developing-the-new-feature-bug-fix",
            "text": "Now it's time to modify existing code or to contribute new code to the project.",
            "title": "3. Developing the new feature / bug fix"
        },
        {
            "location": "/contributing/#4-testing-your-code",
            "text": "Add the respective unit tests and check if they pass:  $ nosetests -sv  Use the  --with-coverage  flag to ensure that all code is being covered in the unit tests:  $ nosetests --with-coverage",
            "title": "4. Testing your code"
        },
        {
            "location": "/contributing/#5-documenting-changes",
            "text": "Please add an entry to the  mlxtend/docs/sources/changelog.md  file.\nIf it is a new feature, it would also be nice if you could update the documentation in appropriate location in  mlxtend/sources .",
            "title": "5. Documenting changes"
        },
        {
            "location": "/contributing/#6-committing-changes",
            "text": "When you are ready to commit the changes, please provide a meaningful  commit  message:  $ git add <modifies_files> # or `git add .`\n$ git commit -m '<meaningful commit message>'",
            "title": "6. Committing changes"
        },
        {
            "location": "/contributing/#7-optional-squashing-commits",
            "text": "If you made multiple smaller commits, it would be nice if you could group them into a larger, summarizing commit. First, list your recent commit via  $ git log  which will list the commits from newest to oldest in the following format by default:  commit 046e3af8a9127df8eac879454f029937c8a31c41\nAuthor: rasbt <mail@sebastianraschka.com>\nDate:   Tue Nov 24 03:46:37 2015 -0500\n\n    fixed setup.py\n\ncommit c3c00f6ba0e8f48bbe1c9081b8ae3817e57ecc5c\nAuthor: rasbt <mail@sebastianraschka.com>\nDate:   Tue Nov 24 03:04:39 2015 -0500\n\n        documented feature x\n\ncommit d87934fe8726c46f0b166d6290a3bf38915d6e75\nAuthor: rasbt <mail@sebastianraschka.com>\nDate:   Tue Nov 24 02:44:45 2015 -0500\n\n        added support for feature x  Assuming that it would make sense to group these 3 commits into one, we can execute  $ git rebase -i HEAD~3  which will bring our default git editor with the following contents:  pick d87934f added support for feature x\npick c3c00f6 documented feature x\npick 046e3af fixed setup.py  Since  c3c00f6  and  046e3af  are related to the original commit of  feature x , let's keep the  d87934f  and squash the 2 following commits into this initial one by changes the lines to  pick d87934f added support for feature x\nsquash c3c00f6 documented feature x\nsquash 046e3af fixed setup.py  Now, save the changes in your editor. Now, quitting the editor will apply the  rebase  changes, and the editor will open a second time, prompting you to enter a new commit message. In this case, we could enter  support for feature x  to summarize the contributions.",
            "title": "7. Optional: squashing commits"
        },
        {
            "location": "/contributing/#8-uploading-changes",
            "text": "Push your changes to a topic branch to the git server by executing:  $ git push origin <feature_branch>",
            "title": "8. Uploading changes"
        },
        {
            "location": "/contributing/#9-submitting-a-pull-request",
            "text": "Go to your GitHub repository online, select the new feature branch, and submit a new pull request:",
            "title": "9. Submitting a pull request"
        },
        {
            "location": "/contributing/#notes-for-developers",
            "text": "",
            "title": "Notes for Developers"
        },
        {
            "location": "/contributing/#building-the-documentation",
            "text": "The documentation is built via  MkDocs ; to ensure that the documentation is rendered correctly, you can view the documentation locally by executing  mkdocs serve  from the  mlxtend/docs  directory.  For example,  ~/github/mlxtend/docs$ mkdocs serve",
            "title": "Building the documentation"
        },
        {
            "location": "/contributing/#1-building-the-api-documentation",
            "text": "To build the API documentation, navigate to  mlxtend/docs  and execute the  make_api.py  file from this directory via  ~/github/mlxtend/docs$ python make_api.py  This should place the API documentation into the correct directories into the two directories:   mlxtend/docs/sources/api_modules  mlxtend/docs/sources/api_subpackes",
            "title": "1. Building the API documentation"
        },
        {
            "location": "/contributing/#2-editing-the-user-guide",
            "text": "The documents containing code examples for the \"User Guide\" are generated from IPython Notebook files. In order to convert a IPython notebook file to markdown after editing, please follow the following steps:   Modify or edit the existing notebook.  Execute all cells in the current notebook and make sure that no errors occur.  Convert the notebook to markdown using the  ipynb2markdown.py  converter   ~/github/mlxtend/docs$ python ipynb2markdown.py --ipynb_path ./sources/user_guide/subpackage/notebookname.ipynb  Note     If you are adding a new document, please also include it in the pages section in the  mlxtend/docs/mkdocs.yml  file.",
            "title": "2. Editing the User Guide"
        },
        {
            "location": "/contributing/#3-building-static-html-files-of-the-documentation",
            "text": "First, please check the documenation via localhost (http://127.0.0.1:8000/):  ~/github/mlxtend/docs$ mkdocs serve  Next, build the static HTML files of the mlxtend documentation via  ~/github/mlxtend/docs$ mkdocs build --clean  To deploy the documentation, execute  ~/github/mlxtend/docs$ mkdocs gh-deploy --clean",
            "title": "3. Building static HTML files of the documentation"
        },
        {
            "location": "/contributing/#uploading-a-new-version-to-pypi",
            "text": "",
            "title": "Uploading a new version to PyPI"
        },
        {
            "location": "/contributing/#1-creating-a-new-testing-environment",
            "text": "Assuming we are using  conda , create a new python environment via  $ conda create -n 'mlxtend-testing' python=3 pandas  Next, activate the environment by executing  $ source activate mlxtend-testing",
            "title": "1. Creating a new testing environment"
        },
        {
            "location": "/contributing/#2-installing-the-package-from-local-files",
            "text": "Test the installation by executing  $ python setup.py install --record files.txt  the  --record files.txt  flag will create a  files.txt  file listing the locations where these files will be installed.  Try to import the package to see if it works, for example, by executing  $ python -c 'import mlxtend; print(mlxtend.__file__)'  If everything seems to be fine, remove the installation via  $ cat files.txt | xargs rm -rf ; rm files.txt  Next, test if  pip  is able to install the packages. First, navigate to a different directory, and from there, install the package:  $ pip install code/mlxtend/  and uninstall it again  $ pip uninstall mlxtend",
            "title": "2. Installing the package from local files"
        },
        {
            "location": "/contributing/#3-deploying-the-package",
            "text": "Consider deploying the package to the PyPI test server first. The setup instructions can be found  here .  $ python setup.py sdist upload -r https://testpypi.python.org/pypi  Test if it can be installed from there by executing  $ pip install -i https://testpypi.python.org/pypi mlxtend  and uninstall it  $ pip uninstall mlxtend  After this dry-run succeeded, repeat this process using the \"real\" PyPI:  $ python setup.py sdist upload",
            "title": "3. Deploying the package"
        },
        {
            "location": "/contributing/#4-removing-the-virtual-environment",
            "text": "Finally, to cleanup our local drive, remove the virtual testing environment via  $ conda remove --name 'mlxtend-testing' --all",
            "title": "4. Removing the virtual environment"
        },
        {
            "location": "/license/",
            "text": "License\n\n\n\n\nNew BSD License\n\n\nCopyright (c) 2014-2016, Sebastian Raschka. All rights reserved.\n\n\nRedistribution and use in source and binary forms, with or without\nmodification, are permitted provided that the following conditions are met:\n\n\n\n\n\n\nRedistributions of source code must retain the above copyright notice, this\n  list of conditions and the following disclaimer.\n\n\n\n\n\n\nRedistributions in binary form must reproduce the above copyright notice,\n  this list of conditions and the following disclaimer in the documentation\n  and/or other materials provided with the distribution.\n\n\n\n\n\n\nNeither the name of mlxtend nor the names of its\n  contributors may be used to endorse or promote products derived from\n  this software without specific prior written permission.\n\n\n\n\n\n\nTHIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS \"AS IS\"\nAND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE\nIMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE\nDISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE LIABLE\nFOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL\nDAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR\nSERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER\nCAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY,\nOR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE\nOF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.",
            "title": "License"
        },
        {
            "location": "/license/#license",
            "text": "New BSD License  Copyright (c) 2014-2016, Sebastian Raschka. All rights reserved.  Redistribution and use in source and binary forms, with or without\nmodification, are permitted provided that the following conditions are met:    Redistributions of source code must retain the above copyright notice, this\n  list of conditions and the following disclaimer.    Redistributions in binary form must reproduce the above copyright notice,\n  this list of conditions and the following disclaimer in the documentation\n  and/or other materials provided with the distribution.    Neither the name of mlxtend nor the names of its\n  contributors may be used to endorse or promote products derived from\n  this software without specific prior written permission.    THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS \"AS IS\"\nAND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE\nIMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE\nDISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE LIABLE\nFOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL\nDAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR\nSERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER\nCAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY,\nOR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE\nOF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.",
            "title": "License"
        }
    ]
}
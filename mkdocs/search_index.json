{
    "docs": [
        {
            "location": "/",
            "text": "Welcome to mlxtend\u2019s documentation!\n\n\nMlxtend (machine learning extensions) is a Python library of useful tools for the day-to-day data science tasks.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLinks\n\n\n\n\nDocumentation:\n\n\nhtml: \nhttp://rasbt.github.io/mlxtend/\n\n\npdf: \nhttp://sebastianraschka.com/pdf/software/mlxtend-latest.pdf\n\n\n\n\n\n\nSource code repository: \nhttps://github.com/rasbt/mlxtend\n\n\nPyPI: \nhttps://pypi.python.org/pypi/mlxtend\n\n\nQuestions? Check out the \nGoogle Groups mailing list\n\n\n\n\n\n\n\nExamples\n\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport matplotlib.gridspec as gridspec\nimport itertools\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import SVC\nfrom sklearn.ensemble import RandomForestClassifier\nfrom mlxtend.classifier import EnsembleVoteClassifier\nfrom mlxtend.data import iris_data\nfrom mlxtend.evaluate import plot_decision_regions\n\n# Initializing Classifiers\nclf1 = LogisticRegression(random_state=0)\nclf2 = RandomForestClassifier(random_state=0)\nclf3 = SVC(random_state=0, probability=True)\neclf = EnsembleVoteClassifier(clfs=[clf1, clf2, clf3],\n                              weights=[2, 1, 1], voting='soft')\n\n# Loading some example data\nX, y = iris_data()\nX = X[:,[0, 2]]\n\n# Plotting Decision Regions\n\ngs = gridspec.GridSpec(2, 2)\nfig = plt.figure(figsize=(10, 8))\n\nlabels = ['Logistic Regression',\n          'Random Forest',\n          'RBF kernel SVM',\n          'Ensemble']\n\nfor clf, lab, grd in zip([clf1, clf2, clf3, eclf],\n                         labels,\n                         itertools.product([0, 1],\n                         repeat=2)):\n    clf.fit(X, y)\n    ax = plt.subplot(gs[grd[0], grd[1]])\n    fig = plot_decision_regions(X=X, y=y,\n                                clf=clf, legend=2)\n    plt.title(lab)\n\nplt.show()\n\n\n\n\n\n\n\n\nIf you use mlxtend as part of your workflow in a scientific publication, please consider citing the mlxtend repository with the following DOI:\n\n\n\n\n@misc{raschkas_2016_49235,\n  author       = {Raschka, Sebastian},\n  title        = {Mlxtend},\n  month        = apr,\n  year         = 2016,\n  doi          = {10.5281/zenodo.49235},\n  url          = {http://dx.doi.org/10.5281/zenodo.49235}\n}\n\n\n\n\nLicense\n\n\n\n\nThis project is released under a permissive new BSD open source \nlicense\n and commercially usable. There is no warranty; not even for merchantability or fitness for a particular purpose.\n\n\nIn addition, you may use, copy, modify and redistribute all artistic creative works (figures and images) included in this distribution under the directory\naccording to the terms and conditions of the Creative Commons Attribution 4.0 International License.  See the file \nLICENSE-CC-BY.txt\n for details. (Computer-generated graphics such as the plots produced by matplotlib fall under the BSD license mentioned above).\n\n\n\n\nContact\n\n\nI received a lot of feedback and questions about mlxtend recently, and I thought that it would be worthwhile to set up a public communication channel. Before you write an email with a question about mlxtend, please consider posting it here since it can also be useful to others! Please join the \nGoogle Groups Mailing List\n!\n\n\nIf Google Groups is not for you, please feel free to write me an \nemail\n or consider filing an issue on \nGitHub's issue tracker\n for new feature requests or bug reports. In addition, I setup a \nGitter channel\n for live discussions.",
            "title": "Home"
        },
        {
            "location": "/#welcome-to-mlxtends-documentation",
            "text": "Mlxtend (machine learning extensions) is a Python library of useful tools for the day-to-day data science tasks.",
            "title": "Welcome to mlxtend\u2019s documentation!"
        },
        {
            "location": "/#links",
            "text": "Documentation:  html:  http://rasbt.github.io/mlxtend/  pdf:  http://sebastianraschka.com/pdf/software/mlxtend-latest.pdf    Source code repository:  https://github.com/rasbt/mlxtend  PyPI:  https://pypi.python.org/pypi/mlxtend  Questions? Check out the  Google Groups mailing list",
            "title": "Links"
        },
        {
            "location": "/#examples",
            "text": "import numpy as np\nimport matplotlib.pyplot as plt\nimport matplotlib.gridspec as gridspec\nimport itertools\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import SVC\nfrom sklearn.ensemble import RandomForestClassifier\nfrom mlxtend.classifier import EnsembleVoteClassifier\nfrom mlxtend.data import iris_data\nfrom mlxtend.evaluate import plot_decision_regions\n\n# Initializing Classifiers\nclf1 = LogisticRegression(random_state=0)\nclf2 = RandomForestClassifier(random_state=0)\nclf3 = SVC(random_state=0, probability=True)\neclf = EnsembleVoteClassifier(clfs=[clf1, clf2, clf3],\n                              weights=[2, 1, 1], voting='soft')\n\n# Loading some example data\nX, y = iris_data()\nX = X[:,[0, 2]]\n\n# Plotting Decision Regions\n\ngs = gridspec.GridSpec(2, 2)\nfig = plt.figure(figsize=(10, 8))\n\nlabels = ['Logistic Regression',\n          'Random Forest',\n          'RBF kernel SVM',\n          'Ensemble']\n\nfor clf, lab, grd in zip([clf1, clf2, clf3, eclf],\n                         labels,\n                         itertools.product([0, 1],\n                         repeat=2)):\n    clf.fit(X, y)\n    ax = plt.subplot(gs[grd[0], grd[1]])\n    fig = plot_decision_regions(X=X, y=y,\n                                clf=clf, legend=2)\n    plt.title(lab)\n\nplt.show()    If you use mlxtend as part of your workflow in a scientific publication, please consider citing the mlxtend repository with the following DOI:   @misc{raschkas_2016_49235,\n  author       = {Raschka, Sebastian},\n  title        = {Mlxtend},\n  month        = apr,\n  year         = 2016,\n  doi          = {10.5281/zenodo.49235},\n  url          = {http://dx.doi.org/10.5281/zenodo.49235}\n}",
            "title": "Examples"
        },
        {
            "location": "/#license",
            "text": "This project is released under a permissive new BSD open source  license  and commercially usable. There is no warranty; not even for merchantability or fitness for a particular purpose.  In addition, you may use, copy, modify and redistribute all artistic creative works (figures and images) included in this distribution under the directory\naccording to the terms and conditions of the Creative Commons Attribution 4.0 International License.  See the file  LICENSE-CC-BY.txt  for details. (Computer-generated graphics such as the plots produced by matplotlib fall under the BSD license mentioned above).",
            "title": "License"
        },
        {
            "location": "/#contact",
            "text": "I received a lot of feedback and questions about mlxtend recently, and I thought that it would be worthwhile to set up a public communication channel. Before you write an email with a question about mlxtend, please consider posting it here since it can also be useful to others! Please join the  Google Groups Mailing List !  If Google Groups is not for you, please feel free to write me an  email  or consider filing an issue on  GitHub's issue tracker  for new feature requests or bug reports. In addition, I setup a  Gitter channel  for live discussions.",
            "title": "Contact"
        },
        {
            "location": "/USER_GUIDE_INDEX/",
            "text": "User Guide Index\n\n\nclassifier\n\n\n\n\nEnsembleVoteClassifier\n\n\nStackingClassifier\n\n\nPerceptron\n\n\nAdaline\n\n\nLogisticRegression\n\n\nMultiLayerPerceptron\n\n\nSoftmaxRegression\n\n\n\n\ntf_classifier\n (TensorFlow Classifier)\n\n\n\n\nTfSoftmaxRegression\n\n\nTfMultiLayerPerceptron\n\n\n\n\nregressor\n\n\n\n\nLinearRegression\n\n\nStackingRegressor\n\n\n\n\ntf_regressor\n (TensorFlow Regressor)\n\n\n\n\nTfLinearRegression\n (new in 0.4.1dev)\n\n\n\n\nregression_utils\n\n\n\n\nplot_linear_regression\n\n\n\n\nfeature_selection\n\n\n\n\nSequentialFeatureSelector\n\n\nColumnSelector\n\n\n\n\nfeature_extraction\n\n\n\n\nPrincipalComponentAnalysis\n\n\nLinearDiscriminantAnalysis\n\n\nRBFKernelPCA\n\n\n\n\ncluster\n\n\n\n\nKmeans\n (new in 0.4.1dev)\n\n\n\n\ntf_cluster\n\n\n\n\nTfKmeans\n (new in 0.4.1dev)\n\n\n\n\nevaluate\n\n\n\n\nconfusion_matrix\n\n\nplot_decision_regions\n\n\nplot_learning_curves\n\n\nscoring\n\n\n\n\npreprocessing\n\n\n\n\nMeanCenterer\n\n\nminmax_scaling\n\n\nshuffle_arrays_unison\n\n\nstandardize\n\n\none-hot_encoding\n\n\nDenseTransformer\n\n\nCopyTransformer\n\n\n\n\ndata\n\n\n\n\nautompg_data\n\n\nboston_housing_data\n\n\niris_data\n\n\nmnist_data\n\n\nload_mnist\n\n\nwine_data\n\n\nthree_blobs\n\n\n\n\nfile_io\n\n\n\n\nfind_filegroups\n\n\nfind_files\n\n\n\n\ngeneral plotting\n\n\n\n\ncategory_scatter\n\n\nenrichment_plot\n\n\nstacked_barplot\n\n\n\n\nmath\n\n\n\n\nnum_combinations\n\n\nnum_permutations\n\n\n\n\ntext\n\n\n\n\ngeneralize_names\n\n\ngeneralize_names_duplcheck\n\n\ntokenizer.\n\n\n\n\nutils\n\n\n\n\nCounter\n\n\n\n\ngeneral concepts\n\n\n\n\nactivation-functions\n\n\ngradient-optimization\n\n\nlinear-gradient-derivative\n\n\nregularization-linear",
            "title": "USER GUIDE INDEX"
        },
        {
            "location": "/USER_GUIDE_INDEX/#user-guide-index",
            "text": "",
            "title": "User Guide Index"
        },
        {
            "location": "/USER_GUIDE_INDEX/#classifier",
            "text": "EnsembleVoteClassifier  StackingClassifier  Perceptron  Adaline  LogisticRegression  MultiLayerPerceptron  SoftmaxRegression",
            "title": "classifier"
        },
        {
            "location": "/USER_GUIDE_INDEX/#tf_classifier-tensorflow-classifier",
            "text": "TfSoftmaxRegression  TfMultiLayerPerceptron",
            "title": "tf_classifier (TensorFlow Classifier)"
        },
        {
            "location": "/USER_GUIDE_INDEX/#regressor",
            "text": "LinearRegression  StackingRegressor",
            "title": "regressor"
        },
        {
            "location": "/USER_GUIDE_INDEX/#tf_regressor-tensorflow-regressor",
            "text": "TfLinearRegression  (new in 0.4.1dev)",
            "title": "tf_regressor (TensorFlow Regressor)"
        },
        {
            "location": "/USER_GUIDE_INDEX/#regression_utils",
            "text": "plot_linear_regression",
            "title": "regression_utils"
        },
        {
            "location": "/USER_GUIDE_INDEX/#feature_selection",
            "text": "SequentialFeatureSelector  ColumnSelector",
            "title": "feature_selection"
        },
        {
            "location": "/USER_GUIDE_INDEX/#feature_extraction",
            "text": "PrincipalComponentAnalysis  LinearDiscriminantAnalysis  RBFKernelPCA",
            "title": "feature_extraction"
        },
        {
            "location": "/USER_GUIDE_INDEX/#cluster",
            "text": "Kmeans  (new in 0.4.1dev)",
            "title": "cluster"
        },
        {
            "location": "/USER_GUIDE_INDEX/#tf_cluster",
            "text": "TfKmeans  (new in 0.4.1dev)",
            "title": "tf_cluster"
        },
        {
            "location": "/USER_GUIDE_INDEX/#evaluate",
            "text": "confusion_matrix  plot_decision_regions  plot_learning_curves  scoring",
            "title": "evaluate"
        },
        {
            "location": "/USER_GUIDE_INDEX/#preprocessing",
            "text": "MeanCenterer  minmax_scaling  shuffle_arrays_unison  standardize  one-hot_encoding  DenseTransformer  CopyTransformer",
            "title": "preprocessing"
        },
        {
            "location": "/USER_GUIDE_INDEX/#data",
            "text": "autompg_data  boston_housing_data  iris_data  mnist_data  load_mnist  wine_data  three_blobs",
            "title": "data"
        },
        {
            "location": "/USER_GUIDE_INDEX/#file_io",
            "text": "find_filegroups  find_files",
            "title": "file_io"
        },
        {
            "location": "/USER_GUIDE_INDEX/#general-plotting",
            "text": "category_scatter  enrichment_plot  stacked_barplot",
            "title": "general plotting"
        },
        {
            "location": "/USER_GUIDE_INDEX/#math",
            "text": "num_combinations  num_permutations",
            "title": "math"
        },
        {
            "location": "/USER_GUIDE_INDEX/#text",
            "text": "generalize_names  generalize_names_duplcheck  tokenizer.",
            "title": "text"
        },
        {
            "location": "/USER_GUIDE_INDEX/#utils",
            "text": "Counter",
            "title": "utils"
        },
        {
            "location": "/USER_GUIDE_INDEX/#general-concepts",
            "text": "activation-functions  gradient-optimization  linear-gradient-derivative  regularization-linear",
            "title": "general concepts"
        },
        {
            "location": "/user_guide/classifier/EnsembleVoteClassifier/",
            "text": "EnsembleVoteClassifier\n\n\nImplementation of a majority voting \nEnsembleVoteClassifier\n for classification.\n\n\n\n\nfrom mlxtend.classifier import EnsembleVoteClassifier\n\n\n\n\nOverview\n\n\nThe \nEnsembleVoteClassifier\n is a meta-classifier for combining similar or conceptually different machine learning classifiers for classification via majority or plurality voting. (For simplicity, we will refer to both majority and plurality voting as majority voting.)\n\n\n\n\nThe \nEnsembleVoteClassifier\n implements \"hard\" and \"soft\" voting. In hard voting, we predict the final class label as the class label that has been predicted most frequently by the classification models. In soft voting, we predict the class labels by averaging the class-probabilities (only recommended if the classifiers are well-calibrated).\n\n\n\n\nNote\n\n\nIf you are interested in using the \nEnsembleVoteClassifier\n, please note that it is now also available through scikit learn (>0.17) as \nVotingClassifier\n.\n\n\nMajority Voting / Hard Voting\n\n\nHard voting is the simplest case of majority voting. Here, we predict the class label \n$\\hat{y}$\n via majority (plurality) voting of each classifier \n$C_j$\n:\n\n\n$$\\hat{y}=mode\\{C_1(\\mathbf{x}), C_2(\\mathbf{x}), ..., C_m(\\mathbf{x})\\}$$\n\n\nAssuming that we combine three classifiers that classify a training sample as follows:\n\n\n\n\nclassifier 1 -> class 0\n\n\nclassifier 2 -> class 0\n\n\nclassifier 3 -> class 1\n\n\n\n\n$$\\hat{y}=mode\\{0, 0, 1\\} = 0$$\n\n\nVia majority vote, we would we would classify the sample as \"class 0.\"\n\n\nWeighted Majority Vote\n\n\nIn addition to the simple majority vote (hard voting) as described in the previous section, we can compute a weighted majority vote by associating a weight \n$w_j$\n with classifier \n$C_j$\n:\n\n\n$$\\hat{y} = \\arg \\max_i \\sum^{m}_{j=1} w_j \\chi_A \\big(C_j(\\mathbf{x})=i\\big),$$\n\n\nwhere \n$\\chi_A$\n is the characteristic function \n$[C_j(\\mathbf{x}) = i \\; \\in A]$\n, and \n$A$\n is the set of unique class labels. \n\n\nContinuing with the example from the previous section\n\n\n\n\nclassifier 1 -> class 0\n\n\nclassifier 2 -> class 0\n\n\nclassifier 3 -> class 1\n\n\n\n\nassigning the weights {0.2, 0.2, 0.6} would yield a prediction \n$\\hat{y} = 1$\n:\n\n\n$$\\arg \\max_i [0.2 \\times i_0 + 0.2 \\times i_0 + 0.6 \\times i_1] = 1$$\n\n\nSoft Voting\n\n\nIn soft voting, we predict the class labels based on the predicted probabilities \n$p$\n for classifier -- this approach is only recommended if the classifiers are well-calibrated.\n\n\n$$\\hat{y} = \\arg \\max_i \\sum^{m}_{j=1} w_j p_{ij},$$\n\n\nwhere \n$w_j$\n is the weight that can be assigned to the \n$j$\nth classifier.\n\n\nAssuming the example in the previous section was a binary classification task with class labels \n$i \\in \\{0, 1\\}$\n, our ensemble could make the following prediction:\n\n\n\n\n$C_1(\\mathbf{x}) \\rightarrow [0.9, 0.1]$\n\n\n$C_2(\\mathbf{x}) \\rightarrow [0.8, 0.2]$\n\n\n$C_3(\\mathbf{x}) \\rightarrow [0.4, 0.6]$\n\n\n\n\nUsing uniform weights, we compute the average probabilities:\n\n\n$$p(i_0 \\mid \\mathbf{x}) = \\frac{0.9 + 0.8 + 0.4}{3} = 0.7 \\\\\\\\\np(i_1 \\mid \\mathbf{x}) = \\frac{0.1 + 0.2 + 0.6}{3} = 0.3$$\n\n\n$$\\hat{y} = \\arg \\max_i \\big[p(i_0 \\mid \\mathbf{x}), p(i_1 \\mid \\mathbf{x}) \\big] = 0$$\n\n\nHowever, assigning the weights {0.1, 0.1, 0.8} would yield a prediction \n$\\hat{y} = 1$\n:\n\n\n$$p(i_0 \\mid \\mathbf{x}) = {0.1 \\times 0.9 + 0.1 \\times 0.8 + 0.8 \\times  0.4} = 0.49 \\\\\\\\\np(i_1 \\mid \\mathbf{x}) = {0.1 \\times 0.1 + 0.2 \\times 0.1 + 0.8 \\times 0.6} = 0.51$$\n\n\n$$\\hat{y} = \\arg \\max_i \\big[p(i_0 \\mid \\mathbf{x}), p(i_1 \\mid \\mathbf{x}) \\big] = 1$$\n\n\nReferences\n\n\n\n\n[1] S. Raschka. \nPython Machine Learning\n. Packt Publishing Ltd., 2015.\n\n\n\n\nExamples\n\n\nExample 1 -  Classifying Iris Flowers Using Different Classification Models\n\n\nfrom sklearn import datasets\n\niris = datasets.load_iris()\nX, y = iris.data[:, 1:3], iris.target\n\n\n\n\nfrom sklearn import cross_validation\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.naive_bayes import GaussianNB \nfrom sklearn.ensemble import RandomForestClassifier\nimport numpy as np\n\nclf1 = LogisticRegression(random_state=1)\nclf2 = RandomForestClassifier(random_state=1)\nclf3 = GaussianNB()\n\nprint('5-fold cross validation:\\n')\n\nlabels = ['Logistic Regression', 'Random Forest', 'Naive Bayes']\n\nfor clf, label in zip([clf1, clf2, clf3], labels):\n\n    scores = cross_validation.cross_val_score(clf, X, y, \n                                              cv=5, \n                                              scoring='accuracy')\n    print(\"Accuracy: %0.2f (+/- %0.2f) [%s]\"\n          % (scores.mean(), scores.std(), label))\n\n\n\n\n5-fold cross validation:\n\nAccuracy: 0.90 (+/- 0.05) [Logistic Regression]\nAccuracy: 0.93 (+/- 0.05) [Random Forest]\nAccuracy: 0.91 (+/- 0.04) [Naive Bayes]\n\n\n\nfrom mlxtend.classifier import EnsembleVoteClassifier\n\neclf = EnsembleVoteClassifier(clfs=[clf1, clf2, clf3], weights=[1,1,1])\n\nlabels = ['Logistic Regression', 'Random Forest', 'Naive Bayes', 'Ensemble']\nfor clf, label in zip([clf1, clf2, clf3, eclf], labels):\n\n    scores = cross_validation.cross_val_score(clf, X, y, \n                                              cv=5, \n                                              scoring='accuracy')\n    print(\"Accuracy: %0.2f (+/- %0.2f) [%s]\" \n          % (scores.mean(), scores.std(), label))\n\n\n\n\nAccuracy: 0.90 (+/- 0.05) [Logistic Regression]\nAccuracy: 0.93 (+/- 0.05) [Random Forest]\nAccuracy: 0.91 (+/- 0.04) [Naive Bayes]\nAccuracy: 0.95 (+/- 0.05) [Ensemble]\n\n\n\nPlotting Decision Regions\n\n\nimport matplotlib.pyplot as plt\nfrom mlxtend.evaluate import plot_decision_regions\nimport matplotlib.gridspec as gridspec\nimport itertools\n\ngs = gridspec.GridSpec(2, 2)\n\nfig = plt.figure(figsize=(10,8))\n\nlabels = ['Logistic Regression', 'Random Forest', 'Naive Bayes', 'Ensemble']\nfor clf, lab, grd in zip([clf1, clf2, clf3, eclf],\n                         labels,\n                         itertools.product([0, 1], repeat=2)):\n\n    clf.fit(X, y)\n    ax = plt.subplot(gs[grd[0], grd[1]])\n    fig = plot_decision_regions(X=X, y=y, clf=clf)\n    plt.title(lab)\n\n\n\n\n\n\nExample 2 - Grid Search\n\n\nfrom sklearn import datasets\n\niris = datasets.load_iris()\nX, y = iris.data[:, 1:3], iris.target\n\n\n\n\nfrom sklearn.grid_search import GridSearchCV\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.naive_bayes import GaussianNB \nfrom sklearn.ensemble import RandomForestClassifier\nfrom mlxtend.classifier import EnsembleVoteClassifier\n\nclf1 = LogisticRegression(random_state=1)\nclf2 = RandomForestClassifier(random_state=1)\nclf3 = GaussianNB()\neclf = EnsembleVoteClassifier(clfs=[clf1, clf2, clf3], voting='soft')\n\nparams = {'logisticregression__C': [1.0, 100.0],\n          'randomforestclassifier__n_estimators': [20, 200],}\n\ngrid = GridSearchCV(estimator=eclf, param_grid=params, cv=5)\ngrid.fit(iris.data, iris.target)\n\nfor params, mean_score, scores in grid.grid_scores_:\n    print(\"%0.3f (+/-%0.03f) for %r\"\n        % (mean_score, scores.std() / 2, params))\n\n\n\n\n0.953 (+/-0.013) for {'randomforestclassifier__n_estimators': 20, 'logisticregression__C': 1.0}\n0.960 (+/-0.012) for {'randomforestclassifier__n_estimators': 200, 'logisticregression__C': 1.0}\n0.960 (+/-0.012) for {'randomforestclassifier__n_estimators': 20, 'logisticregression__C': 100.0}\n0.953 (+/-0.017) for {'randomforestclassifier__n_estimators': 200, 'logisticregression__C': 100.0}\n\n\n\nNote\n: If the \nEnsembleClassifier\n is initialized with multiple similar estimator objects, the estimator names are modified with consecutive integer indices, for example:\n\n\nclf1 = LogisticRegression(random_state=1)\nclf2 = RandomForestClassifier(random_state=1)\neclf = EnsembleVoteClassifier(clfs=[clf1, clf1, clf2], \n                              voting='soft')\n\nparams = {'logisticregression-1__C': [1.0, 100.0],\n          'logisticregression-2__C': [1.0, 100.0],\n          'randomforestclassifier__n_estimators': [20, 200],}\n\ngrid = GridSearchCV(estimator=eclf, param_grid=params, cv=5)\ngrid = grid.fit(iris.data, iris.target)\n\n\n\n\nExample 3 - Majority voting with classifiers trained on different feature subsets\n\n\nFeature selection algorithms implemented in scikit-learn as well as the \nSequentialFeatureSelector\n implement a \ntransform\n method that passes the reduced feature subset to the next item in a \nPipeline\n.\n\n\nFor example, the method\n\n\ndef transform(self, X):\n    return X[:, self.k_feature_idx_]\n\n\n\nreturns the best feature columns, \nk_feature_idx_\n, given a dataset X.\n\n\nThus, we simply need to construct a \nPipeline\n consisting of the feature selector and the classifier in order to select different feature subsets for different algorithms. During \nfitting\n, the optimal feature subsets are automatically determined via the \nGridSearchCV\n object, and by calling \npredict\n, the fitted feature selector in the pipeline only passes these columns along, which resulted in the best performance for the respective classifier.\n\n\nfrom sklearn import datasets\n\niris = datasets.load_iris()\nX, y = iris.data[:, :], iris.target\n\nfrom sklearn.grid_search import GridSearchCV\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.naive_bayes import GaussianNB \nfrom sklearn.ensemble import RandomForestClassifier\nfrom mlxtend.classifier import EnsembleVoteClassifier\nfrom sklearn.pipeline import Pipeline\nfrom mlxtend.feature_selection import SequentialFeatureSelector\n\nclf1 = LogisticRegression(random_state=1)\nclf2 = RandomForestClassifier(random_state=1)\nclf3 = GaussianNB()\n\n# Creating a feature-selection-classifier pipeline\n\nsfs1 = SequentialFeatureSelector(clf1, \n                                 k_features=4,\n                                 forward=True, \n                                 floating=False, \n                                 scoring='accuracy',\n                                 print_progress=False,\n                                 cv=0)\n\nclf1_pipe = Pipeline([('sfs', sfs1),\n                      ('logreg', clf1)])\n\neclf = EnsembleVoteClassifier(clfs=[clf1_pipe, clf2, clf3], \n                              voting='soft')\n\n\nparams = {'pipeline__sfs__k_features': [1, 2, 3],\n          'pipeline__logreg__C': [1.0, 100.0],\n          'randomforestclassifier__n_estimators': [20, 200]}\n\ngrid = GridSearchCV(estimator=eclf, param_grid=params, cv=5)\ngrid.fit(iris.data, iris.target)\n\nfor params, mean_score, scores in grid.grid_scores_:\n    print(\"%0.3f (+/-%0.03f) for %r\"\n        % (mean_score, scores.std() / 2, params))\n\n\n\n\n0.953 (+/-0.013) for {'pipeline__logreg__C': 1.0, 'randomforestclassifier__n_estimators': 20, 'pipeline__sfs__k_features': 1}\n0.947 (+/-0.017) for {'pipeline__logreg__C': 1.0, 'randomforestclassifier__n_estimators': 200, 'pipeline__sfs__k_features': 1}\n0.953 (+/-0.013) for {'pipeline__logreg__C': 1.0, 'randomforestclassifier__n_estimators': 20, 'pipeline__sfs__k_features': 2}\n0.947 (+/-0.017) for {'pipeline__logreg__C': 1.0, 'randomforestclassifier__n_estimators': 200, 'pipeline__sfs__k_features': 2}\n0.953 (+/-0.013) for {'pipeline__logreg__C': 1.0, 'randomforestclassifier__n_estimators': 20, 'pipeline__sfs__k_features': 3}\n0.953 (+/-0.017) for {'pipeline__logreg__C': 1.0, 'randomforestclassifier__n_estimators': 200, 'pipeline__sfs__k_features': 3}\n0.947 (+/-0.017) for {'pipeline__logreg__C': 100.0, 'randomforestclassifier__n_estimators': 20, 'pipeline__sfs__k_features': 1}\n0.953 (+/-0.017) for {'pipeline__logreg__C': 100.0, 'randomforestclassifier__n_estimators': 200, 'pipeline__sfs__k_features': 1}\n0.947 (+/-0.017) for {'pipeline__logreg__C': 100.0, 'randomforestclassifier__n_estimators': 20, 'pipeline__sfs__k_features': 2}\n0.947 (+/-0.017) for {'pipeline__logreg__C': 100.0, 'randomforestclassifier__n_estimators': 200, 'pipeline__sfs__k_features': 2}\n0.960 (+/-0.012) for {'pipeline__logreg__C': 100.0, 'randomforestclassifier__n_estimators': 20, 'pipeline__sfs__k_features': 3}\n0.953 (+/-0.017) for {'pipeline__logreg__C': 100.0, 'randomforestclassifier__n_estimators': 200, 'pipeline__sfs__k_features': 3}\n\n\n\nThe best parameters determined via GridSearch are:\n\n\ngrid.best_params_\n\n\n\n\n{'pipeline__logreg__C': 100.0,\n 'pipeline__sfs__k_features': 3,\n 'randomforestclassifier__n_estimators': 20}\n\n\n\nNow, we assign these parameters to the ensemble voting classifier, fit the models on the complete training set, and perform a prediction on 3 samples from the Iris dataset.\n\n\neclf = eclf.set_params(**grid.best_params_)\neclf.fit(X, y).predict(X[[1, 51, 149]])\n\n\n\n\narray([0, 1, 2])\n\n\n\nManual Approach\n\n\nAlternatively, we can select different columns \"manually\" using the \nColumnSelector\n object. In this example, we select only the first (sepal length) and third (petal length) column for the logistic regression classifier (\nclf1\n).\n\n\nfrom mlxtend.feature_selection import ColumnSelector\n\ncol_sel = ColumnSelector(cols=[0, 2])\n\nclf1_pipe = Pipeline([('sel', col_sel),\n                      ('logreg', clf1)])\n\neclf = EnsembleVoteClassifier(clfs=[clf1_pipe, clf2, clf3],\n                              voting='soft')\neclf.fit(X, y).predict(X[[1, 51, 149]])\n\n\n\n\narray([0, 1, 2])\n\n\n\nFurthermore, we can fit the \nSequentialFeatureSelector\n separately, outside the grid search hyperparameter optimization pipeline. Here, we determine the best features first, and then we construct a pipeline using these \"fixed,\" best features as seed for the \nColumnSelector\n:\n\n\nsfs1 = SequentialFeatureSelector(clf1, \n                                 k_features=2,\n                                 forward=True, \n                                 floating=False, \n                                 scoring='accuracy',\n                                 print_progress=True,\n                                 cv=0)\n\nsfs1.fit(X, y)\n\nprint('Best features', sfs1.k_feature_idx_)\n\ncol_sel = ColumnSelector(cols=sfs1.k_feature_idx_)\n\nclf1_pipe = Pipeline([('sel', col_sel),\n                      ('logreg', clf1)])\n\n\n\n\nFeatures: 2/2\n\nBest features (0, 2)\n\n\n\neclf = EnsembleVoteClassifier(clfs=[clf1_pipe, clf2, clf3], \n                              voting='soft')\neclf.fit(X, y).predict(X[[1, 51, 149]])\n\n\n\n\narray([0, 1, 2])\n\n\n\nAPI\n\n\nEnsembleVoteClassifier(clfs, voting='hard', weights=None, verbose=0)\n\n\nSoft Voting/Majority Rule classifier for scikit-learn estimators.\n\n\nParameters\n\n\n\n\n\n\nclfs\n : array-like, shape = [n_classifiers]\n\n\nA list of classifiers.\nInvoking the \nfit\n method on the \nVotingClassifier\n will fit clones\nof those original classifiers that will\nbe stored in the class attribute\n\nself.clfs_\n.\n\n\n\n\n\n\nvoting\n : str, {'hard', 'soft'} (default='hard')\n\n\nIf 'hard', uses predicted class labels for majority rule voting.\nElse if 'soft', predicts the class label based on the argmax of\nthe sums of the predicted probalities, which is recommended for\nan ensemble of well-calibrated classifiers.\n\n\n\n\n\n\nweights\n : array-like, shape = [n_classifiers], optional (default=\nNone\n)\n\n\nSequence of weights (\nfloat\n or \nint\n) to weight the occurances of\npredicted class labels (\nhard\n voting) or class probabilities\nbefore averaging (\nsoft\n voting). Uses uniform weights if \nNone\n.\n\n\n\n\n\n\nverbose\n : int, optional (default=0)\n\n\nControls the verbosity of the building process.\n- \nverbose=0\n (default): Prints nothing\n- \nverbose=1\n: Prints the number & name of the clf being fitted\n- \nverbose=2\n: Prints info about the parameters of the clf being fitted\n- \nverbose>2\n: Changes \nverbose\n param of the underlying clf to\nself.verbose - 2\n\n\n\n\n\n\nAttributes\n\n\n\n\n\n\nclasses_\n : array-like, shape = [n_predictions]\n\n\n\n\n\n\nclf\n : array-like, shape = [n_predictions]\n\n\nThe unmodified input classifiers\n\n\n\n\n\n\nclf_\n : array-like, shape = [n_predictions]\n\n\nFitted clones of the input classifiers\n\n\n\n\n\n\nExamples\n\n\n>>> import numpy as np\n>>> from sklearn.linear_model import LogisticRegression\n>>> from sklearn.naive_bayes import GaussianNB\n>>> from sklearn.ensemble import RandomForestClassifier\n>>> from mlxtend.sklearn import EnsembleVoteClassifier\n>>> clf1 = LogisticRegression(random_seed=1)\n>>> clf2 = RandomForestClassifier(random_seed=1)\n>>> clf3 = GaussianNB()\n>>> X = np.array([[-1, -1], [-2, -1], [-3, -2], [1, 1], [2, 1], [3, 2]])\n>>> y = np.array([1, 1, 1, 2, 2, 2])\n>>> eclf1 = EnsembleVoteClassifier(clfs=[clf1, clf2, clf3],\n... voting='hard', verbose=1)\n>>> eclf1 = eclf1.fit(X, y)\n>>> print(eclf1.predict(X))\n[1 1 1 2 2 2]\n>>> eclf2 = EnsembleVoteClassifier(clfs=[clf1, clf2, clf3], voting='soft')\n>>> eclf2 = eclf2.fit(X, y)\n>>> print(eclf2.predict(X))\n[1 1 1 2 2 2]\n>>> eclf3 = EnsembleVoteClassifier(clfs=[clf1, clf2, clf3],\n...                          voting='soft', weights=[2,1,1])\n>>> eclf3 = eclf3.fit(X, y)\n>>> print(eclf3.predict(X))\n[1 1 1 2 2 2]\n>>>\n\n\n\nMethods\n\n\n\n\n\nfit(X, y)\n\n\nLearn weight coefficients from training data for each classifier.\n\n\nParameters\n\n\n\n\n\n\nX\n : {array-like, sparse matrix}, shape = [n_samples, n_features]\n\n\nTraining vectors, where n_samples is the number of samples and\nn_features is the number of features.\n\n\n\n\n\n\ny\n : array-like, shape = [n_samples]\n\n\nTarget values.\n\n\n\n\n\n\nReturns\n\n\n\n\nself\n : object\n\n\n\n\n\n\n\nfit_transform(X, y=None, \nfit_params)\n\n\nFit to data, then transform it.\n\n\nFits transformer to X and y with optional parameters fit_params\nand returns a transformed version of X.\n\n\n\nParameters\n\n\n\n\n\n\nX\n : numpy array of shape [n_samples, n_features]\n\n\nTraining set.\n\n\n\n\n\n\ny\n : numpy array of shape [n_samples]\n\n\nTarget values.\n\n\n\n\n\n\nReturns\n\n\n\n\n\n\nX_new\n : numpy array of shape [n_samples, n_features_new]\n\n\nTransformed array.\n\n\n\n\n\n\n\n\n\nget_params(deep=True)\n\n\nReturn estimator parameter names for GridSearch support.\n\n\n\n\n\npredict(X)\n\n\nPredict class labels for X.\n\n\nParameters\n\n\n\n\n\n\nX\n : {array-like, sparse matrix}, shape = [n_samples, n_features]\n\n\nTraining vectors, where n_samples is the number of samples and\nn_features is the number of features.\n\n\n\n\n\n\nReturns\n\n\n\n\n\n\nmaj\n : array-like, shape = [n_samples]\n\n\nPredicted class labels.\n\n\n\n\n\n\n\n\n\npredict_proba(X)\n\n\nPredict class probabilities for X.\n\n\nParameters\n\n\n\n\n\n\nX\n : {array-like, sparse matrix}, shape = [n_samples, n_features]\n\n\nTraining vectors, where n_samples is the number of samples and\nn_features is the number of features.\n\n\n\n\n\n\nReturns\n\n\n\n\n\n\navg\n : array-like, shape = [n_samples, n_classes]\n\n\nWeighted average probability for each class per sample.\n\n\n\n\n\n\n\n\n\nscore(X, y, sample_weight=None)\n\n\nReturns the mean accuracy on the given test data and labels.\n\n\nIn multi-label classification, this is the subset accuracy\nwhich is a harsh metric since you require for each sample that\neach label set be correctly predicted.\n\n\n\nParameters\n\n\n\n\n\n\nX\n : array-like, shape = (n_samples, n_features)\n\n\nTest samples.\n\n\n\n\n\n\ny\n : array-like, shape = (n_samples) or (n_samples, n_outputs)\n\n\nTrue labels for X.\n\n\n\n\n\n\nsample_weight\n : array-like, shape = [n_samples], optional\n\n\nSample weights.\n\n\n\n\n\n\nReturns\n\n\n\n\n\n\nscore\n : float\n\n\nMean accuracy of self.predict(X) wrt. y.\n\n\n\n\n\n\n\n\n\nset_params(\nparams)\n\n\nSet the parameters of this estimator.\n\n\nThe method works on simple estimators as well as on nested objects\n(such as pipelines). The former have parameters of the form\n``<component>__<parameter>`` so that it's possible to update each\ncomponent of a nested object.\n\n\n\nReturns\n\n\nself\n\n\n\n\n\ntransform(X)\n\n\nReturn class labels or probabilities for X for each estimator.\n\n\nParameters\n\n\n\n\n\n\nX\n : {array-like, sparse matrix}, shape = [n_samples, n_features]\n\n\nTraining vectors, where n_samples is the number of samples and\nn_features is the number of features.\n\n\n\n\n\n\nReturns\n\n\n\n\n\n\nIf\nvoting='soft'`` : array-like = [n_classifiers, n_samples, n_classes]\n\n\nClass probabilties calculated by each classifier.\n\n\n\n\n\n\nIf\nvoting='hard'`` : array-like = [n_classifiers, n_samples]\n\n\nClass labels predicted by each classifier.",
            "title": "EnsembleVoteClassifier"
        },
        {
            "location": "/user_guide/classifier/EnsembleVoteClassifier/#ensemblevoteclassifier",
            "text": "Implementation of a majority voting  EnsembleVoteClassifier  for classification.   from mlxtend.classifier import EnsembleVoteClassifier",
            "title": "EnsembleVoteClassifier"
        },
        {
            "location": "/user_guide/classifier/EnsembleVoteClassifier/#overview",
            "text": "The  EnsembleVoteClassifier  is a meta-classifier for combining similar or conceptually different machine learning classifiers for classification via majority or plurality voting. (For simplicity, we will refer to both majority and plurality voting as majority voting.)   The  EnsembleVoteClassifier  implements \"hard\" and \"soft\" voting. In hard voting, we predict the final class label as the class label that has been predicted most frequently by the classification models. In soft voting, we predict the class labels by averaging the class-probabilities (only recommended if the classifiers are well-calibrated).   Note  If you are interested in using the  EnsembleVoteClassifier , please note that it is now also available through scikit learn (>0.17) as  VotingClassifier .",
            "title": "Overview"
        },
        {
            "location": "/user_guide/classifier/EnsembleVoteClassifier/#majority-voting-hard-voting",
            "text": "Hard voting is the simplest case of majority voting. Here, we predict the class label  $\\hat{y}$  via majority (plurality) voting of each classifier  $C_j$ :  $$\\hat{y}=mode\\{C_1(\\mathbf{x}), C_2(\\mathbf{x}), ..., C_m(\\mathbf{x})\\}$$  Assuming that we combine three classifiers that classify a training sample as follows:   classifier 1 -> class 0  classifier 2 -> class 0  classifier 3 -> class 1   $$\\hat{y}=mode\\{0, 0, 1\\} = 0$$  Via majority vote, we would we would classify the sample as \"class 0.\"",
            "title": "Majority Voting / Hard Voting"
        },
        {
            "location": "/user_guide/classifier/EnsembleVoteClassifier/#weighted-majority-vote",
            "text": "In addition to the simple majority vote (hard voting) as described in the previous section, we can compute a weighted majority vote by associating a weight  $w_j$  with classifier  $C_j$ :  $$\\hat{y} = \\arg \\max_i \\sum^{m}_{j=1} w_j \\chi_A \\big(C_j(\\mathbf{x})=i\\big),$$  where  $\\chi_A$  is the characteristic function  $[C_j(\\mathbf{x}) = i \\; \\in A]$ , and  $A$  is the set of unique class labels.   Continuing with the example from the previous section   classifier 1 -> class 0  classifier 2 -> class 0  classifier 3 -> class 1   assigning the weights {0.2, 0.2, 0.6} would yield a prediction  $\\hat{y} = 1$ :  $$\\arg \\max_i [0.2 \\times i_0 + 0.2 \\times i_0 + 0.6 \\times i_1] = 1$$",
            "title": "Weighted Majority Vote"
        },
        {
            "location": "/user_guide/classifier/EnsembleVoteClassifier/#soft-voting",
            "text": "In soft voting, we predict the class labels based on the predicted probabilities  $p$  for classifier -- this approach is only recommended if the classifiers are well-calibrated.  $$\\hat{y} = \\arg \\max_i \\sum^{m}_{j=1} w_j p_{ij},$$  where  $w_j$  is the weight that can be assigned to the  $j$ th classifier.  Assuming the example in the previous section was a binary classification task with class labels  $i \\in \\{0, 1\\}$ , our ensemble could make the following prediction:   $C_1(\\mathbf{x}) \\rightarrow [0.9, 0.1]$  $C_2(\\mathbf{x}) \\rightarrow [0.8, 0.2]$  $C_3(\\mathbf{x}) \\rightarrow [0.4, 0.6]$   Using uniform weights, we compute the average probabilities:  $$p(i_0 \\mid \\mathbf{x}) = \\frac{0.9 + 0.8 + 0.4}{3} = 0.7 \\\\\\\\\np(i_1 \\mid \\mathbf{x}) = \\frac{0.1 + 0.2 + 0.6}{3} = 0.3$$  $$\\hat{y} = \\arg \\max_i \\big[p(i_0 \\mid \\mathbf{x}), p(i_1 \\mid \\mathbf{x}) \\big] = 0$$  However, assigning the weights {0.1, 0.1, 0.8} would yield a prediction  $\\hat{y} = 1$ :  $$p(i_0 \\mid \\mathbf{x}) = {0.1 \\times 0.9 + 0.1 \\times 0.8 + 0.8 \\times  0.4} = 0.49 \\\\\\\\\np(i_1 \\mid \\mathbf{x}) = {0.1 \\times 0.1 + 0.2 \\times 0.1 + 0.8 \\times 0.6} = 0.51$$  $$\\hat{y} = \\arg \\max_i \\big[p(i_0 \\mid \\mathbf{x}), p(i_1 \\mid \\mathbf{x}) \\big] = 1$$",
            "title": "Soft Voting"
        },
        {
            "location": "/user_guide/classifier/EnsembleVoteClassifier/#references",
            "text": "[1] S. Raschka.  Python Machine Learning . Packt Publishing Ltd., 2015.",
            "title": "References"
        },
        {
            "location": "/user_guide/classifier/EnsembleVoteClassifier/#examples",
            "text": "",
            "title": "Examples"
        },
        {
            "location": "/user_guide/classifier/EnsembleVoteClassifier/#example-1-classifying-iris-flowers-using-different-classification-models",
            "text": "from sklearn import datasets\n\niris = datasets.load_iris()\nX, y = iris.data[:, 1:3], iris.target  from sklearn import cross_validation\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.naive_bayes import GaussianNB \nfrom sklearn.ensemble import RandomForestClassifier\nimport numpy as np\n\nclf1 = LogisticRegression(random_state=1)\nclf2 = RandomForestClassifier(random_state=1)\nclf3 = GaussianNB()\n\nprint('5-fold cross validation:\\n')\n\nlabels = ['Logistic Regression', 'Random Forest', 'Naive Bayes']\n\nfor clf, label in zip([clf1, clf2, clf3], labels):\n\n    scores = cross_validation.cross_val_score(clf, X, y, \n                                              cv=5, \n                                              scoring='accuracy')\n    print(\"Accuracy: %0.2f (+/- %0.2f) [%s]\"\n          % (scores.mean(), scores.std(), label))  5-fold cross validation:\n\nAccuracy: 0.90 (+/- 0.05) [Logistic Regression]\nAccuracy: 0.93 (+/- 0.05) [Random Forest]\nAccuracy: 0.91 (+/- 0.04) [Naive Bayes]  from mlxtend.classifier import EnsembleVoteClassifier\n\neclf = EnsembleVoteClassifier(clfs=[clf1, clf2, clf3], weights=[1,1,1])\n\nlabels = ['Logistic Regression', 'Random Forest', 'Naive Bayes', 'Ensemble']\nfor clf, label in zip([clf1, clf2, clf3, eclf], labels):\n\n    scores = cross_validation.cross_val_score(clf, X, y, \n                                              cv=5, \n                                              scoring='accuracy')\n    print(\"Accuracy: %0.2f (+/- %0.2f) [%s]\" \n          % (scores.mean(), scores.std(), label))  Accuracy: 0.90 (+/- 0.05) [Logistic Regression]\nAccuracy: 0.93 (+/- 0.05) [Random Forest]\nAccuracy: 0.91 (+/- 0.04) [Naive Bayes]\nAccuracy: 0.95 (+/- 0.05) [Ensemble]",
            "title": "Example 1 -  Classifying Iris Flowers Using Different Classification Models"
        },
        {
            "location": "/user_guide/classifier/EnsembleVoteClassifier/#plotting-decision-regions",
            "text": "import matplotlib.pyplot as plt\nfrom mlxtend.evaluate import plot_decision_regions\nimport matplotlib.gridspec as gridspec\nimport itertools\n\ngs = gridspec.GridSpec(2, 2)\n\nfig = plt.figure(figsize=(10,8))\n\nlabels = ['Logistic Regression', 'Random Forest', 'Naive Bayes', 'Ensemble']\nfor clf, lab, grd in zip([clf1, clf2, clf3, eclf],\n                         labels,\n                         itertools.product([0, 1], repeat=2)):\n\n    clf.fit(X, y)\n    ax = plt.subplot(gs[grd[0], grd[1]])\n    fig = plot_decision_regions(X=X, y=y, clf=clf)\n    plt.title(lab)",
            "title": "Plotting Decision Regions"
        },
        {
            "location": "/user_guide/classifier/EnsembleVoteClassifier/#example-2-grid-search",
            "text": "from sklearn import datasets\n\niris = datasets.load_iris()\nX, y = iris.data[:, 1:3], iris.target  from sklearn.grid_search import GridSearchCV\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.naive_bayes import GaussianNB \nfrom sklearn.ensemble import RandomForestClassifier\nfrom mlxtend.classifier import EnsembleVoteClassifier\n\nclf1 = LogisticRegression(random_state=1)\nclf2 = RandomForestClassifier(random_state=1)\nclf3 = GaussianNB()\neclf = EnsembleVoteClassifier(clfs=[clf1, clf2, clf3], voting='soft')\n\nparams = {'logisticregression__C': [1.0, 100.0],\n          'randomforestclassifier__n_estimators': [20, 200],}\n\ngrid = GridSearchCV(estimator=eclf, param_grid=params, cv=5)\ngrid.fit(iris.data, iris.target)\n\nfor params, mean_score, scores in grid.grid_scores_:\n    print(\"%0.3f (+/-%0.03f) for %r\"\n        % (mean_score, scores.std() / 2, params))  0.953 (+/-0.013) for {'randomforestclassifier__n_estimators': 20, 'logisticregression__C': 1.0}\n0.960 (+/-0.012) for {'randomforestclassifier__n_estimators': 200, 'logisticregression__C': 1.0}\n0.960 (+/-0.012) for {'randomforestclassifier__n_estimators': 20, 'logisticregression__C': 100.0}\n0.953 (+/-0.017) for {'randomforestclassifier__n_estimators': 200, 'logisticregression__C': 100.0}  Note : If the  EnsembleClassifier  is initialized with multiple similar estimator objects, the estimator names are modified with consecutive integer indices, for example:  clf1 = LogisticRegression(random_state=1)\nclf2 = RandomForestClassifier(random_state=1)\neclf = EnsembleVoteClassifier(clfs=[clf1, clf1, clf2], \n                              voting='soft')\n\nparams = {'logisticregression-1__C': [1.0, 100.0],\n          'logisticregression-2__C': [1.0, 100.0],\n          'randomforestclassifier__n_estimators': [20, 200],}\n\ngrid = GridSearchCV(estimator=eclf, param_grid=params, cv=5)\ngrid = grid.fit(iris.data, iris.target)",
            "title": "Example 2 - Grid Search"
        },
        {
            "location": "/user_guide/classifier/EnsembleVoteClassifier/#example-3-majority-voting-with-classifiers-trained-on-different-feature-subsets",
            "text": "Feature selection algorithms implemented in scikit-learn as well as the  SequentialFeatureSelector  implement a  transform  method that passes the reduced feature subset to the next item in a  Pipeline .  For example, the method  def transform(self, X):\n    return X[:, self.k_feature_idx_]  returns the best feature columns,  k_feature_idx_ , given a dataset X.  Thus, we simply need to construct a  Pipeline  consisting of the feature selector and the classifier in order to select different feature subsets for different algorithms. During  fitting , the optimal feature subsets are automatically determined via the  GridSearchCV  object, and by calling  predict , the fitted feature selector in the pipeline only passes these columns along, which resulted in the best performance for the respective classifier.  from sklearn import datasets\n\niris = datasets.load_iris()\nX, y = iris.data[:, :], iris.target\n\nfrom sklearn.grid_search import GridSearchCV\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.naive_bayes import GaussianNB \nfrom sklearn.ensemble import RandomForestClassifier\nfrom mlxtend.classifier import EnsembleVoteClassifier\nfrom sklearn.pipeline import Pipeline\nfrom mlxtend.feature_selection import SequentialFeatureSelector\n\nclf1 = LogisticRegression(random_state=1)\nclf2 = RandomForestClassifier(random_state=1)\nclf3 = GaussianNB()\n\n# Creating a feature-selection-classifier pipeline\n\nsfs1 = SequentialFeatureSelector(clf1, \n                                 k_features=4,\n                                 forward=True, \n                                 floating=False, \n                                 scoring='accuracy',\n                                 print_progress=False,\n                                 cv=0)\n\nclf1_pipe = Pipeline([('sfs', sfs1),\n                      ('logreg', clf1)])\n\neclf = EnsembleVoteClassifier(clfs=[clf1_pipe, clf2, clf3], \n                              voting='soft')\n\n\nparams = {'pipeline__sfs__k_features': [1, 2, 3],\n          'pipeline__logreg__C': [1.0, 100.0],\n          'randomforestclassifier__n_estimators': [20, 200]}\n\ngrid = GridSearchCV(estimator=eclf, param_grid=params, cv=5)\ngrid.fit(iris.data, iris.target)\n\nfor params, mean_score, scores in grid.grid_scores_:\n    print(\"%0.3f (+/-%0.03f) for %r\"\n        % (mean_score, scores.std() / 2, params))  0.953 (+/-0.013) for {'pipeline__logreg__C': 1.0, 'randomforestclassifier__n_estimators': 20, 'pipeline__sfs__k_features': 1}\n0.947 (+/-0.017) for {'pipeline__logreg__C': 1.0, 'randomforestclassifier__n_estimators': 200, 'pipeline__sfs__k_features': 1}\n0.953 (+/-0.013) for {'pipeline__logreg__C': 1.0, 'randomforestclassifier__n_estimators': 20, 'pipeline__sfs__k_features': 2}\n0.947 (+/-0.017) for {'pipeline__logreg__C': 1.0, 'randomforestclassifier__n_estimators': 200, 'pipeline__sfs__k_features': 2}\n0.953 (+/-0.013) for {'pipeline__logreg__C': 1.0, 'randomforestclassifier__n_estimators': 20, 'pipeline__sfs__k_features': 3}\n0.953 (+/-0.017) for {'pipeline__logreg__C': 1.0, 'randomforestclassifier__n_estimators': 200, 'pipeline__sfs__k_features': 3}\n0.947 (+/-0.017) for {'pipeline__logreg__C': 100.0, 'randomforestclassifier__n_estimators': 20, 'pipeline__sfs__k_features': 1}\n0.953 (+/-0.017) for {'pipeline__logreg__C': 100.0, 'randomforestclassifier__n_estimators': 200, 'pipeline__sfs__k_features': 1}\n0.947 (+/-0.017) for {'pipeline__logreg__C': 100.0, 'randomforestclassifier__n_estimators': 20, 'pipeline__sfs__k_features': 2}\n0.947 (+/-0.017) for {'pipeline__logreg__C': 100.0, 'randomforestclassifier__n_estimators': 200, 'pipeline__sfs__k_features': 2}\n0.960 (+/-0.012) for {'pipeline__logreg__C': 100.0, 'randomforestclassifier__n_estimators': 20, 'pipeline__sfs__k_features': 3}\n0.953 (+/-0.017) for {'pipeline__logreg__C': 100.0, 'randomforestclassifier__n_estimators': 200, 'pipeline__sfs__k_features': 3}  The best parameters determined via GridSearch are:  grid.best_params_  {'pipeline__logreg__C': 100.0,\n 'pipeline__sfs__k_features': 3,\n 'randomforestclassifier__n_estimators': 20}  Now, we assign these parameters to the ensemble voting classifier, fit the models on the complete training set, and perform a prediction on 3 samples from the Iris dataset.  eclf = eclf.set_params(**grid.best_params_)\neclf.fit(X, y).predict(X[[1, 51, 149]])  array([0, 1, 2])",
            "title": "Example 3 - Majority voting with classifiers trained on different feature subsets"
        },
        {
            "location": "/user_guide/classifier/EnsembleVoteClassifier/#manual-approach",
            "text": "Alternatively, we can select different columns \"manually\" using the  ColumnSelector  object. In this example, we select only the first (sepal length) and third (petal length) column for the logistic regression classifier ( clf1 ).  from mlxtend.feature_selection import ColumnSelector\n\ncol_sel = ColumnSelector(cols=[0, 2])\n\nclf1_pipe = Pipeline([('sel', col_sel),\n                      ('logreg', clf1)])\n\neclf = EnsembleVoteClassifier(clfs=[clf1_pipe, clf2, clf3],\n                              voting='soft')\neclf.fit(X, y).predict(X[[1, 51, 149]])  array([0, 1, 2])  Furthermore, we can fit the  SequentialFeatureSelector  separately, outside the grid search hyperparameter optimization pipeline. Here, we determine the best features first, and then we construct a pipeline using these \"fixed,\" best features as seed for the  ColumnSelector :  sfs1 = SequentialFeatureSelector(clf1, \n                                 k_features=2,\n                                 forward=True, \n                                 floating=False, \n                                 scoring='accuracy',\n                                 print_progress=True,\n                                 cv=0)\n\nsfs1.fit(X, y)\n\nprint('Best features', sfs1.k_feature_idx_)\n\ncol_sel = ColumnSelector(cols=sfs1.k_feature_idx_)\n\nclf1_pipe = Pipeline([('sel', col_sel),\n                      ('logreg', clf1)])  Features: 2/2\n\nBest features (0, 2)  eclf = EnsembleVoteClassifier(clfs=[clf1_pipe, clf2, clf3], \n                              voting='soft')\neclf.fit(X, y).predict(X[[1, 51, 149]])  array([0, 1, 2])",
            "title": "Manual Approach"
        },
        {
            "location": "/user_guide/classifier/EnsembleVoteClassifier/#api",
            "text": "EnsembleVoteClassifier(clfs, voting='hard', weights=None, verbose=0)  Soft Voting/Majority Rule classifier for scikit-learn estimators.  Parameters    clfs  : array-like, shape = [n_classifiers]  A list of classifiers.\nInvoking the  fit  method on the  VotingClassifier  will fit clones\nof those original classifiers that will\nbe stored in the class attribute self.clfs_ .    voting  : str, {'hard', 'soft'} (default='hard')  If 'hard', uses predicted class labels for majority rule voting.\nElse if 'soft', predicts the class label based on the argmax of\nthe sums of the predicted probalities, which is recommended for\nan ensemble of well-calibrated classifiers.    weights  : array-like, shape = [n_classifiers], optional (default= None )  Sequence of weights ( float  or  int ) to weight the occurances of\npredicted class labels ( hard  voting) or class probabilities\nbefore averaging ( soft  voting). Uses uniform weights if  None .    verbose  : int, optional (default=0)  Controls the verbosity of the building process.\n-  verbose=0  (default): Prints nothing\n-  verbose=1 : Prints the number & name of the clf being fitted\n-  verbose=2 : Prints info about the parameters of the clf being fitted\n-  verbose>2 : Changes  verbose  param of the underlying clf to\nself.verbose - 2    Attributes    classes_  : array-like, shape = [n_predictions]    clf  : array-like, shape = [n_predictions]  The unmodified input classifiers    clf_  : array-like, shape = [n_predictions]  Fitted clones of the input classifiers    Examples  >>> import numpy as np\n>>> from sklearn.linear_model import LogisticRegression\n>>> from sklearn.naive_bayes import GaussianNB\n>>> from sklearn.ensemble import RandomForestClassifier\n>>> from mlxtend.sklearn import EnsembleVoteClassifier\n>>> clf1 = LogisticRegression(random_seed=1)\n>>> clf2 = RandomForestClassifier(random_seed=1)\n>>> clf3 = GaussianNB()\n>>> X = np.array([[-1, -1], [-2, -1], [-3, -2], [1, 1], [2, 1], [3, 2]])\n>>> y = np.array([1, 1, 1, 2, 2, 2])\n>>> eclf1 = EnsembleVoteClassifier(clfs=[clf1, clf2, clf3],\n... voting='hard', verbose=1)\n>>> eclf1 = eclf1.fit(X, y)\n>>> print(eclf1.predict(X))\n[1 1 1 2 2 2]\n>>> eclf2 = EnsembleVoteClassifier(clfs=[clf1, clf2, clf3], voting='soft')\n>>> eclf2 = eclf2.fit(X, y)\n>>> print(eclf2.predict(X))\n[1 1 1 2 2 2]\n>>> eclf3 = EnsembleVoteClassifier(clfs=[clf1, clf2, clf3],\n...                          voting='soft', weights=[2,1,1])\n>>> eclf3 = eclf3.fit(X, y)\n>>> print(eclf3.predict(X))\n[1 1 1 2 2 2]\n>>>",
            "title": "API"
        },
        {
            "location": "/user_guide/classifier/EnsembleVoteClassifier/#methods",
            "text": "fit(X, y)  Learn weight coefficients from training data for each classifier.  Parameters    X  : {array-like, sparse matrix}, shape = [n_samples, n_features]  Training vectors, where n_samples is the number of samples and\nn_features is the number of features.    y  : array-like, shape = [n_samples]  Target values.    Returns   self  : object    fit_transform(X, y=None,  fit_params)  Fit to data, then transform it.  Fits transformer to X and y with optional parameters fit_params\nand returns a transformed version of X.  Parameters    X  : numpy array of shape [n_samples, n_features]  Training set.    y  : numpy array of shape [n_samples]  Target values.    Returns    X_new  : numpy array of shape [n_samples, n_features_new]  Transformed array.     get_params(deep=True)  Return estimator parameter names for GridSearch support.   predict(X)  Predict class labels for X.  Parameters    X  : {array-like, sparse matrix}, shape = [n_samples, n_features]  Training vectors, where n_samples is the number of samples and\nn_features is the number of features.    Returns    maj  : array-like, shape = [n_samples]  Predicted class labels.     predict_proba(X)  Predict class probabilities for X.  Parameters    X  : {array-like, sparse matrix}, shape = [n_samples, n_features]  Training vectors, where n_samples is the number of samples and\nn_features is the number of features.    Returns    avg  : array-like, shape = [n_samples, n_classes]  Weighted average probability for each class per sample.     score(X, y, sample_weight=None)  Returns the mean accuracy on the given test data and labels.  In multi-label classification, this is the subset accuracy\nwhich is a harsh metric since you require for each sample that\neach label set be correctly predicted.  Parameters    X  : array-like, shape = (n_samples, n_features)  Test samples.    y  : array-like, shape = (n_samples) or (n_samples, n_outputs)  True labels for X.    sample_weight  : array-like, shape = [n_samples], optional  Sample weights.    Returns    score  : float  Mean accuracy of self.predict(X) wrt. y.     set_params( params)  Set the parameters of this estimator.  The method works on simple estimators as well as on nested objects\n(such as pipelines). The former have parameters of the form\n``<component>__<parameter>`` so that it's possible to update each\ncomponent of a nested object.  Returns  self   transform(X)  Return class labels or probabilities for X for each estimator.  Parameters    X  : {array-like, sparse matrix}, shape = [n_samples, n_features]  Training vectors, where n_samples is the number of samples and\nn_features is the number of features.    Returns    If voting='soft'`` : array-like = [n_classifiers, n_samples, n_classes]  Class probabilties calculated by each classifier.    If voting='hard'`` : array-like = [n_classifiers, n_samples]  Class labels predicted by each classifier.",
            "title": "Methods"
        },
        {
            "location": "/user_guide/classifier/StackingClassifier/",
            "text": "StackingClassifier\n\n\nAn ensemble-learning meta-classifier for stacking.\n\n\n\n\nfrom mlxtend.classifier import StackingClassifier\n\n\n\n\nOverview\n\n\nStacking is an ensemble learning technique to combine multiple classification models via a meta-classifier. The individual classification models are trained based on the complete training set; then, the meta-classifier is fitted based on the outputs -- meta-features -- of the individual classification models in the ensemble.\nThe meta-classifier can either be trained on the predicted class labels or probabilities from the ensemble.\n\n\n\n\nThe algorithm can be summarized as follows (source: [1]):\n\n\n\n\nReferences\n\n\n\n\n[1] Tang, J., S. Alelyani, and H. Liu. \"\nData Classification: Algorithms and Applications.\n\" Data Mining and Knowledge Discovery Series, CRC Press (2015): pp. 498-500.\n\n\n[2] Wolpert, David H. \"\nStacked generalization.\n\" Neural networks 5.2 (1992): 241-259.\n\n\n\n\nExamples\n\n\nExample 1 - Simple Stacked Classification\n\n\nfrom sklearn import datasets\n\niris = datasets.load_iris()\nX, y = iris.data[:, 1:3], iris.target\n\n\n\n\nfrom sklearn import cross_validation\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.naive_bayes import GaussianNB \nfrom sklearn.ensemble import RandomForestClassifier\nfrom mlxtend.classifier import StackingClassifier\nimport numpy as np\n\nclf1 = KNeighborsClassifier(n_neighbors=1)\nclf2 = RandomForestClassifier(random_state=1)\nclf3 = GaussianNB()\nlr = LogisticRegression()\nsclf = StackingClassifier(classifiers=[clf1, clf2, clf3], \n                          meta_classifier=lr)\n\nprint('3-fold cross validation:\\n')\n\nfor clf, label in zip([clf1, clf2, clf3, sclf], \n                      ['KNN', \n                       'Random Forest', \n                       'Naive Bayes',\n                       'StackingClassifier']):\n\n    scores = cross_validation.cross_val_score(clf, X, y, \n                                              cv=3, scoring='accuracy')\n    print(\"Accuracy: %0.2f (+/- %0.2f) [%s]\" \n          % (scores.mean(), scores.std(), label))\n\n\n\n\n3-fold cross validation:\n\nAccuracy: 0.91 (+/- 0.01) [KNN]\nAccuracy: 0.91 (+/- 0.06) [Random Forest]\nAccuracy: 0.92 (+/- 0.03) [Naive Bayes]\nAccuracy: 0.95 (+/- 0.03) [StackingClassifier]\n\n\n\nimport matplotlib.pyplot as plt\nfrom mlxtend.evaluate import plot_decision_regions\nimport matplotlib.gridspec as gridspec\nimport itertools\n\ngs = gridspec.GridSpec(2, 2)\n\nfig = plt.figure(figsize=(10,8))\n\nfor clf, lab, grd in zip([clf1, clf2, clf3, sclf], \n                         ['KNN', \n                          'Random Forest', \n                          'Naive Bayes',\n                          'StackingClassifier'],\n                          itertools.product([0, 1], repeat=2)):\n\n    clf.fit(X, y)\n    ax = plt.subplot(gs[grd[0], grd[1]])\n    fig = plot_decision_regions(X=X, y=y, clf=clf)\n    plt.title(lab)\n\n\n\n\n\n\nExample 2 - Stacked Classification and GridSearch\n\n\nTo set up a parameter grid for scikit-learn's \nGridSearch\n, we simply provide the estimator's names in the parameter grid -- in the special case of the meta-regressor, we append the \n'meta-'\n prefix.\n\n\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.naive_bayes import GaussianNB \nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.grid_search import GridSearchCV\nfrom mlxtend.classifier import StackingClassifier\n\n# Initializing models\n\nclf1 = KNeighborsClassifier(n_neighbors=1)\nclf2 = RandomForestClassifier(random_state=1)\nclf3 = GaussianNB()\nlr = LogisticRegression()\nsclf = StackingClassifier(classifiers=[clf1, clf2, clf3], \n                          meta_classifier=lr)\n\nparams = {'kneighborsclassifier__n_neighbors': [1, 5],\n          'randomforestclassifier__n_estimators': [10, 50],\n          'meta-logisticregression__C': [0.1, 10.0]}\n\ngrid = GridSearchCV(estimator=sclf, \n                    param_grid=params, \n                    cv=5,\n                    refit=True)\ngrid.fit(X, y)\n\nfor params, mean_score, scores in grid.grid_scores_:\n    print(\"%0.3f (+/-%0.03f) for %r\"\n        % (mean_score, scores.std() / 2, params))\n\n\n\n\n0.667 (+/-0.000) for {'randomforestclassifier__n_estimators': 10, 'meta-logisticregression__C': 0.1, 'kneighborsclassifier__n_neighbors': 1}\n0.667 (+/-0.000) for {'randomforestclassifier__n_estimators': 50, 'meta-logisticregression__C': 0.1, 'kneighborsclassifier__n_neighbors': 1}\n0.927 (+/-0.022) for {'randomforestclassifier__n_estimators': 10, 'meta-logisticregression__C': 10.0, 'kneighborsclassifier__n_neighbors': 1}\n0.913 (+/-0.027) for {'randomforestclassifier__n_estimators': 50, 'meta-logisticregression__C': 10.0, 'kneighborsclassifier__n_neighbors': 1}\n0.667 (+/-0.000) for {'randomforestclassifier__n_estimators': 10, 'meta-logisticregression__C': 0.1, 'kneighborsclassifier__n_neighbors': 5}\n0.667 (+/-0.000) for {'randomforestclassifier__n_estimators': 50, 'meta-logisticregression__C': 0.1, 'kneighborsclassifier__n_neighbors': 5}\n0.933 (+/-0.024) for {'randomforestclassifier__n_estimators': 10, 'meta-logisticregression__C': 10.0, 'kneighborsclassifier__n_neighbors': 5}\n0.940 (+/-0.019) for {'randomforestclassifier__n_estimators': 50, 'meta-logisticregression__C': 10.0, 'kneighborsclassifier__n_neighbors': 5}\n\n\n\nIn case we are planning to use a regression algorithm multiple times, all we need to do is to add an additional number suffix in the parameter grid as shown below:\n\n\nfrom sklearn.grid_search import GridSearchCV\n\n# Initializing models\n\nclf1 = KNeighborsClassifier(n_neighbors=1)\nclf2 = RandomForestClassifier(random_state=1)\nclf3 = GaussianNB()\nlr = LogisticRegression()\nsclf = StackingClassifier(classifiers=[clf1, clf1, clf2, clf3], \n                          meta_classifier=lr)\n\nparams = {'kneighborsclassifier-1__n_neighbors': [1, 5],\n          'kneighborsclassifier-2__n_neighbors': [1, 5],\n          'randomforestclassifier__n_estimators': [10, 50],\n          'meta-logisticregression__C': [0.1, 10.0]}\n\ngrid = GridSearchCV(estimator=sclf, \n                    param_grid=params, \n                    cv=5,\n                    refit=True)\ngrid.fit(X, y)\n\nfor params, mean_score, scores in grid.grid_scores_:\n    print(\"%0.3f (+/-%0.03f) for %r\"\n        % (mean_score, scores.std() / 2, params))\n\n\n\n\n0.667 (+/-0.000) for {'kneighborsclassifier-2__n_neighbors': 1, 'randomforestclassifier__n_estimators': 10, 'meta-logisticregression__C': 0.1, 'kneighborsclassifier-1__n_neighbors': 1}\n0.667 (+/-0.000) for {'kneighborsclassifier-2__n_neighbors': 1, 'randomforestclassifier__n_estimators': 50, 'meta-logisticregression__C': 0.1, 'kneighborsclassifier-1__n_neighbors': 1}\n0.907 (+/-0.029) for {'kneighborsclassifier-2__n_neighbors': 1, 'randomforestclassifier__n_estimators': 10, 'meta-logisticregression__C': 10.0, 'kneighborsclassifier-1__n_neighbors': 1}\n0.913 (+/-0.027) for {'kneighborsclassifier-2__n_neighbors': 1, 'randomforestclassifier__n_estimators': 50, 'meta-logisticregression__C': 10.0, 'kneighborsclassifier-1__n_neighbors': 1}\n0.667 (+/-0.000) for {'kneighborsclassifier-2__n_neighbors': 5, 'randomforestclassifier__n_estimators': 10, 'meta-logisticregression__C': 0.1, 'kneighborsclassifier-1__n_neighbors': 1}\n0.667 (+/-0.000) for {'kneighborsclassifier-2__n_neighbors': 5, 'randomforestclassifier__n_estimators': 50, 'meta-logisticregression__C': 0.1, 'kneighborsclassifier-1__n_neighbors': 1}\n0.927 (+/-0.022) for {'kneighborsclassifier-2__n_neighbors': 5, 'randomforestclassifier__n_estimators': 10, 'meta-logisticregression__C': 10.0, 'kneighborsclassifier-1__n_neighbors': 1}\n0.913 (+/-0.027) for {'kneighborsclassifier-2__n_neighbors': 5, 'randomforestclassifier__n_estimators': 50, 'meta-logisticregression__C': 10.0, 'kneighborsclassifier-1__n_neighbors': 1}\n0.667 (+/-0.000) for {'kneighborsclassifier-2__n_neighbors': 1, 'randomforestclassifier__n_estimators': 10, 'meta-logisticregression__C': 0.1, 'kneighborsclassifier-1__n_neighbors': 5}\n0.667 (+/-0.000) for {'kneighborsclassifier-2__n_neighbors': 1, 'randomforestclassifier__n_estimators': 50, 'meta-logisticregression__C': 0.1, 'kneighborsclassifier-1__n_neighbors': 5}\n0.927 (+/-0.022) for {'kneighborsclassifier-2__n_neighbors': 1, 'randomforestclassifier__n_estimators': 10, 'meta-logisticregression__C': 10.0, 'kneighborsclassifier-1__n_neighbors': 5}\n0.913 (+/-0.027) for {'kneighborsclassifier-2__n_neighbors': 1, 'randomforestclassifier__n_estimators': 50, 'meta-logisticregression__C': 10.0, 'kneighborsclassifier-1__n_neighbors': 5}\n0.667 (+/-0.000) for {'kneighborsclassifier-2__n_neighbors': 5, 'randomforestclassifier__n_estimators': 10, 'meta-logisticregression__C': 0.1, 'kneighborsclassifier-1__n_neighbors': 5}\n0.667 (+/-0.000) for {'kneighborsclassifier-2__n_neighbors': 5, 'randomforestclassifier__n_estimators': 50, 'meta-logisticregression__C': 0.1, 'kneighborsclassifier-1__n_neighbors': 5}\n0.933 (+/-0.024) for {'kneighborsclassifier-2__n_neighbors': 5, 'randomforestclassifier__n_estimators': 10, 'meta-logisticregression__C': 10.0, 'kneighborsclassifier-1__n_neighbors': 5}\n0.940 (+/-0.019) for {'kneighborsclassifier-2__n_neighbors': 5, 'randomforestclassifier__n_estimators': 50, 'meta-logisticregression__C': 10.0, 'kneighborsclassifier-1__n_neighbors': 5}\n\n\n\nAPI\n\n\nStackingClassifier(classifiers, meta_classifier, use_probas=False, verbose=0)\n\n\nA Stacking classifier for scikit-learn estimators for classification.\n\n\nParameters\n\n\n\n\n\n\nclassifiers\n : array-like, shape = [n_regressors]\n\n\nA list of classifiers.\nInvoking the \nfit\n method on the \nStackingClassifer\n will fit clones\nof these original classifiers that will\nbe stored in the class attribute\n\nself.clfs_\n.\n\n\n\n\n\n\nmeta_classifier\n : object\n\n\nThe meta-classifier to be fitted on the ensemble of\nclassifiers\n\n\n\n\n\n\nuse_probas\n : bool (default: False)\n\n\nIf True, trains meta-classifier based on predicted probabilities\ninstead of class labels.\n\n\n\n\n\n\nverbose\n : int, optional (default=0)\n\n\nControls the verbosity of the building process.\n- \nverbose=0\n (default): Prints nothing\n- \nverbose=1\n: Prints the number & name of the regressor being fitted\n- \nverbose=2\n: Prints info about the parameters of the\nregressor being fitted\n- \nverbose>2\n: Changes \nverbose\n param of the underlying regressor to\nself.verbose - 2\n\n\n\n\n\n\nAttributes\n\n\n\n\n\n\nclfs_\n : list, shape=[n_classifiers]\n\n\nFitted classifiers (clones of the original classifiers)\n\n\n\n\n\n\nmeta_clf_\n : estimator\n\n\nFitted meta-classifier (clone of the original meta-estimator)\n\n\n\n\n\n\nMethods\n\n\n\n\n\nfit(X, y)\n\n\nFit ensemble classifers and the meta-classifier.\n\n\nParameters\n\n\n\n\n\n\nX\n : {array-like, sparse matrix}, shape = [n_samples, n_features]\n\n\nTraining vectors, where n_samples is the number of samples and\nn_features is the number of features.\n\n\n\n\n\n\ny\n : array-like, shape = [n_samples]\n\n\nTarget values.\n\n\n\n\n\n\nReturns\n\n\n\n\nself\n : object\n\n\n\n\n\n\n\nfit_transform(X, y=None, \nfit_params)\n\n\nFit to data, then transform it.\n\n\nFits transformer to X and y with optional parameters fit_params\nand returns a transformed version of X.\n\n\n\nParameters\n\n\n\n\n\n\nX\n : numpy array of shape [n_samples, n_features]\n\n\nTraining set.\n\n\n\n\n\n\ny\n : numpy array of shape [n_samples]\n\n\nTarget values.\n\n\n\n\n\n\nReturns\n\n\n\n\n\n\nX_new\n : numpy array of shape [n_samples, n_features_new]\n\n\nTransformed array.\n\n\n\n\n\n\n\n\n\nget_params(deep=True)\n\n\nReturn estimator parameter names for GridSearch support.\n\n\n\n\n\npredict(X)\n\n\nPredict target values for X.\n\n\nParameters\n\n\n\n\n\n\nX\n : {array-like, sparse matrix}, shape = [n_samples, n_features]\n\n\nTraining vectors, where n_samples is the number of samples and\nn_features is the number of features.\n\n\n\n\n\n\nReturns\n\n\n\n\n\n\nlabels\n : array-like, shape = [n_samples]\n\n\nPredicted class labels.\n\n\n\n\n\n\n\n\n\npredict_proba(X)\n\n\nPredict class probabilities for X.\n\n\nParameters\n\n\n\n\n\n\nX\n : {array-like, sparse matrix}, shape = [n_samples, n_features]\n\n\nTraining vectors, where n_samples is the number of samples and\nn_features is the number of features.\n\n\n\n\n\n\nReturns\n\n\n\n\n\n\nproba\n : array-like, shape = [n_samples, n_classes]\n\n\nProbability for each class per sample.\n\n\n\n\n\n\n\n\n\nscore(X, y, sample_weight=None)\n\n\nReturns the mean accuracy on the given test data and labels.\n\n\nIn multi-label classification, this is the subset accuracy\nwhich is a harsh metric since you require for each sample that\neach label set be correctly predicted.\n\n\n\nParameters\n\n\n\n\n\n\nX\n : array-like, shape = (n_samples, n_features)\n\n\nTest samples.\n\n\n\n\n\n\ny\n : array-like, shape = (n_samples) or (n_samples, n_outputs)\n\n\nTrue labels for X.\n\n\n\n\n\n\nsample_weight\n : array-like, shape = [n_samples], optional\n\n\nSample weights.\n\n\n\n\n\n\nReturns\n\n\n\n\n\n\nscore\n : float\n\n\nMean accuracy of self.predict(X) wrt. y.\n\n\n\n\n\n\n\n\n\nset_params(\nparams)\n\n\nSet the parameters of this estimator.\n\n\nThe method works on simple estimators as well as on nested objects\n(such as pipelines). The former have parameters of the form\n``<component>__<parameter>`` so that it's possible to update each\ncomponent of a nested object.\n\n\n\nReturns\n\n\nself",
            "title": "StackingClassifier"
        },
        {
            "location": "/user_guide/classifier/StackingClassifier/#stackingclassifier",
            "text": "An ensemble-learning meta-classifier for stacking.   from mlxtend.classifier import StackingClassifier",
            "title": "StackingClassifier"
        },
        {
            "location": "/user_guide/classifier/StackingClassifier/#overview",
            "text": "Stacking is an ensemble learning technique to combine multiple classification models via a meta-classifier. The individual classification models are trained based on the complete training set; then, the meta-classifier is fitted based on the outputs -- meta-features -- of the individual classification models in the ensemble.\nThe meta-classifier can either be trained on the predicted class labels or probabilities from the ensemble.   The algorithm can be summarized as follows (source: [1]):",
            "title": "Overview"
        },
        {
            "location": "/user_guide/classifier/StackingClassifier/#references",
            "text": "[1] Tang, J., S. Alelyani, and H. Liu. \" Data Classification: Algorithms and Applications. \" Data Mining and Knowledge Discovery Series, CRC Press (2015): pp. 498-500.  [2] Wolpert, David H. \" Stacked generalization. \" Neural networks 5.2 (1992): 241-259.",
            "title": "References"
        },
        {
            "location": "/user_guide/classifier/StackingClassifier/#examples",
            "text": "",
            "title": "Examples"
        },
        {
            "location": "/user_guide/classifier/StackingClassifier/#example-1-simple-stacked-classification",
            "text": "from sklearn import datasets\n\niris = datasets.load_iris()\nX, y = iris.data[:, 1:3], iris.target  from sklearn import cross_validation\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.naive_bayes import GaussianNB \nfrom sklearn.ensemble import RandomForestClassifier\nfrom mlxtend.classifier import StackingClassifier\nimport numpy as np\n\nclf1 = KNeighborsClassifier(n_neighbors=1)\nclf2 = RandomForestClassifier(random_state=1)\nclf3 = GaussianNB()\nlr = LogisticRegression()\nsclf = StackingClassifier(classifiers=[clf1, clf2, clf3], \n                          meta_classifier=lr)\n\nprint('3-fold cross validation:\\n')\n\nfor clf, label in zip([clf1, clf2, clf3, sclf], \n                      ['KNN', \n                       'Random Forest', \n                       'Naive Bayes',\n                       'StackingClassifier']):\n\n    scores = cross_validation.cross_val_score(clf, X, y, \n                                              cv=3, scoring='accuracy')\n    print(\"Accuracy: %0.2f (+/- %0.2f) [%s]\" \n          % (scores.mean(), scores.std(), label))  3-fold cross validation:\n\nAccuracy: 0.91 (+/- 0.01) [KNN]\nAccuracy: 0.91 (+/- 0.06) [Random Forest]\nAccuracy: 0.92 (+/- 0.03) [Naive Bayes]\nAccuracy: 0.95 (+/- 0.03) [StackingClassifier]  import matplotlib.pyplot as plt\nfrom mlxtend.evaluate import plot_decision_regions\nimport matplotlib.gridspec as gridspec\nimport itertools\n\ngs = gridspec.GridSpec(2, 2)\n\nfig = plt.figure(figsize=(10,8))\n\nfor clf, lab, grd in zip([clf1, clf2, clf3, sclf], \n                         ['KNN', \n                          'Random Forest', \n                          'Naive Bayes',\n                          'StackingClassifier'],\n                          itertools.product([0, 1], repeat=2)):\n\n    clf.fit(X, y)\n    ax = plt.subplot(gs[grd[0], grd[1]])\n    fig = plot_decision_regions(X=X, y=y, clf=clf)\n    plt.title(lab)",
            "title": "Example 1 - Simple Stacked Classification"
        },
        {
            "location": "/user_guide/classifier/StackingClassifier/#example-2-stacked-classification-and-gridsearch",
            "text": "To set up a parameter grid for scikit-learn's  GridSearch , we simply provide the estimator's names in the parameter grid -- in the special case of the meta-regressor, we append the  'meta-'  prefix.  from sklearn.linear_model import LogisticRegression\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.naive_bayes import GaussianNB \nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.grid_search import GridSearchCV\nfrom mlxtend.classifier import StackingClassifier\n\n# Initializing models\n\nclf1 = KNeighborsClassifier(n_neighbors=1)\nclf2 = RandomForestClassifier(random_state=1)\nclf3 = GaussianNB()\nlr = LogisticRegression()\nsclf = StackingClassifier(classifiers=[clf1, clf2, clf3], \n                          meta_classifier=lr)\n\nparams = {'kneighborsclassifier__n_neighbors': [1, 5],\n          'randomforestclassifier__n_estimators': [10, 50],\n          'meta-logisticregression__C': [0.1, 10.0]}\n\ngrid = GridSearchCV(estimator=sclf, \n                    param_grid=params, \n                    cv=5,\n                    refit=True)\ngrid.fit(X, y)\n\nfor params, mean_score, scores in grid.grid_scores_:\n    print(\"%0.3f (+/-%0.03f) for %r\"\n        % (mean_score, scores.std() / 2, params))  0.667 (+/-0.000) for {'randomforestclassifier__n_estimators': 10, 'meta-logisticregression__C': 0.1, 'kneighborsclassifier__n_neighbors': 1}\n0.667 (+/-0.000) for {'randomforestclassifier__n_estimators': 50, 'meta-logisticregression__C': 0.1, 'kneighborsclassifier__n_neighbors': 1}\n0.927 (+/-0.022) for {'randomforestclassifier__n_estimators': 10, 'meta-logisticregression__C': 10.0, 'kneighborsclassifier__n_neighbors': 1}\n0.913 (+/-0.027) for {'randomforestclassifier__n_estimators': 50, 'meta-logisticregression__C': 10.0, 'kneighborsclassifier__n_neighbors': 1}\n0.667 (+/-0.000) for {'randomforestclassifier__n_estimators': 10, 'meta-logisticregression__C': 0.1, 'kneighborsclassifier__n_neighbors': 5}\n0.667 (+/-0.000) for {'randomforestclassifier__n_estimators': 50, 'meta-logisticregression__C': 0.1, 'kneighborsclassifier__n_neighbors': 5}\n0.933 (+/-0.024) for {'randomforestclassifier__n_estimators': 10, 'meta-logisticregression__C': 10.0, 'kneighborsclassifier__n_neighbors': 5}\n0.940 (+/-0.019) for {'randomforestclassifier__n_estimators': 50, 'meta-logisticregression__C': 10.0, 'kneighborsclassifier__n_neighbors': 5}  In case we are planning to use a regression algorithm multiple times, all we need to do is to add an additional number suffix in the parameter grid as shown below:  from sklearn.grid_search import GridSearchCV\n\n# Initializing models\n\nclf1 = KNeighborsClassifier(n_neighbors=1)\nclf2 = RandomForestClassifier(random_state=1)\nclf3 = GaussianNB()\nlr = LogisticRegression()\nsclf = StackingClassifier(classifiers=[clf1, clf1, clf2, clf3], \n                          meta_classifier=lr)\n\nparams = {'kneighborsclassifier-1__n_neighbors': [1, 5],\n          'kneighborsclassifier-2__n_neighbors': [1, 5],\n          'randomforestclassifier__n_estimators': [10, 50],\n          'meta-logisticregression__C': [0.1, 10.0]}\n\ngrid = GridSearchCV(estimator=sclf, \n                    param_grid=params, \n                    cv=5,\n                    refit=True)\ngrid.fit(X, y)\n\nfor params, mean_score, scores in grid.grid_scores_:\n    print(\"%0.3f (+/-%0.03f) for %r\"\n        % (mean_score, scores.std() / 2, params))  0.667 (+/-0.000) for {'kneighborsclassifier-2__n_neighbors': 1, 'randomforestclassifier__n_estimators': 10, 'meta-logisticregression__C': 0.1, 'kneighborsclassifier-1__n_neighbors': 1}\n0.667 (+/-0.000) for {'kneighborsclassifier-2__n_neighbors': 1, 'randomforestclassifier__n_estimators': 50, 'meta-logisticregression__C': 0.1, 'kneighborsclassifier-1__n_neighbors': 1}\n0.907 (+/-0.029) for {'kneighborsclassifier-2__n_neighbors': 1, 'randomforestclassifier__n_estimators': 10, 'meta-logisticregression__C': 10.0, 'kneighborsclassifier-1__n_neighbors': 1}\n0.913 (+/-0.027) for {'kneighborsclassifier-2__n_neighbors': 1, 'randomforestclassifier__n_estimators': 50, 'meta-logisticregression__C': 10.0, 'kneighborsclassifier-1__n_neighbors': 1}\n0.667 (+/-0.000) for {'kneighborsclassifier-2__n_neighbors': 5, 'randomforestclassifier__n_estimators': 10, 'meta-logisticregression__C': 0.1, 'kneighborsclassifier-1__n_neighbors': 1}\n0.667 (+/-0.000) for {'kneighborsclassifier-2__n_neighbors': 5, 'randomforestclassifier__n_estimators': 50, 'meta-logisticregression__C': 0.1, 'kneighborsclassifier-1__n_neighbors': 1}\n0.927 (+/-0.022) for {'kneighborsclassifier-2__n_neighbors': 5, 'randomforestclassifier__n_estimators': 10, 'meta-logisticregression__C': 10.0, 'kneighborsclassifier-1__n_neighbors': 1}\n0.913 (+/-0.027) for {'kneighborsclassifier-2__n_neighbors': 5, 'randomforestclassifier__n_estimators': 50, 'meta-logisticregression__C': 10.0, 'kneighborsclassifier-1__n_neighbors': 1}\n0.667 (+/-0.000) for {'kneighborsclassifier-2__n_neighbors': 1, 'randomforestclassifier__n_estimators': 10, 'meta-logisticregression__C': 0.1, 'kneighborsclassifier-1__n_neighbors': 5}\n0.667 (+/-0.000) for {'kneighborsclassifier-2__n_neighbors': 1, 'randomforestclassifier__n_estimators': 50, 'meta-logisticregression__C': 0.1, 'kneighborsclassifier-1__n_neighbors': 5}\n0.927 (+/-0.022) for {'kneighborsclassifier-2__n_neighbors': 1, 'randomforestclassifier__n_estimators': 10, 'meta-logisticregression__C': 10.0, 'kneighborsclassifier-1__n_neighbors': 5}\n0.913 (+/-0.027) for {'kneighborsclassifier-2__n_neighbors': 1, 'randomforestclassifier__n_estimators': 50, 'meta-logisticregression__C': 10.0, 'kneighborsclassifier-1__n_neighbors': 5}\n0.667 (+/-0.000) for {'kneighborsclassifier-2__n_neighbors': 5, 'randomforestclassifier__n_estimators': 10, 'meta-logisticregression__C': 0.1, 'kneighborsclassifier-1__n_neighbors': 5}\n0.667 (+/-0.000) for {'kneighborsclassifier-2__n_neighbors': 5, 'randomforestclassifier__n_estimators': 50, 'meta-logisticregression__C': 0.1, 'kneighborsclassifier-1__n_neighbors': 5}\n0.933 (+/-0.024) for {'kneighborsclassifier-2__n_neighbors': 5, 'randomforestclassifier__n_estimators': 10, 'meta-logisticregression__C': 10.0, 'kneighborsclassifier-1__n_neighbors': 5}\n0.940 (+/-0.019) for {'kneighborsclassifier-2__n_neighbors': 5, 'randomforestclassifier__n_estimators': 50, 'meta-logisticregression__C': 10.0, 'kneighborsclassifier-1__n_neighbors': 5}",
            "title": "Example 2 - Stacked Classification and GridSearch"
        },
        {
            "location": "/user_guide/classifier/StackingClassifier/#api",
            "text": "StackingClassifier(classifiers, meta_classifier, use_probas=False, verbose=0)  A Stacking classifier for scikit-learn estimators for classification.  Parameters    classifiers  : array-like, shape = [n_regressors]  A list of classifiers.\nInvoking the  fit  method on the  StackingClassifer  will fit clones\nof these original classifiers that will\nbe stored in the class attribute self.clfs_ .    meta_classifier  : object  The meta-classifier to be fitted on the ensemble of\nclassifiers    use_probas  : bool (default: False)  If True, trains meta-classifier based on predicted probabilities\ninstead of class labels.    verbose  : int, optional (default=0)  Controls the verbosity of the building process.\n-  verbose=0  (default): Prints nothing\n-  verbose=1 : Prints the number & name of the regressor being fitted\n-  verbose=2 : Prints info about the parameters of the\nregressor being fitted\n-  verbose>2 : Changes  verbose  param of the underlying regressor to\nself.verbose - 2    Attributes    clfs_  : list, shape=[n_classifiers]  Fitted classifiers (clones of the original classifiers)    meta_clf_  : estimator  Fitted meta-classifier (clone of the original meta-estimator)",
            "title": "API"
        },
        {
            "location": "/user_guide/classifier/StackingClassifier/#methods",
            "text": "fit(X, y)  Fit ensemble classifers and the meta-classifier.  Parameters    X  : {array-like, sparse matrix}, shape = [n_samples, n_features]  Training vectors, where n_samples is the number of samples and\nn_features is the number of features.    y  : array-like, shape = [n_samples]  Target values.    Returns   self  : object    fit_transform(X, y=None,  fit_params)  Fit to data, then transform it.  Fits transformer to X and y with optional parameters fit_params\nand returns a transformed version of X.  Parameters    X  : numpy array of shape [n_samples, n_features]  Training set.    y  : numpy array of shape [n_samples]  Target values.    Returns    X_new  : numpy array of shape [n_samples, n_features_new]  Transformed array.     get_params(deep=True)  Return estimator parameter names for GridSearch support.   predict(X)  Predict target values for X.  Parameters    X  : {array-like, sparse matrix}, shape = [n_samples, n_features]  Training vectors, where n_samples is the number of samples and\nn_features is the number of features.    Returns    labels  : array-like, shape = [n_samples]  Predicted class labels.     predict_proba(X)  Predict class probabilities for X.  Parameters    X  : {array-like, sparse matrix}, shape = [n_samples, n_features]  Training vectors, where n_samples is the number of samples and\nn_features is the number of features.    Returns    proba  : array-like, shape = [n_samples, n_classes]  Probability for each class per sample.     score(X, y, sample_weight=None)  Returns the mean accuracy on the given test data and labels.  In multi-label classification, this is the subset accuracy\nwhich is a harsh metric since you require for each sample that\neach label set be correctly predicted.  Parameters    X  : array-like, shape = (n_samples, n_features)  Test samples.    y  : array-like, shape = (n_samples) or (n_samples, n_outputs)  True labels for X.    sample_weight  : array-like, shape = [n_samples], optional  Sample weights.    Returns    score  : float  Mean accuracy of self.predict(X) wrt. y.     set_params( params)  Set the parameters of this estimator.  The method works on simple estimators as well as on nested objects\n(such as pipelines). The former have parameters of the form\n``<component>__<parameter>`` so that it's possible to update each\ncomponent of a nested object.  Returns  self",
            "title": "Methods"
        },
        {
            "location": "/user_guide/classifier/Perceptron/",
            "text": "Perceptron\n\n\nImplementation of a Perceptron learning algorithm for classification.\n\n\n\n\nfrom mlxtend.classifier import Perceptron\n\n\n\n\nOverview\n\n\nThe idea behind this \"thresholded\" perceptron was to mimic how a single neuron in the brain works: It either \"fires\" or not. \nA perceptron receives multiple input signals, and if the sum of the input signals exceed a certain threshold it either returns a signal or remains \"silent\" otherwise. What made this a \"machine learning\" algorithm was Frank Rosenblatt's idea of the perceptron learning rule: The perceptron algorithm is about learning the weights for the input signals in order to draw linear decision boundary that allows us to discriminate between the two linearly separable classes +1 and -1.\n\n\n\n\nBasic Notation\n\n\nBefore we dive deeper into the algorithm(s) for learning the weights of the perceptron classifier, let us take a brief look at the basic notation. In the following sections, we will label the \npositive\n and \nnegative\n class in our binary classification setting as \"1\" and \"-1\", respectively. Next, we define an activation function \n$g(\\mathbf{z})$\n that takes a linear combination of the input values \n$\\mathbf{x}$\n and weights \n$\\mathbf{w}$\n as input (\n$\\mathbf{z} = w_1x_{1} + \\dots + w_mx_{m}$\n), and if \n$g(\\mathbf{z})$\n is greater than a defined threshold \n$\\theta$\n we predict 1 and -1 otherwise; in this case, this activation function \n$g$\n is a simple \"unit step function,\" which is sometimes also called \"Heaviside step function.\" \n\n\n$$\n g(z) =\\begin{cases}\n    1 & \\text{if $z \\ge \\theta$}\\\\\n    -1 & \\text{otherwise}.\n  \\end{cases}\n$$\n\n\nwhere\n\n\n$$z =  w_1x_{1} + \\dots + w_mx_{m} = \\sum_{j=1}^{m} x_{j}w_{j} \\\\ = \\mathbf{w}^T\\mathbf{x}$$\n\n\n$\\mathbf{w}$\n is the feature vector, and \n$\\mathbf{x}$\n is an \n$m$\n-dimensional sample from the training dataset:\n\n\n$$ \n\\mathbf{w} = \\begin{bmatrix}\n    w_{1}  \\\\\n    \\vdots \\\\\n    w_{m}\n\\end{bmatrix}\n\\quad  \\mathbf{x} = \\begin{bmatrix}\n    x_{1}  \\\\\n    \\vdots \\\\\n    x_{m}\n\\end{bmatrix}$$\n\n\nIn order to simplify the notation, we bring \n$\\theta$\n to the left side of the equation and define \n$w_0 = -\\theta  \\text{ and } x_0=1$\n \n\n\nso that \n\n\n$$\n g({z}) =\\begin{cases}\n    1 & \\text{if $z \\ge 0$}\\\\\n    -1 & \\text{otherwise}.\n  \\end{cases}\n$$\n\n\nand\n\n\n$$z = w_0x_{0} + w_1x_{1} + \\dots + w_mx_{m} = \\sum_{j=0}^{m} x_{j}w_{j} \\\\ = \\mathbf{w}^T\\mathbf{x}.$$\n\n\nPerceptron Rule\n\n\nRosenblatt's initial perceptron rule is fairly simple and can be summarized by the following steps: \n\n\n\n\nInitialize the weights to 0 or small random numbers.\n\n\nFor each training sample \n$\\mathbf{x^{(i)}}$\n:\n\n\nCalculate the \noutput\n value.\n\n\nUpdate the weights.\n\n\n\n\n\n\n\n\nThe output value is the class label predicted by the unit step function that we defined earlier (output \n$=g(\\mathbf{z})$\n) and the weight update can be written more formally as  \n$w_j := w_j + \\Delta w_j$\n.\n\n\nThe value for updating the weights at each increment is calculated by the learning rule\n\n\n$\\Delta w_j = \\eta \\; (\\text{target}^{(i)} - \\text{output}^{(i)})\\;x^{(i)}_{j}$\n\n\nwhere \n$\\eta$\n is the learning rate (a constant between 0.0 and 1.0), \"target\" is the true class label, and the \"output\" is the predicted class label.\n\n\naIt is important to note that all weights in the weight vector are being updated simultaneously. Concretely, for a 2-dimensional dataset, we would write the update as:\n\n\n$\\Delta w_0 = \\eta(\\text{target}^{(i)} - \\text{output}^{(i)})$\n\n\n$\\Delta w_1 = \\eta(\\text{target}^{(i)} - \\text{output}^{(i)})\\;x^{(i)}_{1}$\n\n\n$\\Delta w_2 = \\eta(\\text{target}^{(i)} - \\text{output}^{(i)})\\;x^{(i)}_{2}$\n  \n\n\nBefore we implement the perceptron rule in Python, let us make a simple thought experiment to illustrate how beautifully simple this learning rule really is. In the two scenarios where the perceptron predicts the class label correctly, the weights remain unchanged:\n\n\n\n\n$\\Delta w_j = \\eta(-1^{(i)} - -1^{(i)})\\;x^{(i)}_{j} = 0$\n \n\n\n$\\Delta w_j = \\eta(1^{(i)} - 1^{(i)})\\;x^{(i)}_{j} = 0$\n \n\n\n\n\nHowever, in case of a wrong prediction, the weights are being \"pushed\" towards the direction of the positive or negative target class, respectively:\n\n\n\n\n$\\Delta w_j = \\eta(1^{(i)} - -1^{(i)})\\;x^{(i)}_{j} = \\eta(2)\\;x^{(i)}_{j}$\n \n\n\n$\\Delta w_j = \\eta(-1^{(i)} - 1^{(i)})\\;x^{(i)}_{j} = \\eta(-2)\\;x^{(i)}_{j}$\n \n\n\n\n\nIt is important to note that the convergence of the perceptron is only guaranteed if the two classes are linearly separable. If the two classes can't be separated by a linear decision boundary, we can set a maximum number of passes over the training dataset (\"epochs\") and/or a threshold for the number of tolerated misclassifications.\n\n\nReferences\n\n\n\n\nF. Rosenblatt. The perceptron, a perceiving and recognizing automaton Project Para. Cornell Aeronautical Laboratory, 1957.\n\n\n\n\nExamples\n\n\nExample 1 - Classification of Iris Flowers\n\n\nfrom mlxtend.data import iris_data\nfrom mlxtend.evaluate import plot_decision_regions\nfrom mlxtend.classifier import Perceptron\nimport matplotlib.pyplot as plt\n\n# Loading Data\n\nX, y = iris_data()\nX = X[:, [0, 3]] # sepal length and petal width\nX = X[0:100] # class 0 and class 1\ny = y[0:100] # class 0 and class 1\n\n# standardize\nX[:,0] = (X[:,0] - X[:,0].mean()) / X[:,0].std()\nX[:,1] = (X[:,1] - X[:,1].mean()) / X[:,1].std()\n\n\n# Rosenblatt Perceptron\n\nppn = Perceptron(epochs=5, \n                 eta=0.05, \n                 random_seed=0,\n                 print_progress=3)\nppn.fit(X, y)\n\nplot_decision_regions(X, y, clf=ppn)\nplt.title('Perceptron - Rosenblatt Perceptron Rule')\nplt.show()\n\nprint('Bias & Weights: %s' % ppn.w_)\n\nplt.plot(range(len(ppn.cost_)), ppn.cost_)\nplt.xlabel('Iterations')\nplt.ylabel('Missclassifications')\nplt.show()\n\n\n\n\nIteration: 5/5 | Elapsed: 00:00:00 | ETA: 00:00:00\n\n\n\n\n\nBias & Weights: [[-0.04500809]\n [ 0.11048855]]\n\n\n\n\n\nAPI\n\n\nPerceptron(eta=0.1, epochs=50, random_seed=None, print_progress=0)\n\n\nPerceptron classifier.\n\n\nNote that this implementation of the Perceptron expects binary class labels\nin {0, 1}.\n\n\n\nParameters\n\n\n\n\n\n\neta\n : float (default: 0.1)\n\n\nLearning rate (between 0.0 and 1.0)\n\n\n\n\n\n\nepochs\n : int (default: 50)\n\n\nNumber of passes over the training dataset.\nPrior to each epoch, the dataset is shuffled to prevent cycles.\n\n\n\n\n\n\nrandom_seed\n : int\n\n\nRandom state for initializing random weights and shuffling.\n\n\n\n\n\n\nprint_progress\n : int (default: 0)\n\n\nPrints progress in fitting to stderr.\n0: No output\n1: Epochs elapsed and cost\n2: 1 plus time elapsed\n3: 2 plus estimated time until completion\n\n\n\n\n\n\nAttributes\n\n\n\n\n\n\nw_\n : 2d-array, shape={n_features, 1}\n\n\nModel weights after fitting.\n\n\n\n\n\n\nb_\n : 1d-array, shape={1,}\n\n\nBias unit after fitting.\n\n\n\n\n\n\ncost_\n : list\n\n\nNumber of misclassifications in every epoch.\n\n\n\n\n\n\nMethods\n\n\n\n\n\nfit(X, y, init_params=True)\n\n\nLearn model from training data.\n\n\nParameters\n\n\n\n\n\n\nX\n : {array-like, sparse matrix}, shape = [n_samples, n_features]\n\n\nTraining vectors, where n_samples is the number of samples and\nn_features is the number of features.\n\n\n\n\n\n\ny\n : array-like, shape = [n_samples]\n\n\nTarget values.\n\n\n\n\n\n\ninit_params\n : bool (default: True)\n\n\nRe-initializes model parameters prior to fitting.\nSet False to continue training with weights from\na previous model fitting.\n\n\n\n\n\n\nReturns\n\n\n\n\nself\n : object\n\n\n\n\n\n\n\npredict(X)\n\n\nPredict targets from X.\n\n\nParameters\n\n\n\n\n\n\nX\n : {array-like, sparse matrix}, shape = [n_samples, n_features]\n\n\nTraining vectors, where n_samples is the number of samples and\nn_features is the number of features.\n\n\n\n\n\n\nReturns\n\n\n\n\n\n\ntarget_values\n : array-like, shape = [n_samples]\n\n\nPredicted target values.\n\n\n\n\n\n\n\n\n\nscore(X, y)\n\n\nCompute the prediction accuracy\n\n\nParameters\n\n\n\n\n\n\nX\n : {array-like, sparse matrix}, shape = [n_samples, n_features]\n\n\nTraining vectors, where n_samples is the number of samples and\nn_features is the number of features.\n\n\n\n\n\n\ny\n : array-like, shape = [n_samples]\n\n\nTarget values (true class labels).\n\n\n\n\n\n\nReturns\n\n\n\n\n\n\nacc\n : float\n\n\nThe prediction accuracy as a float\nbetween 0.0 and 1.0 (perfect score).",
            "title": "Perceptron"
        },
        {
            "location": "/user_guide/classifier/Perceptron/#perceptron",
            "text": "Implementation of a Perceptron learning algorithm for classification.   from mlxtend.classifier import Perceptron",
            "title": "Perceptron"
        },
        {
            "location": "/user_guide/classifier/Perceptron/#overview",
            "text": "The idea behind this \"thresholded\" perceptron was to mimic how a single neuron in the brain works: It either \"fires\" or not. \nA perceptron receives multiple input signals, and if the sum of the input signals exceed a certain threshold it either returns a signal or remains \"silent\" otherwise. What made this a \"machine learning\" algorithm was Frank Rosenblatt's idea of the perceptron learning rule: The perceptron algorithm is about learning the weights for the input signals in order to draw linear decision boundary that allows us to discriminate between the two linearly separable classes +1 and -1.",
            "title": "Overview"
        },
        {
            "location": "/user_guide/classifier/Perceptron/#basic-notation",
            "text": "Before we dive deeper into the algorithm(s) for learning the weights of the perceptron classifier, let us take a brief look at the basic notation. In the following sections, we will label the  positive  and  negative  class in our binary classification setting as \"1\" and \"-1\", respectively. Next, we define an activation function  $g(\\mathbf{z})$  that takes a linear combination of the input values  $\\mathbf{x}$  and weights  $\\mathbf{w}$  as input ( $\\mathbf{z} = w_1x_{1} + \\dots + w_mx_{m}$ ), and if  $g(\\mathbf{z})$  is greater than a defined threshold  $\\theta$  we predict 1 and -1 otherwise; in this case, this activation function  $g$  is a simple \"unit step function,\" which is sometimes also called \"Heaviside step function.\"   $$\n g(z) =\\begin{cases}\n    1 & \\text{if $z \\ge \\theta$}\\\\\n    -1 & \\text{otherwise}.\n  \\end{cases}\n$$  where  $$z =  w_1x_{1} + \\dots + w_mx_{m} = \\sum_{j=1}^{m} x_{j}w_{j} \\\\ = \\mathbf{w}^T\\mathbf{x}$$  $\\mathbf{w}$  is the feature vector, and  $\\mathbf{x}$  is an  $m$ -dimensional sample from the training dataset:  $$ \n\\mathbf{w} = \\begin{bmatrix}\n    w_{1}  \\\\\n    \\vdots \\\\\n    w_{m}\n\\end{bmatrix}\n\\quad  \\mathbf{x} = \\begin{bmatrix}\n    x_{1}  \\\\\n    \\vdots \\\\\n    x_{m}\n\\end{bmatrix}$$  In order to simplify the notation, we bring  $\\theta$  to the left side of the equation and define  $w_0 = -\\theta  \\text{ and } x_0=1$    so that   $$\n g({z}) =\\begin{cases}\n    1 & \\text{if $z \\ge 0$}\\\\\n    -1 & \\text{otherwise}.\n  \\end{cases}\n$$  and  $$z = w_0x_{0} + w_1x_{1} + \\dots + w_mx_{m} = \\sum_{j=0}^{m} x_{j}w_{j} \\\\ = \\mathbf{w}^T\\mathbf{x}.$$",
            "title": "Basic Notation"
        },
        {
            "location": "/user_guide/classifier/Perceptron/#perceptron-rule",
            "text": "Rosenblatt's initial perceptron rule is fairly simple and can be summarized by the following steps:    Initialize the weights to 0 or small random numbers.  For each training sample  $\\mathbf{x^{(i)}}$ :  Calculate the  output  value.  Update the weights.     The output value is the class label predicted by the unit step function that we defined earlier (output  $=g(\\mathbf{z})$ ) and the weight update can be written more formally as   $w_j := w_j + \\Delta w_j$ .  The value for updating the weights at each increment is calculated by the learning rule  $\\Delta w_j = \\eta \\; (\\text{target}^{(i)} - \\text{output}^{(i)})\\;x^{(i)}_{j}$  where  $\\eta$  is the learning rate (a constant between 0.0 and 1.0), \"target\" is the true class label, and the \"output\" is the predicted class label.  aIt is important to note that all weights in the weight vector are being updated simultaneously. Concretely, for a 2-dimensional dataset, we would write the update as:  $\\Delta w_0 = \\eta(\\text{target}^{(i)} - \\text{output}^{(i)})$  $\\Delta w_1 = \\eta(\\text{target}^{(i)} - \\text{output}^{(i)})\\;x^{(i)}_{1}$  $\\Delta w_2 = \\eta(\\text{target}^{(i)} - \\text{output}^{(i)})\\;x^{(i)}_{2}$     Before we implement the perceptron rule in Python, let us make a simple thought experiment to illustrate how beautifully simple this learning rule really is. In the two scenarios where the perceptron predicts the class label correctly, the weights remain unchanged:   $\\Delta w_j = \\eta(-1^{(i)} - -1^{(i)})\\;x^{(i)}_{j} = 0$    $\\Delta w_j = \\eta(1^{(i)} - 1^{(i)})\\;x^{(i)}_{j} = 0$     However, in case of a wrong prediction, the weights are being \"pushed\" towards the direction of the positive or negative target class, respectively:   $\\Delta w_j = \\eta(1^{(i)} - -1^{(i)})\\;x^{(i)}_{j} = \\eta(2)\\;x^{(i)}_{j}$    $\\Delta w_j = \\eta(-1^{(i)} - 1^{(i)})\\;x^{(i)}_{j} = \\eta(-2)\\;x^{(i)}_{j}$     It is important to note that the convergence of the perceptron is only guaranteed if the two classes are linearly separable. If the two classes can't be separated by a linear decision boundary, we can set a maximum number of passes over the training dataset (\"epochs\") and/or a threshold for the number of tolerated misclassifications.",
            "title": "Perceptron Rule"
        },
        {
            "location": "/user_guide/classifier/Perceptron/#references",
            "text": "F. Rosenblatt. The perceptron, a perceiving and recognizing automaton Project Para. Cornell Aeronautical Laboratory, 1957.",
            "title": "References"
        },
        {
            "location": "/user_guide/classifier/Perceptron/#examples",
            "text": "",
            "title": "Examples"
        },
        {
            "location": "/user_guide/classifier/Perceptron/#example-1-classification-of-iris-flowers",
            "text": "from mlxtend.data import iris_data\nfrom mlxtend.evaluate import plot_decision_regions\nfrom mlxtend.classifier import Perceptron\nimport matplotlib.pyplot as plt\n\n# Loading Data\n\nX, y = iris_data()\nX = X[:, [0, 3]] # sepal length and petal width\nX = X[0:100] # class 0 and class 1\ny = y[0:100] # class 0 and class 1\n\n# standardize\nX[:,0] = (X[:,0] - X[:,0].mean()) / X[:,0].std()\nX[:,1] = (X[:,1] - X[:,1].mean()) / X[:,1].std()\n\n\n# Rosenblatt Perceptron\n\nppn = Perceptron(epochs=5, \n                 eta=0.05, \n                 random_seed=0,\n                 print_progress=3)\nppn.fit(X, y)\n\nplot_decision_regions(X, y, clf=ppn)\nplt.title('Perceptron - Rosenblatt Perceptron Rule')\nplt.show()\n\nprint('Bias & Weights: %s' % ppn.w_)\n\nplt.plot(range(len(ppn.cost_)), ppn.cost_)\nplt.xlabel('Iterations')\nplt.ylabel('Missclassifications')\nplt.show()  Iteration: 5/5 | Elapsed: 00:00:00 | ETA: 00:00:00   Bias & Weights: [[-0.04500809]\n [ 0.11048855]]",
            "title": "Example 1 - Classification of Iris Flowers"
        },
        {
            "location": "/user_guide/classifier/Perceptron/#api",
            "text": "Perceptron(eta=0.1, epochs=50, random_seed=None, print_progress=0)  Perceptron classifier.  Note that this implementation of the Perceptron expects binary class labels\nin {0, 1}.  Parameters    eta  : float (default: 0.1)  Learning rate (between 0.0 and 1.0)    epochs  : int (default: 50)  Number of passes over the training dataset.\nPrior to each epoch, the dataset is shuffled to prevent cycles.    random_seed  : int  Random state for initializing random weights and shuffling.    print_progress  : int (default: 0)  Prints progress in fitting to stderr.\n0: No output\n1: Epochs elapsed and cost\n2: 1 plus time elapsed\n3: 2 plus estimated time until completion    Attributes    w_  : 2d-array, shape={n_features, 1}  Model weights after fitting.    b_  : 1d-array, shape={1,}  Bias unit after fitting.    cost_  : list  Number of misclassifications in every epoch.",
            "title": "API"
        },
        {
            "location": "/user_guide/classifier/Perceptron/#methods",
            "text": "fit(X, y, init_params=True)  Learn model from training data.  Parameters    X  : {array-like, sparse matrix}, shape = [n_samples, n_features]  Training vectors, where n_samples is the number of samples and\nn_features is the number of features.    y  : array-like, shape = [n_samples]  Target values.    init_params  : bool (default: True)  Re-initializes model parameters prior to fitting.\nSet False to continue training with weights from\na previous model fitting.    Returns   self  : object    predict(X)  Predict targets from X.  Parameters    X  : {array-like, sparse matrix}, shape = [n_samples, n_features]  Training vectors, where n_samples is the number of samples and\nn_features is the number of features.    Returns    target_values  : array-like, shape = [n_samples]  Predicted target values.     score(X, y)  Compute the prediction accuracy  Parameters    X  : {array-like, sparse matrix}, shape = [n_samples, n_features]  Training vectors, where n_samples is the number of samples and\nn_features is the number of features.    y  : array-like, shape = [n_samples]  Target values (true class labels).    Returns    acc  : float  The prediction accuracy as a float\nbetween 0.0 and 1.0 (perfect score).",
            "title": "Methods"
        },
        {
            "location": "/user_guide/classifier/Adaline/",
            "text": "Adaptive Linear Neuron -- Adaline\n\n\nAn implementation of the ADAptive LInear NEuron, Adaline, for binary classification tasks.\n\n\n\n\nfrom mlxtend.classifier import Adaline\n\n\n\n\nOverview\n\n\nAn illustration of the ADAptive LInear NEuron (Adaline) -- a single-layer artificial linear neuron with a threshold unit:\n\n\n\n\nThe Adaline classifier is closely related to the Ordinary Least Squares (OLS) Linear Regression algorithm; in OLS regression we find the line (or hyperplane) that minimizes the vertical offsets. Or in other words, we define the best-fitting line as the line that minimizes the sum of squared errors (SSE) or mean squared error (MSE) between our target variable (y) and our predicted output over all samples \n$i$\n in our dataset of size \n$n$\n.\n\n\n$$ SSE =  \\sum_i (\\text{target}^{(i)} - \\text{output}^{(i)})^2$$\n\n\n$$MSE = \\frac{1}{n} \\times SSE$$\n\n\nLinearRegression\n implements a linear regression model for performing ordinary least squares regression, and in Adaline, we add a threshold function \n$g(\\cdot)$\n to convert the continuous outcome to a categorical class label:\n\n\n$$y = g({z}) =\n\\begin{cases}\n1 & \\text{if z $\\ge$ 0}\\\\\n-1 & \\text{otherwise}.\n\\end{cases}\n$$\n\n\nAn Adaline model can be trained by one of the following three approaches:\n\n\n\n\nNormal Equations\n\n\nGradient Descent\n\n\nStochastic Gradient Descent\n\n\n\n\nNormal Equations (closed-form solution)\n\n\nThe closed-form solution should be preferred for \"smaller\" datasets where calculating (a \"costly\") matrix inverse is not a concern. For very large datasets, or datasets where the inverse of \n$[X^T X]$\n may not exist (the matrix is non-invertible or singular, e.g., in case of perfect multicollinearity), the gradient descent or stochastic gradient descent approaches are to be preferred.\n\n\nThe linear function (linear regression model) is defined as:\n\n\n$$z = w_0x_0 + w_1x_1 + ... + w_mx_m = \\sum_{j=0}^{m} w_j x_j = \\mathbf{w}^T\\mathbf{x}$$\n\n\nwhere \n$y$\n is the response variable, \n$\\mathbf{x}$\n is an \n$m$\n-dimensional sample vector, and \n$\\mathbf{w}$\n is the weight vector (vector of coefficients). Note that \n$w_0$\n represents the y-axis intercept of the model and therefore \n$x_0=1$\n.  \n\n\nUsing the closed-form solution (normal equation), we compute the weights of the model as follows:\n\n\n$$ \\mathbf{w} = (\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{X}^Ty$$\n\n\nGradient Descent (GD)  and Stochastic Gradient Descent (SGD)\n\n\nIn the current implementation, the Adaline model is learned via Gradient Descent or Stochastic Gradient Descent.\n\n\nSee \nGradient Descent and Stochastic Gradient Descent\n and \nDeriving the Gradient Descent Rule for Linear Regression and Adaline\n for details.\n\n\nRandom shuffling is implemented as:\n\n\n\n\nfor one or more epochs\n\n\nrandomly shuffle samples in the training set\n\n\nfor training sample \ni\n\n\ncompute gradients and perform weight updates\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nReferences\n\n\n\n\nB. Widrow, M. E. Hoff, et al. \nAdaptive switching circuits\n. 1960.\n\n\n\n\nExamples\n\n\nExample 1 - Closed Form Solution\n\n\nfrom mlxtend.data import iris_data\nfrom mlxtend.evaluate import plot_decision_regions\nfrom mlxtend.classifier import Adaline\nimport matplotlib.pyplot as plt\n\n# Loading Data\n\nX, y = iris_data()\nX = X[:, [0, 3]] # sepal length and petal width\nX = X[0:100] # class 0 and class 1\ny = y[0:100] # class 0 and class 1\n\n# standardize\nX[:,0] = (X[:,0] - X[:,0].mean()) / X[:,0].std()\nX[:,1] = (X[:,1] - X[:,1].mean()) / X[:,1].std()\n\n\nada = Adaline(epochs=30, \n              eta=0.01, \n              minibatches=None, \n              random_seed=1)\nada.fit(X, y)\nplot_decision_regions(X, y, clf=ada)\nplt.title('Adaline - Stochastic Gradient Descent')\n\nplt.show()\n\n\n\n\n\n\nExample 2 - Gradient Descent\n\n\nfrom mlxtend.data import iris_data\nfrom mlxtend.evaluate import plot_decision_regions\nfrom mlxtend.classifier import Adaline\nimport matplotlib.pyplot as plt\n\n# Loading Data\n\nX, y = iris_data()\nX = X[:, [0, 3]] # sepal length and petal width\nX = X[0:100] # class 0 and class 1\ny = y[0:100] # class 0 and class 1\n\n# standardize\nX[:,0] = (X[:,0] - X[:,0].mean()) / X[:,0].std()\nX[:,1] = (X[:,1] - X[:,1].mean()) / X[:,1].std()\n\n\nada = Adaline(epochs=30, \n              eta=0.01, \n              minibatches=1, # for Gradient Descent Learning\n              random_seed=1,\n              print_progress=3)\n\nada.fit(X, y)\nplot_decision_regions(X, y, clf=ada)\nplt.title('Adaline - Stochastic Gradient Descent')\nplt.show()\n\nplt.plot(range(len(ada.cost_)), ada.cost_)\nplt.xlabel('Iterations')\nplt.ylabel('Cost')\n\n\n\n\nIteration: 30/30 | Cost 3.79 | Elapsed: 0:00:00 | ETA: 0:00:00\n\n\n\n\n\n<matplotlib.text.Text at 0x10ca096a0>\n\n\n\n\n\nExample 3 - Stochastic Gradient Descent\n\n\nfrom mlxtend.data import iris_data\nfrom mlxtend.evaluate import plot_decision_regions\nfrom mlxtend.classifier import Adaline\nimport matplotlib.pyplot as plt\n\n# Loading Data\n\nX, y = iris_data()\nX = X[:, [0, 3]] # sepal length and petal width\nX = X[0:100] # class 0 and class 1\ny = y[0:100] # class 0 and class 1\n\n# standardize\nX[:,0] = (X[:,0] - X[:,0].mean()) / X[:,0].std()\nX[:,1] = (X[:,1] - X[:,1].mean()) / X[:,1].std()\n\n\nada = Adaline(epochs=15, \n              eta=0.02, \n              minibatches=len(y), # for SGD learning \n              random_seed=1,\n              print_progress=3)\n\nada.fit(X, y)\nplot_decision_regions(X, y, clf=ada)\nplt.title('Adaline - Stochastic Gradient Descent')\nplt.show()\n\nplt.plot(range(len(ada.cost_)), ada.cost_)\nplt.xlabel('Iterations')\nplt.ylabel('Cost')\nplt.show()\n\n\n\n\nIteration: 15/15 | Cost 3.81 | Elapsed: 0:00:00 | ETA: 0:00:00\n\n\n\n\n\n\n\nExample 4 - Stochastic Gradient Descent with Minibatches\n\n\nfrom mlxtend.data import iris_data\nfrom mlxtend.evaluate import plot_decision_regions\nfrom mlxtend.classifier import Adaline\nimport matplotlib.pyplot as plt\n\n# Loading Data\n\nX, y = iris_data()\nX = X[:, [0, 3]] # sepal length and petal width\nX = X[0:100] # class 0 and class 1\ny = y[0:100] # class 0 and class 1\n\n# standardize\nX[:,0] = (X[:,0] - X[:,0].mean()) / X[:,0].std()\nX[:,1] = (X[:,1] - X[:,1].mean()) / X[:,1].std()\n\n\nada = Adaline(epochs=15, \n              eta=0.02, \n              minibatches=5, # for SGD learning w. minibatch size 20\n              random_seed=1,\n              print_progress=3)\n\nada.fit(X, y)\nplot_decision_regions(X, y, clf=ada)\nplt.title('Adaline - Stochastic Gradient Descent')\nplt.show()\n\nplt.plot(range(len(ada.cost_)), ada.cost_)\nplt.xlabel('Iterations')\nplt.ylabel('Cost')\nplt.show()\n\n\n\n\nIteration: 15/15 | Cost 3.87 | Elapsed: 0:00:00 | ETA: 0:00:00\n\n\n\n\n\n\n\nAPI\n\n\nAdaline(eta=0.01, epochs=50, minibatches=None, random_seed=None, print_progress=0)\n\n\nADAptive LInear NEuron classifier.\n\n\nNote that this implementation of Adaline expects binary class labels\nin {0, 1}.\n\n\n\nParameters\n\n\n\n\n\n\neta\n : float (default: 0.01)\n\n\nsolver rate (between 0.0 and 1.0)\n\n\n\n\n\n\nepochs\n : int (default: 50)\n\n\nPasses over the training dataset.\nPrior to each epoch, the dataset is shuffled\nif \nminibatches > 1\n to prevent cycles in stochastic gradient descent.\n\n\n\n\n\n\nminibatches\n : int (default: None)\n\n\nThe number of minibatches for gradient-based optimization.\nIf None: Normal Equations (closed-form solution)\nIf 1: Gradient Descent learning\nIf len(y): Stochastic Gradient Descent (SGD) online learning\nIf 1 < minibatches < len(y): SGD Minibatch learning\n\n\n\n\n\n\nrandom_seed\n : int (default: None)\n\n\nSet random state for shuffling and initializing the weights.\n\n\n\n\n\n\nprint_progress\n : int (default: 0)\n\n\nPrints progress in fitting to stderr if not solver='normal equation'\n0: No output\n1: Epochs elapsed and cost\n2: 1 plus time elapsed\n3: 2 plus estimated time until completion\n\n\n\n\n\n\nAttributes\n\n\n\n\n\n\nw_\n : 2d-array, shape={n_features, 1}\n\n\nModel weights after fitting.\n\n\n\n\n\n\nb_\n : 1d-array, shape={1,}\n\n\nBias unit after fitting.\n\n\n\n\n\n\ncost_\n : list\n\n\nSum of squared errors after each epoch.\n\n\n\n\n\n\nMethods\n\n\n\n\n\nfit(X, y, init_params=True)\n\n\nLearn model from training data.\n\n\nParameters\n\n\n\n\n\n\nX\n : {array-like, sparse matrix}, shape = [n_samples, n_features]\n\n\nTraining vectors, where n_samples is the number of samples and\nn_features is the number of features.\n\n\n\n\n\n\ny\n : array-like, shape = [n_samples]\n\n\nTarget values.\n\n\n\n\n\n\ninit_params\n : bool (default: True)\n\n\nRe-initializes model parameters prior to fitting.\nSet False to continue training with weights from\na previous model fitting.\n\n\n\n\n\n\nReturns\n\n\n\n\nself\n : object\n\n\n\n\n\n\n\npredict(X)\n\n\nPredict targets from X.\n\n\nParameters\n\n\n\n\n\n\nX\n : {array-like, sparse matrix}, shape = [n_samples, n_features]\n\n\nTraining vectors, where n_samples is the number of samples and\nn_features is the number of features.\n\n\n\n\n\n\nReturns\n\n\n\n\n\n\ntarget_values\n : array-like, shape = [n_samples]\n\n\nPredicted target values.\n\n\n\n\n\n\n\n\n\nscore(X, y)\n\n\nCompute the prediction accuracy\n\n\nParameters\n\n\n\n\n\n\nX\n : {array-like, sparse matrix}, shape = [n_samples, n_features]\n\n\nTraining vectors, where n_samples is the number of samples and\nn_features is the number of features.\n\n\n\n\n\n\ny\n : array-like, shape = [n_samples]\n\n\nTarget values (true class labels).\n\n\n\n\n\n\nReturns\n\n\n\n\n\n\nacc\n : float\n\n\nThe prediction accuracy as a float\nbetween 0.0 and 1.0 (perfect score).",
            "title": "Adaline"
        },
        {
            "location": "/user_guide/classifier/Adaline/#adaptive-linear-neuron-adaline",
            "text": "An implementation of the ADAptive LInear NEuron, Adaline, for binary classification tasks.   from mlxtend.classifier import Adaline",
            "title": "Adaptive Linear Neuron -- Adaline"
        },
        {
            "location": "/user_guide/classifier/Adaline/#overview",
            "text": "An illustration of the ADAptive LInear NEuron (Adaline) -- a single-layer artificial linear neuron with a threshold unit:   The Adaline classifier is closely related to the Ordinary Least Squares (OLS) Linear Regression algorithm; in OLS regression we find the line (or hyperplane) that minimizes the vertical offsets. Or in other words, we define the best-fitting line as the line that minimizes the sum of squared errors (SSE) or mean squared error (MSE) between our target variable (y) and our predicted output over all samples  $i$  in our dataset of size  $n$ .  $$ SSE =  \\sum_i (\\text{target}^{(i)} - \\text{output}^{(i)})^2$$  $$MSE = \\frac{1}{n} \\times SSE$$  LinearRegression  implements a linear regression model for performing ordinary least squares regression, and in Adaline, we add a threshold function  $g(\\cdot)$  to convert the continuous outcome to a categorical class label:  $$y = g({z}) =\n\\begin{cases}\n1 & \\text{if z $\\ge$ 0}\\\\\n-1 & \\text{otherwise}.\n\\end{cases}\n$$  An Adaline model can be trained by one of the following three approaches:   Normal Equations  Gradient Descent  Stochastic Gradient Descent",
            "title": "Overview"
        },
        {
            "location": "/user_guide/classifier/Adaline/#normal-equations-closed-form-solution",
            "text": "The closed-form solution should be preferred for \"smaller\" datasets where calculating (a \"costly\") matrix inverse is not a concern. For very large datasets, or datasets where the inverse of  $[X^T X]$  may not exist (the matrix is non-invertible or singular, e.g., in case of perfect multicollinearity), the gradient descent or stochastic gradient descent approaches are to be preferred.  The linear function (linear regression model) is defined as:  $$z = w_0x_0 + w_1x_1 + ... + w_mx_m = \\sum_{j=0}^{m} w_j x_j = \\mathbf{w}^T\\mathbf{x}$$  where  $y$  is the response variable,  $\\mathbf{x}$  is an  $m$ -dimensional sample vector, and  $\\mathbf{w}$  is the weight vector (vector of coefficients). Note that  $w_0$  represents the y-axis intercept of the model and therefore  $x_0=1$ .    Using the closed-form solution (normal equation), we compute the weights of the model as follows:  $$ \\mathbf{w} = (\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{X}^Ty$$",
            "title": "Normal Equations (closed-form solution)"
        },
        {
            "location": "/user_guide/classifier/Adaline/#gradient-descent-gd-and-stochastic-gradient-descent-sgd",
            "text": "In the current implementation, the Adaline model is learned via Gradient Descent or Stochastic Gradient Descent.  See  Gradient Descent and Stochastic Gradient Descent  and  Deriving the Gradient Descent Rule for Linear Regression and Adaline  for details.  Random shuffling is implemented as:   for one or more epochs  randomly shuffle samples in the training set  for training sample  i  compute gradients and perform weight updates",
            "title": "Gradient Descent (GD)  and Stochastic Gradient Descent (SGD)"
        },
        {
            "location": "/user_guide/classifier/Adaline/#references",
            "text": "B. Widrow, M. E. Hoff, et al.  Adaptive switching circuits . 1960.",
            "title": "References"
        },
        {
            "location": "/user_guide/classifier/Adaline/#examples",
            "text": "",
            "title": "Examples"
        },
        {
            "location": "/user_guide/classifier/Adaline/#example-1-closed-form-solution",
            "text": "from mlxtend.data import iris_data\nfrom mlxtend.evaluate import plot_decision_regions\nfrom mlxtend.classifier import Adaline\nimport matplotlib.pyplot as plt\n\n# Loading Data\n\nX, y = iris_data()\nX = X[:, [0, 3]] # sepal length and petal width\nX = X[0:100] # class 0 and class 1\ny = y[0:100] # class 0 and class 1\n\n# standardize\nX[:,0] = (X[:,0] - X[:,0].mean()) / X[:,0].std()\nX[:,1] = (X[:,1] - X[:,1].mean()) / X[:,1].std()\n\n\nada = Adaline(epochs=30, \n              eta=0.01, \n              minibatches=None, \n              random_seed=1)\nada.fit(X, y)\nplot_decision_regions(X, y, clf=ada)\nplt.title('Adaline - Stochastic Gradient Descent')\n\nplt.show()",
            "title": "Example 1 - Closed Form Solution"
        },
        {
            "location": "/user_guide/classifier/Adaline/#example-2-gradient-descent",
            "text": "from mlxtend.data import iris_data\nfrom mlxtend.evaluate import plot_decision_regions\nfrom mlxtend.classifier import Adaline\nimport matplotlib.pyplot as plt\n\n# Loading Data\n\nX, y = iris_data()\nX = X[:, [0, 3]] # sepal length and petal width\nX = X[0:100] # class 0 and class 1\ny = y[0:100] # class 0 and class 1\n\n# standardize\nX[:,0] = (X[:,0] - X[:,0].mean()) / X[:,0].std()\nX[:,1] = (X[:,1] - X[:,1].mean()) / X[:,1].std()\n\n\nada = Adaline(epochs=30, \n              eta=0.01, \n              minibatches=1, # for Gradient Descent Learning\n              random_seed=1,\n              print_progress=3)\n\nada.fit(X, y)\nplot_decision_regions(X, y, clf=ada)\nplt.title('Adaline - Stochastic Gradient Descent')\nplt.show()\n\nplt.plot(range(len(ada.cost_)), ada.cost_)\nplt.xlabel('Iterations')\nplt.ylabel('Cost')  Iteration: 30/30 | Cost 3.79 | Elapsed: 0:00:00 | ETA: 0:00:00   <matplotlib.text.Text at 0x10ca096a0>",
            "title": "Example 2 - Gradient Descent"
        },
        {
            "location": "/user_guide/classifier/Adaline/#example-3-stochastic-gradient-descent",
            "text": "from mlxtend.data import iris_data\nfrom mlxtend.evaluate import plot_decision_regions\nfrom mlxtend.classifier import Adaline\nimport matplotlib.pyplot as plt\n\n# Loading Data\n\nX, y = iris_data()\nX = X[:, [0, 3]] # sepal length and petal width\nX = X[0:100] # class 0 and class 1\ny = y[0:100] # class 0 and class 1\n\n# standardize\nX[:,0] = (X[:,0] - X[:,0].mean()) / X[:,0].std()\nX[:,1] = (X[:,1] - X[:,1].mean()) / X[:,1].std()\n\n\nada = Adaline(epochs=15, \n              eta=0.02, \n              minibatches=len(y), # for SGD learning \n              random_seed=1,\n              print_progress=3)\n\nada.fit(X, y)\nplot_decision_regions(X, y, clf=ada)\nplt.title('Adaline - Stochastic Gradient Descent')\nplt.show()\n\nplt.plot(range(len(ada.cost_)), ada.cost_)\nplt.xlabel('Iterations')\nplt.ylabel('Cost')\nplt.show()  Iteration: 15/15 | Cost 3.81 | Elapsed: 0:00:00 | ETA: 0:00:00",
            "title": "Example 3 - Stochastic Gradient Descent"
        },
        {
            "location": "/user_guide/classifier/Adaline/#example-4-stochastic-gradient-descent-with-minibatches",
            "text": "from mlxtend.data import iris_data\nfrom mlxtend.evaluate import plot_decision_regions\nfrom mlxtend.classifier import Adaline\nimport matplotlib.pyplot as plt\n\n# Loading Data\n\nX, y = iris_data()\nX = X[:, [0, 3]] # sepal length and petal width\nX = X[0:100] # class 0 and class 1\ny = y[0:100] # class 0 and class 1\n\n# standardize\nX[:,0] = (X[:,0] - X[:,0].mean()) / X[:,0].std()\nX[:,1] = (X[:,1] - X[:,1].mean()) / X[:,1].std()\n\n\nada = Adaline(epochs=15, \n              eta=0.02, \n              minibatches=5, # for SGD learning w. minibatch size 20\n              random_seed=1,\n              print_progress=3)\n\nada.fit(X, y)\nplot_decision_regions(X, y, clf=ada)\nplt.title('Adaline - Stochastic Gradient Descent')\nplt.show()\n\nplt.plot(range(len(ada.cost_)), ada.cost_)\nplt.xlabel('Iterations')\nplt.ylabel('Cost')\nplt.show()  Iteration: 15/15 | Cost 3.87 | Elapsed: 0:00:00 | ETA: 0:00:00",
            "title": "Example 4 - Stochastic Gradient Descent with Minibatches"
        },
        {
            "location": "/user_guide/classifier/Adaline/#api",
            "text": "Adaline(eta=0.01, epochs=50, minibatches=None, random_seed=None, print_progress=0)  ADAptive LInear NEuron classifier.  Note that this implementation of Adaline expects binary class labels\nin {0, 1}.  Parameters    eta  : float (default: 0.01)  solver rate (between 0.0 and 1.0)    epochs  : int (default: 50)  Passes over the training dataset.\nPrior to each epoch, the dataset is shuffled\nif  minibatches > 1  to prevent cycles in stochastic gradient descent.    minibatches  : int (default: None)  The number of minibatches for gradient-based optimization.\nIf None: Normal Equations (closed-form solution)\nIf 1: Gradient Descent learning\nIf len(y): Stochastic Gradient Descent (SGD) online learning\nIf 1 < minibatches < len(y): SGD Minibatch learning    random_seed  : int (default: None)  Set random state for shuffling and initializing the weights.    print_progress  : int (default: 0)  Prints progress in fitting to stderr if not solver='normal equation'\n0: No output\n1: Epochs elapsed and cost\n2: 1 plus time elapsed\n3: 2 plus estimated time until completion    Attributes    w_  : 2d-array, shape={n_features, 1}  Model weights after fitting.    b_  : 1d-array, shape={1,}  Bias unit after fitting.    cost_  : list  Sum of squared errors after each epoch.",
            "title": "API"
        },
        {
            "location": "/user_guide/classifier/Adaline/#methods",
            "text": "fit(X, y, init_params=True)  Learn model from training data.  Parameters    X  : {array-like, sparse matrix}, shape = [n_samples, n_features]  Training vectors, where n_samples is the number of samples and\nn_features is the number of features.    y  : array-like, shape = [n_samples]  Target values.    init_params  : bool (default: True)  Re-initializes model parameters prior to fitting.\nSet False to continue training with weights from\na previous model fitting.    Returns   self  : object    predict(X)  Predict targets from X.  Parameters    X  : {array-like, sparse matrix}, shape = [n_samples, n_features]  Training vectors, where n_samples is the number of samples and\nn_features is the number of features.    Returns    target_values  : array-like, shape = [n_samples]  Predicted target values.     score(X, y)  Compute the prediction accuracy  Parameters    X  : {array-like, sparse matrix}, shape = [n_samples, n_features]  Training vectors, where n_samples is the number of samples and\nn_features is the number of features.    y  : array-like, shape = [n_samples]  Target values (true class labels).    Returns    acc  : float  The prediction accuracy as a float\nbetween 0.0 and 1.0 (perfect score).",
            "title": "Methods"
        },
        {
            "location": "/user_guide/classifier/LogisticRegression/",
            "text": "Logistic Regression\n\n\nA logistic regression class for binary classification tasks.\n\n\n\n\nfrom mlxtend.classifier import LogisticRegression\n\n\n\n\nOverview\n\n\n\n\nRelated to the \nPerceptron\n and \n'Adaline'\n, a Logistic Regression model is a linear model for binary classification. However, instead of minimizing a linear cost function such as the sum of squared errors (SSE) in Adaline, we minimize a sigmoid function, i.e., the logistic function:\n\n\n$$\\phi(z) = \\frac{1}{1 + e^{-z}},$$\n\n\nwhere \n$z$\n is defined as the net input\n\n\n$$z = w_0x_0 + w_1x_1 + ... + w_mx_m = \\sum_{j=0}^{m} w_j x_j= \\mathbf{w}^T\\mathbf{x}.$$\n \n\n\nThe net input is in turn based on the logit function\n\n\n$$logit(p(y=1 \\mid \\mathbf{x})) = z.$$\n\n\nHere, \n$p(y=1 \\mid \\mathbf{x})$\n is the conditional probability that a particular sample belongs to class 1 given its features \n$\\mathbf{x}$\n. The logit function takes inputs in the range [0, 1] and transform them to values over the entire real number range. In contrast, the logistic function takes input values over the entire real number range and transforms them to values in the range [0, 1]. In other words, the logistic function is the inverse of the logit function, and it lets us predict the conditional probability that a certain sample belongs to class 1 (or class 0).\n\n\n\n\nAfter model fitting, the conditional probability \n$p(y=1 \\mid \\mathbf{x})$\n is converted to a binary class label via a threshold function \n$g(\\cdot)$\n:\n\n\n$$y = g({z}) = \n \\begin{cases}\n  1 & \\text{if $\\phi(z) \\ge 0.5$}\\\\\n  0 & \\text{otherwise.}\n   \\end{cases}\n$$\n\n\nor equivalently:\n\n\n$$y = g({z}) = \n\\begin{cases}\n1 & \\text{if z $\\ge$ 0}\\\\\n0 & \\text{otherwise}.\n\\end{cases}\n$$\n\n\nObjective Function -- Log-Likelihood\n\n\nIn order to parameterize a logistic regression model, we maximize the likelihood \n$L(\\cdot)$\n (or minimize the logistic cost function).\n\n\nWe write the likelihood as \n\n\n$$L(\\mathbf{w}) = P(\\mathbf{y} \\mid \\mathbf{x};\\mathbf{w}) = \\prod_{i=1}^{n} P\\big(y^{(i)} \\mid x^{(i)}; \\mathbf{w}\\big) = \\prod^{n}_{i=1}\\bigg(\\phi\\big(z^{(i)}\\big)\\bigg)^{y^{(i)}} \\bigg(1-\\phi\\big(z^{(i)}\\big)\\bigg)^{1-y^{(i)}},$$\n\n\nunder the assumption that the training samples are independent of each other.\n\n\nIn practice, it is easier to maximize the (natural) log of this equation, which is called\nthe log-likelihood function:\n\n\n$$l(\\mathbf{w}) = \\log L(\\mathbf{w}) = \\sum^{n}_{i=1} y^{(i)} \\log \\bigg(\\phi\\big(z^{(i)}\\big)\\bigg) + \\big( 1 - y^{(i)}\\big) \\log \\big(1-\\phi\\big(z^{(i)}\\big)\\big)$$\n\n\nOne advantage of taking the log is to avoid numeric underflow (and challenges with floating point math) for very small likelihoods. Another advantage is that we can obtain the derivative more easily, using the addition trick to rewrite the product of factors as a summation term, which we can then maximize using optimization algorithms such as gradient ascent.\n\n\nObjective Function -- Logistic Cost Function\n\n\nAn alternative to maximizing the log-likelihood, we can define a cost function \n$J(\\cdot)$\n to be minimized; we rewrite the log-likelihood as:\n\n\n$$J(\\mathbf{w}) = \\sum_{i=1}^{m} - y^{(i)} log \\bigg( \\phi\\big(z^{(i)}\\big) \\bigg) - \\big(1 - y^{(i)}\\big) log\\bigg(1-\\phi\\big(z^{(i)}\\big)\\bigg)$$\n\n\n$$\n J\\big(\\phi(z), y; \\mathbf{w}\\big) =\\begin{cases}\n    -log\\big(\\phi(z) \\big) & \\text{if $y = 1$}\\\\\n    -log\\big(1- \\phi(z) \\big) & \\text{if $y = 0$}\n  \\end{cases}\n$$\n\n\n\n\nAs we can see in the figure above, we penalize wrong predictions with an increasingly larger cost.\n\n\nGradient Descent (GD)  and Stochastic Gradient Descent (SGD) Optimization\n\n\nGradient Ascent and the log-likelihood\n\n\nTo learn the weight coefficient of a logistic regression model via gradient-based optimization, we compute the partial derivative of the log-likelihood function -- w.r.t. the \nj\nth weight -- as follows:\n\n\n$$\\frac{\\partial}{\\partial w_j} l(\\mathbf{w}) = \\bigg(y \\frac{1}{\\phi(z)} - (1-y) \\frac{1}{1-\\phi{(z)}} \\bigg) \\frac{\\partial}{\\partial w_j}\\phi(z)$$\n\n\nAs an intermediate step, we compute the partial derivative of the sigmoid function, which will come in handy later:\n\n\n\\begin{align}\n&\\frac{\\partial}{\\partial z} \\phi(z) = \\frac{\\partial}{{\\partial z}} \\frac{1}{1+e^{-z}} \\\\\n&= \\frac{1}{(1 + e^{-z})^{2}} e^{-z}\\\\\n&= \\frac{1}{1+e^{-z}} \\bigg(1 - \\frac{1}{1+e^{-z}} \\bigg)\\\\\n&= \\phi(z)\\big(1-\\phi(z)\\big)\n\\end{align}\n\n\nNow, we re-substitute \n$$\\frac{\\partial}{\\partial z} \\phi(z) = \\phi(z) \\big(1 - \\phi(z)\\big)$$\n back into in the log-likelihood partial derivative equation and obtain the equation shown below:\n\n\n\\begin{align}\n& \\bigg(y \\frac{1}{\\phi{(z)}} - (1 - y) \\frac{1}{1 - \\phi(z)} \\bigg) \\frac{\\partial}{\\partial w_j} \\phi(z) \\\\\n&= \\bigg(y \\frac{1}{\\phi{(z)}} - (1 - y) \\frac{1}{1 - \\phi(z)} \\bigg) \\phi(z) \\big(1 - \\phi(z)\\big) \\frac{\\partial}{\\partial w_j}z\\\\\n&= \\big(y(1-\\phi(z)\\big) - (1 - y) \\phi(z)\\big)x_j\\\\\n&=\\big(y - \\phi(z)\\big)x_j\n\\end{align}\n\n\nNow, in order to find the weights of the model, we take a step proportional to the positive direction of the gradient to maximize the log-likelihood. Futhermore, we add a coefficient, the learning rate \n$\\eta$\n to the weight update:\n\n\n\\begin{align}\n& w_j := w_j + \\eta \\frac{\\partial}{\\partial w_j} l(\\mathbf{w})\\\\\n& w_j := w_j + \\eta \\sum^{n}_{i=1} \\big( y^{(i)} - \\phi\\big(z^{(i)}\\big)\\big)x_j^{(i)}\n\\end{align}\n\n\nNote that the gradient (and weight update) is computed from all samples in the training set in gradient ascent/descent in contrast to stochastic gradient ascent/descent. For more information about the differences between gradient descent and stochastic gradient descent, please see the related article \nGradient Descent and Stochastic Gradient Descent\n.\n\n\nThe previous equation shows the weight update for a single weight \n$j$\n. In gradient-based optimization, all weight coefficients are updated simultaneously; the weight update can be written more compactly as \n\n\n$$\\mathbf{w} := \\mathbf{w} + \\Delta\\mathbf{w},$$\n\nwhere\n\n\n$$\\Delta{\\mathbf{w}} = \\eta \\nabla l(\\mathbf{w})$$\n\n\nGradient Descent and the logistic cost function\n\n\nIn the previous section, we derived the gradient of the log-likelihood function, which can be optimized via gradient ascent. Similarly, we can obtain the cost gradient of the logistic cost function \n$J(\\cdot)$\n and minimize it via gradient descent in order to learn the logistic regression model.\n\n\nThe update rule for a single weight:\n\n\n\\begin{align}\n& \\Delta{w_j} = -\\eta \\frac{\\partial J}{\\partial w_j} \\\n& = - \\eta \\sum_{i=1}^{n}\\big(y^{(i)} - \\phi\\big(z^{(i)}\\big) x^{(i)} \\big)\n\\end{align}\n\n\nThe simultaneous weight update:\n\n\n$$\\mathbf{w} := \\mathbf{w} + \\Delta\\mathbf{w}$$\n\n\nwhere\n\n\n$$\\Delta{\\mathbf{w}} = - \\eta \\nabla J(\\mathbf{w}).$$\n\n\nShuffling\n\n\nRandom shuffling is implemented as:\n\n\n\n\nfor one or more epochs\n\n\nrandomly shuffle samples in the training set\n\n\nfor training sample \ni\n\n\ncompute gradients and perform weight updates\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nRegularization\n\n\nAs a way to tackle overfitting, we can add additional bias to the logistic regression model via a regularization terms. Via the L2 regularization term, we reduce the complexity of the model by penalizing large weight coefficients:\n\n\n$$L2: \\frac{\\lambda}{2}\\lVert \\mathbf{w} \\lVert_2 = \\frac{\\lambda}{2} \\sum_{j=1}^{m} w_j^2$$\n\n\nIn order to apply regularization, we just need to add the regularization term to the cost function that we defined for logistic regression to shrink the weights:\n\n\n$$J(\\mathbf{w}) =  \\sum_{i=1}^{m} \\Bigg[ - y^{(i)} log \\bigg( \\phi\\big(z^{(i)}\\big) \\bigg) - \\big(1 - y^{(i)}\\big) log\\bigg(1-\\phi\\big(z^{(i)}\\big)\\bigg) \\Bigg] + \\frac{\\lambda}{2} \\sum_{j=1}^{m} w_j^2$$\n\n\nThe update rule for a single weight:\n\n\n\\begin{align}\n& \\Delta{w_j} = -\\eta \\bigg( \\frac{\\partial J}{\\partial w_j} + \\lambda w_j\\bigg)\\\n& = - \\eta \\sum_{i=1}^{n}\\big(y^{(i)} - \\phi\\big(z^{(i)}\\big) x^{(i)} \\big) - \\eta \\lambda w_j\n\\end{align}\n\n\nThe simultaneous weight update:\n\n\n$$\\mathbf{w} := \\mathbf{w} + \\Delta\\mathbf{w}$$\n\n\nwhere\n\n\n$$\\Delta{\\mathbf{w}} = - \\eta \\big( \\nabla J(\\mathbf{w}) + \\lambda \\mathbf{w}\\big).$$\n\n\nFor more information on regularization, please see \nRegularization of Generalized Linear Models\n.\n\n\nReferences\n\n\n\n\nBishop, Christopher M. \nPattern recognition and machine learning\n. Springer, 2006. pp. 203-213\n\n\n\n\nExamples\n\n\nExample 1 - Gradient Descent\n\n\nfrom mlxtend.data import iris_data\nfrom mlxtend.evaluate import plot_decision_regions\nfrom mlxtend.classifier import LogisticRegression\nimport matplotlib.pyplot as plt\n\n# Loading Data\n\nX, y = iris_data()\nX = X[:, [0, 3]] # sepal length and petal width\nX = X[0:100] # class 0 and class 1\ny = y[0:100] # class 0 and class 1\n\n# standardize\nX[:,0] = (X[:,0] - X[:,0].mean()) / X[:,0].std()\nX[:,1] = (X[:,1] - X[:,1].mean()) / X[:,1].std()\n\nlr = LogisticRegression(eta=0.1, \n                        l2_lambda=0.0, \n                        epochs=100,\n                        minibatches=1, # for Gradient Descent\n                        random_seed=1,\n                        print_progress=3)\nlr.fit(X, y)\n\nplot_decision_regions(X, y, clf=lr)\nplt.title('Logistic Regression - Gradient Descent')\nplt.show()\n\nplt.plot(range(len(lr.cost_)), lr.cost_)\nplt.xlabel('Iterations')\nplt.ylabel('Cost')\nplt.show()\n\n\n\n\nIteration: 100/100 | Cost 0.32 | Elapsed: 0:00:00 | ETA: 0:00:00\n\n\n\n\n\n\n\nPredicting Class Labels\n\n\ny_pred = lr.predict(X)\nprint('Last 3 Class Labels: %s' % y_pred[-3:])\n\n\n\n\nLast 3 Class Labels: [1 1 1]\n\n\n\nPredicting Class Probabilities\n\n\ny_pred = lr.predict_proba(X)\nprint('Last 3 Class Labels: %s' % y_pred[-3:])\n\n\n\n\nLast 3 Class Labels: [ 0.99997968  0.99339873  0.99992707]\n\n\n\nExample 2 - Stochastic Gradient Descent\n\n\nfrom mlxtend.data import iris_data\nfrom mlxtend.evaluate import plot_decision_regions\nfrom mlxtend.classifier import LogisticRegression\nimport matplotlib.pyplot as plt\n\n# Loading Data\n\nX, y = iris_data()\nX = X[:, [0, 3]] # sepal length and petal width\nX = X[0:100] # class 0 and class 1\ny = y[0:100] # class 0 and class 1\n\n# standardize\nX[:,0] = (X[:,0] - X[:,0].mean()) / X[:,0].std()\nX[:,1] = (X[:,1] - X[:,1].mean()) / X[:,1].std()\n\nlr = LogisticRegression(eta=0.5, \n                        epochs=30, \n                        l2_lambda=0.0, \n                        minibatches=len(y), # for SGD learning \n                        random_seed=1,\n                        print_progress=3)\nlr.fit(X, y)\n\nplot_decision_regions(X, y, clf=lr)\nplt.title('Logistic Regression - Stochastic Gradient Descent')\nplt.show()\n\nplt.plot(range(len(lr.cost_)), lr.cost_)\nplt.xlabel('Iterations')\nplt.ylabel('Cost')\nplt.show()\n\n\n\n\nIteration: 30/30 | Cost 0.27 | Elapsed: 0:00:00 | ETA: 0:00:00\n\n\n\n\n\n\n\nExample 3 - Stochastic Gradient Descent w. Minibatches\n\n\nHere, we set \nminibatches\n to 5, which will result in Minibatch Learning with a batch size of 20 samples (since 100 Iris samples divided by 5 minibatches equals 20).\n\n\nfrom mlxtend.data import iris_data\nfrom mlxtend.evaluate import plot_decision_regions\nfrom mlxtend.classifier import LogisticRegression\nimport matplotlib.pyplot as plt\n\n# Loading Data\n\nX, y = iris_data()\nX = X[:, [0, 3]] # sepal length and petal width\nX = X[0:100] # class 0 and class 1\ny = y[0:100] # class 0 and class 1\n\n# standardize\nX[:,0] = (X[:,0] - X[:,0].mean()) / X[:,0].std()\nX[:,1] = (X[:,1] - X[:,1].mean()) / X[:,1].std()\n\nlr = LogisticRegression(eta=0.5, \n                        epochs=30, \n                        l2_lambda=0.0, \n                        minibatches=5, # 100/5 = 20 -> minibatch-s \n                        random_seed=1,\n                        print_progress=3)\nlr.fit(X, y)\n\nplot_decision_regions(X, y, clf=lr)\nplt.title('Logistic Regression - Stochastic Gradient Descent')\nplt.show()\n\nplt.plot(range(len(lr.cost_)), lr.cost_)\nplt.xlabel('Iterations')\nplt.ylabel('Cost')\nplt.show()\n\n\n\n\nIteration: 30/30 | Cost 0.25 | Elapsed: 0:00:00 | ETA: 0:00:00\n\n\n\n\n\n\n\nAPI\n\n\nLogisticRegression(eta=0.01, epochs=50, l2_lambda=0.0, minibatches=1, random_seed=None, print_progress=0)\n\n\nLogistic regression classifier.\n\n\nNote that this implementation of Logistic Regression\nexpects binary class labels in {0, 1}.\n\n\n\nParameters\n\n\n\n\n\n\neta\n : float (default: 0.01)\n\n\nLearning rate (between 0.0 and 1.0)\n\n\n\n\n\n\nepochs\n : int (default: 50)\n\n\nPasses over the training dataset.\nPrior to each epoch, the dataset is shuffled\nif \nminibatches > 1\n to prevent cycles in stochastic gradient descent.\n\n\n\n\n\n\nl2_lambda\n : float\n\n\nRegularization parameter for L2 regularization.\nNo regularization if l2_lambda=0.0.\n\n\n\n\n\n\nminibatches\n : int (default: 1)\n\n\nThe number of minibatches for gradient-based optimization.\nIf 1: Gradient Descent learning\nIf len(y): Stochastic Gradient Descent (SGD) online learning\nIf 1 < minibatches < len(y): SGD Minibatch learning\n\n\n\n\n\n\nrandom_seed\n : int (default: None)\n\n\nSet random state for shuffling and initializing the weights.\n\n\n\n\n\n\nprint_progress\n : int (default: 0)\n\n\nPrints progress in fitting to stderr.\n0: No output\n1: Epochs elapsed and cost\n2: 1 plus time elapsed\n3: 2 plus estimated time until completion\n\n\n\n\n\n\nAttributes\n\n\n\n\n\n\nw_\n : 2d-array, shape={n_features, 1}\n\n\nModel weights after fitting.\n\n\n\n\n\n\nb_\n : 1d-array, shape={1,}\n\n\nBias unit after fitting.\n\n\n\n\n\n\ncost_\n : list\n\n\nList of floats with cross_entropy cost (sgd or gd) for every\nepoch.\n\n\n\n\n\n\nMethods\n\n\n\n\n\nfit(X, y, init_params=True)\n\n\nLearn model from training data.\n\n\nParameters\n\n\n\n\n\n\nX\n : {array-like, sparse matrix}, shape = [n_samples, n_features]\n\n\nTraining vectors, where n_samples is the number of samples and\nn_features is the number of features.\n\n\n\n\n\n\ny\n : array-like, shape = [n_samples]\n\n\nTarget values.\n\n\n\n\n\n\ninit_params\n : bool (default: True)\n\n\nRe-initializes model parameters prior to fitting.\nSet False to continue training with weights from\na previous model fitting.\n\n\n\n\n\n\nReturns\n\n\n\n\nself\n : object\n\n\n\n\n\n\n\npredict(X)\n\n\nPredict targets from X.\n\n\nParameters\n\n\n\n\n\n\nX\n : {array-like, sparse matrix}, shape = [n_samples, n_features]\n\n\nTraining vectors, where n_samples is the number of samples and\nn_features is the number of features.\n\n\n\n\n\n\nReturns\n\n\n\n\n\n\ntarget_values\n : array-like, shape = [n_samples]\n\n\nPredicted target values.\n\n\n\n\n\n\n\n\n\npredict_proba(X)\n\n\nPredict class probabilities of X from the net input.\n\n\nParameters\n\n\n\n\n\n\nX\n : {array-like, sparse matrix}, shape = [n_samples, n_features]\n\n\nTraining vectors, where n_samples is the number of samples and\nn_features is the number of features.\n\n\n\n\n\n\nReturns\n\n\n\n\nClass 1 probability\n : float\n\n\n\n\n\n\n\nscore(X, y)\n\n\nCompute the prediction accuracy\n\n\nParameters\n\n\n\n\n\n\nX\n : {array-like, sparse matrix}, shape = [n_samples, n_features]\n\n\nTraining vectors, where n_samples is the number of samples and\nn_features is the number of features.\n\n\n\n\n\n\ny\n : array-like, shape = [n_samples]\n\n\nTarget values (true class labels).\n\n\n\n\n\n\nReturns\n\n\n\n\n\n\nacc\n : float\n\n\nThe prediction accuracy as a float\nbetween 0.0 and 1.0 (perfect score).",
            "title": "LogisticRegression"
        },
        {
            "location": "/user_guide/classifier/LogisticRegression/#logistic-regression",
            "text": "A logistic regression class for binary classification tasks.   from mlxtend.classifier import LogisticRegression",
            "title": "Logistic Regression"
        },
        {
            "location": "/user_guide/classifier/LogisticRegression/#overview",
            "text": "Related to the  Perceptron  and  'Adaline' , a Logistic Regression model is a linear model for binary classification. However, instead of minimizing a linear cost function such as the sum of squared errors (SSE) in Adaline, we minimize a sigmoid function, i.e., the logistic function:  $$\\phi(z) = \\frac{1}{1 + e^{-z}},$$  where  $z$  is defined as the net input  $$z = w_0x_0 + w_1x_1 + ... + w_mx_m = \\sum_{j=0}^{m} w_j x_j= \\mathbf{w}^T\\mathbf{x}.$$    The net input is in turn based on the logit function  $$logit(p(y=1 \\mid \\mathbf{x})) = z.$$  Here,  $p(y=1 \\mid \\mathbf{x})$  is the conditional probability that a particular sample belongs to class 1 given its features  $\\mathbf{x}$ . The logit function takes inputs in the range [0, 1] and transform them to values over the entire real number range. In contrast, the logistic function takes input values over the entire real number range and transforms them to values in the range [0, 1]. In other words, the logistic function is the inverse of the logit function, and it lets us predict the conditional probability that a certain sample belongs to class 1 (or class 0).   After model fitting, the conditional probability  $p(y=1 \\mid \\mathbf{x})$  is converted to a binary class label via a threshold function  $g(\\cdot)$ :  $$y = g({z}) = \n \\begin{cases}\n  1 & \\text{if $\\phi(z) \\ge 0.5$}\\\\\n  0 & \\text{otherwise.}\n   \\end{cases}\n$$  or equivalently:  $$y = g({z}) = \n\\begin{cases}\n1 & \\text{if z $\\ge$ 0}\\\\\n0 & \\text{otherwise}.\n\\end{cases}\n$$",
            "title": "Overview"
        },
        {
            "location": "/user_guide/classifier/LogisticRegression/#objective-function-log-likelihood",
            "text": "In order to parameterize a logistic regression model, we maximize the likelihood  $L(\\cdot)$  (or minimize the logistic cost function).  We write the likelihood as   $$L(\\mathbf{w}) = P(\\mathbf{y} \\mid \\mathbf{x};\\mathbf{w}) = \\prod_{i=1}^{n} P\\big(y^{(i)} \\mid x^{(i)}; \\mathbf{w}\\big) = \\prod^{n}_{i=1}\\bigg(\\phi\\big(z^{(i)}\\big)\\bigg)^{y^{(i)}} \\bigg(1-\\phi\\big(z^{(i)}\\big)\\bigg)^{1-y^{(i)}},$$  under the assumption that the training samples are independent of each other.  In practice, it is easier to maximize the (natural) log of this equation, which is called\nthe log-likelihood function:  $$l(\\mathbf{w}) = \\log L(\\mathbf{w}) = \\sum^{n}_{i=1} y^{(i)} \\log \\bigg(\\phi\\big(z^{(i)}\\big)\\bigg) + \\big( 1 - y^{(i)}\\big) \\log \\big(1-\\phi\\big(z^{(i)}\\big)\\big)$$  One advantage of taking the log is to avoid numeric underflow (and challenges with floating point math) for very small likelihoods. Another advantage is that we can obtain the derivative more easily, using the addition trick to rewrite the product of factors as a summation term, which we can then maximize using optimization algorithms such as gradient ascent.",
            "title": "Objective Function -- Log-Likelihood"
        },
        {
            "location": "/user_guide/classifier/LogisticRegression/#objective-function-logistic-cost-function",
            "text": "An alternative to maximizing the log-likelihood, we can define a cost function  $J(\\cdot)$  to be minimized; we rewrite the log-likelihood as:  $$J(\\mathbf{w}) = \\sum_{i=1}^{m} - y^{(i)} log \\bigg( \\phi\\big(z^{(i)}\\big) \\bigg) - \\big(1 - y^{(i)}\\big) log\\bigg(1-\\phi\\big(z^{(i)}\\big)\\bigg)$$  $$\n J\\big(\\phi(z), y; \\mathbf{w}\\big) =\\begin{cases}\n    -log\\big(\\phi(z) \\big) & \\text{if $y = 1$}\\\\\n    -log\\big(1- \\phi(z) \\big) & \\text{if $y = 0$}\n  \\end{cases}\n$$   As we can see in the figure above, we penalize wrong predictions with an increasingly larger cost.",
            "title": "Objective Function -- Logistic Cost Function"
        },
        {
            "location": "/user_guide/classifier/LogisticRegression/#gradient-descent-gd-and-stochastic-gradient-descent-sgd-optimization",
            "text": "",
            "title": "Gradient Descent (GD)  and Stochastic Gradient Descent (SGD) Optimization"
        },
        {
            "location": "/user_guide/classifier/LogisticRegression/#gradient-ascent-and-the-log-likelihood",
            "text": "To learn the weight coefficient of a logistic regression model via gradient-based optimization, we compute the partial derivative of the log-likelihood function -- w.r.t. the  j th weight -- as follows:  $$\\frac{\\partial}{\\partial w_j} l(\\mathbf{w}) = \\bigg(y \\frac{1}{\\phi(z)} - (1-y) \\frac{1}{1-\\phi{(z)}} \\bigg) \\frac{\\partial}{\\partial w_j}\\phi(z)$$  As an intermediate step, we compute the partial derivative of the sigmoid function, which will come in handy later:  \\begin{align}\n&\\frac{\\partial}{\\partial z} \\phi(z) = \\frac{\\partial}{{\\partial z}} \\frac{1}{1+e^{-z}} \\\\\n&= \\frac{1}{(1 + e^{-z})^{2}} e^{-z}\\\\\n&= \\frac{1}{1+e^{-z}} \\bigg(1 - \\frac{1}{1+e^{-z}} \\bigg)\\\\\n&= \\phi(z)\\big(1-\\phi(z)\\big)\n\\end{align}  Now, we re-substitute  $$\\frac{\\partial}{\\partial z} \\phi(z) = \\phi(z) \\big(1 - \\phi(z)\\big)$$  back into in the log-likelihood partial derivative equation and obtain the equation shown below:  \\begin{align}\n& \\bigg(y \\frac{1}{\\phi{(z)}} - (1 - y) \\frac{1}{1 - \\phi(z)} \\bigg) \\frac{\\partial}{\\partial w_j} \\phi(z) \\\\\n&= \\bigg(y \\frac{1}{\\phi{(z)}} - (1 - y) \\frac{1}{1 - \\phi(z)} \\bigg) \\phi(z) \\big(1 - \\phi(z)\\big) \\frac{\\partial}{\\partial w_j}z\\\\\n&= \\big(y(1-\\phi(z)\\big) - (1 - y) \\phi(z)\\big)x_j\\\\\n&=\\big(y - \\phi(z)\\big)x_j\n\\end{align}  Now, in order to find the weights of the model, we take a step proportional to the positive direction of the gradient to maximize the log-likelihood. Futhermore, we add a coefficient, the learning rate  $\\eta$  to the weight update:  \\begin{align}\n& w_j := w_j + \\eta \\frac{\\partial}{\\partial w_j} l(\\mathbf{w})\\\\\n& w_j := w_j + \\eta \\sum^{n}_{i=1} \\big( y^{(i)} - \\phi\\big(z^{(i)}\\big)\\big)x_j^{(i)}\n\\end{align}  Note that the gradient (and weight update) is computed from all samples in the training set in gradient ascent/descent in contrast to stochastic gradient ascent/descent. For more information about the differences between gradient descent and stochastic gradient descent, please see the related article  Gradient Descent and Stochastic Gradient Descent .  The previous equation shows the weight update for a single weight  $j$ . In gradient-based optimization, all weight coefficients are updated simultaneously; the weight update can be written more compactly as   $$\\mathbf{w} := \\mathbf{w} + \\Delta\\mathbf{w},$$ \nwhere  $$\\Delta{\\mathbf{w}} = \\eta \\nabla l(\\mathbf{w})$$",
            "title": "Gradient Ascent and the log-likelihood"
        },
        {
            "location": "/user_guide/classifier/LogisticRegression/#gradient-descent-and-the-logistic-cost-function",
            "text": "In the previous section, we derived the gradient of the log-likelihood function, which can be optimized via gradient ascent. Similarly, we can obtain the cost gradient of the logistic cost function  $J(\\cdot)$  and minimize it via gradient descent in order to learn the logistic regression model.  The update rule for a single weight:  \\begin{align}\n& \\Delta{w_j} = -\\eta \\frac{\\partial J}{\\partial w_j} \\\n& = - \\eta \\sum_{i=1}^{n}\\big(y^{(i)} - \\phi\\big(z^{(i)}\\big) x^{(i)} \\big)\n\\end{align}  The simultaneous weight update:  $$\\mathbf{w} := \\mathbf{w} + \\Delta\\mathbf{w}$$  where  $$\\Delta{\\mathbf{w}} = - \\eta \\nabla J(\\mathbf{w}).$$",
            "title": "Gradient Descent and the logistic cost function"
        },
        {
            "location": "/user_guide/classifier/LogisticRegression/#shuffling",
            "text": "Random shuffling is implemented as:   for one or more epochs  randomly shuffle samples in the training set  for training sample  i  compute gradients and perform weight updates",
            "title": "Shuffling"
        },
        {
            "location": "/user_guide/classifier/LogisticRegression/#regularization",
            "text": "As a way to tackle overfitting, we can add additional bias to the logistic regression model via a regularization terms. Via the L2 regularization term, we reduce the complexity of the model by penalizing large weight coefficients:  $$L2: \\frac{\\lambda}{2}\\lVert \\mathbf{w} \\lVert_2 = \\frac{\\lambda}{2} \\sum_{j=1}^{m} w_j^2$$  In order to apply regularization, we just need to add the regularization term to the cost function that we defined for logistic regression to shrink the weights:  $$J(\\mathbf{w}) =  \\sum_{i=1}^{m} \\Bigg[ - y^{(i)} log \\bigg( \\phi\\big(z^{(i)}\\big) \\bigg) - \\big(1 - y^{(i)}\\big) log\\bigg(1-\\phi\\big(z^{(i)}\\big)\\bigg) \\Bigg] + \\frac{\\lambda}{2} \\sum_{j=1}^{m} w_j^2$$  The update rule for a single weight:  \\begin{align}\n& \\Delta{w_j} = -\\eta \\bigg( \\frac{\\partial J}{\\partial w_j} + \\lambda w_j\\bigg)\\\n& = - \\eta \\sum_{i=1}^{n}\\big(y^{(i)} - \\phi\\big(z^{(i)}\\big) x^{(i)} \\big) - \\eta \\lambda w_j\n\\end{align}  The simultaneous weight update:  $$\\mathbf{w} := \\mathbf{w} + \\Delta\\mathbf{w}$$  where  $$\\Delta{\\mathbf{w}} = - \\eta \\big( \\nabla J(\\mathbf{w}) + \\lambda \\mathbf{w}\\big).$$  For more information on regularization, please see  Regularization of Generalized Linear Models .",
            "title": "Regularization"
        },
        {
            "location": "/user_guide/classifier/LogisticRegression/#references",
            "text": "Bishop, Christopher M.  Pattern recognition and machine learning . Springer, 2006. pp. 203-213",
            "title": "References"
        },
        {
            "location": "/user_guide/classifier/LogisticRegression/#examples",
            "text": "",
            "title": "Examples"
        },
        {
            "location": "/user_guide/classifier/LogisticRegression/#example-1-gradient-descent",
            "text": "from mlxtend.data import iris_data\nfrom mlxtend.evaluate import plot_decision_regions\nfrom mlxtend.classifier import LogisticRegression\nimport matplotlib.pyplot as plt\n\n# Loading Data\n\nX, y = iris_data()\nX = X[:, [0, 3]] # sepal length and petal width\nX = X[0:100] # class 0 and class 1\ny = y[0:100] # class 0 and class 1\n\n# standardize\nX[:,0] = (X[:,0] - X[:,0].mean()) / X[:,0].std()\nX[:,1] = (X[:,1] - X[:,1].mean()) / X[:,1].std()\n\nlr = LogisticRegression(eta=0.1, \n                        l2_lambda=0.0, \n                        epochs=100,\n                        minibatches=1, # for Gradient Descent\n                        random_seed=1,\n                        print_progress=3)\nlr.fit(X, y)\n\nplot_decision_regions(X, y, clf=lr)\nplt.title('Logistic Regression - Gradient Descent')\nplt.show()\n\nplt.plot(range(len(lr.cost_)), lr.cost_)\nplt.xlabel('Iterations')\nplt.ylabel('Cost')\nplt.show()  Iteration: 100/100 | Cost 0.32 | Elapsed: 0:00:00 | ETA: 0:00:00",
            "title": "Example 1 - Gradient Descent"
        },
        {
            "location": "/user_guide/classifier/LogisticRegression/#predicting-class-labels",
            "text": "y_pred = lr.predict(X)\nprint('Last 3 Class Labels: %s' % y_pred[-3:])  Last 3 Class Labels: [1 1 1]",
            "title": "Predicting Class Labels"
        },
        {
            "location": "/user_guide/classifier/LogisticRegression/#predicting-class-probabilities",
            "text": "y_pred = lr.predict_proba(X)\nprint('Last 3 Class Labels: %s' % y_pred[-3:])  Last 3 Class Labels: [ 0.99997968  0.99339873  0.99992707]",
            "title": "Predicting Class Probabilities"
        },
        {
            "location": "/user_guide/classifier/LogisticRegression/#example-2-stochastic-gradient-descent",
            "text": "from mlxtend.data import iris_data\nfrom mlxtend.evaluate import plot_decision_regions\nfrom mlxtend.classifier import LogisticRegression\nimport matplotlib.pyplot as plt\n\n# Loading Data\n\nX, y = iris_data()\nX = X[:, [0, 3]] # sepal length and petal width\nX = X[0:100] # class 0 and class 1\ny = y[0:100] # class 0 and class 1\n\n# standardize\nX[:,0] = (X[:,0] - X[:,0].mean()) / X[:,0].std()\nX[:,1] = (X[:,1] - X[:,1].mean()) / X[:,1].std()\n\nlr = LogisticRegression(eta=0.5, \n                        epochs=30, \n                        l2_lambda=0.0, \n                        minibatches=len(y), # for SGD learning \n                        random_seed=1,\n                        print_progress=3)\nlr.fit(X, y)\n\nplot_decision_regions(X, y, clf=lr)\nplt.title('Logistic Regression - Stochastic Gradient Descent')\nplt.show()\n\nplt.plot(range(len(lr.cost_)), lr.cost_)\nplt.xlabel('Iterations')\nplt.ylabel('Cost')\nplt.show()  Iteration: 30/30 | Cost 0.27 | Elapsed: 0:00:00 | ETA: 0:00:00",
            "title": "Example 2 - Stochastic Gradient Descent"
        },
        {
            "location": "/user_guide/classifier/LogisticRegression/#example-3-stochastic-gradient-descent-w-minibatches",
            "text": "Here, we set  minibatches  to 5, which will result in Minibatch Learning with a batch size of 20 samples (since 100 Iris samples divided by 5 minibatches equals 20).  from mlxtend.data import iris_data\nfrom mlxtend.evaluate import plot_decision_regions\nfrom mlxtend.classifier import LogisticRegression\nimport matplotlib.pyplot as plt\n\n# Loading Data\n\nX, y = iris_data()\nX = X[:, [0, 3]] # sepal length and petal width\nX = X[0:100] # class 0 and class 1\ny = y[0:100] # class 0 and class 1\n\n# standardize\nX[:,0] = (X[:,0] - X[:,0].mean()) / X[:,0].std()\nX[:,1] = (X[:,1] - X[:,1].mean()) / X[:,1].std()\n\nlr = LogisticRegression(eta=0.5, \n                        epochs=30, \n                        l2_lambda=0.0, \n                        minibatches=5, # 100/5 = 20 -> minibatch-s \n                        random_seed=1,\n                        print_progress=3)\nlr.fit(X, y)\n\nplot_decision_regions(X, y, clf=lr)\nplt.title('Logistic Regression - Stochastic Gradient Descent')\nplt.show()\n\nplt.plot(range(len(lr.cost_)), lr.cost_)\nplt.xlabel('Iterations')\nplt.ylabel('Cost')\nplt.show()  Iteration: 30/30 | Cost 0.25 | Elapsed: 0:00:00 | ETA: 0:00:00",
            "title": "Example 3 - Stochastic Gradient Descent w. Minibatches"
        },
        {
            "location": "/user_guide/classifier/LogisticRegression/#api",
            "text": "LogisticRegression(eta=0.01, epochs=50, l2_lambda=0.0, minibatches=1, random_seed=None, print_progress=0)  Logistic regression classifier.  Note that this implementation of Logistic Regression\nexpects binary class labels in {0, 1}.  Parameters    eta  : float (default: 0.01)  Learning rate (between 0.0 and 1.0)    epochs  : int (default: 50)  Passes over the training dataset.\nPrior to each epoch, the dataset is shuffled\nif  minibatches > 1  to prevent cycles in stochastic gradient descent.    l2_lambda  : float  Regularization parameter for L2 regularization.\nNo regularization if l2_lambda=0.0.    minibatches  : int (default: 1)  The number of minibatches for gradient-based optimization.\nIf 1: Gradient Descent learning\nIf len(y): Stochastic Gradient Descent (SGD) online learning\nIf 1 < minibatches < len(y): SGD Minibatch learning    random_seed  : int (default: None)  Set random state for shuffling and initializing the weights.    print_progress  : int (default: 0)  Prints progress in fitting to stderr.\n0: No output\n1: Epochs elapsed and cost\n2: 1 plus time elapsed\n3: 2 plus estimated time until completion    Attributes    w_  : 2d-array, shape={n_features, 1}  Model weights after fitting.    b_  : 1d-array, shape={1,}  Bias unit after fitting.    cost_  : list  List of floats with cross_entropy cost (sgd or gd) for every\nepoch.",
            "title": "API"
        },
        {
            "location": "/user_guide/classifier/LogisticRegression/#methods",
            "text": "fit(X, y, init_params=True)  Learn model from training data.  Parameters    X  : {array-like, sparse matrix}, shape = [n_samples, n_features]  Training vectors, where n_samples is the number of samples and\nn_features is the number of features.    y  : array-like, shape = [n_samples]  Target values.    init_params  : bool (default: True)  Re-initializes model parameters prior to fitting.\nSet False to continue training with weights from\na previous model fitting.    Returns   self  : object    predict(X)  Predict targets from X.  Parameters    X  : {array-like, sparse matrix}, shape = [n_samples, n_features]  Training vectors, where n_samples is the number of samples and\nn_features is the number of features.    Returns    target_values  : array-like, shape = [n_samples]  Predicted target values.     predict_proba(X)  Predict class probabilities of X from the net input.  Parameters    X  : {array-like, sparse matrix}, shape = [n_samples, n_features]  Training vectors, where n_samples is the number of samples and\nn_features is the number of features.    Returns   Class 1 probability  : float    score(X, y)  Compute the prediction accuracy  Parameters    X  : {array-like, sparse matrix}, shape = [n_samples, n_features]  Training vectors, where n_samples is the number of samples and\nn_features is the number of features.    y  : array-like, shape = [n_samples]  Target values (true class labels).    Returns    acc  : float  The prediction accuracy as a float\nbetween 0.0 and 1.0 (perfect score).",
            "title": "Methods"
        },
        {
            "location": "/user_guide/classifier/SoftmaxRegression/",
            "text": "Softmax Regression\n\n\nA logistic regression class for multi-class classification tasks.\n\n\n\n\nfrom mlxtend.classifier import SoftmaxRegression\n\n\n\n\nOverview\n\n\nSoftmax Regression\n (synonyms: \nMultinomial Logistic\n, \nMaximum Entropy Classifier\n, or just \nMulti-class Logistic Regression\n) is a generalization of logistic regression that we can use for multi-class classification (under the assumption that the classes are  mutually exclusive). In contrast, we use the (standard) \nLogistic Regression\n model in binary classification tasks.\n\n\nBelow is a schematic of a \nLogistic Regression\n model, for more details, please see the \nLogisticRegression\n manual\n.\n\n\n\n\nIn \nSoftmax Regression\n (SMR), we replace the sigmoid logistic function by the so-called \nsoftmax\n function \n$\\phi_{softmax}(\\cdot)$\n.\n\n\n$$P(y=j \\mid z^{(i)}) = \\phi_{softmax}(z^{(i)}) = \\frac{e^{z^{(i)}}}{\\sum_{j=0}^{k} e^{z_{k}^{(i)}}},$$\n\n\nwhere we define the net input \nz\n as \n\n\n$$z = w_1x_1 + ... + w_mx_m  + b= \\sum_{l=0}^{m} w_l x_l + b= \\mathbf{w}^T\\mathbf{x} + b.$$\n \n\n\n(\nw\n is the weight vector, \n$\\mathbf{x}$\n is the feature vector of 1 training sample, and \n$b$\n is the bias unit.) \n\nNow, this softmax function computes the probability that this training sample \n$\\mathbf{x}^{(i)}$\n belongs to class \n$j$\n given the weight and net input \n$z^{(i)}$\n. So, we compute the probability \n$p(y = j \\mid \\mathbf{x^{(i)}; w}_j)$\n for each class label in  \n$j = 1, \\ldots, k.$\n. Note the normalization term in the denominator which causes these class probabilities to sum up to one.\n\n\n\n\nTo illustrate the concept of softmax, let us walk through a concrete example. Let's assume we have a training set consisting of 4 samples from 3 different classes (0, 1, and 2)\n\n\n\n\n$x_0 \\rightarrow \\text{class }0$\n\n\n$x_1 \\rightarrow \\text{class }1$\n\n\n$x_2 \\rightarrow \\text{class }2$\n\n\n$x_3 \\rightarrow \\text{class }2$\n\n\n\n\nimport numpy as np\ny = np.array([0, 1, 2, 2])\n\n\n\n\nFirst, we want to encode the class labels into a format that we can more easily work with; we apply one-hot encoding:\n\n\ny_enc = (np.arange(np.max(y) + 1) == y[:, None]).astype(float)\nprint('one-hot encoding:\\n', y_enc)\n\n\n\n\none-hot encoding:\n [[ 1.  0.  0.]\n [ 0.  1.  0.]\n [ 0.  0.  1.]\n [ 0.  0.  1.]]\n\n\n\nA sample that belongs to class 0 (the first row) has a 1 in the first cell, a sample that belongs to class 2 has a 1 in the second cell of its row, and so forth.\n\n\nNext, let us define the feature matrix of our 4 training samples. Here, we assume that our dataset consists of 2 features; thus, we create a 4x2 dimensional matrix of our samples and features.\nSimilarly, we create a 2x3 dimensional weight matrix (one row per feature and one column for each class).\n\n\nX = np.array([[0.1, 0.5],\n              [1.1, 2.3],\n              [-1.1, -2.3],\n              [-1.5, -2.5]])\n\nW = np.array([[0.1, 0.2, 0.3],\n              [0.1, 0.2, 0.3]])\n\nbias = np.array([0.01, 0.1, 0.1])\n\nprint('Inputs X:\\n', X)\nprint('\\nWeights W:\\n', W)\nprint('\\nbias:\\n', bias)\n\n\n\n\nInputs X:\n [[ 0.1  0.5]\n [ 1.1  2.3]\n [-1.1 -2.3]\n [-1.5 -2.5]]\n\nWeights W:\n [[ 0.1  0.2  0.3]\n [ 0.1  0.2  0.3]]\n\nbias:\n [ 0.01  0.1   0.1 ]\n\n\n\nTo compute the net input, we multiply the 4x2 matrix feature matrix \nX\n with the 2x3 (n_features x n_classes) weight matrix \nW\n, which yields a 4x3 output matrix (n_samples x n_classes) to which we then add the bias unit: \n\n\n$$\\mathbf{Z} = \\mathbf{X}\\mathbf{W} + \\mathbf{b}.$$\n\n\nX = np.array([[0.1, 0.5],\n              [1.1, 2.3],\n              [-1.1, -2.3],\n              [-1.5, -2.5]])\n\nW = np.array([[0.1, 0.2, 0.3],\n              [0.1, 0.2, 0.3]])\n\nbias = np.array([0.01, 0.1, 0.1])\n\nprint('Inputs X:\\n', X)\nprint('\\nWeights W:\\n', W)\nprint('\\nbias:\\n', bias)\n\n\n\n\nInputs X:\n [[ 0.1  0.5]\n [ 1.1  2.3]\n [-1.1 -2.3]\n [-1.5 -2.5]]\n\nWeights W:\n [[ 0.1  0.2  0.3]\n [ 0.1  0.2  0.3]]\n\nbias:\n [ 0.01  0.1   0.1 ]\n\n\n\ndef net_input(X, W, b):\n    return (X.dot(W) + b)\n\nnet_in = net_input(X, W, bias)\nprint('net input:\\n', net_in)\n\n\n\n\nnet input:\n [[ 0.07  0.22  0.28]\n [ 0.35  0.78  1.12]\n [-0.33 -0.58 -0.92]\n [-0.39 -0.7  -1.1 ]]\n\n\n\nNow, it's time to compute the softmax activation that we discussed earlier:\n\n\n$$P(y=j \\mid z^{(i)}) = \\phi_{softmax}(z^{(i)}) = \\frac{e^{z^{(i)}}}{\\sum_{j=0}^{k} e^{z_{k}^{(i)}}}.$$\n\n\ndef softmax(z):\n    return (np.exp(z.T) / np.sum(np.exp(z), axis=1)).T\n\nsmax = softmax(net_in)\nprint('softmax:\\n', smax)\n\n\n\n\nsoftmax:\n [[ 0.29450637  0.34216758  0.36332605]\n [ 0.21290077  0.32728332  0.45981591]\n [ 0.42860913  0.33380113  0.23758974]\n [ 0.44941979  0.32962558  0.22095463]]\n\n\n\nAs we can see, the values for each sample (row) nicely sum up to 1 now. E.g., we can say that the first sample \n\n\n[ 0.29450637  0.34216758  0.36332605]\n has a 29.45% probability to belong to class 0.\n\n\nNow, in order to turn these probabilities back into class labels, we could simply take the argmax-index position of each row:\n\n\n[[ 0.29450637  0.34216758  \n0.36332605\n] -> 2 \n\n[ 0.21290077  0.32728332  \n0.45981591\n]  -> 2\n\n[ \n0.42860913\n  0.33380113  0.23758974]  -> 0\n\n[ \n0.44941979\n  0.32962558  0.22095463]] -> 0  \n\n\ndef to_classlabel(z):\n    return z.argmax(axis=1)\n\nprint('predicted class labels: ', to_classlabel(smax))\n\n\n\n\npredicted class labels:  [2 2 0 0]\n\n\n\nAs we can see, our predictions are terribly wrong, since the correct class labels are \n[0, 1, 2, 2]\n. Now, in order to train our logistic model (e.g., via an optimization algorithm such as gradient descent), we need to define a cost function \n$J(\\cdot)$\n that we want to minimize:\n\n\n$$J(\\mathbf{W}; \\mathbf{b}) = \\frac{1}{n} \\sum_{i=1}^{n} H(T_i, O_i),$$\n\n\nwhich is the average of all cross-entropies over our \n$n$\n training samples. The cross-entropy  function is defined as\n\n\n$$H(T_i, O_i) = -\\sum_m T_i \\cdot log(O_i).$$\n\n\nHere the \n$T$\n stands for \"target\" (i.e., the \ntrue\n class labels) and the \n$O$\n stands for output -- the computed \nprobability\n via softmax; \nnot\n the predicted class label.\n\n\ndef cross_entropy(output, y_target):\n    return - np.sum(np.log(output) * (y_target), axis=1)\n\nxent = cross_entropy(smax, y_enc)\nprint('Cross Entropy:', xent)\n\n\n\n\nCross Entropy: [ 1.22245465  1.11692907  1.43720989  1.50979788]\n\n\n\ndef cost(output, y_target):\n    return np.mean(cross_entropy(output, y_target))\n\nJ_cost = cost(smax, y_enc)\nprint('Cost: ', J_cost)\n\n\n\n\nCost:  1.32159787159\n\n\n\nIn order to learn our softmax model -- determining the weight coefficients -- via gradient descent, we then need to compute the derivative \n\n\n$$\\nabla \\mathbf{w}_j \\, J(\\mathbf{W}; \\mathbf{b}).$$\n\n\nI don't want to walk through the tedious details here, but this cost derivative turns out to be simply:\n\n\n$$\\nabla \\mathbf{w}_j \\, J(\\mathbf{W}; \\mathbf{b}) = \\frac{1}{n} \\sum^{n}_{i=0} \\big[\\mathbf{x}^{(i)}\\ \\big(O_i - T_i \\big) \\big]$$\n\n\nWe can then use the cost derivate to update the weights in opposite direction of the cost gradient with learning rate \n$\\eta$\n:\n\n\n$$\\mathbf{w}_j := \\mathbf{w}_j - \\eta \\nabla \\mathbf{w}_j \\, J(\\mathbf{W}; \\mathbf{b})$$\n \n\n\nfor each class \n$$j \\in \\{0, 1, ..., k\\}$$\n\n\n(note that \n$\\mathbf{w}_j$\n is the weight vector for the class \n$y=j$\n), and we update the bias units\n\n\n$$\\mathbf{b}_j := \\mathbf{b}_j   - \\eta \\bigg[ \\frac{1}{n} \\sum^{n}_{i=0} \\big(O_i - T_i  \\big) \\bigg].$$\n \n\n\nAs a penalty against complexity, an approach to reduce the variance of our model and decrease the degree of overfitting by adding additional bias, we can further add a regularization term such as the L2 term with the regularization parameter \n$\\lambda$\n:\n\n\nL2:        \n$\\frac{\\lambda}{2} ||\\mathbf{w}||_{2}^{2}$\n, \n\n\nwhere \n\n\n$$||\\mathbf{w}||_{2}^{2} = \\sum^{m}_{l=0} \\sum^{k}_{j=0} w_{i, j}$$\n\n\nso that our cost function becomes\n\n\n$$J(\\mathbf{W}; \\mathbf{b}) = \\frac{1}{n} \\sum_{i=1}^{n} H(T_i, O_i) + \\frac{\\lambda}{2} ||\\mathbf{w}||_{2}^{2}$$\n\n\nand we define the \"regularized\" weight update as\n\n\n$$\\mathbf{w}_j := \\mathbf{w}_j -  \\eta \\big[\\nabla \\mathbf{w}_j \\, J(\\mathbf{W}) + \\lambda \\mathbf{w}_j \\big].$$\n\n\n(Please note that we don't regularize the bias term.)\n\n\nExamples\n\n\nExample 1 - Gradient Descent\n\n\nfrom mlxtend.data import iris_data\nfrom mlxtend.evaluate import plot_decision_regions\nfrom mlxtend.classifier import SoftmaxRegression\nimport matplotlib.pyplot as plt\n\n# Loading Data\n\nX, y = iris_data()\nX = X[:, [0, 3]] # sepal length and petal width\n\n# standardize\nX[:,0] = (X[:,0] - X[:,0].mean()) / X[:,0].std()\nX[:,1] = (X[:,1] - X[:,1].mean()) / X[:,1].std()\n\nlr = SoftmaxRegression(eta=0.01, \n                       epochs=500, \n                       minibatches=1, \n                       random_seed=1,\n                       print_progress=3)\nlr.fit(X, y)\n\nplot_decision_regions(X, y, clf=lr)\nplt.title('Softmax Regression - Gradient Descent')\nplt.show()\n\nplt.plot(range(len(lr.cost_)), lr.cost_)\nplt.xlabel('Iterations')\nplt.ylabel('Cost')\nplt.show()\n\n\n\n\nIteration: 500/500 | Cost 0.06 | Elapsed: 0:00:00 | ETA: 0:00:00\n\n\n\n\n\n\n\nPredicting Class Labels\n\n\ny_pred = lr.predict(X)\nprint('Last 3 Class Labels: %s' % y_pred[-3:])\n\n\n\n\nLast 3 Class Labels: [2 2 2]\n\n\n\nPredicting Class Probabilities\n\n\ny_pred = lr.predict_proba(X)\nprint('Last 3 Class Labels:\\n %s' % y_pred[-3:])\n\n\n\n\nLast 3 Class Labels:\n [[  9.18728149e-09   1.68894679e-02   9.83110523e-01]\n [  2.97052325e-11   7.26356627e-04   9.99273643e-01]\n [  1.57464093e-06   1.57779528e-01   8.42218897e-01]]\n\n\n\nExample 2 - Stochastic Gradient Descent\n\n\nfrom mlxtend.data import iris_data\nfrom mlxtend.evaluate import plot_decision_regions\nfrom mlxtend.classifier import SoftmaxRegression\nimport matplotlib.pyplot as plt\n\n# Loading Data\n\nX, y = iris_data()\nX = X[:, [0, 3]] # sepal length and petal width\n\n# standardize\nX[:,0] = (X[:,0] - X[:,0].mean()) / X[:,0].std()\nX[:,1] = (X[:,1] - X[:,1].mean()) / X[:,1].std()\n\nlr = SoftmaxRegression(eta=0.01, epochs=300, minibatches=len(y), random_seed=1)\nlr.fit(X, y)\n\nplot_decision_regions(X, y, clf=lr)\nplt.title('Softmax Regression - Stochastic Gradient Descent')\nplt.show()\n\nplt.plot(range(len(lr.cost_)), lr.cost_)\nplt.xlabel('Iterations')\nplt.ylabel('Cost')\nplt.show()\n\n\n\n\n\n\n\n\nAPI\n\n\nSoftmaxRegression(eta=0.01, epochs=50, l2=0.0, minibatches=1, n_classes=None, random_seed=None, print_progress=0)\n\n\nSoftmax regression classifier.\n\n\nParameters\n\n\n\n\n\n\neta\n : float (default: 0.01)\n\n\nLearning rate (between 0.0 and 1.0)\n\n\n\n\n\n\nepochs\n : int (default: 50)\n\n\nPasses over the training dataset.\nPrior to each epoch, the dataset is shuffled\nif \nminibatches > 1\n to prevent cycles in stochastic gradient descent.\n\n\n\n\n\n\nl2\n : float\n\n\nRegularization parameter for L2 regularization.\nNo regularization if l2=0.0.\n\n\n\n\n\n\nminibatches\n : int (default: 1)\n\n\nThe number of minibatches for gradient-based optimization.\nIf 1: Gradient Descent learning\nIf len(y): Stochastic Gradient Descent (SGD) online learning\nIf 1 < minibatches < len(y): SGD Minibatch learning\n\n\n\n\n\n\nn_classes\n : int (default: None)\n\n\nA positive integer to declare the number of class labels\nif not all class labels are present in a partial training set.\nGets the number of class labels automatically if None.\n\n\n\n\n\n\nrandom_seed\n : int (default: None)\n\n\nSet random state for shuffling and initializing the weights.\n\n\n\n\n\n\nprint_progress\n : int (default: 0)\n\n\nPrints progress in fitting to stderr.\n0: No output\n1: Epochs elapsed and cost\n2: 1 plus time elapsed\n3: 2 plus estimated time until completion\n\n\n\n\n\n\nAttributes\n\n\n\n\n\n\nw_\n : 2d-array, shape={n_features, 1}\n\n\nModel weights after fitting.\n\n\n\n\n\n\nb_\n : 1d-array, shape={1,}\n\n\nBias unit after fitting.\n\n\n\n\n\n\ncost_\n : list\n\n\nList of floats, the average cross_entropy for each epoch.\n\n\n\n\n\n\nMethods\n\n\n\n\n\nfit(X, y, init_params=True)\n\n\nLearn model from training data.\n\n\nParameters\n\n\n\n\n\n\nX\n : {array-like, sparse matrix}, shape = [n_samples, n_features]\n\n\nTraining vectors, where n_samples is the number of samples and\nn_features is the number of features.\n\n\n\n\n\n\ny\n : array-like, shape = [n_samples]\n\n\nTarget values.\n\n\n\n\n\n\ninit_params\n : bool (default: True)\n\n\nRe-initializes model parameters prior to fitting.\nSet False to continue training with weights from\na previous model fitting.\n\n\n\n\n\n\nReturns\n\n\n\n\nself\n : object\n\n\n\n\n\n\n\npredict(X)\n\n\nPredict targets from X.\n\n\nParameters\n\n\n\n\n\n\nX\n : {array-like, sparse matrix}, shape = [n_samples, n_features]\n\n\nTraining vectors, where n_samples is the number of samples and\nn_features is the number of features.\n\n\n\n\n\n\nReturns\n\n\n\n\n\n\ntarget_values\n : array-like, shape = [n_samples]\n\n\nPredicted target values.\n\n\n\n\n\n\n\n\n\npredict_proba(X)\n\n\nPredict class probabilities of X from the net input.\n\n\nParameters\n\n\n\n\n\n\nX\n : {array-like, sparse matrix}, shape = [n_samples, n_features]\n\n\nTraining vectors, where n_samples is the number of samples and\nn_features is the number of features.\n\n\n\n\n\n\nReturns\n\n\n\n\nClass probabilties\n : array-like, shape= [n_samples, n_classes]\n\n\n\n\n\n\n\nscore(X, y)\n\n\nCompute the prediction accuracy\n\n\nParameters\n\n\n\n\n\n\nX\n : {array-like, sparse matrix}, shape = [n_samples, n_features]\n\n\nTraining vectors, where n_samples is the number of samples and\nn_features is the number of features.\n\n\n\n\n\n\ny\n : array-like, shape = [n_samples]\n\n\nTarget values (true class labels).\n\n\n\n\n\n\nReturns\n\n\n\n\n\n\nacc\n : float\n\n\nThe prediction accuracy as a float\nbetween 0.0 and 1.0 (perfect score).",
            "title": "SoftmaxRegression"
        },
        {
            "location": "/user_guide/classifier/SoftmaxRegression/#softmax-regression",
            "text": "A logistic regression class for multi-class classification tasks.   from mlxtend.classifier import SoftmaxRegression",
            "title": "Softmax Regression"
        },
        {
            "location": "/user_guide/classifier/SoftmaxRegression/#overview",
            "text": "Softmax Regression  (synonyms:  Multinomial Logistic ,  Maximum Entropy Classifier , or just  Multi-class Logistic Regression ) is a generalization of logistic regression that we can use for multi-class classification (under the assumption that the classes are  mutually exclusive). In contrast, we use the (standard)  Logistic Regression  model in binary classification tasks.  Below is a schematic of a  Logistic Regression  model, for more details, please see the  LogisticRegression  manual .   In  Softmax Regression  (SMR), we replace the sigmoid logistic function by the so-called  softmax  function  $\\phi_{softmax}(\\cdot)$ .  $$P(y=j \\mid z^{(i)}) = \\phi_{softmax}(z^{(i)}) = \\frac{e^{z^{(i)}}}{\\sum_{j=0}^{k} e^{z_{k}^{(i)}}},$$  where we define the net input  z  as   $$z = w_1x_1 + ... + w_mx_m  + b= \\sum_{l=0}^{m} w_l x_l + b= \\mathbf{w}^T\\mathbf{x} + b.$$    ( w  is the weight vector,  $\\mathbf{x}$  is the feature vector of 1 training sample, and  $b$  is the bias unit.)  \nNow, this softmax function computes the probability that this training sample  $\\mathbf{x}^{(i)}$  belongs to class  $j$  given the weight and net input  $z^{(i)}$ . So, we compute the probability  $p(y = j \\mid \\mathbf{x^{(i)}; w}_j)$  for each class label in   $j = 1, \\ldots, k.$ . Note the normalization term in the denominator which causes these class probabilities to sum up to one.   To illustrate the concept of softmax, let us walk through a concrete example. Let's assume we have a training set consisting of 4 samples from 3 different classes (0, 1, and 2)   $x_0 \\rightarrow \\text{class }0$  $x_1 \\rightarrow \\text{class }1$  $x_2 \\rightarrow \\text{class }2$  $x_3 \\rightarrow \\text{class }2$   import numpy as np\ny = np.array([0, 1, 2, 2])  First, we want to encode the class labels into a format that we can more easily work with; we apply one-hot encoding:  y_enc = (np.arange(np.max(y) + 1) == y[:, None]).astype(float)\nprint('one-hot encoding:\\n', y_enc)  one-hot encoding:\n [[ 1.  0.  0.]\n [ 0.  1.  0.]\n [ 0.  0.  1.]\n [ 0.  0.  1.]]  A sample that belongs to class 0 (the first row) has a 1 in the first cell, a sample that belongs to class 2 has a 1 in the second cell of its row, and so forth.  Next, let us define the feature matrix of our 4 training samples. Here, we assume that our dataset consists of 2 features; thus, we create a 4x2 dimensional matrix of our samples and features.\nSimilarly, we create a 2x3 dimensional weight matrix (one row per feature and one column for each class).  X = np.array([[0.1, 0.5],\n              [1.1, 2.3],\n              [-1.1, -2.3],\n              [-1.5, -2.5]])\n\nW = np.array([[0.1, 0.2, 0.3],\n              [0.1, 0.2, 0.3]])\n\nbias = np.array([0.01, 0.1, 0.1])\n\nprint('Inputs X:\\n', X)\nprint('\\nWeights W:\\n', W)\nprint('\\nbias:\\n', bias)  Inputs X:\n [[ 0.1  0.5]\n [ 1.1  2.3]\n [-1.1 -2.3]\n [-1.5 -2.5]]\n\nWeights W:\n [[ 0.1  0.2  0.3]\n [ 0.1  0.2  0.3]]\n\nbias:\n [ 0.01  0.1   0.1 ]  To compute the net input, we multiply the 4x2 matrix feature matrix  X  with the 2x3 (n_features x n_classes) weight matrix  W , which yields a 4x3 output matrix (n_samples x n_classes) to which we then add the bias unit:   $$\\mathbf{Z} = \\mathbf{X}\\mathbf{W} + \\mathbf{b}.$$  X = np.array([[0.1, 0.5],\n              [1.1, 2.3],\n              [-1.1, -2.3],\n              [-1.5, -2.5]])\n\nW = np.array([[0.1, 0.2, 0.3],\n              [0.1, 0.2, 0.3]])\n\nbias = np.array([0.01, 0.1, 0.1])\n\nprint('Inputs X:\\n', X)\nprint('\\nWeights W:\\n', W)\nprint('\\nbias:\\n', bias)  Inputs X:\n [[ 0.1  0.5]\n [ 1.1  2.3]\n [-1.1 -2.3]\n [-1.5 -2.5]]\n\nWeights W:\n [[ 0.1  0.2  0.3]\n [ 0.1  0.2  0.3]]\n\nbias:\n [ 0.01  0.1   0.1 ]  def net_input(X, W, b):\n    return (X.dot(W) + b)\n\nnet_in = net_input(X, W, bias)\nprint('net input:\\n', net_in)  net input:\n [[ 0.07  0.22  0.28]\n [ 0.35  0.78  1.12]\n [-0.33 -0.58 -0.92]\n [-0.39 -0.7  -1.1 ]]  Now, it's time to compute the softmax activation that we discussed earlier:  $$P(y=j \\mid z^{(i)}) = \\phi_{softmax}(z^{(i)}) = \\frac{e^{z^{(i)}}}{\\sum_{j=0}^{k} e^{z_{k}^{(i)}}}.$$  def softmax(z):\n    return (np.exp(z.T) / np.sum(np.exp(z), axis=1)).T\n\nsmax = softmax(net_in)\nprint('softmax:\\n', smax)  softmax:\n [[ 0.29450637  0.34216758  0.36332605]\n [ 0.21290077  0.32728332  0.45981591]\n [ 0.42860913  0.33380113  0.23758974]\n [ 0.44941979  0.32962558  0.22095463]]  As we can see, the values for each sample (row) nicely sum up to 1 now. E.g., we can say that the first sample   [ 0.29450637  0.34216758  0.36332605]  has a 29.45% probability to belong to class 0.  Now, in order to turn these probabilities back into class labels, we could simply take the argmax-index position of each row:  [[ 0.29450637  0.34216758   0.36332605 ] -> 2  \n[ 0.21290077  0.32728332   0.45981591 ]  -> 2 \n[  0.42860913   0.33380113  0.23758974]  -> 0 \n[  0.44941979   0.32962558  0.22095463]] -> 0    def to_classlabel(z):\n    return z.argmax(axis=1)\n\nprint('predicted class labels: ', to_classlabel(smax))  predicted class labels:  [2 2 0 0]  As we can see, our predictions are terribly wrong, since the correct class labels are  [0, 1, 2, 2] . Now, in order to train our logistic model (e.g., via an optimization algorithm such as gradient descent), we need to define a cost function  $J(\\cdot)$  that we want to minimize:  $$J(\\mathbf{W}; \\mathbf{b}) = \\frac{1}{n} \\sum_{i=1}^{n} H(T_i, O_i),$$  which is the average of all cross-entropies over our  $n$  training samples. The cross-entropy  function is defined as  $$H(T_i, O_i) = -\\sum_m T_i \\cdot log(O_i).$$  Here the  $T$  stands for \"target\" (i.e., the  true  class labels) and the  $O$  stands for output -- the computed  probability  via softmax;  not  the predicted class label.  def cross_entropy(output, y_target):\n    return - np.sum(np.log(output) * (y_target), axis=1)\n\nxent = cross_entropy(smax, y_enc)\nprint('Cross Entropy:', xent)  Cross Entropy: [ 1.22245465  1.11692907  1.43720989  1.50979788]  def cost(output, y_target):\n    return np.mean(cross_entropy(output, y_target))\n\nJ_cost = cost(smax, y_enc)\nprint('Cost: ', J_cost)  Cost:  1.32159787159  In order to learn our softmax model -- determining the weight coefficients -- via gradient descent, we then need to compute the derivative   $$\\nabla \\mathbf{w}_j \\, J(\\mathbf{W}; \\mathbf{b}).$$  I don't want to walk through the tedious details here, but this cost derivative turns out to be simply:  $$\\nabla \\mathbf{w}_j \\, J(\\mathbf{W}; \\mathbf{b}) = \\frac{1}{n} \\sum^{n}_{i=0} \\big[\\mathbf{x}^{(i)}\\ \\big(O_i - T_i \\big) \\big]$$  We can then use the cost derivate to update the weights in opposite direction of the cost gradient with learning rate  $\\eta$ :  $$\\mathbf{w}_j := \\mathbf{w}_j - \\eta \\nabla \\mathbf{w}_j \\, J(\\mathbf{W}; \\mathbf{b})$$    for each class  $$j \\in \\{0, 1, ..., k\\}$$  (note that  $\\mathbf{w}_j$  is the weight vector for the class  $y=j$ ), and we update the bias units  $$\\mathbf{b}_j := \\mathbf{b}_j   - \\eta \\bigg[ \\frac{1}{n} \\sum^{n}_{i=0} \\big(O_i - T_i  \\big) \\bigg].$$    As a penalty against complexity, an approach to reduce the variance of our model and decrease the degree of overfitting by adding additional bias, we can further add a regularization term such as the L2 term with the regularization parameter  $\\lambda$ :  L2:         $\\frac{\\lambda}{2} ||\\mathbf{w}||_{2}^{2}$ ,   where   $$||\\mathbf{w}||_{2}^{2} = \\sum^{m}_{l=0} \\sum^{k}_{j=0} w_{i, j}$$  so that our cost function becomes  $$J(\\mathbf{W}; \\mathbf{b}) = \\frac{1}{n} \\sum_{i=1}^{n} H(T_i, O_i) + \\frac{\\lambda}{2} ||\\mathbf{w}||_{2}^{2}$$  and we define the \"regularized\" weight update as  $$\\mathbf{w}_j := \\mathbf{w}_j -  \\eta \\big[\\nabla \\mathbf{w}_j \\, J(\\mathbf{W}) + \\lambda \\mathbf{w}_j \\big].$$  (Please note that we don't regularize the bias term.)",
            "title": "Overview"
        },
        {
            "location": "/user_guide/classifier/SoftmaxRegression/#examples",
            "text": "",
            "title": "Examples"
        },
        {
            "location": "/user_guide/classifier/SoftmaxRegression/#example-1-gradient-descent",
            "text": "from mlxtend.data import iris_data\nfrom mlxtend.evaluate import plot_decision_regions\nfrom mlxtend.classifier import SoftmaxRegression\nimport matplotlib.pyplot as plt\n\n# Loading Data\n\nX, y = iris_data()\nX = X[:, [0, 3]] # sepal length and petal width\n\n# standardize\nX[:,0] = (X[:,0] - X[:,0].mean()) / X[:,0].std()\nX[:,1] = (X[:,1] - X[:,1].mean()) / X[:,1].std()\n\nlr = SoftmaxRegression(eta=0.01, \n                       epochs=500, \n                       minibatches=1, \n                       random_seed=1,\n                       print_progress=3)\nlr.fit(X, y)\n\nplot_decision_regions(X, y, clf=lr)\nplt.title('Softmax Regression - Gradient Descent')\nplt.show()\n\nplt.plot(range(len(lr.cost_)), lr.cost_)\nplt.xlabel('Iterations')\nplt.ylabel('Cost')\nplt.show()  Iteration: 500/500 | Cost 0.06 | Elapsed: 0:00:00 | ETA: 0:00:00",
            "title": "Example 1 - Gradient Descent"
        },
        {
            "location": "/user_guide/classifier/SoftmaxRegression/#predicting-class-labels",
            "text": "y_pred = lr.predict(X)\nprint('Last 3 Class Labels: %s' % y_pred[-3:])  Last 3 Class Labels: [2 2 2]",
            "title": "Predicting Class Labels"
        },
        {
            "location": "/user_guide/classifier/SoftmaxRegression/#predicting-class-probabilities",
            "text": "y_pred = lr.predict_proba(X)\nprint('Last 3 Class Labels:\\n %s' % y_pred[-3:])  Last 3 Class Labels:\n [[  9.18728149e-09   1.68894679e-02   9.83110523e-01]\n [  2.97052325e-11   7.26356627e-04   9.99273643e-01]\n [  1.57464093e-06   1.57779528e-01   8.42218897e-01]]",
            "title": "Predicting Class Probabilities"
        },
        {
            "location": "/user_guide/classifier/SoftmaxRegression/#example-2-stochastic-gradient-descent",
            "text": "from mlxtend.data import iris_data\nfrom mlxtend.evaluate import plot_decision_regions\nfrom mlxtend.classifier import SoftmaxRegression\nimport matplotlib.pyplot as plt\n\n# Loading Data\n\nX, y = iris_data()\nX = X[:, [0, 3]] # sepal length and petal width\n\n# standardize\nX[:,0] = (X[:,0] - X[:,0].mean()) / X[:,0].std()\nX[:,1] = (X[:,1] - X[:,1].mean()) / X[:,1].std()\n\nlr = SoftmaxRegression(eta=0.01, epochs=300, minibatches=len(y), random_seed=1)\nlr.fit(X, y)\n\nplot_decision_regions(X, y, clf=lr)\nplt.title('Softmax Regression - Stochastic Gradient Descent')\nplt.show()\n\nplt.plot(range(len(lr.cost_)), lr.cost_)\nplt.xlabel('Iterations')\nplt.ylabel('Cost')\nplt.show()",
            "title": "Example 2 - Stochastic Gradient Descent"
        },
        {
            "location": "/user_guide/classifier/SoftmaxRegression/#api",
            "text": "SoftmaxRegression(eta=0.01, epochs=50, l2=0.0, minibatches=1, n_classes=None, random_seed=None, print_progress=0)  Softmax regression classifier.  Parameters    eta  : float (default: 0.01)  Learning rate (between 0.0 and 1.0)    epochs  : int (default: 50)  Passes over the training dataset.\nPrior to each epoch, the dataset is shuffled\nif  minibatches > 1  to prevent cycles in stochastic gradient descent.    l2  : float  Regularization parameter for L2 regularization.\nNo regularization if l2=0.0.    minibatches  : int (default: 1)  The number of minibatches for gradient-based optimization.\nIf 1: Gradient Descent learning\nIf len(y): Stochastic Gradient Descent (SGD) online learning\nIf 1 < minibatches < len(y): SGD Minibatch learning    n_classes  : int (default: None)  A positive integer to declare the number of class labels\nif not all class labels are present in a partial training set.\nGets the number of class labels automatically if None.    random_seed  : int (default: None)  Set random state for shuffling and initializing the weights.    print_progress  : int (default: 0)  Prints progress in fitting to stderr.\n0: No output\n1: Epochs elapsed and cost\n2: 1 plus time elapsed\n3: 2 plus estimated time until completion    Attributes    w_  : 2d-array, shape={n_features, 1}  Model weights after fitting.    b_  : 1d-array, shape={1,}  Bias unit after fitting.    cost_  : list  List of floats, the average cross_entropy for each epoch.",
            "title": "API"
        },
        {
            "location": "/user_guide/classifier/SoftmaxRegression/#methods",
            "text": "fit(X, y, init_params=True)  Learn model from training data.  Parameters    X  : {array-like, sparse matrix}, shape = [n_samples, n_features]  Training vectors, where n_samples is the number of samples and\nn_features is the number of features.    y  : array-like, shape = [n_samples]  Target values.    init_params  : bool (default: True)  Re-initializes model parameters prior to fitting.\nSet False to continue training with weights from\na previous model fitting.    Returns   self  : object    predict(X)  Predict targets from X.  Parameters    X  : {array-like, sparse matrix}, shape = [n_samples, n_features]  Training vectors, where n_samples is the number of samples and\nn_features is the number of features.    Returns    target_values  : array-like, shape = [n_samples]  Predicted target values.     predict_proba(X)  Predict class probabilities of X from the net input.  Parameters    X  : {array-like, sparse matrix}, shape = [n_samples, n_features]  Training vectors, where n_samples is the number of samples and\nn_features is the number of features.    Returns   Class probabilties  : array-like, shape= [n_samples, n_classes]    score(X, y)  Compute the prediction accuracy  Parameters    X  : {array-like, sparse matrix}, shape = [n_samples, n_features]  Training vectors, where n_samples is the number of samples and\nn_features is the number of features.    y  : array-like, shape = [n_samples]  Target values (true class labels).    Returns    acc  : float  The prediction accuracy as a float\nbetween 0.0 and 1.0 (perfect score).",
            "title": "Methods"
        },
        {
            "location": "/user_guide/classifier/MultiLayerPerceptron/",
            "text": "Neural Network - Multilayer Perceptron\n\n\nImplementation of a multilayer perceptron, a feedforward artificial neural network.\n\n\n\n\nfrom mlxtend.classifier import MultiLayerPerceptron\n\n\n\n\nOverview\n\n\nAlthough the code is fully working and can be used for common classification tasks, this implementation is not geared towards efficiency but clarity \u2013 the original code was written for demonstration purposes.\n\n\nBasic Architecture\n\n\n  \n\n\nThe neurons \n$x_0$\n and \n$a_0$\n represent the bias units (\n$x_0=1$\n, \n$a_0=1$\n). \n\n\nThe \n$i$\nth superscript denotes the \n$i$\nth layer, and the \nj\nth subscripts stands for the index of the respective unit. For example, \n$a_{1}^{(2)}$\n refers to the first activation unit \nafter\n the bias unit (i.e., 2nd activation unit) in the 2nd layer (here: the hidden layer)\n\n\n\\begin{align}\n    \\mathbf{a^{(2)}} &= \\begin{bmatrix}\n           a_{0}^{(2)} \\\n           a_{1}^{(2)} \\\n           \\vdots \\\n           a_{m}^{(2)}\n         \\end{bmatrix}.\n  \\end{align}\n\n\nEach layer \n$(l)$\n in a multi-layer perceptron, a directed graph, is fully connected to the next layer \n$(l+1)$\n. We write the weight coefficient that connects the \n$k$\nth unit in the \n$l$\nth layer to the \n$j$\nth unit in layer \n$l+1$\n as \n$w^{(l)}_{j, k}$\n.\n\n\nFor example, the weight coefficient that connects the units\n\n\n$a_0^{(2)} \\rightarrow a_1^{(3)}$\n\n\nwould be written as \n$w_{1,0}^{(2)}$\n.\n\n\nActivation\n\n\nIn the current implementation, the activations of the hidden layer(s) are computed via the logistic (sigmoid) function \n$\\phi(z) = \\frac{1}{1 + e^{-z}}.$\n\n\n\n\n(For more details on the logistic function, please see \nclassifier.LogisticRegression\n; a general overview of different activation function can be found \nhere\n.)\n\n\nFurthermore, the MLP uses the softmax function in the output layer, For more details on the logistic function, please see \nclassifier.SoftmaxRegression\n.\n\n\nReferences\n\n\n\n\nD. R. G. H. R. Williams and G. Hinton. \nLearning representations by back-propagating errors\n. Nature, pages 323\u2013533, 1986.\n\n\nC. M. Bishop. \nNeural networks for pattern recognition\n. Oxford University Press, 1995.\n\n\nT. Hastie, J. Friedman, and R. Tibshirani. \nThe Elements of Statistical Learning\n, Volume 2. Springer, 2009.\n\n\n\n\nExamples\n\n\nExample 1 - Classifying Iris Flowers\n\n\nLoad 2 features from Iris (petal length and petal width) for visualization purposes:\n\n\nfrom mlxtend.data import iris_data\nX, y = iris_data()\nX = X[:, [0, 3]]    \n\n# standardize training data\nX_std = (X - X.mean(axis=0)) / X.std(axis=0)\n\n\n\n\nTrain neural network for 3 output flower classes ('Setosa', 'Versicolor', 'Virginica'), regular gradient decent (\nminibatches=1\n), 30 hidden units, and no regularization.\n\n\nGradient Descent\n\n\nSetting the \nminibatches\n to \n1\n will result in gradient descent training; please see \nGradient Descent vs. Stochastic Gradient Descent\n for details.\n\n\nfrom mlxtend.classifier import MultiLayerPerceptron as MLP\n\nnn1 = MLP(hidden_layers=[50], \n          l2=0.00, \n          l1=0.0, \n          epochs=150, \n          eta=0.05, \n          momentum=0.1,\n          decrease_const=0.0,\n          minibatches=1, \n          random_seed=1,\n          print_progress=3)\n\nnn1 = nn1.fit(X_std, y)\n\n\n\n\nIteration: 150/150 | Cost 0.06 | Elapsed: 0:00:00 | ETA: 0:00:00\n\n\n\nfrom mlxtend.evaluate import plot_decision_regions\nimport matplotlib.pyplot as plt\n\nfig = plot_decision_regions(X=X_std, y=y, clf=nn1, legend=2)\nplt.title('Multi-layer Perceptron w. 1 hidden layer (logistic sigmoid)')\nplt.show()\n\n\n\n\n\n\nimport matplotlib.pyplot as plt\nplt.plot(range(len(nn1.cost_)), nn1.cost_)\nplt.ylabel('Cost')\nplt.xlabel('Epochs')\nplt.show()\n\n\n\n\n\n\nprint('Accuracy: %.2f%%' % (100 * nn1.score(X_std, y)))\n\n\n\n\nAccuracy: 96.67%\n\n\n\nStochastic Gradient Descent\n\n\nSetting \nminibatches\n to \nn_samples\n will result in stochastic gradient descent training; please see \nGradient Descent vs. Stochastic Gradient Descent\n for details.\n\n\nnn2 = MLP(hidden_layers=[50], \n          l2=0.00, \n          l1=0.0, \n          epochs=5, \n          eta=0.005, \n          momentum=0.1,\n          decrease_const=0.0,\n          minibatches=len(y), \n          random_seed=1,\n          print_progress=3)\n\nnn2.fit(X_std, y)\n\nplt.plot(range(len(nn2.cost_)), nn2.cost_)\nplt.ylabel('Cost')\nplt.xlabel('Epochs')\nplt.show()\n\n\n\n\nIteration: 5/5 | Cost 0.11 | Elapsed: 00:00:00 | ETA: 00:00:00\n\n\n\n\n\nContinue the training for 25 epochs...\n\n\nnn2.epochs = 25\nnn2 = nn2.fit(X_std, y)\n\n\n\n\nIteration: 25/25 | Cost 0.07 | Elapsed: 0:00:00 | ETA: 0:00:00\n\n\n\nplt.plot(range(len(nn2.cost_)), nn2.cost_)\nplt.ylabel('Cost')\nplt.xlabel('Epochs')\nplt.show()\n\n\n\n\n\n\nExample 2 - Classifying Handwritten Digits from a 10% MNIST Subset\n\n\nLoad a \n5000-sample subset\n of the \nMNIST dataset\n (please see \ndata.load_mnist\n if you want to download and read in the complete MNIST dataset).\n\n\nfrom mlxtend.data import mnist_data\nfrom mlxtend.preprocessing import shuffle_arrays_unison\n\nX, y = mnist_data()\nX, y = shuffle_arrays_unison((X, y), random_seed=1)\nX_train, y_train = X[:500], y[:500]\nX_test, y_test = X[500:], y[500:]\n\n\n\n\nVisualize a sample from the MNIST dataset to check if it was loaded correctly:\n\n\nimport matplotlib.pyplot as plt\n\ndef plot_digit(X, y, idx):\n    img = X[idx].reshape(28,28)\n    plt.imshow(img, cmap='Greys',  interpolation='nearest')\n    plt.title('true label: %d' % y[idx])\n    plt.show()\n\nplot_digit(X, y, 3500)    \n\n\n\n\n\n\nStandardize pixel values:\n\n\nimport numpy as np\nfrom mlxtend.preprocessing import standardize\n\nX_train_std, params = standardize(X_train, \n                                  columns=range(X_train.shape[1]), \n                                  return_params=True)\n\nX_test_std = standardize(X_test,\n                         columns=range(X_test.shape[1]),\n                         params=params)\n\n\n\n\nInitialize the neural network to recognize the 10 different digits (0-10) using 300 epochs and mini-batch learning.\n\n\nnn1 = MLP(hidden_layers=[150], \n          l2=0.00, \n          l1=0.0, \n          epochs=100, \n          eta=0.005, \n          momentum=0.0,\n          decrease_const=0.0,\n          minibatches=100, \n          random_seed=1,\n          print_progress=3)\n\n\n\n\nLearn the features while printing the progress to get an idea about how long it may take.\n\n\nimport matplotlib.pyplot as plt\n\nnn1.fit(X_train_std, y_train)\n\nplt.plot(range(len(nn1.cost_)), nn1.cost_)\nplt.ylabel('Cost')\nplt.xlabel('Epochs')\nplt.show()\n\n\n\n\nIteration: 100/100 | Cost 0.01 | Elapsed: 0:00:22 | ETA: 0:00:00\n\n\n\n\n\nprint('Train Accuracy: %.2f%%' % (100 * nn1.score(X_train_std, y_train)))\nprint('Test Accuracy: %.2f%%' % (100 * nn1.score(X_test_std, y_test)))\n\n\n\n\nTrain Accuracy: 100.00%\nTest Accuracy: 84.62%\n\n\n\nPlease note\n that this neural network has been trained on only 10% of the MNIST data for technical demonstration purposes, hence, the lousy predictive performance.\n\n\nAPI\n\n\nNeuralNetMLP(n_output, n_features, n_hidden=30, l1=0.0, l2=0.0, epochs=500, eta=0.001, alpha=0.0, decrease_const=0.0, shuffle_init=True, shuffle_epoch=True, minibatches=1, zero_init_weight=False, random_seed=None, print_progress=0)\n\n\nFeedforward neural network / Multi-layer perceptron classifier.\n\n\nParameters\n\n\n\n\n\n\nn_output\n : int\n\n\nNumber of output units, should be equal to the\nnumber of unique class labels.\n\n\n\n\n\n\nn_features\n : int\n\n\nNumber of features (dimensions) in the target dataset.\nShould be equal to the number of columns in the X array.\n\n\n\n\n\n\nn_hidden\n : int (default: 30)\n\n\nNumber of hidden units.\n\n\n\n\n\n\nl1\n : float (default: 0.0)\n\n\nLambda value for L1-regularization.\nNo regularization if l1=0.0 (default)\n\n\n\n\n\n\nl2\n : float (default: 0.0)\n\n\nLambda value for L2-regularization.\nNo regularization if l2=0.0 (default)\n\n\n\n\n\n\nepochs\n : int (default: 500)\n\n\nNumber of passes over the training set.\n\n\n\n\n\n\neta\n : float (default: 0.001)\n\n\nLearning rate.\n\n\n\n\n\n\nalpha\n : float (default: 0.0)\n\n\nMomentum constant. Factor multiplied with the\ngradient of the previous epoch t-1 to improve\nlearning speed\nw(t) := w(t) - (grad(t) + alpha*grad(t-1))\n\n\n\n\n\n\ndecrease_const\n : float (default: 0.0)\n\n\nDecrease constant. Shrinks the learning rate\nafter each epoch via eta / (1 + epoch*decrease_const)\n\n\n\n\n\n\nrandom_weights\n : list (default: [-1.0, 1.0])\n\n\nMin and max values for initializing the random weights.\nInitializes weights to 0 if None or False.\n\n\n\n\n\n\nshuffle_init\n : bool (default: True)\n\n\nShuffles (a copy of the) training data before training.\n\n\n\n\n\n\nshuffle_epoch\n : bool (default: True)\n\n\nShuffles training data before every epoch if True to prevent circles.\n\n\n\n\n\n\nminibatches\n : int (default: 1)\n\n\nDivides training data into k minibatches for efficiency.\nNormal gradient descent learning if k=1 (default).\n\n\n\n\n\n\nrandom_seed\n : int (default: None)\n\n\nSet random seed for shuffling and initializing the weights.\n\n\n\n\n\n\nzero_init_weight\n : bool (default: False)\n\n\nIf True, weights are initialized to zero instead of small random\nnumbers following a standard normal distribution with mean=0 and\nstddev=1.\n\n\n\n\n\n\nprint_progress\n : int (default: 0)\n\n\nPrints progress in fitting to stderr.\n0: No output\n1: Epochs elapsed and cost\n2: 1 plus time elapsed\n3: 2 plus estimated time until completion\n\n\n\n\n\n\nAttributes\n\n\n\n\n\n\ncost_\n : list\n\n\nSum of squared errors after each epoch.\n\n\n\n\n\n\nMethods\n\n\n\n\n\nfit(X, y)\n\n\nLearn weight coefficients from training data.\n\n\nParameters\n\n\n\n\n\n\nX\n : array, shape = [n_samples, n_features]\n\n\nInput layer with original features.\n\n\n\n\n\n\ny\n : array, shape = [n_samples]\n\n\nTarget class labels.\n\n\n\n\n\n\nReturns:\n\n\nself\n\n\n\n\n\npredict(X)\n\n\nPredict class labels of X.\n\n\nParameters\n\n\n\n\n\n\nX\n : {array-like, sparse matrix}, shape = [n_samples, n_features]\n\n\nTraining vectors, where n_samples is the number of samples and\nn_features is the number of features.\n\n\n\n\n\n\nReturns\n\n\n\n\n\n\nclass_labels\n : array-like, shape = [n_samples]\n\n\nPredicted class labels.\n\n\n\n\n\n\n\n\n\nscore(X, y)\n\n\nCompute the prediction accuracy\n\n\nParameters\n\n\n\n\n\n\nX\n : {array-like, sparse matrix}, shape = [n_samples, n_features]\n\n\nTraining vectors, where n_samples is the number of samples and\nn_features is the number of features.\n\n\n\n\n\n\ny\n : array-like, shape = [n_samples]\n\n\nTarget values (true class labels).\n\n\n\n\n\n\nReturns\n\n\n\n\n\n\nacc\n : float\n\n\nThe prediction accuracy as a float\nbetween 0.0 and 1.0 (perfect score).",
            "title": "MultiLayerPerceptron"
        },
        {
            "location": "/user_guide/classifier/MultiLayerPerceptron/#neural-network-multilayer-perceptron",
            "text": "Implementation of a multilayer perceptron, a feedforward artificial neural network.   from mlxtend.classifier import MultiLayerPerceptron",
            "title": "Neural Network - Multilayer Perceptron"
        },
        {
            "location": "/user_guide/classifier/MultiLayerPerceptron/#overview",
            "text": "Although the code is fully working and can be used for common classification tasks, this implementation is not geared towards efficiency but clarity \u2013 the original code was written for demonstration purposes.",
            "title": "Overview"
        },
        {
            "location": "/user_guide/classifier/MultiLayerPerceptron/#basic-architecture",
            "text": "The neurons  $x_0$  and  $a_0$  represent the bias units ( $x_0=1$ ,  $a_0=1$ ).   The  $i$ th superscript denotes the  $i$ th layer, and the  j th subscripts stands for the index of the respective unit. For example,  $a_{1}^{(2)}$  refers to the first activation unit  after  the bias unit (i.e., 2nd activation unit) in the 2nd layer (here: the hidden layer)  \\begin{align}\n    \\mathbf{a^{(2)}} &= \\begin{bmatrix}\n           a_{0}^{(2)} \\\n           a_{1}^{(2)} \\\n           \\vdots \\\n           a_{m}^{(2)}\n         \\end{bmatrix}.\n  \\end{align}  Each layer  $(l)$  in a multi-layer perceptron, a directed graph, is fully connected to the next layer  $(l+1)$ . We write the weight coefficient that connects the  $k$ th unit in the  $l$ th layer to the  $j$ th unit in layer  $l+1$  as  $w^{(l)}_{j, k}$ .  For example, the weight coefficient that connects the units  $a_0^{(2)} \\rightarrow a_1^{(3)}$  would be written as  $w_{1,0}^{(2)}$ .",
            "title": "Basic Architecture"
        },
        {
            "location": "/user_guide/classifier/MultiLayerPerceptron/#activation",
            "text": "In the current implementation, the activations of the hidden layer(s) are computed via the logistic (sigmoid) function  $\\phi(z) = \\frac{1}{1 + e^{-z}}.$   (For more details on the logistic function, please see  classifier.LogisticRegression ; a general overview of different activation function can be found  here .)  Furthermore, the MLP uses the softmax function in the output layer, For more details on the logistic function, please see  classifier.SoftmaxRegression .",
            "title": "Activation"
        },
        {
            "location": "/user_guide/classifier/MultiLayerPerceptron/#references",
            "text": "D. R. G. H. R. Williams and G. Hinton.  Learning representations by back-propagating errors . Nature, pages 323\u2013533, 1986.  C. M. Bishop.  Neural networks for pattern recognition . Oxford University Press, 1995.  T. Hastie, J. Friedman, and R. Tibshirani.  The Elements of Statistical Learning , Volume 2. Springer, 2009.",
            "title": "References"
        },
        {
            "location": "/user_guide/classifier/MultiLayerPerceptron/#examples",
            "text": "",
            "title": "Examples"
        },
        {
            "location": "/user_guide/classifier/MultiLayerPerceptron/#example-1-classifying-iris-flowers",
            "text": "Load 2 features from Iris (petal length and petal width) for visualization purposes:  from mlxtend.data import iris_data\nX, y = iris_data()\nX = X[:, [0, 3]]    \n\n# standardize training data\nX_std = (X - X.mean(axis=0)) / X.std(axis=0)  Train neural network for 3 output flower classes ('Setosa', 'Versicolor', 'Virginica'), regular gradient decent ( minibatches=1 ), 30 hidden units, and no regularization.",
            "title": "Example 1 - Classifying Iris Flowers"
        },
        {
            "location": "/user_guide/classifier/MultiLayerPerceptron/#gradient-descent",
            "text": "Setting the  minibatches  to  1  will result in gradient descent training; please see  Gradient Descent vs. Stochastic Gradient Descent  for details.  from mlxtend.classifier import MultiLayerPerceptron as MLP\n\nnn1 = MLP(hidden_layers=[50], \n          l2=0.00, \n          l1=0.0, \n          epochs=150, \n          eta=0.05, \n          momentum=0.1,\n          decrease_const=0.0,\n          minibatches=1, \n          random_seed=1,\n          print_progress=3)\n\nnn1 = nn1.fit(X_std, y)  Iteration: 150/150 | Cost 0.06 | Elapsed: 0:00:00 | ETA: 0:00:00  from mlxtend.evaluate import plot_decision_regions\nimport matplotlib.pyplot as plt\n\nfig = plot_decision_regions(X=X_std, y=y, clf=nn1, legend=2)\nplt.title('Multi-layer Perceptron w. 1 hidden layer (logistic sigmoid)')\nplt.show()   import matplotlib.pyplot as plt\nplt.plot(range(len(nn1.cost_)), nn1.cost_)\nplt.ylabel('Cost')\nplt.xlabel('Epochs')\nplt.show()   print('Accuracy: %.2f%%' % (100 * nn1.score(X_std, y)))  Accuracy: 96.67%",
            "title": "Gradient Descent"
        },
        {
            "location": "/user_guide/classifier/MultiLayerPerceptron/#stochastic-gradient-descent",
            "text": "Setting  minibatches  to  n_samples  will result in stochastic gradient descent training; please see  Gradient Descent vs. Stochastic Gradient Descent  for details.  nn2 = MLP(hidden_layers=[50], \n          l2=0.00, \n          l1=0.0, \n          epochs=5, \n          eta=0.005, \n          momentum=0.1,\n          decrease_const=0.0,\n          minibatches=len(y), \n          random_seed=1,\n          print_progress=3)\n\nnn2.fit(X_std, y)\n\nplt.plot(range(len(nn2.cost_)), nn2.cost_)\nplt.ylabel('Cost')\nplt.xlabel('Epochs')\nplt.show()  Iteration: 5/5 | Cost 0.11 | Elapsed: 00:00:00 | ETA: 00:00:00   Continue the training for 25 epochs...  nn2.epochs = 25\nnn2 = nn2.fit(X_std, y)  Iteration: 25/25 | Cost 0.07 | Elapsed: 0:00:00 | ETA: 0:00:00  plt.plot(range(len(nn2.cost_)), nn2.cost_)\nplt.ylabel('Cost')\nplt.xlabel('Epochs')\nplt.show()",
            "title": "Stochastic Gradient Descent"
        },
        {
            "location": "/user_guide/classifier/MultiLayerPerceptron/#example-2-classifying-handwritten-digits-from-a-10-mnist-subset",
            "text": "Load a  5000-sample subset  of the  MNIST dataset  (please see  data.load_mnist  if you want to download and read in the complete MNIST dataset).  from mlxtend.data import mnist_data\nfrom mlxtend.preprocessing import shuffle_arrays_unison\n\nX, y = mnist_data()\nX, y = shuffle_arrays_unison((X, y), random_seed=1)\nX_train, y_train = X[:500], y[:500]\nX_test, y_test = X[500:], y[500:]  Visualize a sample from the MNIST dataset to check if it was loaded correctly:  import matplotlib.pyplot as plt\n\ndef plot_digit(X, y, idx):\n    img = X[idx].reshape(28,28)\n    plt.imshow(img, cmap='Greys',  interpolation='nearest')\n    plt.title('true label: %d' % y[idx])\n    plt.show()\n\nplot_digit(X, y, 3500)       Standardize pixel values:  import numpy as np\nfrom mlxtend.preprocessing import standardize\n\nX_train_std, params = standardize(X_train, \n                                  columns=range(X_train.shape[1]), \n                                  return_params=True)\n\nX_test_std = standardize(X_test,\n                         columns=range(X_test.shape[1]),\n                         params=params)  Initialize the neural network to recognize the 10 different digits (0-10) using 300 epochs and mini-batch learning.  nn1 = MLP(hidden_layers=[150], \n          l2=0.00, \n          l1=0.0, \n          epochs=100, \n          eta=0.005, \n          momentum=0.0,\n          decrease_const=0.0,\n          minibatches=100, \n          random_seed=1,\n          print_progress=3)  Learn the features while printing the progress to get an idea about how long it may take.  import matplotlib.pyplot as plt\n\nnn1.fit(X_train_std, y_train)\n\nplt.plot(range(len(nn1.cost_)), nn1.cost_)\nplt.ylabel('Cost')\nplt.xlabel('Epochs')\nplt.show()  Iteration: 100/100 | Cost 0.01 | Elapsed: 0:00:22 | ETA: 0:00:00   print('Train Accuracy: %.2f%%' % (100 * nn1.score(X_train_std, y_train)))\nprint('Test Accuracy: %.2f%%' % (100 * nn1.score(X_test_std, y_test)))  Train Accuracy: 100.00%\nTest Accuracy: 84.62%  Please note  that this neural network has been trained on only 10% of the MNIST data for technical demonstration purposes, hence, the lousy predictive performance.",
            "title": "Example 2 - Classifying Handwritten Digits from a 10% MNIST Subset"
        },
        {
            "location": "/user_guide/classifier/MultiLayerPerceptron/#api",
            "text": "NeuralNetMLP(n_output, n_features, n_hidden=30, l1=0.0, l2=0.0, epochs=500, eta=0.001, alpha=0.0, decrease_const=0.0, shuffle_init=True, shuffle_epoch=True, minibatches=1, zero_init_weight=False, random_seed=None, print_progress=0)  Feedforward neural network / Multi-layer perceptron classifier.  Parameters    n_output  : int  Number of output units, should be equal to the\nnumber of unique class labels.    n_features  : int  Number of features (dimensions) in the target dataset.\nShould be equal to the number of columns in the X array.    n_hidden  : int (default: 30)  Number of hidden units.    l1  : float (default: 0.0)  Lambda value for L1-regularization.\nNo regularization if l1=0.0 (default)    l2  : float (default: 0.0)  Lambda value for L2-regularization.\nNo regularization if l2=0.0 (default)    epochs  : int (default: 500)  Number of passes over the training set.    eta  : float (default: 0.001)  Learning rate.    alpha  : float (default: 0.0)  Momentum constant. Factor multiplied with the\ngradient of the previous epoch t-1 to improve\nlearning speed\nw(t) := w(t) - (grad(t) + alpha*grad(t-1))    decrease_const  : float (default: 0.0)  Decrease constant. Shrinks the learning rate\nafter each epoch via eta / (1 + epoch*decrease_const)    random_weights  : list (default: [-1.0, 1.0])  Min and max values for initializing the random weights.\nInitializes weights to 0 if None or False.    shuffle_init  : bool (default: True)  Shuffles (a copy of the) training data before training.    shuffle_epoch  : bool (default: True)  Shuffles training data before every epoch if True to prevent circles.    minibatches  : int (default: 1)  Divides training data into k minibatches for efficiency.\nNormal gradient descent learning if k=1 (default).    random_seed  : int (default: None)  Set random seed for shuffling and initializing the weights.    zero_init_weight  : bool (default: False)  If True, weights are initialized to zero instead of small random\nnumbers following a standard normal distribution with mean=0 and\nstddev=1.    print_progress  : int (default: 0)  Prints progress in fitting to stderr.\n0: No output\n1: Epochs elapsed and cost\n2: 1 plus time elapsed\n3: 2 plus estimated time until completion    Attributes    cost_  : list  Sum of squared errors after each epoch.",
            "title": "API"
        },
        {
            "location": "/user_guide/classifier/MultiLayerPerceptron/#methods",
            "text": "fit(X, y)  Learn weight coefficients from training data.  Parameters    X  : array, shape = [n_samples, n_features]  Input layer with original features.    y  : array, shape = [n_samples]  Target class labels.    Returns:  self   predict(X)  Predict class labels of X.  Parameters    X  : {array-like, sparse matrix}, shape = [n_samples, n_features]  Training vectors, where n_samples is the number of samples and\nn_features is the number of features.    Returns    class_labels  : array-like, shape = [n_samples]  Predicted class labels.     score(X, y)  Compute the prediction accuracy  Parameters    X  : {array-like, sparse matrix}, shape = [n_samples, n_features]  Training vectors, where n_samples is the number of samples and\nn_features is the number of features.    y  : array-like, shape = [n_samples]  Target values (true class labels).    Returns    acc  : float  The prediction accuracy as a float\nbetween 0.0 and 1.0 (perfect score).",
            "title": "Methods"
        },
        {
            "location": "/user_guide/tf_classifier/TfMultiLayerPerceptron/",
            "text": "TensorFlow Multi-Layer Perceptron\n\n\nA multi-layer perceptron class for binary and multi-class classification tasks.\n\n\n\n\nfrom mlxtend.tf_classifier import TfMultiLayerPerceptron\n\n\n\n\nOverview\n\n\n(A more detailed tutorial on multi-layer perceptrons is in preparation.)\n\n\n\n\n\n\nPlease also see the activation function cheatsheet at \ngeneral_concepts.activation-functions\n.\n\n\nReferences\n\n\n\n\nSrivastava, Nitish, et al. \n\"Dropout: A simple way to prevent neural networks from overfitting.\"\n The Journal of Machine Learning Research 15.1 (2014): 1929-1958.\n\n\n\n\nExamples\n\n\nExample 1 - Gradient Descent\n\n\nEach integer in the \nhidden_layers\n list argument specifies the number of neurons for the respective layer; via the \nactivations\n, we specify the activation functions for the individual hidden layer. Below, we initialize a multi-layer perceptron with 1 hidden layer using the logistic sigmoid activation. Furthermore, we train the network via simple gradient descent training by setting \noptimizer='gradientdescent'\n and \nminibatches=1\n. \n\n\nfrom mlxtend.tf_classifier import TfMultiLayerPerceptron\nfrom mlxtend.data import iris_data\nfrom mlxtend.evaluate import plot_decision_regions\nimport matplotlib.pyplot as plt\n\n# Loading Data\nX, y = iris_data()\nX = X[:, [0, 3]] # sepal length and petal width\n\n# standardize\nX[:,0] = (X[:,0] - X[:,0].mean()) / X[:,0].std()\nX[:,1] = (X[:,1] - X[:,1].mean()) / X[:,1].std()\n\nmlp = TfMultiLayerPerceptron(eta=0.5, \n                             epochs=20, \n                             hidden_layers=[10],\n                             activations=['logistic'],\n                             optimizer='gradientdescent',\n                             print_progress=3, \n                             minibatches=1, \n                             random_seed=1)\n\nmlp.fit(X, y)\n\nplt.plot(range(len(mlp.cost_)), mlp.cost_)\nplt.xlabel('Iterations')\nplt.ylabel('Cost')\nplt.show()\n\n\n\n\nIteration: 20/20 | Cost 0.55 | Elapsed: 0:00:00 | ETA: 0:00:00\n\n\n\n\n\nContinue the training if cost could be further decreased via additional epochs. Instead of training the classifier another 20 epochs, we modify the epochs and set them to 550. Also, we want to make sure to set \ninit_weights\n to \nFalse\n in order to re-use the model parameters from the previous training.\n\n\nmlp.epochs = 550\nmlp.fit(X, y, init_params=False)\nplt.plot(range(len(mlp.cost_)), mlp.cost_)\nplt.xlabel('Iterations')\nplt.ylabel('Cost')\nplt.show()\n\n\n\n\nIteration: 550/550 | Cost 0.12 | Elapsed: 0:00:02 | ETA: 0:00:00\n\n\n\n\n\nplot_decision_regions(X=X, y=y, clf=mlp)\nplt.title('Logistic Sigmoid MLP - Gradient Descent')\nplt.show()\n\n\n\n\n\n\nPredicting Class Labels\n\n\nprint('Predicted class labels:', mlp.predict(X[[0, 99, 149]]))\n\n\n\n\nPredicted class labels: [0 1 2]\n\n\n\nPredicting Class Probabilities\n\n\nprint('Predicted class probabilities:\\n', mlp.predict_proba(X[[0, 99, 149]]))\n\n\n\n\nPredicted class probabilities:\n [[  9.92696404e-01   7.30253849e-03   1.04788933e-06]\n [  1.58569030e-03   9.77745533e-01   2.06688400e-02]\n [  5.52368192e-06   1.68280497e-01   8.31713974e-01]]\n\n\n\nExample 2 - Stochastic Gradient Descent\n\n\nStochastic gradient descent training sample by sample can be achieved by setting the number of minibatches equal to the number of samples in the training dataset; everything between \nminibatches=1\n and \nminibatches=len(y)\n is \"minibatch\" stochastic gradient descent. Below, we train a network using 10 minibatches.\n\n\nfrom mlxtend.data import iris_data\nfrom mlxtend.evaluate import plot_decision_regions\nfrom mlxtend.tf_classifier import TfMultiLayerPerceptron\nimport matplotlib.pyplot as plt\n\n# Loading Data\n\nX, y = iris_data()\nX = X[:, [0, 3]] # sepal length and petal width\n\n# standardize\nX[:,0] = (X[:,0] - X[:,0].mean()) / X[:,0].std()\nX[:,1] = (X[:,1] - X[:,1].mean()) / X[:,1].std()\n\nmlp = TfMultiLayerPerceptron(eta=0.5, \n                             epochs=100, \n                             hidden_layers=[10],\n                             activations=['logistic'],\n                             print_progress=3, \n                             optimizer='gradientdescent',\n                             minibatches=10, \n                             random_seed=1)\n\nmlp.fit(X, y)\n\nplt.plot(range(len(mlp.cost_)), mlp.cost_)\nplt.xlabel('Iterations')\nplt.ylabel('Cost')\nplt.show()\n\nplot_decision_regions(X, y, clf=mlp)\nplt.title('MLP - Minibatch Stochastic Gradient Descent')\nplt.show()\n\n\n\n\nIteration: 100/100 | Cost 0.13 | Elapsed: 0:00:03 | ETA: 0:00:00\n\n\n\n\n\n\n\nExample 3 - MNIST\n\n\nPlease note that \nmnist_data\n just contains a random 5000-sample subset of MNIST (~10% of the original dataset size) suitable for demonstration purposes regarding computational efficiency.\n\n\nAlthough it may be overkill, and the network may terribly overfit the training data, let us initialize a more complex neural network with 2 hidden layers and 200 ReLU (Rectifier Linear Units) each.\n\n\nfrom mlxtend.data import mnist_data\nfrom mlxtend.tf_classifier import TfMultiLayerPerceptron\nimport matplotlib.pyplot as plt\n\nX, y = mnist_data()\n\nmlp = TfMultiLayerPerceptron(eta=0.01, \n                             epochs=30, \n                             hidden_layers=[200, 200],\n                             activations=['relu', 'relu'],\n                             print_progress=3, \n                             minibatches=5, \n                             optimizer='adam',\n                             random_seed=1)\n\nmlp.fit(X, y)\n\nplt.plot(range(len(mlp.cost_)), mlp.cost_)\nplt.xlabel('Iterations')\nplt.ylabel('Cost')\nplt.show()\n\n\n\n\nIteration: 30/30 | Cost 40.04 | Elapsed: 0:00:14 | ETA: 0:00:00\n\n\n\n\n\nimport numpy as np\ny_pred = mlp.predict(X)\nprint('Training Accuracy: %.2f%%' % (mlp.score(X, y) * 100))\n\n\n\n\nTraining Accuracy: 99.76%\n\n\n\ndef plot_digit(X, y, idx):\n    img = X[idx].reshape(28,28)\n    plt.imshow(img, cmap='Greys',  interpolation='nearest')\n    plt.title('true label: %d' % y[idx])\n    plt.show()\nplot_digit(X, y, 4999)  \n\n\n\n\n\n\nprint('Prediction: %d' % mlp.predict(X[4999, None]))\n\n\n\n\nPrediction: 9\n\n\n\nAPI\n\n\nTfMultiLayerPerceptron(eta=0.5, epochs=50, hidden_layers=[50, 10], n_classes=None, activations=['logistic', 'logistic'], optimizer='gradientdescent', momentum=0.0, l1=0.0, l2=0.0, dropout=1.0, decay=[0.0, 1.0], minibatches=1, random_seed=None, print_progress=0, dtype=None)\n\n\nMulti-layer perceptron classifier.\n\n\nParameters\n\n\n\n\n\n\neta\n : float (default: 0.5)\n\n\nLearning rate (between 0.0 and 1.0)\n\n\n\n\n\n\nepochs\n : int (default: 50)\n\n\nPasses over the training dataset.\nPrior to each epoch, the dataset is shuffled\nif \nminibatches > 1\n to prevent cycles in stochastic gradient descent.\n\n\n\n\n\n\nhidden_layers\n : list (default: [50, 10])\n\n\nNumber of units per hidden layer. By default 50 units in the\nfirst hidden layer, and 10 hidden units in the second hidden layer.\n\n\n\n\n\n\nn_classes\n : int (default: None)\n\n\nA positive integer to declare the number of class labels\nif not all class labels are present in a partial training set.\nGets the number of class labels automatically if None.\n\n\n\n\n\n\nactivations\n : list (default: ['logistic', 'logistic'])\n\n\nActivation functions for each layer.\nAvailable actiavtion functions:\n\"logistic\", \"relu\", \"tanh\", \"relu6\", \"elu\", \"softplus\", \"softsign\"\n\n\n\n\n\n\noptimizer\n : str (default: \"gradientdescent\")\n\n\nOptimizer to minimize the cost function:\n\"gradientdescent\", \"momentum\", \"adam\", \"ftrl\", \"adagrad\"\n\n\n\n\n\n\nmomentum\n : float (default: 0.0)\n\n\nMomentum constant for momentum learning; only applies if\noptimizer='momentum'\n\n\n\n\n\n\nl1\n : float (default: 0.0)\n\n\nL1 regularization strength; only applies if optimizer='ftrl'\n\n\n\n\n\n\nl2\n : float (default: 0.0)\n\n\nregularization strength; only applies if optimizer='ftrl'\n\n\n\n\n\n\ndropout\n : float (default: 1.0)\n\n\nA float between in the range (0.0, 1.0] to specify\nthe probability that each element is kept.\n\n\n\n\n\n\ndecay\n : list, shape=[decay_rate, decay_steps] (default: [0.0, 1])\n\n\nParameter to specify the exponential decay of the learning rate eta\nfor adaptive learning (eta * decay_rate ^ (epoch / decay_steps)).\n\n\n\n\n\n\nminibatches\n : int (default: 1)\n\n\nDivide the training data into \nk\n minibatches\nfor accelerated stochastic gradient descent learning.\nGradient Descent Learning if \nminibatches\n = 1\nStochastic Gradient Descent learning if \nminibatches\n = len(y)\nMinibatch learning if \nminibatches\n > 1\n\n\n\n\n\n\nrandom_seed\n : int (default: None)\n\n\nSet random state for shuffling and initializing the weights.\n\n\n\n\n\n\nprint_progress\n : int (default: 0)\n\n\nPrints progress in fitting to stderr.\n0: No output\n1: Epochs elapsed and cost\n2: 1 plus time elapsed\n3: 2 plus estimated time until completion\n\n\n\n\n\n\ndtype\n : Array-type (default: None)\n\n\nUses tensorflow.float32 if None.\n\n\n\n\n\n\nAttributes\n\n\n\n\n\n\nw_\n : 2d-array, shape=[n_features, n_classes]\n\n\nWeights after fitting.\n\n\n\n\n\n\nb_\n : 1D-array, shape=[n_classes]\n\n\nBias units after fitting.\n\n\n\n\n\n\ncost_\n : list\n\n\nList of floats, the average cross_entropy for each epoch.\n\n\n\n\n\n\nMethods\n\n\n\n\n\nfit(X, y, init_params=True)\n\n\nLearn model from training data.\n\n\nParameters\n\n\n\n\n\n\nX\n : {array-like, sparse matrix}, shape = [n_samples, n_features]\n\n\nTraining vectors, where n_samples is the number of samples and\nn_features is the number of features.\n\n\n\n\n\n\ny\n : array-like, shape = [n_samples]\n\n\nTarget values.\n\n\n\n\n\n\ninit_params\n : bool (default: True)\n\n\nRe-initializes model parameters prior to fitting.\nSet False to continue training with weights from\na previous model fitting.\n\n\n\n\n\n\nReturns\n\n\n\n\nself\n : object\n\n\n\n\n\n\n\npredict(X)\n\n\nPredict class labels of X.\n\n\nParameters\n\n\n\n\n\n\nX\n : {array-like, sparse matrix}, shape = [n_samples, n_features]\n\n\nTraining vectors, where n_samples is the number of samples and\nn_features is the number of features.\n\n\n\n\n\n\nReturns\n\n\n\n\n\n\nclass_labels\n : array-like, shape = [n_samples]\n\n\nPredicted class labels.\n\n\n\n\n\n\n\n\n\npredict_proba(X)\n\n\nPredict class probabilities of X from the net input.\n\n\nParameters\n\n\n\n\n\n\nX\n : {array-like, sparse matrix}, shape = [n_samples, n_features]\n\n\nTraining vectors, where n_samples is the number of samples and\nn_features is the number of features.\n\n\n\n\n\n\nReturns\n\n\n\n\nClass probabilties\n : array-like, shape= [n_samples, n_classes]\n\n\n\n\n\n\n\nscore(X, y)\n\n\nCompute the prediction accuracy\n\n\nParameters\n\n\n\n\n\n\nX\n : {array-like, sparse matrix}, shape = [n_samples, n_features]\n\n\nTraining vectors, where n_samples is the number of samples and\nn_features is the number of features.\n\n\n\n\n\n\ny\n : array-like, shape = [n_samples]\n\n\nTarget values (true class labels).\n\n\n\n\n\n\nReturns\n\n\n\n\n\n\nacc\n : float\n\n\nThe prediction accuracy as a float\nbetween 0.0 and 1.0 (perfect score).",
            "title": "TfMultiLayerPerceptron"
        },
        {
            "location": "/user_guide/tf_classifier/TfMultiLayerPerceptron/#tensorflow-multi-layer-perceptron",
            "text": "A multi-layer perceptron class for binary and multi-class classification tasks.   from mlxtend.tf_classifier import TfMultiLayerPerceptron",
            "title": "TensorFlow Multi-Layer Perceptron"
        },
        {
            "location": "/user_guide/tf_classifier/TfMultiLayerPerceptron/#overview",
            "text": "(A more detailed tutorial on multi-layer perceptrons is in preparation.)    Please also see the activation function cheatsheet at  general_concepts.activation-functions .",
            "title": "Overview"
        },
        {
            "location": "/user_guide/tf_classifier/TfMultiLayerPerceptron/#references",
            "text": "Srivastava, Nitish, et al.  \"Dropout: A simple way to prevent neural networks from overfitting.\"  The Journal of Machine Learning Research 15.1 (2014): 1929-1958.",
            "title": "References"
        },
        {
            "location": "/user_guide/tf_classifier/TfMultiLayerPerceptron/#examples",
            "text": "",
            "title": "Examples"
        },
        {
            "location": "/user_guide/tf_classifier/TfMultiLayerPerceptron/#example-1-gradient-descent",
            "text": "Each integer in the  hidden_layers  list argument specifies the number of neurons for the respective layer; via the  activations , we specify the activation functions for the individual hidden layer. Below, we initialize a multi-layer perceptron with 1 hidden layer using the logistic sigmoid activation. Furthermore, we train the network via simple gradient descent training by setting  optimizer='gradientdescent'  and  minibatches=1 .   from mlxtend.tf_classifier import TfMultiLayerPerceptron\nfrom mlxtend.data import iris_data\nfrom mlxtend.evaluate import plot_decision_regions\nimport matplotlib.pyplot as plt\n\n# Loading Data\nX, y = iris_data()\nX = X[:, [0, 3]] # sepal length and petal width\n\n# standardize\nX[:,0] = (X[:,0] - X[:,0].mean()) / X[:,0].std()\nX[:,1] = (X[:,1] - X[:,1].mean()) / X[:,1].std()\n\nmlp = TfMultiLayerPerceptron(eta=0.5, \n                             epochs=20, \n                             hidden_layers=[10],\n                             activations=['logistic'],\n                             optimizer='gradientdescent',\n                             print_progress=3, \n                             minibatches=1, \n                             random_seed=1)\n\nmlp.fit(X, y)\n\nplt.plot(range(len(mlp.cost_)), mlp.cost_)\nplt.xlabel('Iterations')\nplt.ylabel('Cost')\nplt.show()  Iteration: 20/20 | Cost 0.55 | Elapsed: 0:00:00 | ETA: 0:00:00   Continue the training if cost could be further decreased via additional epochs. Instead of training the classifier another 20 epochs, we modify the epochs and set them to 550. Also, we want to make sure to set  init_weights  to  False  in order to re-use the model parameters from the previous training.  mlp.epochs = 550\nmlp.fit(X, y, init_params=False)\nplt.plot(range(len(mlp.cost_)), mlp.cost_)\nplt.xlabel('Iterations')\nplt.ylabel('Cost')\nplt.show()  Iteration: 550/550 | Cost 0.12 | Elapsed: 0:00:02 | ETA: 0:00:00   plot_decision_regions(X=X, y=y, clf=mlp)\nplt.title('Logistic Sigmoid MLP - Gradient Descent')\nplt.show()",
            "title": "Example 1 - Gradient Descent"
        },
        {
            "location": "/user_guide/tf_classifier/TfMultiLayerPerceptron/#predicting-class-labels",
            "text": "print('Predicted class labels:', mlp.predict(X[[0, 99, 149]]))  Predicted class labels: [0 1 2]",
            "title": "Predicting Class Labels"
        },
        {
            "location": "/user_guide/tf_classifier/TfMultiLayerPerceptron/#predicting-class-probabilities",
            "text": "print('Predicted class probabilities:\\n', mlp.predict_proba(X[[0, 99, 149]]))  Predicted class probabilities:\n [[  9.92696404e-01   7.30253849e-03   1.04788933e-06]\n [  1.58569030e-03   9.77745533e-01   2.06688400e-02]\n [  5.52368192e-06   1.68280497e-01   8.31713974e-01]]",
            "title": "Predicting Class Probabilities"
        },
        {
            "location": "/user_guide/tf_classifier/TfMultiLayerPerceptron/#example-2-stochastic-gradient-descent",
            "text": "Stochastic gradient descent training sample by sample can be achieved by setting the number of minibatches equal to the number of samples in the training dataset; everything between  minibatches=1  and  minibatches=len(y)  is \"minibatch\" stochastic gradient descent. Below, we train a network using 10 minibatches.  from mlxtend.data import iris_data\nfrom mlxtend.evaluate import plot_decision_regions\nfrom mlxtend.tf_classifier import TfMultiLayerPerceptron\nimport matplotlib.pyplot as plt\n\n# Loading Data\n\nX, y = iris_data()\nX = X[:, [0, 3]] # sepal length and petal width\n\n# standardize\nX[:,0] = (X[:,0] - X[:,0].mean()) / X[:,0].std()\nX[:,1] = (X[:,1] - X[:,1].mean()) / X[:,1].std()\n\nmlp = TfMultiLayerPerceptron(eta=0.5, \n                             epochs=100, \n                             hidden_layers=[10],\n                             activations=['logistic'],\n                             print_progress=3, \n                             optimizer='gradientdescent',\n                             minibatches=10, \n                             random_seed=1)\n\nmlp.fit(X, y)\n\nplt.plot(range(len(mlp.cost_)), mlp.cost_)\nplt.xlabel('Iterations')\nplt.ylabel('Cost')\nplt.show()\n\nplot_decision_regions(X, y, clf=mlp)\nplt.title('MLP - Minibatch Stochastic Gradient Descent')\nplt.show()  Iteration: 100/100 | Cost 0.13 | Elapsed: 0:00:03 | ETA: 0:00:00",
            "title": "Example 2 - Stochastic Gradient Descent"
        },
        {
            "location": "/user_guide/tf_classifier/TfMultiLayerPerceptron/#example-3-mnist",
            "text": "Please note that  mnist_data  just contains a random 5000-sample subset of MNIST (~10% of the original dataset size) suitable for demonstration purposes regarding computational efficiency.  Although it may be overkill, and the network may terribly overfit the training data, let us initialize a more complex neural network with 2 hidden layers and 200 ReLU (Rectifier Linear Units) each.  from mlxtend.data import mnist_data\nfrom mlxtend.tf_classifier import TfMultiLayerPerceptron\nimport matplotlib.pyplot as plt\n\nX, y = mnist_data()\n\nmlp = TfMultiLayerPerceptron(eta=0.01, \n                             epochs=30, \n                             hidden_layers=[200, 200],\n                             activations=['relu', 'relu'],\n                             print_progress=3, \n                             minibatches=5, \n                             optimizer='adam',\n                             random_seed=1)\n\nmlp.fit(X, y)\n\nplt.plot(range(len(mlp.cost_)), mlp.cost_)\nplt.xlabel('Iterations')\nplt.ylabel('Cost')\nplt.show()  Iteration: 30/30 | Cost 40.04 | Elapsed: 0:00:14 | ETA: 0:00:00   import numpy as np\ny_pred = mlp.predict(X)\nprint('Training Accuracy: %.2f%%' % (mlp.score(X, y) * 100))  Training Accuracy: 99.76%  def plot_digit(X, y, idx):\n    img = X[idx].reshape(28,28)\n    plt.imshow(img, cmap='Greys',  interpolation='nearest')\n    plt.title('true label: %d' % y[idx])\n    plt.show()\nplot_digit(X, y, 4999)     print('Prediction: %d' % mlp.predict(X[4999, None]))  Prediction: 9",
            "title": "Example 3 - MNIST"
        },
        {
            "location": "/user_guide/tf_classifier/TfMultiLayerPerceptron/#api",
            "text": "TfMultiLayerPerceptron(eta=0.5, epochs=50, hidden_layers=[50, 10], n_classes=None, activations=['logistic', 'logistic'], optimizer='gradientdescent', momentum=0.0, l1=0.0, l2=0.0, dropout=1.0, decay=[0.0, 1.0], minibatches=1, random_seed=None, print_progress=0, dtype=None)  Multi-layer perceptron classifier.  Parameters    eta  : float (default: 0.5)  Learning rate (between 0.0 and 1.0)    epochs  : int (default: 50)  Passes over the training dataset.\nPrior to each epoch, the dataset is shuffled\nif  minibatches > 1  to prevent cycles in stochastic gradient descent.    hidden_layers  : list (default: [50, 10])  Number of units per hidden layer. By default 50 units in the\nfirst hidden layer, and 10 hidden units in the second hidden layer.    n_classes  : int (default: None)  A positive integer to declare the number of class labels\nif not all class labels are present in a partial training set.\nGets the number of class labels automatically if None.    activations  : list (default: ['logistic', 'logistic'])  Activation functions for each layer.\nAvailable actiavtion functions:\n\"logistic\", \"relu\", \"tanh\", \"relu6\", \"elu\", \"softplus\", \"softsign\"    optimizer  : str (default: \"gradientdescent\")  Optimizer to minimize the cost function:\n\"gradientdescent\", \"momentum\", \"adam\", \"ftrl\", \"adagrad\"    momentum  : float (default: 0.0)  Momentum constant for momentum learning; only applies if\noptimizer='momentum'    l1  : float (default: 0.0)  L1 regularization strength; only applies if optimizer='ftrl'    l2  : float (default: 0.0)  regularization strength; only applies if optimizer='ftrl'    dropout  : float (default: 1.0)  A float between in the range (0.0, 1.0] to specify\nthe probability that each element is kept.    decay  : list, shape=[decay_rate, decay_steps] (default: [0.0, 1])  Parameter to specify the exponential decay of the learning rate eta\nfor adaptive learning (eta * decay_rate ^ (epoch / decay_steps)).    minibatches  : int (default: 1)  Divide the training data into  k  minibatches\nfor accelerated stochastic gradient descent learning.\nGradient Descent Learning if  minibatches  = 1\nStochastic Gradient Descent learning if  minibatches  = len(y)\nMinibatch learning if  minibatches  > 1    random_seed  : int (default: None)  Set random state for shuffling and initializing the weights.    print_progress  : int (default: 0)  Prints progress in fitting to stderr.\n0: No output\n1: Epochs elapsed and cost\n2: 1 plus time elapsed\n3: 2 plus estimated time until completion    dtype  : Array-type (default: None)  Uses tensorflow.float32 if None.    Attributes    w_  : 2d-array, shape=[n_features, n_classes]  Weights after fitting.    b_  : 1D-array, shape=[n_classes]  Bias units after fitting.    cost_  : list  List of floats, the average cross_entropy for each epoch.",
            "title": "API"
        },
        {
            "location": "/user_guide/tf_classifier/TfMultiLayerPerceptron/#methods",
            "text": "fit(X, y, init_params=True)  Learn model from training data.  Parameters    X  : {array-like, sparse matrix}, shape = [n_samples, n_features]  Training vectors, where n_samples is the number of samples and\nn_features is the number of features.    y  : array-like, shape = [n_samples]  Target values.    init_params  : bool (default: True)  Re-initializes model parameters prior to fitting.\nSet False to continue training with weights from\na previous model fitting.    Returns   self  : object    predict(X)  Predict class labels of X.  Parameters    X  : {array-like, sparse matrix}, shape = [n_samples, n_features]  Training vectors, where n_samples is the number of samples and\nn_features is the number of features.    Returns    class_labels  : array-like, shape = [n_samples]  Predicted class labels.     predict_proba(X)  Predict class probabilities of X from the net input.  Parameters    X  : {array-like, sparse matrix}, shape = [n_samples, n_features]  Training vectors, where n_samples is the number of samples and\nn_features is the number of features.    Returns   Class probabilties  : array-like, shape= [n_samples, n_classes]    score(X, y)  Compute the prediction accuracy  Parameters    X  : {array-like, sparse matrix}, shape = [n_samples, n_features]  Training vectors, where n_samples is the number of samples and\nn_features is the number of features.    y  : array-like, shape = [n_samples]  Target values (true class labels).    Returns    acc  : float  The prediction accuracy as a float\nbetween 0.0 and 1.0 (perfect score).",
            "title": "Methods"
        },
        {
            "location": "/user_guide/tf_classifier/TfSoftmaxRegression/",
            "text": "TensorFlow Softmax Regression\n\n\nA multinomial logistic (aka \"softmax\") regression class for multi-class classification tasks.\n\n\n\n\nfrom mlxtend.tf_classifier import TfSoftmaxRegression\n\n\n\n\nOverview\n\n\n\n\nPlease refer to the documentation of the equivalent NumPy implementation \nclassifier.SoftmaxRegression\n for details.\n\n\nExamples\n\n\nExample 1 - Gradient Descent\n\n\nSimple gradient descent training via \nminibatches=1\n:\n\n\nfrom mlxtend.tf_classifier import TfSoftmaxRegression\nfrom mlxtend.data import iris_data\nfrom mlxtend.evaluate import plot_decision_regions\nimport matplotlib.pyplot as plt\n\n# Loading Data\nX, y = iris_data()\nX = X[:, [0, 3]] # sepal length and petal width\n\n# standardize\nX[:,0] = (X[:,0] - X[:,0].mean()) / X[:,0].std()\nX[:,1] = (X[:,1] - X[:,1].mean()) / X[:,1].std()\n\nlr = TfSoftmaxRegression(eta=0.75, \n                         epochs=20, \n                         print_progress=True, \n                         minibatches=1, \n                         random_seed=1)\n\nlr.fit(X, y)\n\nplt.plot(range(len(lr.cost_)), lr.cost_)\nplt.xlabel('Iterations')\nplt.ylabel('Cost')\nplt.show()\n\n\n\n\nIteration: 20/20 | Cost 0.37\n\n\n\n\n\nContinue the training if cost could be further decreased via additional epochs. Instead of training the classifier another 20 epochs, we modify the epochs and set them to 500. Also, we want to make sure to set \ninit_weights\n to \nFalse\n in order to re-use the model parameters from the previous training.\n\n\nlr.epochs = 500\nlr.fit(X, y, init_params=False)\nplt.plot(range(len(lr.cost_)), lr.cost_)\nplt.xlabel('Iterations')\nplt.ylabel('Cost')\nplt.show()\n\n\n\n\nIteration: 500/500 | Cost 0.13\n\n\n\n\n\nplot_decision_regions(X, y, clf=lr)\nplt.title('Softmax Regression - Gradient Descent')\nplt.show()\n\n\n\n\n\n\nPredicting Class Labels\n\n\nprint('Predicted class labels:', lr.predict(X[[0, 99, 149]]))\n\n\n\n\nPredicted class labels: [0 1 2]\n\n\n\nPredicting Class Probabilities\n\n\nprint('Predicted class probabilities:\\n', lr.predict_proba(X[[0, 99, 149]]))\n\n\n\n\nPredicted class probabilities:\n [[  9.92753923e-01   7.24608498e-03   6.90939350e-09]\n [  5.35254739e-03   9.61461246e-01   3.31862085e-02]\n [  1.28487918e-05   2.01715842e-01   7.98271239e-01]]\n\n\n\nExample 2 - Stochastic Gradient Descent\n\n\nStochastic gradient descent training sample by sample can be achieved by setting the number of minibatches equal to the number of samples in the training dataset; everything between \nminibatches=1\n and \nminibatches=len(y)\n is \"minibatch\" stochastic gradient descent.\n\n\nfrom mlxtend.data import iris_data\nfrom mlxtend.evaluate import plot_decision_regions\nfrom mlxtend.tf_classifier import TfSoftmaxRegression\nimport matplotlib.pyplot as plt\n\n# Loading Data\n\nX, y = iris_data()\nX = X[:, [0, 3]] # sepal length and petal width\n\n# standardize\nX[:,0] = (X[:,0] - X[:,0].mean()) / X[:,0].std()\nX[:,1] = (X[:,1] - X[:,1].mean()) / X[:,1].std()\n\nlr = TfSoftmaxRegression(eta=0.5, \n                         epochs=10, \n                         minibatches=len(y), \n                         print_progress=True, \n                         random_seed=1)\nlr.fit(X, y)\n\nplt.plot(range(len(lr.cost_)), lr.cost_)\nplt.xlabel('Iterations')\nplt.ylabel('Cost')\nplt.show()\n\nplot_decision_regions(X, y, clf=lr)\nplt.title('Softmax Regression - Gradient Descent')\nplt.show()\n\n\n\n\nIteration: 10/10 | Cost 0.15\n\n\n\n\n\n\n\nExample 3 - MNIST\n\n\nNote that \nmnist_data\n just contains a random 5000-sample subset of MNIST (~10% of the original dataset size).\n\n\nfrom mlxtend.data import mnist_data\nfrom mlxtend.tf_classifier import TfSoftmaxRegression\nimport matplotlib.pyplot as plt\n\nX, y = mnist_data()\n\nlr = TfSoftmaxRegression(eta=0.01, \n                         epochs=100, \n                         minibatches=20, \n                         print_progress=True, \n                         random_seed=1)\nlr.fit(X, y)\n\nplt.plot(range(len(lr.cost_)), lr.cost_)\nplt.xlabel('Iterations')\nplt.ylabel('Cost')\nplt.show()\n\n\n\n\nIteration: 100/100 | Cost 6.26\n\n\n\n\n\nimport numpy as np\ny_pred = lr.predict(X)\nprint('Training Accuracy: %.2f%%' % (lr.score(X, y) * 100))\n\n\n\n\nTraining Accuracy: 93.86%\n\n\n\ndef plot_digit(X, y, idx):\n    img = X[idx].reshape(28,28)\n    plt.imshow(img, cmap='Greys',  interpolation='nearest')\n    plt.title('true label: %d' % y[idx])\n    plt.show()\nplot_digit(X, y, 0)  \n\n\n\n\n\n\nprint('Prediction: %d' % lr.predict(X[0, None]))\n\n\n\n\nPrediction: 0\n\n\n\nAPI\n\n\nTfSoftmaxRegression(eta=0.5, epochs=50, n_classes=None, minibatches=1, random_seed=None, print_progress=0, dtype=None)\n\n\nSoftmax regression classifier.\n\n\nParameters\n\n\n\n\n\n\neta\n : float (default: 0.5)\n\n\nLearning rate (between 0.0 and 1.0)\n\n\n\n\n\n\nepochs\n : int (default: 50)\n\n\nPasses over the training dataset.\nPrior to each epoch, the dataset is shuffled\nif \nminibatches > 1\n to prevent cycles in stochastic gradient descent.\n\n\n\n\n\n\nn_classes\n : int (default: None)\n\n\nA positive integer to declare the number of class labels\nif not all class labels are present in a partial training set.\nGets the number of class labels automatically if None.\n\n\n\n\n\n\nminibatches\n : int (default: 1)\n\n\nDivide the training data into \nk\n minibatches\nfor accelerated stochastic gradient descent learning.\nGradient Descent Learning if \nminibatches\n = 1\nStochastic Gradient Descent learning if \nminibatches\n = len(y)\nMinibatch learning if \nminibatches\n > 1\n\n\n\n\n\n\nrandom_seed\n : int (default: None)\n\n\nSet random state for shuffling and initializing the weights.\n\n\n\n\n\n\nprint_progress\n : int (default: 0)\n\n\nPrints progress in fitting to stderr.\n0: No output\n1: Epochs elapsed and cost\n2: 1 plus time elapsed\n3: 2 plus estimated time until completion\n\n\n\n\n\n\ndtype\n : Array-type (default: None)\n\n\nUses tensorflow.float32 if None.\n\n\n\n\n\n\nAttributes\n\n\n\n\n\n\nw_\n : 2d-array, shape=[n_features, n_classes]\n\n\nWeights after fitting.\n\n\n\n\n\n\nb_\n : 1D-array, shape=[n_classes]\n\n\nBias units after fitting.\n\n\n\n\n\n\ncost_\n : list\n\n\nList of floats, the average cross_entropy for each epoch.\n\n\n\n\n\n\nMethods\n\n\n\n\n\nfit(X, y, init_params=True)\n\n\nLearn model from training data.\n\n\nParameters\n\n\n\n\n\n\nX\n : {array-like, sparse matrix}, shape = [n_samples, n_features]\n\n\nTraining vectors, where n_samples is the number of samples and\nn_features is the number of features.\n\n\n\n\n\n\ny\n : array-like, shape = [n_samples]\n\n\nTarget values.\n\n\n\n\n\n\ninit_params\n : bool (default: True)\n\n\nRe-initializes model parameters prior to fitting.\nSet False to continue training with weights from\na previous model fitting.\n\n\n\n\n\n\nReturns\n\n\n\n\nself\n : object\n\n\n\n\n\n\n\npredict(X)\n\n\nPredict targets from X.\n\n\nParameters\n\n\n\n\n\n\nX\n : {array-like, sparse matrix}, shape = [n_samples, n_features]\n\n\nTraining vectors, where n_samples is the number of samples and\nn_features is the number of features.\n\n\n\n\n\n\nReturns\n\n\n\n\n\n\ntarget_values\n : array-like, shape = [n_samples]\n\n\nPredicted target values.\n\n\n\n\n\n\n\n\n\npredict_proba(X)\n\n\nPredict class probabilities of X from the net input.\n\n\nParameters\n\n\n\n\n\n\nX\n : {array-like, sparse matrix}, shape = [n_samples, n_features]\n\n\nTraining vectors, where n_samples is the number of samples and\nn_features is the number of features.\n\n\n\n\n\n\nReturns\n\n\n\n\nClass probabilties\n : array-like, shape= [n_samples, n_classes]\n\n\n\n\n\n\n\nscore(X, y)\n\n\nCompute the prediction accuracy\n\n\nParameters\n\n\n\n\n\n\nX\n : {array-like, sparse matrix}, shape = [n_samples, n_features]\n\n\nTraining vectors, where n_samples is the number of samples and\nn_features is the number of features.\n\n\n\n\n\n\ny\n : array-like, shape = [n_samples]\n\n\nTarget values (true class labels).\n\n\n\n\n\n\nReturns\n\n\n\n\n\n\nacc\n : float\n\n\nThe prediction accuracy as a float\nbetween 0.0 and 1.0 (perfect score).",
            "title": "TfSoftmaxRegression"
        },
        {
            "location": "/user_guide/tf_classifier/TfSoftmaxRegression/#tensorflow-softmax-regression",
            "text": "A multinomial logistic (aka \"softmax\") regression class for multi-class classification tasks.   from mlxtend.tf_classifier import TfSoftmaxRegression",
            "title": "TensorFlow Softmax Regression"
        },
        {
            "location": "/user_guide/tf_classifier/TfSoftmaxRegression/#overview",
            "text": "Please refer to the documentation of the equivalent NumPy implementation  classifier.SoftmaxRegression  for details.",
            "title": "Overview"
        },
        {
            "location": "/user_guide/tf_classifier/TfSoftmaxRegression/#examples",
            "text": "",
            "title": "Examples"
        },
        {
            "location": "/user_guide/tf_classifier/TfSoftmaxRegression/#example-1-gradient-descent",
            "text": "Simple gradient descent training via  minibatches=1 :  from mlxtend.tf_classifier import TfSoftmaxRegression\nfrom mlxtend.data import iris_data\nfrom mlxtend.evaluate import plot_decision_regions\nimport matplotlib.pyplot as plt\n\n# Loading Data\nX, y = iris_data()\nX = X[:, [0, 3]] # sepal length and petal width\n\n# standardize\nX[:,0] = (X[:,0] - X[:,0].mean()) / X[:,0].std()\nX[:,1] = (X[:,1] - X[:,1].mean()) / X[:,1].std()\n\nlr = TfSoftmaxRegression(eta=0.75, \n                         epochs=20, \n                         print_progress=True, \n                         minibatches=1, \n                         random_seed=1)\n\nlr.fit(X, y)\n\nplt.plot(range(len(lr.cost_)), lr.cost_)\nplt.xlabel('Iterations')\nplt.ylabel('Cost')\nplt.show()  Iteration: 20/20 | Cost 0.37   Continue the training if cost could be further decreased via additional epochs. Instead of training the classifier another 20 epochs, we modify the epochs and set them to 500. Also, we want to make sure to set  init_weights  to  False  in order to re-use the model parameters from the previous training.  lr.epochs = 500\nlr.fit(X, y, init_params=False)\nplt.plot(range(len(lr.cost_)), lr.cost_)\nplt.xlabel('Iterations')\nplt.ylabel('Cost')\nplt.show()  Iteration: 500/500 | Cost 0.13   plot_decision_regions(X, y, clf=lr)\nplt.title('Softmax Regression - Gradient Descent')\nplt.show()",
            "title": "Example 1 - Gradient Descent"
        },
        {
            "location": "/user_guide/tf_classifier/TfSoftmaxRegression/#predicting-class-labels",
            "text": "print('Predicted class labels:', lr.predict(X[[0, 99, 149]]))  Predicted class labels: [0 1 2]",
            "title": "Predicting Class Labels"
        },
        {
            "location": "/user_guide/tf_classifier/TfSoftmaxRegression/#predicting-class-probabilities",
            "text": "print('Predicted class probabilities:\\n', lr.predict_proba(X[[0, 99, 149]]))  Predicted class probabilities:\n [[  9.92753923e-01   7.24608498e-03   6.90939350e-09]\n [  5.35254739e-03   9.61461246e-01   3.31862085e-02]\n [  1.28487918e-05   2.01715842e-01   7.98271239e-01]]",
            "title": "Predicting Class Probabilities"
        },
        {
            "location": "/user_guide/tf_classifier/TfSoftmaxRegression/#example-2-stochastic-gradient-descent",
            "text": "Stochastic gradient descent training sample by sample can be achieved by setting the number of minibatches equal to the number of samples in the training dataset; everything between  minibatches=1  and  minibatches=len(y)  is \"minibatch\" stochastic gradient descent.  from mlxtend.data import iris_data\nfrom mlxtend.evaluate import plot_decision_regions\nfrom mlxtend.tf_classifier import TfSoftmaxRegression\nimport matplotlib.pyplot as plt\n\n# Loading Data\n\nX, y = iris_data()\nX = X[:, [0, 3]] # sepal length and petal width\n\n# standardize\nX[:,0] = (X[:,0] - X[:,0].mean()) / X[:,0].std()\nX[:,1] = (X[:,1] - X[:,1].mean()) / X[:,1].std()\n\nlr = TfSoftmaxRegression(eta=0.5, \n                         epochs=10, \n                         minibatches=len(y), \n                         print_progress=True, \n                         random_seed=1)\nlr.fit(X, y)\n\nplt.plot(range(len(lr.cost_)), lr.cost_)\nplt.xlabel('Iterations')\nplt.ylabel('Cost')\nplt.show()\n\nplot_decision_regions(X, y, clf=lr)\nplt.title('Softmax Regression - Gradient Descent')\nplt.show()  Iteration: 10/10 | Cost 0.15",
            "title": "Example 2 - Stochastic Gradient Descent"
        },
        {
            "location": "/user_guide/tf_classifier/TfSoftmaxRegression/#example-3-mnist",
            "text": "Note that  mnist_data  just contains a random 5000-sample subset of MNIST (~10% of the original dataset size).  from mlxtend.data import mnist_data\nfrom mlxtend.tf_classifier import TfSoftmaxRegression\nimport matplotlib.pyplot as plt\n\nX, y = mnist_data()\n\nlr = TfSoftmaxRegression(eta=0.01, \n                         epochs=100, \n                         minibatches=20, \n                         print_progress=True, \n                         random_seed=1)\nlr.fit(X, y)\n\nplt.plot(range(len(lr.cost_)), lr.cost_)\nplt.xlabel('Iterations')\nplt.ylabel('Cost')\nplt.show()  Iteration: 100/100 | Cost 6.26   import numpy as np\ny_pred = lr.predict(X)\nprint('Training Accuracy: %.2f%%' % (lr.score(X, y) * 100))  Training Accuracy: 93.86%  def plot_digit(X, y, idx):\n    img = X[idx].reshape(28,28)\n    plt.imshow(img, cmap='Greys',  interpolation='nearest')\n    plt.title('true label: %d' % y[idx])\n    plt.show()\nplot_digit(X, y, 0)     print('Prediction: %d' % lr.predict(X[0, None]))  Prediction: 0",
            "title": "Example 3 - MNIST"
        },
        {
            "location": "/user_guide/tf_classifier/TfSoftmaxRegression/#api",
            "text": "TfSoftmaxRegression(eta=0.5, epochs=50, n_classes=None, minibatches=1, random_seed=None, print_progress=0, dtype=None)  Softmax regression classifier.  Parameters    eta  : float (default: 0.5)  Learning rate (between 0.0 and 1.0)    epochs  : int (default: 50)  Passes over the training dataset.\nPrior to each epoch, the dataset is shuffled\nif  minibatches > 1  to prevent cycles in stochastic gradient descent.    n_classes  : int (default: None)  A positive integer to declare the number of class labels\nif not all class labels are present in a partial training set.\nGets the number of class labels automatically if None.    minibatches  : int (default: 1)  Divide the training data into  k  minibatches\nfor accelerated stochastic gradient descent learning.\nGradient Descent Learning if  minibatches  = 1\nStochastic Gradient Descent learning if  minibatches  = len(y)\nMinibatch learning if  minibatches  > 1    random_seed  : int (default: None)  Set random state for shuffling and initializing the weights.    print_progress  : int (default: 0)  Prints progress in fitting to stderr.\n0: No output\n1: Epochs elapsed and cost\n2: 1 plus time elapsed\n3: 2 plus estimated time until completion    dtype  : Array-type (default: None)  Uses tensorflow.float32 if None.    Attributes    w_  : 2d-array, shape=[n_features, n_classes]  Weights after fitting.    b_  : 1D-array, shape=[n_classes]  Bias units after fitting.    cost_  : list  List of floats, the average cross_entropy for each epoch.",
            "title": "API"
        },
        {
            "location": "/user_guide/tf_classifier/TfSoftmaxRegression/#methods",
            "text": "fit(X, y, init_params=True)  Learn model from training data.  Parameters    X  : {array-like, sparse matrix}, shape = [n_samples, n_features]  Training vectors, where n_samples is the number of samples and\nn_features is the number of features.    y  : array-like, shape = [n_samples]  Target values.    init_params  : bool (default: True)  Re-initializes model parameters prior to fitting.\nSet False to continue training with weights from\na previous model fitting.    Returns   self  : object    predict(X)  Predict targets from X.  Parameters    X  : {array-like, sparse matrix}, shape = [n_samples, n_features]  Training vectors, where n_samples is the number of samples and\nn_features is the number of features.    Returns    target_values  : array-like, shape = [n_samples]  Predicted target values.     predict_proba(X)  Predict class probabilities of X from the net input.  Parameters    X  : {array-like, sparse matrix}, shape = [n_samples, n_features]  Training vectors, where n_samples is the number of samples and\nn_features is the number of features.    Returns   Class probabilties  : array-like, shape= [n_samples, n_classes]    score(X, y)  Compute the prediction accuracy  Parameters    X  : {array-like, sparse matrix}, shape = [n_samples, n_features]  Training vectors, where n_samples is the number of samples and\nn_features is the number of features.    y  : array-like, shape = [n_samples]  Target values (true class labels).    Returns    acc  : float  The prediction accuracy as a float\nbetween 0.0 and 1.0 (perfect score).",
            "title": "Methods"
        },
        {
            "location": "/user_guide/regressor/LinearRegression/",
            "text": "LinearRegression\n\n\nA implementation of Ordinary Least Squares simple and multiple linear regression.\n\n\n\n\nfrom mlxtend.regressor import LinearRegression\n\n\n\n\nOverview\n\n\nIllustration of a simple linear regression model:\n\n\n\n\nIn Ordinary Least Squares (OLS) Linear Regression, our goal is to find the line (or hyperplane) that minimizes the vertical offsets. Or in other words, we define the best-fitting line as the line that minimizes the sum of squared errors (SSE) or mean squared error (MSE) between our target variable (y) and our predicted output over all samples \n$i$\n in our dataset of size \n$n$\n.\n\n\n$$SSE =  \\sum_i \\big(\\text{target}^{(i)} - \\text{output}^{(i)}\\big)^2$$\n\n\n$$MSE = \\frac{1}{n} \\times SSE$$\n\n\nNow, \nLinearRegression\n implements a linear regression model for performing ordinary least squares regression using one of the following three approaches:\n\n\n\n\nNormal Equations\n\n\nGradient Descent\n\n\nStochastic Gradient Descent\n\n\n\n\nNormal Equations (closed-form solution)\n\n\nThe closed-form solution should be preferred for \"smaller\" datasets where calculating (a \"costly\") matrix inverse is not a concern. For very large datasets, or datasets where the inverse of \n$[X^T X]$\n may not exist (the matrix is non-invertible or singular, e.g., in case of perfect multicollinearity), the gradient descent or stochastic gradient descent approaches are to be preferred.\n\n\nThe linear function (linear regression model) is defined as:\n\n\n$$y = w_0x_0 + w_1x_1 + ... + w_mx_m = \\sum_{i=0}^{n} = \\mathbf{w}^T\\mathbf{x}$$\n\n\nwhere \n$y$\n is the response variable, \n$\\mathbf{x}$\n is an \n$m$\n-dimensional sample vector, and \n$\\mathbf{w}$\n is the weight vector (vector of coefficients). Note that \n$w_0$\n represents the y-axis intercept of the model and therefore \n$x_0=1$\n.  \n\n\nUsing the closed-form solution (normal equation), we compute the weights of the model as follows:\n\n\n$$ \\mathbf{w} = (\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{X}^Ty$$\n\n\nGradient Descent (GD)  and Stochastic Gradient Descent (SGD)\n\n\nSee \nGradient Descent and Stochastic Gradient Descent\n and \nDeriving the Gradient Descent Rule for Linear Regression and Adaline\n for details.\n\n\nRandom shuffling is implemented as:\n\n\n\n\nfor one or more epochs\n\n\nrandomly shuffle samples in the training set\n\n\nfor training sample \ni\n\n\ncompute gradients and perform weight updates\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nReferences\n\n\n\n\nF. Galton. \nRegression towards mediocrity in hereditary stature\n. Journal of the Anthropological Institute of Great Britain and Ireland, pages 246\u2013263, 1886.\n\n\nA. I. Khuri. \nIntroduction to linear regression analysis\n, by Douglas C. Montgomery, Elizabeth A. Peck, G. Geoffrey Vining. International Statistical Review, 81(2):318\u2013319, 2013.\n\n\nD. S. G. Pollock. \nThe Classical Linear Regression Model\n.\n\n\n\n\nExamples\n\n\nExample 1 - Closed Form Solution\n\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom mlxtend.regressor import LinearRegression\n\nX = np.array([ 1.0, 2.1, 3.6, 4.2, 6])[:, np.newaxis]\ny = np.array([ 1.0, 2.0, 3.0, 4.0, 5.0])\n\nne_lr = LinearRegression(minibatches=None)\nne_lr.fit(X, y)\n\nprint('Intercept: %.2f' % ne_lr.b_)\nprint('Slope: %.2f' % ne_lr.w_[0])\n\ndef lin_regplot(X, y, model):\n    plt.scatter(X, y, c='blue')\n    plt.plot(X, model.predict(X), color='red')    \n    return\n\nlin_regplot(X, y, ne_lr)\nplt.show()\n\n\n\n\nIntercept: 0.25\nSlope: 0.81\n\n\n\n\n\nExample 2 - Gradient Descent\n\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom mlxtend.regressor import LinearRegression\n\nX = np.array([ 1.0, 2.1, 3.6, 4.2, 6])[:, np.newaxis]\ny = np.array([ 1.0, 2.0, 3.0, 4.0, 5.0])\n\ngd_lr = LinearRegression(eta=0.005, \n                         epochs=100,\n                         minibatches=1,\n                         random_seed=123,\n                         print_progress=3)\ngd_lr.fit(X, y)\n\nprint('Intercept: %.2f' % gd_lr.w_)\nprint('Slope: %.2f' % gd_lr.b_)\n\ndef lin_regplot(X, y, model):\n    plt.scatter(X, y, c='blue')\n    plt.plot(X, model.predict(X), color='red')    \n    return\n\nlin_regplot(X, y, gd_lr)\nplt.show()\n\n\n\n\nIteration: 100/100 | Cost 0.08 | Elapsed: 0:00:00 | ETA: 0:00:00\n\nIntercept: 0.82\nSlope: 0.22\n\n\n\n\n\n# Visualizing the cost to check for convergence and plotting the linear model:\n\nplt.plot(range(1, gd_lr.epochs+1), gd_lr.cost_)\nplt.xlabel('Epochs')\nplt.ylabel('Cost')\nplt.ylim([0, 0.2])\nplt.tight_layout()\nplt.show()    \n\n\n\n\n\n\nExample 3 - Stochastic Gradient Descent\n\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom mlxtend.regressor import LinearRegression\n\nX = np.array([ 1.0, 2.1, 3.6, 4.2, 6])[:, np.newaxis]\ny = np.array([ 1.0, 2.0, 3.0, 4.0, 5.0])\n\nsgd_lr = LinearRegression(eta=0.01, \n                          epochs=100, \n                          random_seed=0, \n                          minibatches=len(y))\nsgd_lr.fit(X, y)\n\nprint('Intercept: %.2f' % sgd_lr.b_)\nprint('Slope: %.2f' % sgd_lr.w_)\n\ndef lin_regplot(X, y, model):\n    plt.scatter(X, y, c='blue')\n    plt.plot(X, model.predict(X), color='red')    \n    return\n\nlin_regplot(X, y, sgd_lr)\nplt.show()\n\n\n\n\nIntercept: 0.24\nSlope: 0.82\n\n\n\n\n\nplt.plot(range(1, sgd_lr.epochs+1), sgd_lr.cost_)\nplt.xlabel('Epochs')\nplt.ylabel('Cost')\nplt.ylim([0, 0.2])\nplt.tight_layout()\nplt.show()  \n\n\n\n\n\n\nExample 3 - Stochastic Gradient Descent with Minibatches\n\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom mlxtend.regressor import LinearRegression\n\nX = np.array([ 1.0, 2.1, 3.6, 4.2, 6])[:, np.newaxis]\ny = np.array([ 1.0, 2.0, 3.0, 4.0, 5.0])\n\nsgd_lr = LinearRegression(eta=0.01, \n                          epochs=100, \n                          random_seed=0, \n                          minibatches=3)\nsgd_lr.fit(X, y)\n\nprint('Intercept: %.2f' % sgd_lr.b_)\nprint('Slope: %.2f' % sgd_lr.w_)\n\ndef lin_regplot(X, y, model):\n    plt.scatter(X, y, c='blue')\n    plt.plot(X, model.predict(X), color='red')    \n    return\n\nlin_regplot(X, y, sgd_lr)\nplt.show()\n\n\n\n\nIntercept: 0.24\nSlope: 0.82\n\n\n\n\n\nplt.plot(range(1, sgd_lr.epochs+1), sgd_lr.cost_)\nplt.xlabel('Epochs')\nplt.ylabel('Cost')\nplt.ylim([0, 0.2])\nplt.tight_layout()\nplt.show()  \n\n\n\n\n\n\nAPI\n\n\nLinearRegression(eta=0.01, epochs=50, minibatches=None, random_seed=None, print_progress=0)\n\n\nOrdinary least squares linear regression.\n\n\nParameters\n\n\n\n\n\n\neta\n : float (default: 0.01)\n\n\nsolver rate (between 0.0 and 1.0)\n\n\n\n\n\n\nepochs\n : int (default: 50)\n\n\nPasses over the training dataset.\nPrior to each epoch, the dataset is shuffled\nif \nminibatches > 1\n to prevent cycles in stochastic gradient descent.\n\n\n\n\n\n\nminibatches\n : int (default: None)\n\n\nThe number of minibatches for gradient-based optimization.\nIf None: Normal Equations (closed-form solution)\nIf 1: Gradient Descent learning\nIf len(y): Stochastic Gradient Descent learning\nIf 1 < minibatches < len(y): Minibatch learning\n\n\n\n\n\n\nrandom_seed\n : int (default: None)\n\n\nSet random state for shuffling and initializing the weights.\n\n\n\n\n\n\nprint_progress\n : int (default: 0)\n\n\nPrints progress in fitting to stderr if not solver='normal equation'\n0: No output\n1: Epochs elapsed and cost\n2: 1 plus time elapsed\n3: 2 plus estimated time until completion\n\n\n\n\n\n\nAttributes\n\n\n\n\n\n\nw_\n : 2d-array, shape={n_features, 1}\n\n\nModel weights after fitting.\n\n\n\n\n\n\nb_\n : 1d-array, shape={1,}\n\n\nBias unit after fitting.\n\n\n\n\n\n\ncost_\n : list\n\n\nSum of squared errors after each epoch;\nignored if solver='normal equation'\n\n\n\n\n\n\nMethods\n\n\n\n\n\nfit(X, y, init_params=True)\n\n\nLearn model from training data.\n\n\nParameters\n\n\n\n\n\n\nX\n : {array-like, sparse matrix}, shape = [n_samples, n_features]\n\n\nTraining vectors, where n_samples is the number of samples and\nn_features is the number of features.\n\n\n\n\n\n\ny\n : array-like, shape = [n_samples]\n\n\nTarget values.\n\n\n\n\n\n\ninit_params\n : bool (default: True)\n\n\nRe-initializes model parameters prior to fitting.\nSet False to continue training with weights from\na previous model fitting.\n\n\n\n\n\n\nReturns\n\n\n\n\nself\n : object\n\n\n\n\n\n\n\npredict(X)\n\n\nPredict targets from X.\n\n\nParameters\n\n\n\n\n\n\nX\n : {array-like, sparse matrix}, shape = [n_samples, n_features]\n\n\nTraining vectors, where n_samples is the number of samples and\nn_features is the number of features.\n\n\n\n\n\n\nReturns\n\n\n\n\n\n\ntarget_values\n : array-like, shape = [n_samples]\n\n\nPredicted target values.",
            "title": "LinearRegression"
        },
        {
            "location": "/user_guide/regressor/LinearRegression/#linearregression",
            "text": "A implementation of Ordinary Least Squares simple and multiple linear regression.   from mlxtend.regressor import LinearRegression",
            "title": "LinearRegression"
        },
        {
            "location": "/user_guide/regressor/LinearRegression/#overview",
            "text": "Illustration of a simple linear regression model:   In Ordinary Least Squares (OLS) Linear Regression, our goal is to find the line (or hyperplane) that minimizes the vertical offsets. Or in other words, we define the best-fitting line as the line that minimizes the sum of squared errors (SSE) or mean squared error (MSE) between our target variable (y) and our predicted output over all samples  $i$  in our dataset of size  $n$ .  $$SSE =  \\sum_i \\big(\\text{target}^{(i)} - \\text{output}^{(i)}\\big)^2$$  $$MSE = \\frac{1}{n} \\times SSE$$  Now,  LinearRegression  implements a linear regression model for performing ordinary least squares regression using one of the following three approaches:   Normal Equations  Gradient Descent  Stochastic Gradient Descent",
            "title": "Overview"
        },
        {
            "location": "/user_guide/regressor/LinearRegression/#normal-equations-closed-form-solution",
            "text": "The closed-form solution should be preferred for \"smaller\" datasets where calculating (a \"costly\") matrix inverse is not a concern. For very large datasets, or datasets where the inverse of  $[X^T X]$  may not exist (the matrix is non-invertible or singular, e.g., in case of perfect multicollinearity), the gradient descent or stochastic gradient descent approaches are to be preferred.  The linear function (linear regression model) is defined as:  $$y = w_0x_0 + w_1x_1 + ... + w_mx_m = \\sum_{i=0}^{n} = \\mathbf{w}^T\\mathbf{x}$$  where  $y$  is the response variable,  $\\mathbf{x}$  is an  $m$ -dimensional sample vector, and  $\\mathbf{w}$  is the weight vector (vector of coefficients). Note that  $w_0$  represents the y-axis intercept of the model and therefore  $x_0=1$ .    Using the closed-form solution (normal equation), we compute the weights of the model as follows:  $$ \\mathbf{w} = (\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{X}^Ty$$",
            "title": "Normal Equations (closed-form solution)"
        },
        {
            "location": "/user_guide/regressor/LinearRegression/#gradient-descent-gd-and-stochastic-gradient-descent-sgd",
            "text": "See  Gradient Descent and Stochastic Gradient Descent  and  Deriving the Gradient Descent Rule for Linear Regression and Adaline  for details.  Random shuffling is implemented as:   for one or more epochs  randomly shuffle samples in the training set  for training sample  i  compute gradients and perform weight updates",
            "title": "Gradient Descent (GD)  and Stochastic Gradient Descent (SGD)"
        },
        {
            "location": "/user_guide/regressor/LinearRegression/#references",
            "text": "F. Galton.  Regression towards mediocrity in hereditary stature . Journal of the Anthropological Institute of Great Britain and Ireland, pages 246\u2013263, 1886.  A. I. Khuri.  Introduction to linear regression analysis , by Douglas C. Montgomery, Elizabeth A. Peck, G. Geoffrey Vining. International Statistical Review, 81(2):318\u2013319, 2013.  D. S. G. Pollock.  The Classical Linear Regression Model .",
            "title": "References"
        },
        {
            "location": "/user_guide/regressor/LinearRegression/#examples",
            "text": "",
            "title": "Examples"
        },
        {
            "location": "/user_guide/regressor/LinearRegression/#example-1-closed-form-solution",
            "text": "import numpy as np\nimport matplotlib.pyplot as plt\nfrom mlxtend.regressor import LinearRegression\n\nX = np.array([ 1.0, 2.1, 3.6, 4.2, 6])[:, np.newaxis]\ny = np.array([ 1.0, 2.0, 3.0, 4.0, 5.0])\n\nne_lr = LinearRegression(minibatches=None)\nne_lr.fit(X, y)\n\nprint('Intercept: %.2f' % ne_lr.b_)\nprint('Slope: %.2f' % ne_lr.w_[0])\n\ndef lin_regplot(X, y, model):\n    plt.scatter(X, y, c='blue')\n    plt.plot(X, model.predict(X), color='red')    \n    return\n\nlin_regplot(X, y, ne_lr)\nplt.show()  Intercept: 0.25\nSlope: 0.81",
            "title": "Example 1 - Closed Form Solution"
        },
        {
            "location": "/user_guide/regressor/LinearRegression/#example-2-gradient-descent",
            "text": "import numpy as np\nimport matplotlib.pyplot as plt\nfrom mlxtend.regressor import LinearRegression\n\nX = np.array([ 1.0, 2.1, 3.6, 4.2, 6])[:, np.newaxis]\ny = np.array([ 1.0, 2.0, 3.0, 4.0, 5.0])\n\ngd_lr = LinearRegression(eta=0.005, \n                         epochs=100,\n                         minibatches=1,\n                         random_seed=123,\n                         print_progress=3)\ngd_lr.fit(X, y)\n\nprint('Intercept: %.2f' % gd_lr.w_)\nprint('Slope: %.2f' % gd_lr.b_)\n\ndef lin_regplot(X, y, model):\n    plt.scatter(X, y, c='blue')\n    plt.plot(X, model.predict(X), color='red')    \n    return\n\nlin_regplot(X, y, gd_lr)\nplt.show()  Iteration: 100/100 | Cost 0.08 | Elapsed: 0:00:00 | ETA: 0:00:00\n\nIntercept: 0.82\nSlope: 0.22   # Visualizing the cost to check for convergence and plotting the linear model:\n\nplt.plot(range(1, gd_lr.epochs+1), gd_lr.cost_)\nplt.xlabel('Epochs')\nplt.ylabel('Cost')\nplt.ylim([0, 0.2])\nplt.tight_layout()\nplt.show()",
            "title": "Example 2 - Gradient Descent"
        },
        {
            "location": "/user_guide/regressor/LinearRegression/#example-3-stochastic-gradient-descent",
            "text": "import numpy as np\nimport matplotlib.pyplot as plt\nfrom mlxtend.regressor import LinearRegression\n\nX = np.array([ 1.0, 2.1, 3.6, 4.2, 6])[:, np.newaxis]\ny = np.array([ 1.0, 2.0, 3.0, 4.0, 5.0])\n\nsgd_lr = LinearRegression(eta=0.01, \n                          epochs=100, \n                          random_seed=0, \n                          minibatches=len(y))\nsgd_lr.fit(X, y)\n\nprint('Intercept: %.2f' % sgd_lr.b_)\nprint('Slope: %.2f' % sgd_lr.w_)\n\ndef lin_regplot(X, y, model):\n    plt.scatter(X, y, c='blue')\n    plt.plot(X, model.predict(X), color='red')    \n    return\n\nlin_regplot(X, y, sgd_lr)\nplt.show()  Intercept: 0.24\nSlope: 0.82   plt.plot(range(1, sgd_lr.epochs+1), sgd_lr.cost_)\nplt.xlabel('Epochs')\nplt.ylabel('Cost')\nplt.ylim([0, 0.2])\nplt.tight_layout()\nplt.show()",
            "title": "Example 3 - Stochastic Gradient Descent"
        },
        {
            "location": "/user_guide/regressor/LinearRegression/#example-3-stochastic-gradient-descent-with-minibatches",
            "text": "import numpy as np\nimport matplotlib.pyplot as plt\nfrom mlxtend.regressor import LinearRegression\n\nX = np.array([ 1.0, 2.1, 3.6, 4.2, 6])[:, np.newaxis]\ny = np.array([ 1.0, 2.0, 3.0, 4.0, 5.0])\n\nsgd_lr = LinearRegression(eta=0.01, \n                          epochs=100, \n                          random_seed=0, \n                          minibatches=3)\nsgd_lr.fit(X, y)\n\nprint('Intercept: %.2f' % sgd_lr.b_)\nprint('Slope: %.2f' % sgd_lr.w_)\n\ndef lin_regplot(X, y, model):\n    plt.scatter(X, y, c='blue')\n    plt.plot(X, model.predict(X), color='red')    \n    return\n\nlin_regplot(X, y, sgd_lr)\nplt.show()  Intercept: 0.24\nSlope: 0.82   plt.plot(range(1, sgd_lr.epochs+1), sgd_lr.cost_)\nplt.xlabel('Epochs')\nplt.ylabel('Cost')\nplt.ylim([0, 0.2])\nplt.tight_layout()\nplt.show()",
            "title": "Example 3 - Stochastic Gradient Descent with Minibatches"
        },
        {
            "location": "/user_guide/regressor/LinearRegression/#api",
            "text": "LinearRegression(eta=0.01, epochs=50, minibatches=None, random_seed=None, print_progress=0)  Ordinary least squares linear regression.  Parameters    eta  : float (default: 0.01)  solver rate (between 0.0 and 1.0)    epochs  : int (default: 50)  Passes over the training dataset.\nPrior to each epoch, the dataset is shuffled\nif  minibatches > 1  to prevent cycles in stochastic gradient descent.    minibatches  : int (default: None)  The number of minibatches for gradient-based optimization.\nIf None: Normal Equations (closed-form solution)\nIf 1: Gradient Descent learning\nIf len(y): Stochastic Gradient Descent learning\nIf 1 < minibatches < len(y): Minibatch learning    random_seed  : int (default: None)  Set random state for shuffling and initializing the weights.    print_progress  : int (default: 0)  Prints progress in fitting to stderr if not solver='normal equation'\n0: No output\n1: Epochs elapsed and cost\n2: 1 plus time elapsed\n3: 2 plus estimated time until completion    Attributes    w_  : 2d-array, shape={n_features, 1}  Model weights after fitting.    b_  : 1d-array, shape={1,}  Bias unit after fitting.    cost_  : list  Sum of squared errors after each epoch;\nignored if solver='normal equation'",
            "title": "API"
        },
        {
            "location": "/user_guide/regressor/LinearRegression/#methods",
            "text": "fit(X, y, init_params=True)  Learn model from training data.  Parameters    X  : {array-like, sparse matrix}, shape = [n_samples, n_features]  Training vectors, where n_samples is the number of samples and\nn_features is the number of features.    y  : array-like, shape = [n_samples]  Target values.    init_params  : bool (default: True)  Re-initializes model parameters prior to fitting.\nSet False to continue training with weights from\na previous model fitting.    Returns   self  : object    predict(X)  Predict targets from X.  Parameters    X  : {array-like, sparse matrix}, shape = [n_samples, n_features]  Training vectors, where n_samples is the number of samples and\nn_features is the number of features.    Returns    target_values  : array-like, shape = [n_samples]  Predicted target values.",
            "title": "Methods"
        },
        {
            "location": "/user_guide/regressor/StackingRegressor/",
            "text": "StackingRegressor\n\n\nAn ensemble-learning meta-regressor for stacking regression\n\n\n\n\nfrom mlxtend.regressor import StackingRegressor\n\n\n\n\nOverview\n\n\nStacking regression is an ensemble learning technique to combine multiple regression models via a meta-regressor. The individual regression models are trained based on the complete training set; then, the meta-regressor is fitted based on the outputs -- meta-features -- of the individual regression models in the ensemble.\n\n\n\n\nReferences\n\n\n\n\nBreiman, Leo. \"\nStacked regressions.\n\" Machine learning 24.1 (1996): 49-64.\n\n\n\n\nExamples\n\n\nExample 1 - Simple Stacked Regression\n\n\nfrom mlxtend.regressor import StackingRegressor\nfrom mlxtend.data import boston_housing_data\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.linear_model import Ridge\nfrom sklearn.svm import SVR\nimport matplotlib.pyplot as plt\nimport numpy as np\n\n# Generating a sample dataset\nnp.random.seed(1)\nX = np.sort(5 * np.random.rand(40, 1), axis=0)\ny = np.sin(X).ravel()\ny[::5] += 3 * (0.5 - np.random.rand(8))\n\n\n\n\n# Initializing models\n\nlr = LinearRegression()\nsvr_lin = SVR(kernel='linear')\nridge = Ridge(random_state=1)\nsvr_rbf = SVR(kernel='rbf')\n\nstregr = StackingRegressor(regressors=[svr_lin, lr, ridge], \n                           meta_regressor=svr_rbf)\n\n# Training the stacking classifier\n\nstregr.fit(X, y)\nstregr.predict(X)\n\n# Evaluate and visualize the fit\n\nprint(\"Mean Squared Error: %.4f\"\n      % np.mean((stregr.predict(X) - y) ** 2))\nprint('Variance Score: %.4f' % stregr.score(X, y))\n\nwith plt.style.context(('seaborn-whitegrid')):\n    plt.scatter(X, y, c='lightgray')\n    plt.plot(X, stregr.predict(X), c='darkgreen', lw=2)\n\nplt.show()\n\n\n\n\nMean Squared Error: 0.2039\nVariance Score: 0.7049\n\n\n\n\n\nstregr\n\n\n\n\nStackingRegressor(meta_regressor=SVR(C=1.0, cache_size=200, coef0=0.0, degree=3, epsilon=0.1, gamma='auto',\n  kernel='rbf', max_iter=-1, shrinking=True, tol=0.001, verbose=False),\n         regressors=[SVR(C=1.0, cache_size=200, coef0=0.0, degree=3, epsilon=0.1, gamma='auto',\n  kernel='linear', max_iter=-1, shrinking=True, tol=0.001, verbose=False), LinearRegression(copy_X=True, fit_intercept=True, n_jobs=1, normalize=False), Ridge(alpha=1.0, copy_X=True, fit_intercept=True, max_iter=None,\n   normalize=False, random_state=1, solver='auto', tol=0.001)],\n         verbose=0)\n\n\n\nExample 2 - Stacked Regression and GridSearch\n\n\nTo set up a parameter grid for scikit-learn's \nGridSearch\n, we simply provide the estimator's names in the parameter grid -- in the special case of the meta-regressor, we append the \n'meta-'\n prefix.\n\n\nfrom sklearn.grid_search import GridSearchCV\nfrom sklearn.linear_model import Lasso\n\n# Initializing models\n\nlr = LinearRegression()\nsvr_lin = SVR(kernel='linear')\nridge = Ridge(random_state=1)\nlasso = Lasso(random_state=1)\nsvr_rbf = SVR(kernel='rbf')\nregressors = [svr_lin, lr, ridge, lasso]\nstregr = StackingRegressor(regressors=regressors, \n                           meta_regressor=svr_rbf)\n\nparams = {'lasso__alpha': [0.1, 1.0, 10.0],\n          'ridge__alpha': [0.1, 1.0, 10.0],\n          'svr__C': [0.1, 1.0, 10.0],\n          'meta-svr__C': [0.1, 1.0, 10.0, 100.0],\n          'meta-svr__gamma': [0.1, 1.0, 10.0]}\n\ngrid = GridSearchCV(estimator=stregr, \n                    param_grid=params, \n                    cv=5,\n                    refit=True)\ngrid.fit(X, y)\n\nfor params, mean_score, scores in grid.grid_scores_:\n    print(\"%0.3f (+/-%0.03f) for %r\"\n        % (mean_score, scores.std() / 2, params))\n    break\nprint('...\\n\\n')\nprint('Best Parameters:\\n%s' % grid.best_params_)\n\n\n\n\n-9.810 (+/-6.862) for {'lasso__alpha': 0.1, 'meta-svr__C': 0.1, 'meta-svr__gamma': 0.1, 'ridge__alpha': 0.1, 'svr__C': 0.1}\n...\n\n\nBest Parameters:\n{'lasso__alpha': 0.1, 'meta-svr__C': 1.0, 'meta-svr__gamma': 1.0, 'ridge__alpha': 0.1, 'svr__C': 10.0}\n\n\n\n# Evaluate and visualize the fit\nprint(\"Mean Squared Error: %.4f\"\n      % np.mean((grid.predict(X) - y) ** 2))\nprint('Variance Score: %.4f' % grid.score(X, y))\n\nwith plt.style.context(('seaborn-whitegrid')):\n    plt.scatter(X, y, c='lightgray')\n    plt.plot(X, grid.predict(X), c='darkgreen', lw=2)\n\nplt.show()\n\n\n\n\nMean Squared Error: 0.1844\nVariance Score: 0.7331\n\n\n\n\n\nIn case we are planning to use a regression algorithm multiple times, all we need to do is to add an additional number suffix in the parameter grid as shown below:\n\n\nAPI\n\n\nStackingRegressor(regressors, meta_regressor, verbose=0)\n\n\nA Stacking regressor for scikit-learn estimators for regression.\n\n\nParameters\n\n\n\n\n\n\nregressors\n : array-like, shape = [n_regressors]\n\n\nA list of regressors.\nInvoking the \nfit\n method on the \nStackingRegressor\n will fit clones\nof those original regressors that will\nbe stored in the class attribute\n\nself.regr_\n.\n\n\n\n\n\n\nmeta_regressor\n : object\n\n\nThe meta-regressor to be fitted on the ensemble of\nregressors\n\n\n\n\n\n\nverbose\n : int, optional (default=0)\n\n\nControls the verbosity of the building process.\n- \nverbose=0\n (default): Prints nothing\n- \nverbose=1\n: Prints the number & name of the regressor being fitted\n- \nverbose=2\n: Prints info about the parameters of the\nregressor being fitted\n- \nverbose>2\n: Changes \nverbose\n param of the underlying regressor to\nself.verbose - 2\n\n\n\n\n\n\nAttributes\n\n\n\n\n\n\nregr_\n : list, shape=[n_regressors]\n\n\nFitted regressors (clones of the original regressors)\n\n\n\n\n\n\nmeta_regr_\n : estimator\n\n\nFitted meta-regressor (clone of the original meta-estimator)\n\n\n\n\n\n\ncoef_\n : array-like, shape = [n_features]\n\n\nModel coefficients of the fitted meta-estimator\n\n\n\n\n\n\nintercept_\n : float\n\n\nIntercept of the fitted meta-estimator\n\n\n\n\n\n\nMethods\n\n\n\n\n\nfit(X, y)\n\n\nLearn weight coefficients from training data for each regressor.\n\n\nParameters\n\n\n\n\n\n\nX\n : {array-like, sparse matrix}, shape = [n_samples, n_features]\n\n\nTraining vectors, where n_samples is the number of samples and\nn_features is the number of features.\n\n\n\n\n\n\ny\n : array-like, shape = [n_samples]\n\n\nTarget values.\n\n\n\n\n\n\nReturns\n\n\n\n\nself\n : object\n\n\n\n\n\n\n\nfit_transform(X, y=None, \nfit_params)\n\n\nFit to data, then transform it.\n\n\nFits transformer to X and y with optional parameters fit_params\nand returns a transformed version of X.\n\n\n\nParameters\n\n\n\n\n\n\nX\n : numpy array of shape [n_samples, n_features]\n\n\nTraining set.\n\n\n\n\n\n\ny\n : numpy array of shape [n_samples]\n\n\nTarget values.\n\n\n\n\n\n\nReturns\n\n\n\n\n\n\nX_new\n : numpy array of shape [n_samples, n_features_new]\n\n\nTransformed array.\n\n\n\n\n\n\n\n\n\nget_params(deep=True)\n\n\nReturn estimator parameter names for GridSearch support.\n\n\n\n\n\npredict(X)\n\n\nPredict target values for X.\n\n\nParameters\n\n\n\n\n\n\nX\n : {array-like, sparse matrix}, shape = [n_samples, n_features]\n\n\nTraining vectors, where n_samples is the number of samples and\nn_features is the number of features.\n\n\n\n\n\n\nReturns\n\n\n\n\n\n\ny_target\n : array-like, shape = [n_samples]\n\n\nPredicted target values.\n\n\n\n\n\n\n\n\n\nscore(X, y, sample_weight=None)\n\n\nReturns the coefficient of determination R^2 of the prediction.\n\n\nThe coefficient R^2 is defined as (1 - u/v), where u is the regression\n\n\n\nsum of squares ((y_true - y_pred) \n 2).sum() and v is the residual\nsum of squares ((y_true - y_true.mean()) \n 2).sum().\n\n\nBest possible score is 1.0 and it can be negative (because the\n\n\nmodel can be arbitrarily worse). A constant model that always\n    predicts the expected value of y, disregarding the input features,\n    would get a R^2 score of 0.0.\n\n\nParameters\n\n\n\n\n\n\nX\n : array-like, shape = (n_samples, n_features)\n\n\nTest samples.\n\n\n\n\n\n\ny\n : array-like, shape = (n_samples) or (n_samples, n_outputs)\n\n\nTrue values for X.\n\n\n\n\n\n\nsample_weight\n : array-like, shape = [n_samples], optional\n\n\nSample weights.\n\n\n\n\n\n\nReturns\n\n\n\n\n\n\nscore\n : float\n\n\nR^2 of self.predict(X) wrt. y.\n\n\n\n\n\n\n\n\n\nset_params(\nparams)\n\n\nSet the parameters of this estimator.\n\n\nThe method works on simple estimators as well as on nested objects\n(such as pipelines). The former have parameters of the form\n``<component>__<parameter>`` so that it's possible to update each\ncomponent of a nested object.\n\n\n\nReturns\n\n\nself\n\n\nProperties\n\n\n\n\n\ncoef_\n\n\nNone\n\n\n\n\n\nintercept_\n\n\nNone",
            "title": "StackingRegressor"
        },
        {
            "location": "/user_guide/regressor/StackingRegressor/#stackingregressor",
            "text": "An ensemble-learning meta-regressor for stacking regression   from mlxtend.regressor import StackingRegressor",
            "title": "StackingRegressor"
        },
        {
            "location": "/user_guide/regressor/StackingRegressor/#overview",
            "text": "Stacking regression is an ensemble learning technique to combine multiple regression models via a meta-regressor. The individual regression models are trained based on the complete training set; then, the meta-regressor is fitted based on the outputs -- meta-features -- of the individual regression models in the ensemble.",
            "title": "Overview"
        },
        {
            "location": "/user_guide/regressor/StackingRegressor/#references",
            "text": "Breiman, Leo. \" Stacked regressions. \" Machine learning 24.1 (1996): 49-64.",
            "title": "References"
        },
        {
            "location": "/user_guide/regressor/StackingRegressor/#examples",
            "text": "",
            "title": "Examples"
        },
        {
            "location": "/user_guide/regressor/StackingRegressor/#example-1-simple-stacked-regression",
            "text": "from mlxtend.regressor import StackingRegressor\nfrom mlxtend.data import boston_housing_data\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.linear_model import Ridge\nfrom sklearn.svm import SVR\nimport matplotlib.pyplot as plt\nimport numpy as np\n\n# Generating a sample dataset\nnp.random.seed(1)\nX = np.sort(5 * np.random.rand(40, 1), axis=0)\ny = np.sin(X).ravel()\ny[::5] += 3 * (0.5 - np.random.rand(8))  # Initializing models\n\nlr = LinearRegression()\nsvr_lin = SVR(kernel='linear')\nridge = Ridge(random_state=1)\nsvr_rbf = SVR(kernel='rbf')\n\nstregr = StackingRegressor(regressors=[svr_lin, lr, ridge], \n                           meta_regressor=svr_rbf)\n\n# Training the stacking classifier\n\nstregr.fit(X, y)\nstregr.predict(X)\n\n# Evaluate and visualize the fit\n\nprint(\"Mean Squared Error: %.4f\"\n      % np.mean((stregr.predict(X) - y) ** 2))\nprint('Variance Score: %.4f' % stregr.score(X, y))\n\nwith plt.style.context(('seaborn-whitegrid')):\n    plt.scatter(X, y, c='lightgray')\n    plt.plot(X, stregr.predict(X), c='darkgreen', lw=2)\n\nplt.show()  Mean Squared Error: 0.2039\nVariance Score: 0.7049   stregr  StackingRegressor(meta_regressor=SVR(C=1.0, cache_size=200, coef0=0.0, degree=3, epsilon=0.1, gamma='auto',\n  kernel='rbf', max_iter=-1, shrinking=True, tol=0.001, verbose=False),\n         regressors=[SVR(C=1.0, cache_size=200, coef0=0.0, degree=3, epsilon=0.1, gamma='auto',\n  kernel='linear', max_iter=-1, shrinking=True, tol=0.001, verbose=False), LinearRegression(copy_X=True, fit_intercept=True, n_jobs=1, normalize=False), Ridge(alpha=1.0, copy_X=True, fit_intercept=True, max_iter=None,\n   normalize=False, random_state=1, solver='auto', tol=0.001)],\n         verbose=0)",
            "title": "Example 1 - Simple Stacked Regression"
        },
        {
            "location": "/user_guide/regressor/StackingRegressor/#example-2-stacked-regression-and-gridsearch",
            "text": "To set up a parameter grid for scikit-learn's  GridSearch , we simply provide the estimator's names in the parameter grid -- in the special case of the meta-regressor, we append the  'meta-'  prefix.  from sklearn.grid_search import GridSearchCV\nfrom sklearn.linear_model import Lasso\n\n# Initializing models\n\nlr = LinearRegression()\nsvr_lin = SVR(kernel='linear')\nridge = Ridge(random_state=1)\nlasso = Lasso(random_state=1)\nsvr_rbf = SVR(kernel='rbf')\nregressors = [svr_lin, lr, ridge, lasso]\nstregr = StackingRegressor(regressors=regressors, \n                           meta_regressor=svr_rbf)\n\nparams = {'lasso__alpha': [0.1, 1.0, 10.0],\n          'ridge__alpha': [0.1, 1.0, 10.0],\n          'svr__C': [0.1, 1.0, 10.0],\n          'meta-svr__C': [0.1, 1.0, 10.0, 100.0],\n          'meta-svr__gamma': [0.1, 1.0, 10.0]}\n\ngrid = GridSearchCV(estimator=stregr, \n                    param_grid=params, \n                    cv=5,\n                    refit=True)\ngrid.fit(X, y)\n\nfor params, mean_score, scores in grid.grid_scores_:\n    print(\"%0.3f (+/-%0.03f) for %r\"\n        % (mean_score, scores.std() / 2, params))\n    break\nprint('...\\n\\n')\nprint('Best Parameters:\\n%s' % grid.best_params_)  -9.810 (+/-6.862) for {'lasso__alpha': 0.1, 'meta-svr__C': 0.1, 'meta-svr__gamma': 0.1, 'ridge__alpha': 0.1, 'svr__C': 0.1}\n...\n\n\nBest Parameters:\n{'lasso__alpha': 0.1, 'meta-svr__C': 1.0, 'meta-svr__gamma': 1.0, 'ridge__alpha': 0.1, 'svr__C': 10.0}  # Evaluate and visualize the fit\nprint(\"Mean Squared Error: %.4f\"\n      % np.mean((grid.predict(X) - y) ** 2))\nprint('Variance Score: %.4f' % grid.score(X, y))\n\nwith plt.style.context(('seaborn-whitegrid')):\n    plt.scatter(X, y, c='lightgray')\n    plt.plot(X, grid.predict(X), c='darkgreen', lw=2)\n\nplt.show()  Mean Squared Error: 0.1844\nVariance Score: 0.7331   In case we are planning to use a regression algorithm multiple times, all we need to do is to add an additional number suffix in the parameter grid as shown below:",
            "title": "Example 2 - Stacked Regression and GridSearch"
        },
        {
            "location": "/user_guide/regressor/StackingRegressor/#api",
            "text": "StackingRegressor(regressors, meta_regressor, verbose=0)  A Stacking regressor for scikit-learn estimators for regression.  Parameters    regressors  : array-like, shape = [n_regressors]  A list of regressors.\nInvoking the  fit  method on the  StackingRegressor  will fit clones\nof those original regressors that will\nbe stored in the class attribute self.regr_ .    meta_regressor  : object  The meta-regressor to be fitted on the ensemble of\nregressors    verbose  : int, optional (default=0)  Controls the verbosity of the building process.\n-  verbose=0  (default): Prints nothing\n-  verbose=1 : Prints the number & name of the regressor being fitted\n-  verbose=2 : Prints info about the parameters of the\nregressor being fitted\n-  verbose>2 : Changes  verbose  param of the underlying regressor to\nself.verbose - 2    Attributes    regr_  : list, shape=[n_regressors]  Fitted regressors (clones of the original regressors)    meta_regr_  : estimator  Fitted meta-regressor (clone of the original meta-estimator)    coef_  : array-like, shape = [n_features]  Model coefficients of the fitted meta-estimator    intercept_  : float  Intercept of the fitted meta-estimator",
            "title": "API"
        },
        {
            "location": "/user_guide/regressor/StackingRegressor/#methods",
            "text": "fit(X, y)  Learn weight coefficients from training data for each regressor.  Parameters    X  : {array-like, sparse matrix}, shape = [n_samples, n_features]  Training vectors, where n_samples is the number of samples and\nn_features is the number of features.    y  : array-like, shape = [n_samples]  Target values.    Returns   self  : object    fit_transform(X, y=None,  fit_params)  Fit to data, then transform it.  Fits transformer to X and y with optional parameters fit_params\nand returns a transformed version of X.  Parameters    X  : numpy array of shape [n_samples, n_features]  Training set.    y  : numpy array of shape [n_samples]  Target values.    Returns    X_new  : numpy array of shape [n_samples, n_features_new]  Transformed array.     get_params(deep=True)  Return estimator parameter names for GridSearch support.   predict(X)  Predict target values for X.  Parameters    X  : {array-like, sparse matrix}, shape = [n_samples, n_features]  Training vectors, where n_samples is the number of samples and\nn_features is the number of features.    Returns    y_target  : array-like, shape = [n_samples]  Predicted target values.     score(X, y, sample_weight=None)  Returns the coefficient of determination R^2 of the prediction.  The coefficient R^2 is defined as (1 - u/v), where u is the regression  sum of squares ((y_true - y_pred)   2).sum() and v is the residual\nsum of squares ((y_true - y_true.mean())   2).sum().  Best possible score is 1.0 and it can be negative (because the  model can be arbitrarily worse). A constant model that always\n    predicts the expected value of y, disregarding the input features,\n    would get a R^2 score of 0.0.  Parameters    X  : array-like, shape = (n_samples, n_features)  Test samples.    y  : array-like, shape = (n_samples) or (n_samples, n_outputs)  True values for X.    sample_weight  : array-like, shape = [n_samples], optional  Sample weights.    Returns    score  : float  R^2 of self.predict(X) wrt. y.     set_params( params)  Set the parameters of this estimator.  The method works on simple estimators as well as on nested objects\n(such as pipelines). The former have parameters of the form\n``<component>__<parameter>`` so that it's possible to update each\ncomponent of a nested object.  Returns  self",
            "title": "Methods"
        },
        {
            "location": "/user_guide/regressor/StackingRegressor/#properties",
            "text": "coef_  None   intercept_  None",
            "title": "Properties"
        },
        {
            "location": "/user_guide/tf_regressor/TfLinearRegression/",
            "text": "TfLinearRegression\n\n\nA implementation of Ordinary Least Squares simple and multiple linear regression using TensorFlow.\n\n\n\n\nfrom mlxtend.tf_regressor import TfLinearRegression\n\n\n\n\nOverview\n\n\nThe linear regression model is fitted using gradient descent optimization supporting simple and multiple linear regression; for more information on linear regression, please refer to the documentation of the related \n\nmlxtend.regressor.LinearRegression estimator\n and the tutorials:\n\n\n\n\nGradient Descent and Stochastic Gradient Descent\n\n\nDeriving the Gradient Descent Rule for Linear Regression and Adaline\n.\n\n\n\n\nReferences\n\n\n\n\nF. Galton. \nRegression towards mediocrity in hereditary stature\n. Journal of the Anthropological Institute of Great Britain and Ireland, pages 246\u2013263, 1886.\n\n\nA. I. Khuri. \nIntroduction to linear regression analysis\n, by Douglas C. Montgomery, Elizabeth A. Peck, G. Geoffrey Vining. International Statistical Review, 81(2):318\u2013319, 2013.\n\n\nD. S. G. Pollock. \nThe Classical Linear Regression Model\n.\n\n\n\n\nExamples\n\n\nExample 1 - Simple Linear Regression\n\n\nGenerate some sample data\n\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nnp.random.seed(1)\nX = np.array([np.random.normal(1.0, 4.55) for i in range(100)])\ny = np.array([x1 * 0.1 + 0.1 + np.random.normal(0.0, 0.05) for x1 in X])\nX = X[:, np.newaxis]\n\nplt.scatter(X, y)\nplt.show()\n\n\n\n\n\n\nFit regressor and check MSE cost\n\n\nfrom mlxtend.tf_regressor import TfLinearRegression\n\ngd_lr = TfLinearRegression(eta=0.05, \n                           epochs=5,\n                           random_seed=1,\n                           print_progress=3)\ngd_lr.fit(X, y)\n\nprint('Intercept: %.2f' % gd_lr.b_)\nprint('Slope: %.2f' % gd_lr.w_)\n\nplt.plot(range(1, gd_lr.epochs+1), gd_lr.cost_)\nplt.xlabel('epochs')\nplt.ylabel('mean squared error (MSE)')\nplt.show()\n\n\n\n\nIteration: 5/5 | Cost 2.40 | Elapsed: 00:00:00 | ETA: 00:00:00\n\nIntercept: 0.10\nSlope: 0.39\n\n\n\n\n\nContinue training for another 50 epochs\n\n\ngd_lr.epochs = 50\ngd_lr.fit(X, y, init_params=False)\n\nprint('Intercept: %.2f' % gd_lr.b_)\nprint('Slope: %.2f' % gd_lr.w_)\n\nplt.plot(range(1, 56), gd_lr.cost_)\nplt.xlabel('epochs')\nplt.ylabel('mean squared error (MSE)')\nplt.show()\n\n\n\n\nIteration: 50/50 | Cost 0.00 | Elapsed: 0:00:00 | ETA: 0:00:00\n\nIntercept: 0.11\nSlope: 0.10\n\n\n\n\n\nVisualize the regression fit\n\n\ndef lin_regplot(X, y, model):\n    plt.scatter(X, y, c='blue')\n    plt.plot(X, model.predict(X), color='red')    \n    return\n\nlin_regplot(X, y, gd_lr)\nplt.show()\n\n\n\n\n\n\nTip\n: If we are using gradient descent, we should consider standardizing the feature variables for faster convergence:\n\n\nX_std = (X - np.mean(X)) / X.std()\ny_std = (y - np.mean(y)) / y.std()\n\ngd_lr = TfLinearRegression(eta=0.5, \n                           epochs=10,\n                           random_seed=0,\n                           print_progress=3)    \ngd_lr.fit(X_std, y_std)\n\nplt.plot(range(1, gd_lr.epochs + 1), gd_lr.cost_)\nplt.xlabel('Epochs')\nplt.ylabel('Cost')\nplt.tight_layout()\nplt.show()  \n\n\n\n\nIteration: 10/10 | Cost 0.01 | Elapsed: 0:00:00 | ETA: 0:00:00\n\n\n\n\n\nlin_regplot(X_std, y_std, gd_lr)\nplt.show()\n\n\n\n\n\n\nExample 2 - Multiple Linear Regression\n\n\nLoading the Boston Housing Data\n\n\nfrom mlxtend.data import boston_housing_data\nfrom mlxtend.preprocessing import shuffle_arrays_unison\n\nX, y = boston_housing_data()\nX, y = shuffle_arrays_unison([X, y], random_seed=1)\nX_test, y_test = X[:50], y[:50]\nX_train, y_train = X[50:], y[50:]\n\n# standardize\nmu1, mu2 = X_train.mean(axis=0), y_train.mean()\nsigma1, sigma2 = X_train.std(axis=0), y_train.std()\nX_train = (X_train - mu1) / sigma1\nX_test = (X_test - mu1) / sigma1\ny_train = (y_train - mu2) / sigma2\ny_test = (y_test - mu2) / sigma2\n\n\n\n\nFit the regression model\n\n\ngd_lr = TfLinearRegression(eta=0.1, \n                           epochs=200,\n                           random_seed=1,\n                           print_progress=3)    \n\ngd_lr.fit(X_train, y_train)\n\nplt.plot(range(51, gd_lr.epochs + 1), gd_lr.cost_[50:])\nplt.xlabel('Epochs')\nplt.ylabel('Cost')\nplt.tight_layout()\nplt.show()  \n\n\n\n\nIteration: 200/200 | Cost 0.27 | Elapsed: 0:00:00 | ETA: 0:00:00\n\n\n\n\n\nEvaluate the results\n\n\nfrom sklearn.metrics import r2_score\nfrom sklearn.metrics import mean_squared_error\n\ny_train_pred = gd_lr.predict(X_train)\ny_test_pred = gd_lr.predict(X_test)\n\nprint('MSE train: %.3f, test: %.3f' % (\n        mean_squared_error(y_train, y_train_pred),\n        mean_squared_error(y_test, y_test_pred)))\nprint('R^2 train: %.3f, test: %.3f' % (\n        r2_score(y_train, y_train_pred),\n        r2_score(y_test, y_test_pred)))\n\n\n\n\nMSE train: 0.265, test: 0.247\nR^2 train: 0.735, test: 0.780\n\n\n\nAPI\n\n\nTfLinearRegression(eta=0.1, epochs=50, print_progress=0, random_seed=None, dtype=None)\n\n\nEstimator for Linear Regression in TensorFlow using Gradient Descent.\n\n\nAdded in version 0.4.1\n\n\n\nMethods\n\n\n\n\n\nfit(X, y, init_params=True)\n\n\nLearn model from training data.\n\n\nParameters\n\n\n\n\n\n\nX\n : {array-like, sparse matrix}, shape = [n_samples, n_features]\n\n\nTraining vectors, where n_samples is the number of samples and\nn_features is the number of features.\n\n\n\n\n\n\ny\n : array-like, shape = [n_samples]\n\n\nTarget values.\n\n\n\n\n\n\ninit_params\n : bool (default: True)\n\n\nRe-initializes model parameters prior to fitting.\nSet False to continue training with weights from\na previous model fitting.\n\n\n\n\n\n\nReturns\n\n\n\n\nself\n : object\n\n\n\n\n\n\n\npredict(X)\n\n\nPredict class labels of X.\n\n\nParameters\n\n\n\n\n\n\nX\n : {array-like, sparse matrix}, shape = [n_samples, n_features]\n\n\nTraining vectors, where n_samples is the number of samples and\nn_features is the number of features.\n\n\n\n\n\n\nReturns\n\n\n\n\n\n\nclass_labels\n : array-like, shape = [n_samples]\n\n\nPredicted class labels.",
            "title": "TfLinearRegression"
        },
        {
            "location": "/user_guide/tf_regressor/TfLinearRegression/#tflinearregression",
            "text": "A implementation of Ordinary Least Squares simple and multiple linear regression using TensorFlow.   from mlxtend.tf_regressor import TfLinearRegression",
            "title": "TfLinearRegression"
        },
        {
            "location": "/user_guide/tf_regressor/TfLinearRegression/#overview",
            "text": "The linear regression model is fitted using gradient descent optimization supporting simple and multiple linear regression; for more information on linear regression, please refer to the documentation of the related  mlxtend.regressor.LinearRegression estimator  and the tutorials:   Gradient Descent and Stochastic Gradient Descent  Deriving the Gradient Descent Rule for Linear Regression and Adaline .",
            "title": "Overview"
        },
        {
            "location": "/user_guide/tf_regressor/TfLinearRegression/#references",
            "text": "F. Galton.  Regression towards mediocrity in hereditary stature . Journal of the Anthropological Institute of Great Britain and Ireland, pages 246\u2013263, 1886.  A. I. Khuri.  Introduction to linear regression analysis , by Douglas C. Montgomery, Elizabeth A. Peck, G. Geoffrey Vining. International Statistical Review, 81(2):318\u2013319, 2013.  D. S. G. Pollock.  The Classical Linear Regression Model .",
            "title": "References"
        },
        {
            "location": "/user_guide/tf_regressor/TfLinearRegression/#examples",
            "text": "",
            "title": "Examples"
        },
        {
            "location": "/user_guide/tf_regressor/TfLinearRegression/#example-1-simple-linear-regression",
            "text": "",
            "title": "Example 1 - Simple Linear Regression"
        },
        {
            "location": "/user_guide/tf_regressor/TfLinearRegression/#generate-some-sample-data",
            "text": "import numpy as np\nimport matplotlib.pyplot as plt\n\nnp.random.seed(1)\nX = np.array([np.random.normal(1.0, 4.55) for i in range(100)])\ny = np.array([x1 * 0.1 + 0.1 + np.random.normal(0.0, 0.05) for x1 in X])\nX = X[:, np.newaxis]\n\nplt.scatter(X, y)\nplt.show()",
            "title": "Generate some sample data"
        },
        {
            "location": "/user_guide/tf_regressor/TfLinearRegression/#fit-regressor-and-check-mse-cost",
            "text": "from mlxtend.tf_regressor import TfLinearRegression\n\ngd_lr = TfLinearRegression(eta=0.05, \n                           epochs=5,\n                           random_seed=1,\n                           print_progress=3)\ngd_lr.fit(X, y)\n\nprint('Intercept: %.2f' % gd_lr.b_)\nprint('Slope: %.2f' % gd_lr.w_)\n\nplt.plot(range(1, gd_lr.epochs+1), gd_lr.cost_)\nplt.xlabel('epochs')\nplt.ylabel('mean squared error (MSE)')\nplt.show()  Iteration: 5/5 | Cost 2.40 | Elapsed: 00:00:00 | ETA: 00:00:00\n\nIntercept: 0.10\nSlope: 0.39",
            "title": "Fit regressor and check MSE cost"
        },
        {
            "location": "/user_guide/tf_regressor/TfLinearRegression/#continue-training-for-another-50-epochs",
            "text": "gd_lr.epochs = 50\ngd_lr.fit(X, y, init_params=False)\n\nprint('Intercept: %.2f' % gd_lr.b_)\nprint('Slope: %.2f' % gd_lr.w_)\n\nplt.plot(range(1, 56), gd_lr.cost_)\nplt.xlabel('epochs')\nplt.ylabel('mean squared error (MSE)')\nplt.show()  Iteration: 50/50 | Cost 0.00 | Elapsed: 0:00:00 | ETA: 0:00:00\n\nIntercept: 0.11\nSlope: 0.10",
            "title": "Continue training for another 50 epochs"
        },
        {
            "location": "/user_guide/tf_regressor/TfLinearRegression/#visualize-the-regression-fit",
            "text": "def lin_regplot(X, y, model):\n    plt.scatter(X, y, c='blue')\n    plt.plot(X, model.predict(X), color='red')    \n    return\n\nlin_regplot(X, y, gd_lr)\nplt.show()   Tip : If we are using gradient descent, we should consider standardizing the feature variables for faster convergence:  X_std = (X - np.mean(X)) / X.std()\ny_std = (y - np.mean(y)) / y.std()\n\ngd_lr = TfLinearRegression(eta=0.5, \n                           epochs=10,\n                           random_seed=0,\n                           print_progress=3)    \ngd_lr.fit(X_std, y_std)\n\nplt.plot(range(1, gd_lr.epochs + 1), gd_lr.cost_)\nplt.xlabel('Epochs')\nplt.ylabel('Cost')\nplt.tight_layout()\nplt.show()    Iteration: 10/10 | Cost 0.01 | Elapsed: 0:00:00 | ETA: 0:00:00   lin_regplot(X_std, y_std, gd_lr)\nplt.show()",
            "title": "Visualize the regression fit"
        },
        {
            "location": "/user_guide/tf_regressor/TfLinearRegression/#example-2-multiple-linear-regression",
            "text": "",
            "title": "Example 2 - Multiple Linear Regression"
        },
        {
            "location": "/user_guide/tf_regressor/TfLinearRegression/#loading-the-boston-housing-data",
            "text": "from mlxtend.data import boston_housing_data\nfrom mlxtend.preprocessing import shuffle_arrays_unison\n\nX, y = boston_housing_data()\nX, y = shuffle_arrays_unison([X, y], random_seed=1)\nX_test, y_test = X[:50], y[:50]\nX_train, y_train = X[50:], y[50:]\n\n# standardize\nmu1, mu2 = X_train.mean(axis=0), y_train.mean()\nsigma1, sigma2 = X_train.std(axis=0), y_train.std()\nX_train = (X_train - mu1) / sigma1\nX_test = (X_test - mu1) / sigma1\ny_train = (y_train - mu2) / sigma2\ny_test = (y_test - mu2) / sigma2",
            "title": "Loading the Boston Housing Data"
        },
        {
            "location": "/user_guide/tf_regressor/TfLinearRegression/#fit-the-regression-model",
            "text": "gd_lr = TfLinearRegression(eta=0.1, \n                           epochs=200,\n                           random_seed=1,\n                           print_progress=3)    \n\ngd_lr.fit(X_train, y_train)\n\nplt.plot(range(51, gd_lr.epochs + 1), gd_lr.cost_[50:])\nplt.xlabel('Epochs')\nplt.ylabel('Cost')\nplt.tight_layout()\nplt.show()    Iteration: 200/200 | Cost 0.27 | Elapsed: 0:00:00 | ETA: 0:00:00",
            "title": "Fit the regression model"
        },
        {
            "location": "/user_guide/tf_regressor/TfLinearRegression/#evaluate-the-results",
            "text": "from sklearn.metrics import r2_score\nfrom sklearn.metrics import mean_squared_error\n\ny_train_pred = gd_lr.predict(X_train)\ny_test_pred = gd_lr.predict(X_test)\n\nprint('MSE train: %.3f, test: %.3f' % (\n        mean_squared_error(y_train, y_train_pred),\n        mean_squared_error(y_test, y_test_pred)))\nprint('R^2 train: %.3f, test: %.3f' % (\n        r2_score(y_train, y_train_pred),\n        r2_score(y_test, y_test_pred)))  MSE train: 0.265, test: 0.247\nR^2 train: 0.735, test: 0.780",
            "title": "Evaluate the results"
        },
        {
            "location": "/user_guide/tf_regressor/TfLinearRegression/#api",
            "text": "TfLinearRegression(eta=0.1, epochs=50, print_progress=0, random_seed=None, dtype=None)  Estimator for Linear Regression in TensorFlow using Gradient Descent.  Added in version 0.4.1",
            "title": "API"
        },
        {
            "location": "/user_guide/tf_regressor/TfLinearRegression/#methods",
            "text": "fit(X, y, init_params=True)  Learn model from training data.  Parameters    X  : {array-like, sparse matrix}, shape = [n_samples, n_features]  Training vectors, where n_samples is the number of samples and\nn_features is the number of features.    y  : array-like, shape = [n_samples]  Target values.    init_params  : bool (default: True)  Re-initializes model parameters prior to fitting.\nSet False to continue training with weights from\na previous model fitting.    Returns   self  : object    predict(X)  Predict class labels of X.  Parameters    X  : {array-like, sparse matrix}, shape = [n_samples, n_features]  Training vectors, where n_samples is the number of samples and\nn_features is the number of features.    Returns    class_labels  : array-like, shape = [n_samples]  Predicted class labels.",
            "title": "Methods"
        },
        {
            "location": "/user_guide/regression_utils/plot_linear_regression/",
            "text": "Linear Regression Plot\n\n\nA function to plot linear regression fits. \n\n\n\n\nfrom mlxtend.regression_utils import plot_linear_regression\n\n\n\n\nOverview\n\n\nThe \nplot_linear_regression\n is a convenience function that uses scikit-learn's \nlinear_model.LinearRegression\n to fit a linear model and SciPy's \nstats.pearsonr\n to calculate the correlation coefficient. \n\n\nReferences\n\n\n\n\n-\n\n\n\n\nExamples\n\n\nExample 1 - Ordinary Least Squares Simple Linear Regression\n\n\nimport matplotlib.pyplot as plt\nfrom mlxtend.regression_utils import plot_linear_regression\nimport numpy as np\n\nX = np.array([4, 8, 13, 26, 31, 10, 8, 30, 18, 12, 20, 5, 28, 18, 6, 31, 12,\n   12, 27, 11, 6, 14, 25, 7, 13,4, 15, 21, 15])\n\ny = np.array([14, 24, 22, 59, 66, 25, 18, 60, 39, 32, 53, 18, 55, 41, 28, 61, 35,\n   36, 52, 23, 19, 25, 73, 16, 32, 14, 31, 43, 34])\n\nintercept, slope, corr_coeff = plot_linear_regression(X, y)\nplt.show()\n\n\n\n\n\n\nAPI\n\n\nplot_linear_regression(X, y, model=LinearRegression(copy_X=True, fit_intercept=True, n_jobs=1, normalize=False), corr_func='pearsonr', scattercolor='blue', fit_style='k--', legend=True, xlim='auto')\n\n\nPlot a linear regression line fit.\n\n\nParameters\n\n\n\n\n\n\nX\n : numpy array, shape = [n_samples,]\n\n\nSamples.\n\n\n\n\n\n\ny\n : numpy array, shape (n_samples,)\n\n\nTarget values\nmodel: object (default: sklearn.linear_model.LinearRegression)\nEstimator object for regression. Must implement\na .fit() and .predict() method.\ncorr_func: str or function (default: 'pearsonr')\nUses \npearsonr\n from scipy.stats if corr_func='pearsonr'.\nto compute the regression slope. If not 'pearsonr', the \ncorr_func\n,\nthe \ncorr_func\n parameter expects a function of the form\nfunc(\n, \n) as inputs, which is expected to return\na tuple \n(<correlation_coefficient>, <some_unused_value>)\n.\nscattercolor: string (default: blue)\nColor of scatter plot points.\nfit_style: string (default: k--)\nStyle for the line fit.\nlegend: bool (default: True)\nPlots legend with corr_coeff coef.,\nfit coef., and intercept values.\nxlim: array-like (x_min, x_max) or 'auto' (default: 'auto')\nX-axis limits for the linear line fit.\n\n\n\n\n\n\nReturns\n\n\n\n\n\n\nregression_fit\n : tuple\n\n\nintercept, slope, corr_coeff (float, float, float)",
            "title": "Plot linear regression"
        },
        {
            "location": "/user_guide/regression_utils/plot_linear_regression/#linear-regression-plot",
            "text": "A function to plot linear regression fits.    from mlxtend.regression_utils import plot_linear_regression",
            "title": "Linear Regression Plot"
        },
        {
            "location": "/user_guide/regression_utils/plot_linear_regression/#overview",
            "text": "The  plot_linear_regression  is a convenience function that uses scikit-learn's  linear_model.LinearRegression  to fit a linear model and SciPy's  stats.pearsonr  to calculate the correlation coefficient.",
            "title": "Overview"
        },
        {
            "location": "/user_guide/regression_utils/plot_linear_regression/#references",
            "text": "-",
            "title": "References"
        },
        {
            "location": "/user_guide/regression_utils/plot_linear_regression/#examples",
            "text": "",
            "title": "Examples"
        },
        {
            "location": "/user_guide/regression_utils/plot_linear_regression/#example-1-ordinary-least-squares-simple-linear-regression",
            "text": "import matplotlib.pyplot as plt\nfrom mlxtend.regression_utils import plot_linear_regression\nimport numpy as np\n\nX = np.array([4, 8, 13, 26, 31, 10, 8, 30, 18, 12, 20, 5, 28, 18, 6, 31, 12,\n   12, 27, 11, 6, 14, 25, 7, 13,4, 15, 21, 15])\n\ny = np.array([14, 24, 22, 59, 66, 25, 18, 60, 39, 32, 53, 18, 55, 41, 28, 61, 35,\n   36, 52, 23, 19, 25, 73, 16, 32, 14, 31, 43, 34])\n\nintercept, slope, corr_coeff = plot_linear_regression(X, y)\nplt.show()",
            "title": "Example 1 - Ordinary Least Squares Simple Linear Regression"
        },
        {
            "location": "/user_guide/regression_utils/plot_linear_regression/#api",
            "text": "plot_linear_regression(X, y, model=LinearRegression(copy_X=True, fit_intercept=True, n_jobs=1, normalize=False), corr_func='pearsonr', scattercolor='blue', fit_style='k--', legend=True, xlim='auto')  Plot a linear regression line fit.  Parameters    X  : numpy array, shape = [n_samples,]  Samples.    y  : numpy array, shape (n_samples,)  Target values\nmodel: object (default: sklearn.linear_model.LinearRegression)\nEstimator object for regression. Must implement\na .fit() and .predict() method.\ncorr_func: str or function (default: 'pearsonr')\nUses  pearsonr  from scipy.stats if corr_func='pearsonr'.\nto compute the regression slope. If not 'pearsonr', the  corr_func ,\nthe  corr_func  parameter expects a function of the form\nfunc( ,  ) as inputs, which is expected to return\na tuple  (<correlation_coefficient>, <some_unused_value>) .\nscattercolor: string (default: blue)\nColor of scatter plot points.\nfit_style: string (default: k--)\nStyle for the line fit.\nlegend: bool (default: True)\nPlots legend with corr_coeff coef.,\nfit coef., and intercept values.\nxlim: array-like (x_min, x_max) or 'auto' (default: 'auto')\nX-axis limits for the linear line fit.    Returns    regression_fit  : tuple  intercept, slope, corr_coeff (float, float, float)",
            "title": "API"
        },
        {
            "location": "/user_guide/feature_selection/SequentialFeatureSelector/",
            "text": "Sequential Feature Selector\n\n\nImplementation of \nsequential feature algorithms\n (SFAs) -- greedy search algorithms -- that have been developed as a suboptimal solution to the computationally often not feasible exhaustive search.\n\n\n\n\nfrom mlxtend.feature_selection import SequentialFeatureSelector\n\n\n\n\nOverview\n\n\nSequential feature selection algorithms are a family of greedy search algorithms that are used to reduce an initial \nd\n-dimensional feature space to a \nk\n-dimensional feature subspace where \nk < d\n. The motivation behind feature selection algorithms is to automatically select a subset of features that is most relevant to the problem. The goal of feature selection is two-fold: We want to improve the computational efficiency and reduce the generalization error of the model by removing irrelevant features or noise. A wrapper approach such as sequential feature selection is especially useful if embedded feature selection -- for example, a regularization penalty like LASSO -- is not applicable.\n\n\nIn a nutshell, SFAs remove or add one feature at the time based on the classifier performance until a feature subset of the desired size \nk\n is reached. There are 4 different flavors of SFAs available via the \nSequentialFeatureSelector\n:\n\n\n\n\nSequential Forward Selection (SFS)\n\n\nSequential Backward Selection (SBS)\n\n\nSequential Floating Forward Selection (SFFS)\n\n\nSequential Floating Backward Selection (SFBS)\n\n\n\n\nThe \nfloating\n variants, SFFS and SFBS, can be considered as extensions to the simpler SFS and SBS algorithms. The floating algorithms have an additional exclusion or inclusion step to remove features once they were included (or excluded), so that a larger number of feature subset combinations can be sampled. It is important to emphasize that this step is conditional and only occurs if the resulting feature subset is assessed as \"better\" by the criterion function after removal (or addition) of a particular feature. Furthermore, I added an optional check to skip the conditional exclusion steps if the algorithm gets stuck in cycles.  \n\n\n\n\nHow is this different from \nRecursive Feature Elimination\n (RFE)  -- e.g., as implemented in \nsklearn.feature_selection.RFE\n? RFE is computationally less complex using the feature weight coefficients (e.g., linear models) or feature importance (tree-based algorithms) to eliminate features recursively, whereas SFSs eliminate (or add) features based on a user-defined classifier/regression performance metric.\n\n\n\n\nThe SFAs  are outlined in pseudo code below:\n\n\nSequential Forward Selection (SFS)\n\n\nInput:\n \n$Y = \\{y_1, y_2, ..., y_d\\}$\n  \n\n\n\n\nThe \nSFS\n algorithm takes the whole \n$d$\n-dimensional feature set as input.\n\n\n\n\nOutput:\n \n$X_k = \\{x_j \\; | \\;j = 1, 2, ..., k; \\; x_j \\in Y\\}$\n, where \n$k = (0, 1, 2, ..., d)$\n\n\n\n\nSFS returns a subset of features; the number of selected features \n$k$\n, where \n$k < d$\n, has to be specified \na priori\n.\n\n\n\n\nInitialization:\n \n$X_0 = \\emptyset$\n, \n$k = 0$\n\n\n\n\nWe initialize the algorithm with an empty set \n$\\emptyset$\n (\"null set\") so that \n$k = 0$\n (where \n$k$\n is the size of the subset).\n\n\n\n\nStep 1 (Inclusion):\n  \n\n\n$x^+ = \\text{ arg max } J(x_k + x), \\text{ where }  x \\in Y - X_k$\n\n\n$X_k+1 = X_k + x^+$\n\n\n$k = k + 1$\n  \n\n\nGo to Step 1\n \n\n\n\n\nin this step, we add an additional feature, \n$x^+$\n, to our feature subset \n$X_k$\n.\n\n\n$x^+$\n is the feature that maximizes our criterion function, that is, the feature that is associated with the best classifier performance if it is added to \n$X_k$\n.\n\n\nWe repeat this procedure until the termination criterion is satisfied.\n\n\n\n\nTermination:\n \n$k = p$\n\n\n\n\nWe add features from the feature subset \n$X_k$\n until the feature subset of size \n$k$\n contains the number of desired features \n$p$\n that we specified \na priori\n.\n\n\n\n\nSequential Floating Backward (SBS)\n\n\nInput:\n the set of all features, \n$Y = \\{y_1, y_2, ..., y_d\\}$\n  \n\n\n\n\nThe SBS algorithm takes the whole feature set as input.\n\n\n\n\nOutput:\n \n$X_k = \\{x_j \\; | \\;j = 1, 2, ..., k; \\; x_j \\in Y\\}$\n, where \n$k = (0, 1, 2, ..., d)$\n\n\n\n\nSBS returns a subset of features; the number of selected features \n$k$\n, where \n$k < d$\n, has to be specified \na priori\n.\n\n\n\n\nInitialization:\n \n$X_0 = Y$\n, \n$k = d$\n\n\n\n\nWe initialize the algorithm with the given feature set so that the \n$k = d$\n.\n\n\n\n\nStep 1 (Exclusion):\n  \n\n\n$x^- = \\text{ arg max } J(x_k - x), \\text{  where } x \\in X_k$\n\n\n$X_k-1 = X_k - x^-$\n\n\n$k = k - 1$\n\n\nGo to Step 1\n  \n\n\n\n\nIn this step, we remove a feature, \n$x^-$\n from our feature subset \n$X_k$\n.\n\n\n$x^-$\n is the feature that maximizes our criterion function upon re,oval, that is, the feature that is associated with the best classifier performance if it is removed from \n$X_k$\n.\n\n\nWe repeat this procedure until the termination criterion is satisfied.\n\n\n\n\nTermination:\n \n$k = p$\n\n\n\n\nWe add features from the feature subset \n$X_k$\n until the feature subset of size \n$k$\n contains the number of desired features \n$p$\n that we specified \na priori\n.\n\n\n\n\nSequential Floating Backward Selection (SFBS)\n\n\nInput:\n the set of all features, \n$Y = \\{y_1, y_2, ..., y_d\\}$\n  \n\n\n\n\nThe SFBS algorithm takes the whole feature set as input.\n\n\n\n\nOutput:\n \n$X_k = \\{x_j \\; | \\;j = 1, 2, ..., k; \\; x_j \\in Y\\}$\n, where \n$k = (0, 1, 2, ..., d)$\n\n\n\n\nSFBS returns a subset of features; the number of selected features \n$k$\n, where \n$k < d$\n, has to be specified \na priori\n.\n\n\n\n\nInitialization:\n \n$X_0 = Y$\n, \n$k = d$\n\n\n\n\nWe initialize the algorithm with the given feature set so that the \n$k = d$\n.\n\n\n\n\nStep 1 (Exclusion):\n  \n\n\n$x^- = \\text{ arg max } J(x_k - x), \\text{  where } x \\in X_k$\n\n\n$X_k-1 = X_k - x^-$\n\n\n$k = k - 1$\n\n\nGo to Step 2\n  \n\n\n\n\nIn this step, we remove a feature, \n$x^-$\n from our feature subset \n$X_k$\n.\n\n\n$x^-$\n is the feature that maximizes our criterion function upon re,oval, that is, the feature that is associated with the best classifier performance if it is removed from \n$X_k$\n.\n\n\n\n\nStep 2 (Conditional Inclusion):\n\n\n\n\n$x^+ = \\text{ arg max } J(x_k + x), \\text{ where } x \\in Y - X_k$\n\n\nif J(x_k + x) > J(x_k + x)\n:  \n\n\u00a0\u00a0\u00a0\u00a0 \n$X_k+1 = X_k + x^+$\n\n\u00a0\u00a0\u00a0\u00a0 \n$k = k + 1$\n\n\nGo to Step 1\n  \n\n\n\n\nIn Step 2, we search for features that improve the classifier performance if they are added back to the feature subset. If such features exist, we add the feature \n$x^+$\n for which the performance improvement is max.\n\n\nSteps 1 and 2 are repeated until the \nTermination\n criterion is reached.\n\n\n\n\nTermination:\n \n$k = p$\n\n\n\n\nWe add features from the feature subset \n$X_k$\n until the feature subset of size \n$k$\n contains the number of desired features \n$p$\n that we specified \na priori\n.\n\n\n\n\nSequential Floating Forward Selection (SFFS)\n\n\nInput:\n the set of all features, \n$Y = \\{y_1, y_2, ..., y_d\\}$\n  \n\n\n\n\nThe \nSFFS\n algorithm takes the whole feature set as input, if our feature space consists of, e.g. 10, if our feature space consists of 10 dimensions (\nd = 10\n).\n\n\n\n\n\nOutput:\n a subset of features, \n$X_k = \\{x_j \\; | \\;j = 1, 2, ..., k; \\; x_j \\in Y\\}$\n, where \n$k = (0, 1, 2, ..., d)$\n\n\n\n\nThe returned output of the algorithm is a subset of the feature space of a specified size. E.g., a subset of 5 features from a 10-dimensional feature space (\nk = 5, d = 10\n).\n\n\n\n\n\nInitialization:\n \n$X_0 = Y$\n, \n$k = d$\n\n\n\n\nWe initialize the algorithm with an empty set (\"null set\") so that the \nk = 0\n (where \nk\n is the size of the subset)\n\n\n\n\n\nStep 1 (Inclusion):\n\n\n\n\u00a0\u00a0\u00a0\u00a0 \n$x^+ = \\text{ arg max } J(x_k + x), \\text{ where }  x \\in Y - X_k$\n\n\u00a0\u00a0\u00a0\u00a0 \n$X_k+1 = X_k + x^+$\n\n\u00a0\u00a0\u00a0\u00a0 \n$k = k + 1$\n  \n\n\u00a0\u00a0\u00a0\u00a0\nGo to Step 2\n\n\n \n\n\nStep 2 (Conditional Exclusion):\n\n\n\n\u00a0\u00a0\u00a0\u00a0 \n$x^- = \\text{ arg max } J(x_k - x), \\text{ where } x \\in X_k$\n\n\u00a0\u00a0\u00a0\u00a0\n$if \\; J(x_k - x) > J(x_k - x)$\n:  \n\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 \n$X_k-1 = X_k - x^- $\n\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 \n$k = k - 1$\n  \n\n\u00a0\u00a0\u00a0\u00a0\nGo to Step 1\n  \n\n\n\n\nIn step 1, we include the feature from the \nfeature space\n that leads to the best performance increase for our \nfeature subset\n (assessed by the \ncriterion function\n). Then, we go over to step 2\n\n\nIn step 2, we only remove a feature if the resulting subset would gain an increase in performance. We go back to step 1.  \n\n\nSteps 1 and 2 are repeated until the \nTermination\n criterion is reached.\n\n\n\n\n\nTermination:\n stop when \nk\n equals the number of desired features\n\n\nReferences\n\n\n\n\nFerri, F., et al. \n\"Comparative study of techniques for large-scale feature selection.\"\n Pattern Recognition in Practice IV (1994): 403-413.\n\n\n\n\nExamples\n\n\nExample 1 - A simple Sequential Forward Selection example\n\n\nInitializing a simple classifier from scikit-learn:\n\n\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.datasets import load_iris\niris = load_iris()\nX = iris.data\ny = iris.target\nknn = KNeighborsClassifier(n_neighbors=4)\n\n\n\n\nWe start by selection the \"best\" 3 features from the Iris dataset via Sequential Forward Selection (SFS). Here, we set \nforward=True\n and \nfloating=False\n. By choosing \ncv=0\n, we don't perform any cross-validation, therefore, the performance (here: \n'accuracy'\n) is computed entirely on the training set. \n\n\nfrom mlxtend.feature_selection import SequentialFeatureSelector as SFS\n\nsfs1 = SFS(knn, \n           k_features=3, \n           forward=True, \n           floating=False, \n           scoring='accuracy',\n           cv=0)\nsfs1 = sfs1.fit(X, y)\n\n\n\n\nFeatures: 3/3\n\n\n\nVia the \nsubsets_\n attribute, we can take a look at the selected feature indices at each step:\n\n\nsfs1.subsets_\n\n\n\n\n{1: {'avg_score': 0.95999999999999996,\n  'cv_scores': array([ 0.96]),\n  'feature_idx': (3,)},\n 2: {'avg_score': 0.97333333333333338,\n  'cv_scores': array([ 0.97333333]),\n  'feature_idx': (2, 3)},\n 3: {'avg_score': 0.97333333333333338,\n  'cv_scores': array([ 0.97333333]),\n  'feature_idx': (1, 2, 3)}}\n\n\n\nFurthermore, we can access the indices of the 3 best features directly via the \nk_feature_idx_\n attribute:\n\n\nsfs1.k_feature_idx_\n\n\n\n\n(1, 2, 3)\n\n\n\nFinally, the prediction score for these 3 features can be accesses via \nk_score_\n:\n\n\nsfs1.k_score_\n\n\n\n\n0.97333333333333338\n\n\n\nExample 2 - Toggling between SFS, SBS, SFFS, and SFBS\n\n\nUsing the \nforward\n and \nfloating\n parameters, we can toggle between SFS, SBS, SFFS, and SFBS as shown below. Note that we are performing (stratified) 4-fold cross-validation for more robust estimates in contrast to Example 1. Via \nn_jobs=-1\n, we choose to run the cross-validation on all our available CPU cores.\n\n\n# Sequential Forward Selection\nsfs = SFS(knn, \n          k_features=3, \n          forward=True, \n          floating=False, \n          scoring='accuracy',\n          print_progress=False,\n          cv=4,\n          n_jobs=-1)\nsfs = sfs.fit(X, y)\n\nprint('\\nSequential Forward Selection (k=3):')\nprint(sfs.k_feature_idx_)\nprint('CV Score:')\nprint(sfs.k_score_)\n\n###################################################\n\n# Sequential Backward Selection\nsbs = SFS(knn, \n          k_features=3, \n          forward=False, \n          floating=False, \n          scoring='accuracy',\n          print_progress=False,\n          cv=4,\n          n_jobs=-1)\nsbs = sbs.fit(X, y)\n\nprint('\\nSequential Backward Selection (k=3):')\nprint(sbs.k_feature_idx_)\nprint('CV Score:')\nprint(sbs.k_score_)\n\n###################################################\n\n# Sequential Floating Forward Selection\nsffs = SFS(knn, \n           k_features=3, \n           forward=True, \n           floating=True, \n           scoring='accuracy',\n           print_progress=False,\n           cv=4,\n           n_jobs=-1)\nsffs = sffs.fit(X, y)\n\nprint('\\nSequential Floating Forward Selection (k=3):')\nprint(sffs.k_feature_idx_)\nprint('CV Score:')\nprint(sffs.k_score_)\n\n###################################################\n\n# Sequential Floating Backward Selection\nsfbs = SFS(knn, \n           k_features=3, \n           forward=False, \n           floating=True, \n           scoring='accuracy',\n           print_progress=False,\n           cv=4,\n           n_jobs=-1)\nsfbs = sfbs.fit(X, y)\n\nprint('\\nSequential Floating Backward Selection (k=3):')\nprint(sfbs.k_feature_idx_)\nprint('CV Score:')\nprint(sfbs.k_score_)\n\n\n\n\nSequential Forward Selection (k=3):\n(1, 2, 3)\nCV Score:\n0.972756410256\n\nSequential Backward Selection (k=3):\n(1, 2, 3)\nCV Score:\n0.972756410256\n\nSequential Floating Forward Selection (k=3):\n(1, 2, 3)\nCV Score:\n0.972756410256\n\nSequential Floating Backward Selection (k=3):\n(1, 2, 3)\nCV Score:\n0.972756410256\n\n\n\nIn this simple scenario, selecting the best 3 features out of the 4 available features in the Iris set, we end up with similar results regardless of which sequential selection algorithms we used.\n\n\nExample 3 - Visualizing the results in DataFrames\n\n\nFor our convenience, we can visualize the output from the feature selection in a pandas DataFrame format using the \nget_metric_dict\n method of the SequentialFeatureSelector object. The columns \nstd_dev\n and \nstd_err\n represent the standard deviation and standard errors of the cross-validation scores, respectively.\n\n\nBelow, we see the DataFrame of the Sequential Forward Selector from Example 2:\n\n\nimport pandas as pd\npd.DataFrame.from_dict(sfs.get_metric_dict()).T\n\n\n\n\n\n\n\n  \n\n    \n\n      \n\n      \navg_score\n\n      \nci_bound\n\n      \ncv_scores\n\n      \nfeature_idx\n\n      \nstd_dev\n\n      \nstd_err\n\n    \n\n  \n\n  \n\n    \n\n      \n1\n\n      \n0.952991\n\n      \n0.0660624\n\n      \n[0.974358974359, 0.948717948718, 0.88888888888...\n\n      \n(3,)\n\n      \n0.0412122\n\n      \n0.0237939\n\n    \n\n    \n\n      \n2\n\n      \n0.959936\n\n      \n0.0494801\n\n      \n[0.974358974359, 0.948717948718, 0.91666666666...\n\n      \n(2, 3)\n\n      \n0.0308676\n\n      \n0.0178214\n\n    \n\n    \n\n      \n3\n\n      \n0.972756\n\n      \n0.0315204\n\n      \n[0.974358974359, 1.0, 0.944444444444, 0.972222...\n\n      \n(1, 2, 3)\n\n      \n0.0196636\n\n      \n0.0113528\n\n    \n\n  \n\n\n\n\n\n\n\nNow, let's compare it to the Sequential Backward Selector:\n\n\npd.DataFrame.from_dict(sbs.get_metric_dict()).T\n\n\n\n\n\n\n\n  \n\n    \n\n      \n\n      \navg_score\n\n      \nci_bound\n\n      \ncv_scores\n\n      \nfeature_idx\n\n      \nstd_dev\n\n      \nstd_err\n\n    \n\n  \n\n  \n\n    \n\n      \n3\n\n      \n0.972756\n\n      \n0.0315204\n\n      \n[0.974358974359, 1.0, 0.944444444444, 0.972222...\n\n      \n(1, 2, 3)\n\n      \n0.0196636\n\n      \n0.0113528\n\n    \n\n    \n\n      \n4\n\n      \n0.952991\n\n      \n0.0372857\n\n      \n[0.974358974359, 0.948717948718, 0.91666666666...\n\n      \n(0, 1, 2, 3)\n\n      \n0.0232602\n\n      \n0.0134293\n\n    \n\n  \n\n\n\n\n\n\n\nWe can see that both SFS and SFBS found the same \"best\" 3 features, however, the intermediate steps where obviously different.\n\n\nThe \nci_bound\n column in the DataFrames above represents the confidence interval around the computed cross-validation scores. By default, a confidence interval of 95% is used, but we can use different confidence bounds via the \nconfidence_interval\n parameter. E.g., the confidence bounds for a 90% confidence interval can be obtained as follows:\n\n\npd.DataFrame.from_dict(sbs.get_metric_dict(confidence_interval=0.90)).T\n\n\n\n\n\n\n\n  \n\n    \n\n      \n\n      \navg_score\n\n      \nci_bound\n\n      \ncv_scores\n\n      \nfeature_idx\n\n      \nstd_dev\n\n      \nstd_err\n\n    \n\n  \n\n  \n\n    \n\n      \n3\n\n      \n0.972756\n\n      \n0.0242024\n\n      \n[0.974358974359, 1.0, 0.944444444444, 0.972222...\n\n      \n(1, 2, 3)\n\n      \n0.0196636\n\n      \n0.0113528\n\n    \n\n    \n\n      \n4\n\n      \n0.952991\n\n      \n0.0286292\n\n      \n[0.974358974359, 0.948717948718, 0.91666666666...\n\n      \n(0, 1, 2, 3)\n\n      \n0.0232602\n\n      \n0.0134293\n\n    \n\n  \n\n\n\n\n\n\n\nExample 4 - Plotting the results\n\n\nAfter importing the little helper function \nplot_sequential_feature_selection\n, we can also visualize the results using matplotlib figures.\n\n\nfrom mlxtend.feature_selection import plot_sequential_feature_selection as plot_sfs\nimport matplotlib.pyplot as plt\n\nsfs = SFS(knn, \n          k_features=4, \n          forward=True, \n          floating=False, \n          scoring='accuracy',\n          cv=5)\n\nsfs = sfs.fit(X, y)\n\nfig1 = plot_sfs(sfs.get_metric_dict(), kind='std_dev')\n\nplt.ylim([0.8, 1])\nplt.title('Sequential Forward Selection (w. StdDev)')\nplt.grid()\nplt.show()\n\n\n\n\nFeatures: 4/4\n\n\n\n\n\nExample 5 - Sequential Feature Selection for Regression\n\n\nSimilar to the classification examples above, the \nSequentialFeatureSelector\n also supports scikit-learn's estimators\nfor regression.\n\n\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.datasets import load_boston\n\nboston = load_boston()\nX, y = boston.data, boston.target\n\nlr = LinearRegression()\n\nsfs = SFS(lr, \n          k_features=13, \n          forward=True, \n          floating=False, \n          scoring='mean_squared_error',\n          cv=10)\n\nsfs = sfs.fit(X, y)\nfig = plot_sfs(sfs.get_metric_dict(), kind='std_err')\n\nplt.title('Sequential Forward Selection (w. StdDev)')\nplt.grid()\nplt.show()\n\n\n\n\nFeatures: 13/13\n\n\n\n\n\nExample 6 -- Using the Selected Feature Subset For Making New Predictions\n\n\n# Initialize the dataset\n\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.datasets import load_iris\nfrom sklearn.cross_validation import train_test_split\n\niris = load_iris()\nX, y = iris.data, iris.target\nX_train, X_test, y_train, y_test = train_test_split(\n         X, y, test_size=0.33, random_state=1)\n\nknn = KNeighborsClassifier(n_neighbors=4)\n\n\n\n\n# Select the \"best\" three features via\n# 5-fold cross-validation on the training set.\n\nfrom mlxtend.feature_selection import SequentialFeatureSelector as SFS\n\nsfs1 = SFS(knn, \n           k_features=3, \n           forward=True, \n           floating=False, \n           scoring='accuracy',\n           cv=5)\nsfs1 = sfs1.fit(X_train, y_train)\n\n\n\n\nFeatures: 3/3\n\n\n\nprint('Selected features:', sfs1.k_feature_idx_)\n\n\n\n\nSelected features: (1, 2, 3)\n\n\n\n# Generate the new subsets based on the selected features\n# Note that the transform call is equivalent to\n# X_train[:, sfs1.k_feature_idx_]\n\nX_train_sfs = sfs1.transform(X_train)\nX_test_sfs = sfs1.transform(X_test)\n\n# Fit the estimator using the new feature subset\n# and make a prediction on the test data\nknn.fit(X_train_sfs, y_train)\ny_pred = knn.predict(X_test_sfs)\n\n# Compute the accuracy of the prediction\nacc = float((y_test == y_pred).sum()) / y_pred.shape[0]\nprint('Test set accuracy: %.2f %%' % (acc*100))\n\n\n\n\nTest set accuracy: 96.00 %\n\n\n\nExample 7 -- Sequential Feature Selection and GridSearch\n\n\n# Initialize the dataset\n\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.datasets import load_iris\nfrom sklearn.cross_validation import train_test_split\n\niris = load_iris()\nX, y = iris.data, iris.target\nX_train, X_test, y_train, y_test = train_test_split(\n         X, y, test_size=0.33, random_state=1)\n\n\n\n\nUse scikit-learn's \nGridSearch\n to tune the hyperparameters inside and outside the \nSequentialFeatureSelector\n:\n\n\nfrom sklearn.grid_search import GridSearchCV\nfrom sklearn.pipeline import Pipeline\nfrom mlxtend.feature_selection import SequentialFeatureSelector as SFS\nimport mlxtend\n\nknn = KNeighborsClassifier(n_neighbors=2)\n\nsfs1 = SFS(estimator=knn, \n           k_features=3,\n           forward=True, \n           floating=False, \n           scoring='accuracy',\n           print_progress=False,\n           cv=5)\n\npipe = Pipeline([('sfs', sfs1), \n                 ('knn', knn)])\n\nparam_grid = [\n  {'sfs__k_features': [1, 2, 3, 4],\n   'sfs__estimator__n_neighbors': [1, 2, 3, 4]}\n  ]\n\ngs = GridSearchCV(estimator=pipe, \n                  param_grid=param_grid, \n                  scoring='accuracy', \n                  n_jobs=1, \n                  cv=5, \n                  verbose=1, \n                  refit=False)\n\n# run gridearch\ngs = gs.fit(X_train, y_train)\n\n\n\n\nFitting 5 folds for each of 16 candidates, totalling 80 fits\n\n\n[Parallel(n_jobs=1)]: Done  49 tasks       | elapsed:    2.6s\n[Parallel(n_jobs=1)]: Done  80 out of  80 | elapsed:    4.6s finished\n\n\n\n... and the \"best\" parameters determined by GridSearch are ...\n\n\nprint(\"Best parameters via GridSearch\", gs.best_params_)\n\n\n\n\nBest parameters via GridSearch {'sfs__k_features': 3, 'sfs__estimator__n_neighbors': 1}\n\n\n\nObtaining the best \nk\n feature indices after GridSearch\n\n\nIf we are interested in the best \nk\n feature indices via \nSequentialFeatureSelection.k_feature_idx_\n, we have to initialize a \nGridSearchCV\n object with \nrefit=True\n. Now, the grid search object will take the complete training dataset and the best parameters, which it found via cross-validation, to train the estimator pipeline.\n\n\ngs = GridSearchCV(estimator=pipe, \n                  param_grid=param_grid, \n                  scoring='accuracy', \n                  n_jobs=1, \n                  cv=5, \n                  verbose=1, \n                  refit=True)\ngs = gs.fit(X_train, y_train)\n\n\n\n\nFitting 5 folds for each of 16 candidates, totalling 80 fits\n\n\n[Parallel(n_jobs=1)]: Done  49 tasks       | elapsed:    2.6s\n[Parallel(n_jobs=1)]: Done  80 out of  80 | elapsed:    4.4s finished\n\n\n\nAfter running the grid search, we can access the individual pipeline objects of the \nbest_estimator_\n via the \nsteps\n attribute.\n\n\ngs.best_estimator_.steps\n\n\n\n\n[('sfs', SequentialFeatureSelector(clone_estimator=True, cv=5,\n               estimator=KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n             metric_params=None, n_jobs=1, n_neighbors=1, p=2,\n             weights='uniform'),\n               floating=False, forward=True, k_features=3, n_jobs=1,\n               pre_dispatch='2*n_jobs', print_progress=False,\n               scoring='accuracy', skip_if_stuck=True)),\n ('knn',\n  KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n             metric_params=None, n_jobs=1, n_neighbors=2, p=2,\n             weights='uniform'))]\n\n\n\nVia sub-indexing, we can then obtain the best-selected feature subset:\n\n\nprint('Best features:', gs.best_estimator_.steps[0][1].k_feature_idx_)\n\n\n\n\nBest features: (0, 1, 3)\n\n\n\nDuring cross-validation, this feature combination had a CV accuracy of:\n\n\nprint('Best score:', gs.best_score_)\n\n\n\n\nBest score: 0.94\n\n\n\ngs.best_params_\n\n\n\n\n{'sfs__estimator__n_neighbors': 1, 'sfs__k_features': 3}\n\n\n\nAlternatively\n, if we can set the \"best grid search parameters\" in our pipeline manually if we ran \nGridSearchCV\n with \nrefit=False\n. It should yield the same results:\n\n\npipe.set_params(**gs.best_params_).fit(X_train, y_train)\nprint('Best features:', pipe.steps[0][1].k_feature_idx_)\n\n\n\n\nBest features: (0, 1, 3)\n\n\n\nExample 8 -- Selecting the \"best\"  feature combination in a k-range\n\n\nIf \nk_features\n is set to to a tuple \n(min_k, max_k)\n (new in 0.4.2), the SFS will now select the best feature combination that it discovered by iterating from \nk=1\n to \nmax_k\n (forward), or \nmax_k\n to \nmin_k\n (backward). The size of the returned feature subset is then within \nmax_k\n to \nmin_k\n, depending on which combination scored best during cross validation.\n\n\nX.shape\n\n\n\n\n(150, 4)\n\n\n\nfrom mlxtend.feature_selection import SequentialFeatureSelector as SFS\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom mlxtend.data import wine_data\nfrom sklearn.cross_validation import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.pipeline import make_pipeline\n\nX, y = wine_data()\nX_train, X_test, y_train, y_test= train_test_split(X, y, \n                                                   stratify=y,\n                                                   test_size=0.3,\n                                                   random_state=1)\n\nknn = KNeighborsClassifier(n_neighbors=2)\n\nsfs1 = SFS(estimator=knn, \n           k_features=(3, 10),\n           forward=True, \n           floating=False, \n           scoring='accuracy',\n           print_progress=False,\n           cv=5)\n\npipe = make_pipeline(StandardScaler(), sfs1)\n\npipe.fit(X_train, y_train)\n\nprint('best combination (ACC: %.3f): %s\\n' % (sfs1.k_score_, sfs1.k_feature_idx_))\nprint('all subsets:\\n', sfs1.subsets_)\nplot_sfs(sfs1.get_metric_dict(), kind='std_err');\n\n\n\n\nbest combination (ACC: 0.984): (0, 2, 3, 6, 8, 9, 12)\n\nall subsets:\n {1: {'avg_score': 0.79045484949832778, 'feature_idx': (6,), 'cv_scores': array([ 0.76923077,  0.68      ,  0.84      ,  0.75      ,  0.91304348])}, 2: {'avg_score': 0.92686510590858406, 'feature_idx': (9, 6), 'cv_scores': array([ 0.88461538,  0.96      ,  0.96      ,  0.91666667,  0.91304348])}, 3: {'avg_score': 0.95927870680044602, 'feature_idx': (9, 12, 6), 'cv_scores': array([ 0.96153846,  0.96      ,  0.96      ,  0.95833333,  0.95652174])}, 4: {'avg_score': 0.96791973244147156, 'feature_idx': (0, 9, 12, 6), 'cv_scores': array([ 0.92307692,  0.96      ,  1.        ,  1.        ,  0.95652174])}, 5: {'avg_score': 0.98361204013377923, 'feature_idx': (0, 9, 3, 12, 6), 'cv_scores': array([ 0.96153846,  1.        ,  1.        ,  1.        ,  0.95652174])}, 6: {'avg_score': 0.97561204013377922, 'feature_idx': (0, 2, 3, 6, 9, 12), 'cv_scores': array([ 0.96153846,  1.        ,  0.96      ,  1.        ,  0.95652174])}, 7: {'avg_score': 0.98399999999999999, 'feature_idx': (0, 2, 3, 6, 8, 9, 12), 'cv_scores': array([ 1.  ,  0.92,  1.  ,  1.  ,  1.  ])}, 8: {'avg_score': 0.98399999999999999, 'feature_idx': (0, 2, 3, 6, 8, 9, 10, 12), 'cv_scores': array([ 1.  ,  0.96,  0.96,  1.  ,  1.  ])}, 9: {'avg_score': 0.98399999999999999, 'feature_idx': (0, 2, 3, 6, 8, 9, 10, 11, 12), 'cv_scores': array([ 1.  ,  0.96,  0.96,  1.  ,  1.  ])}, 10: {'avg_score': 0.96830769230769231, 'feature_idx': (0, 2, 3, 6, 7, 8, 9, 10, 11, 12), 'cv_scores': array([ 0.96153846,  0.92      ,  0.96      ,  1.        ,  1.        ])}}\n\n\n\n\n\nAPI\n\n\nSequentialFeatureSelector(estimator, k_features='best', forward=True, floating=False, print_progress=True, scoring='accuracy', cv=5, skip_if_stuck=True, n_jobs=1, pre_dispatch='2\nn_jobs', clone_estimator=True)*\n\n\nSequential Feature Selection for Classification and Regression.\n\n\nParameters\n\n\n\n\n\n\nestimator\n : scikit-learn classifier or regressor\n\n\n\n\n\n\nk_features\n : int or tuple (new in 0.4.2) (default: 1)\n\n\nNumber of features to select,\nwhere k_features < the full feature set.\nNew in 0.4.2: A tuple containing a min and max value can be provided,\nand the SFS will consider return any feature combination between\nmin and max that scored highest in cross-validtion. For example,\nthe tuple (1, 4) will return any combination from\n1 up to 4 features instead of a fixed number of features k.\n\n\n\n\n\n\nforward\n : bool (default: True)\n\n\nForward selection if True,\nbackward selection otherwise\n\n\n\n\n\n\nfloating\n : bool (default: False)\n\n\nAdds a conditional exclusion/inclusion if True.\n\n\n\n\n\n\nprint_progress\n : bool (default: True)\n\n\nPrints progress as the number of epochs\nto stderr.\n\n\n\n\n\n\nscoring\n : str, (default='accuracy')\n\n\nScoring metric in {accuracy, f1, precision, recall, roc_auc}\nfor classifiers,\n{'mean_absolute_error', 'mean_squared_error',\n'median_absolute_error', 'r2'} for regressors,\nor a callable object or function with\nsignature \nscorer(estimator, X, y)\n.\n\n\n\n\n\n\ncv\n : int (default: 5)\n\n\nScikit-learn cross-validation generator or \nint\n.\nIf estimator is a classifier (or y consists of integer class labels),\nstratified k-fold is performed, and regular k-fold cross-validation\notherwise.\nNo cross-validation if cv is None, False, or 0.\nskip_if_stuck: bool (default: True)\nSet to True to skip conditional\nexclusion/inclusion if floating=True and\nalgorithm gets stuck in cycles.\n\n\n\n\n\n\nn_jobs\n : int (default: 1)\n\n\nThe number of CPUs to use for cross validation. -1 means 'all CPUs'.\n\n\n\n\n\n\npre_dispatch\n : int, or string (default: '2*n_jobs')\n\n\nControls the number of jobs that get dispatched\nduring parallel execution in cross_val_score.\nReducing this number can be useful to avoid an explosion of\nmemory consumption when more jobs get dispatched than CPUs can process.\nThis parameter can be:\nNone, in which case all the jobs are immediately created and spawned.\nUse this for lightweight and fast-running jobs,\nto avoid delays due to on-demand spawning of the jobs\nAn int, giving the exact number of total jobs that are spawned\nA string, giving an expression as a function\nof n_jobs, as in \n2*n_jobs\n\n\n\n\n\n\nclone_estimator\n : bool (default: True)\n\n\nClones estimator if True; works with the original estimator instance\nif False. Set to False if the estimator doesn't\nimplement scikit-learn's set_params and get_params methods.\nIn addition, it is required to set cv=0, and n_jobs=1.\n\n\n\n\n\n\nAttributes\n\n\n\n\n\n\nk_feature_idx_\n : array-like, shape = [n_predictions]\n\n\nFeature Indices of the selected feature subsets.\n\n\n\n\n\n\nk_score_\n : float\n\n\nCross validation average score of the selected subset.\n\n\n\n\n\n\nsubsets_\n : dict\n\n\nA dictionary of selected feature subsets during the\nsequential selection, where the dictionary keys are\nthe lengths k of these feature subsets. The dictionary\nvalues are dictionaries themselves with the following\nkeys: 'feature_idx' (tuple of indices of the feature subset)\n'cv_scores' (list individual cross-validation scores)\n'avg_score' (average cross-validation score)\n\n\n\n\n\n\nMethods\n\n\n\n\n\nfit(X, y)\n\n\nPerform feature selection and learn model from training data.\n\n\nParameters\n\n\n\n\n\n\nX\n : {array-like, sparse matrix}, shape = [n_samples, n_features]\n\n\nTraining vectors, where n_samples is the number of samples and\nn_features is the number of features.\n\n\n\n\n\n\ny\n : array-like, shape = [n_samples]\n\n\nTarget values.\n\n\n\n\n\n\nReturns\n\n\n\n\nself\n : object\n\n\n\n\n\n\n\nfit_transform(X, y)\n\n\nFit to training data then reduce X to its most important features.\n\n\nParameters\n\n\n\n\n\n\nX\n : {array-like, sparse matrix}, shape = [n_samples, n_features]\n\n\nTraining vectors, where n_samples is the number of samples and\nn_features is the number of features.\n\n\n\n\n\n\nReturns\n\n\nReduced feature subset of X, shape={n_samples, k_features}\n\n\n\n\n\nget_metric_dict(confidence_interval=0.95)\n\n\nReturn metric dictionary\n\n\nParameters\n\n\n\n\n\n\nconfidence_interval\n : float (default: 0.95)\n\n\nA positive float between 0.0 and 1.0 to compute the confidence\ninterval bounds of the CV score averages.\n\n\n\n\n\n\nReturns\n\n\nDictionary with items where each dictionary value is a list\n    with the number of iterations (number of feature subsets) as\n    its length. The dictionary keys corresponding to these lists\n    are as follows:\n    'feature_idx': tuple of the indices of the feature subset\n    'cv_scores': list with individual CV scores\n    'avg_score': of CV average scores\n    'std_dev': standard deviation of the CV score average\n    'std_err': standard error of the CV score average\n    'ci_bound': confidence interval bound of the CV score average\n\n\n\n\n\nget_params(deep=True)\n\n\nGet parameters for this estimator.\n\n\nParameters\n\n\ndeep: boolean, optional\n    If True, will return the parameters for this estimator and\n    contained subobjects that are estimators.\n\n\nReturns\n\n\n\n\n\n\nparams\n : mapping of string to any\n\n\nParameter names mapped to their values.\n\n\n\n\n\n\n\n\n\nset_params(\nparams)\n\n\nSet the parameters of this estimator.\n\n\nThe method works on simple estimators as well as on nested objects\n(such as pipelines). The former have parameters of the form\n``<component>__<parameter>`` so that it's possible to update each\ncomponent of a nested object.\n\n\n\nReturns\n\n\nself\n\n\n\n\n\ntransform(X)\n\n\nReduce X to its most important features.\n\n\nParameters\n\n\n\n\n\n\nX\n : {array-like, sparse matrix}, shape = [n_samples, n_features]\n\n\nTraining vectors, where n_samples is the number of samples and\nn_features is the number of features.\n\n\n\n\n\n\nReturns\n\n\nReduced feature subset of X, shape={n_samples, k_features}\n\n\n\n\nplot_sequential_feature_selection(metric_dict, kind='std_dev', color='blue', bcolor='steelblue', marker='o', alpha=0.2, ylabel='Performance', confidence_interval=0.95)\n\n\nPlot sequential feature selection.\n\n\nParameters\n\n\n\n\n\n\nmetric_dict\n : mlxtend.SequentialFeatureSelector.get_metric_dict() object\n\n\n\n\n\n\nkind\n : str (default: \"std_dev\")\n\n\nThe kind of error bar or confidence interval in\n{'std_dev', 'std_err', 'ci', None}.\n\n\n\n\n\n\ncolor\n : str (default: \"blue\")\n\n\nColor of the lineplot (accepts any matplotlib color name)\n\n\n\n\n\n\nbcolor\n : str (default: \"steelblue\").\n\n\nColor of the error bars / confidence intervals\n(accepts any matplotlib color name).\n\n\n\n\n\n\nmarker\n : str (default: \"o\")\n\n\nMarker of the line plot\n(accepts any matplotlib marker name).\n\n\n\n\n\n\nalpha\n : float in [0, 1] (default: 0.2)\n\n\nTransparency of the error bars / confidence intervals.\n\n\n\n\n\n\nylabel\n : str (default: \"Performance\")\n\n\nY-axis label.\n\n\n\n\n\n\nconfidence_interval\n : float (default: 0.95)\n\n\nConfidence level if \nkind='ci'\n.\n\n\n\n\n\n\nReturns\n\n\n\n\nfig\n : matplotlib.pyplot.figure() object",
            "title": "SequentialFeatureSelector"
        },
        {
            "location": "/user_guide/feature_selection/SequentialFeatureSelector/#sequential-feature-selector",
            "text": "Implementation of  sequential feature algorithms  (SFAs) -- greedy search algorithms -- that have been developed as a suboptimal solution to the computationally often not feasible exhaustive search.   from mlxtend.feature_selection import SequentialFeatureSelector",
            "title": "Sequential Feature Selector"
        },
        {
            "location": "/user_guide/feature_selection/SequentialFeatureSelector/#overview",
            "text": "Sequential feature selection algorithms are a family of greedy search algorithms that are used to reduce an initial  d -dimensional feature space to a  k -dimensional feature subspace where  k < d . The motivation behind feature selection algorithms is to automatically select a subset of features that is most relevant to the problem. The goal of feature selection is two-fold: We want to improve the computational efficiency and reduce the generalization error of the model by removing irrelevant features or noise. A wrapper approach such as sequential feature selection is especially useful if embedded feature selection -- for example, a regularization penalty like LASSO -- is not applicable.  In a nutshell, SFAs remove or add one feature at the time based on the classifier performance until a feature subset of the desired size  k  is reached. There are 4 different flavors of SFAs available via the  SequentialFeatureSelector :   Sequential Forward Selection (SFS)  Sequential Backward Selection (SBS)  Sequential Floating Forward Selection (SFFS)  Sequential Floating Backward Selection (SFBS)   The  floating  variants, SFFS and SFBS, can be considered as extensions to the simpler SFS and SBS algorithms. The floating algorithms have an additional exclusion or inclusion step to remove features once they were included (or excluded), so that a larger number of feature subset combinations can be sampled. It is important to emphasize that this step is conditional and only occurs if the resulting feature subset is assessed as \"better\" by the criterion function after removal (or addition) of a particular feature. Furthermore, I added an optional check to skip the conditional exclusion steps if the algorithm gets stuck in cycles.     How is this different from  Recursive Feature Elimination  (RFE)  -- e.g., as implemented in  sklearn.feature_selection.RFE ? RFE is computationally less complex using the feature weight coefficients (e.g., linear models) or feature importance (tree-based algorithms) to eliminate features recursively, whereas SFSs eliminate (or add) features based on a user-defined classifier/regression performance metric.   The SFAs  are outlined in pseudo code below:",
            "title": "Overview"
        },
        {
            "location": "/user_guide/feature_selection/SequentialFeatureSelector/#sequential-forward-selection-sfs",
            "text": "Input:   $Y = \\{y_1, y_2, ..., y_d\\}$      The  SFS  algorithm takes the whole  $d$ -dimensional feature set as input.   Output:   $X_k = \\{x_j \\; | \\;j = 1, 2, ..., k; \\; x_j \\in Y\\}$ , where  $k = (0, 1, 2, ..., d)$   SFS returns a subset of features; the number of selected features  $k$ , where  $k < d$ , has to be specified  a priori .   Initialization:   $X_0 = \\emptyset$ ,  $k = 0$   We initialize the algorithm with an empty set  $\\emptyset$  (\"null set\") so that  $k = 0$  (where  $k$  is the size of the subset).   Step 1 (Inclusion):     $x^+ = \\text{ arg max } J(x_k + x), \\text{ where }  x \\in Y - X_k$  $X_k+1 = X_k + x^+$  $k = k + 1$     Go to Step 1     in this step, we add an additional feature,  $x^+$ , to our feature subset  $X_k$ .  $x^+$  is the feature that maximizes our criterion function, that is, the feature that is associated with the best classifier performance if it is added to  $X_k$ .  We repeat this procedure until the termination criterion is satisfied.   Termination:   $k = p$   We add features from the feature subset  $X_k$  until the feature subset of size  $k$  contains the number of desired features  $p$  that we specified  a priori .",
            "title": "Sequential Forward Selection (SFS)"
        },
        {
            "location": "/user_guide/feature_selection/SequentialFeatureSelector/#sequential-floating-backward-sbs",
            "text": "Input:  the set of all features,  $Y = \\{y_1, y_2, ..., y_d\\}$      The SBS algorithm takes the whole feature set as input.   Output:   $X_k = \\{x_j \\; | \\;j = 1, 2, ..., k; \\; x_j \\in Y\\}$ , where  $k = (0, 1, 2, ..., d)$   SBS returns a subset of features; the number of selected features  $k$ , where  $k < d$ , has to be specified  a priori .   Initialization:   $X_0 = Y$ ,  $k = d$   We initialize the algorithm with the given feature set so that the  $k = d$ .   Step 1 (Exclusion):     $x^- = \\text{ arg max } J(x_k - x), \\text{  where } x \\in X_k$  $X_k-1 = X_k - x^-$  $k = k - 1$  Go to Step 1      In this step, we remove a feature,  $x^-$  from our feature subset  $X_k$ .  $x^-$  is the feature that maximizes our criterion function upon re,oval, that is, the feature that is associated with the best classifier performance if it is removed from  $X_k$ .  We repeat this procedure until the termination criterion is satisfied.   Termination:   $k = p$   We add features from the feature subset  $X_k$  until the feature subset of size  $k$  contains the number of desired features  $p$  that we specified  a priori .",
            "title": "Sequential Floating Backward (SBS)"
        },
        {
            "location": "/user_guide/feature_selection/SequentialFeatureSelector/#sequential-floating-backward-selection-sfbs",
            "text": "Input:  the set of all features,  $Y = \\{y_1, y_2, ..., y_d\\}$      The SFBS algorithm takes the whole feature set as input.   Output:   $X_k = \\{x_j \\; | \\;j = 1, 2, ..., k; \\; x_j \\in Y\\}$ , where  $k = (0, 1, 2, ..., d)$   SFBS returns a subset of features; the number of selected features  $k$ , where  $k < d$ , has to be specified  a priori .   Initialization:   $X_0 = Y$ ,  $k = d$   We initialize the algorithm with the given feature set so that the  $k = d$ .   Step 1 (Exclusion):     $x^- = \\text{ arg max } J(x_k - x), \\text{  where } x \\in X_k$  $X_k-1 = X_k - x^-$  $k = k - 1$  Go to Step 2      In this step, we remove a feature,  $x^-$  from our feature subset  $X_k$ .  $x^-$  is the feature that maximizes our criterion function upon re,oval, that is, the feature that is associated with the best classifier performance if it is removed from  $X_k$ .   Step 2 (Conditional Inclusion):   $x^+ = \\text{ arg max } J(x_k + x), \\text{ where } x \\in Y - X_k$  if J(x_k + x) > J(x_k + x) :   \n\u00a0\u00a0\u00a0\u00a0  $X_k+1 = X_k + x^+$ \n\u00a0\u00a0\u00a0\u00a0  $k = k + 1$  Go to Step 1      In Step 2, we search for features that improve the classifier performance if they are added back to the feature subset. If such features exist, we add the feature  $x^+$  for which the performance improvement is max.  Steps 1 and 2 are repeated until the  Termination  criterion is reached.   Termination:   $k = p$   We add features from the feature subset  $X_k$  until the feature subset of size  $k$  contains the number of desired features  $p$  that we specified  a priori .",
            "title": "Sequential Floating Backward Selection (SFBS)"
        },
        {
            "location": "/user_guide/feature_selection/SequentialFeatureSelector/#sequential-floating-forward-selection-sffs",
            "text": "Input:  the set of all features,  $Y = \\{y_1, y_2, ..., y_d\\}$      The  SFFS  algorithm takes the whole feature set as input, if our feature space consists of, e.g. 10, if our feature space consists of 10 dimensions ( d = 10 ).   Output:  a subset of features,  $X_k = \\{x_j \\; | \\;j = 1, 2, ..., k; \\; x_j \\in Y\\}$ , where  $k = (0, 1, 2, ..., d)$   The returned output of the algorithm is a subset of the feature space of a specified size. E.g., a subset of 5 features from a 10-dimensional feature space ( k = 5, d = 10 ).   Initialization:   $X_0 = Y$ ,  $k = d$   We initialize the algorithm with an empty set (\"null set\") so that the  k = 0  (where  k  is the size of the subset)   Step 1 (Inclusion):  \n\u00a0\u00a0\u00a0\u00a0  $x^+ = \\text{ arg max } J(x_k + x), \\text{ where }  x \\in Y - X_k$ \n\u00a0\u00a0\u00a0\u00a0  $X_k+1 = X_k + x^+$ \n\u00a0\u00a0\u00a0\u00a0  $k = k + 1$    \n\u00a0\u00a0\u00a0\u00a0 Go to Step 2     Step 2 (Conditional Exclusion):  \n\u00a0\u00a0\u00a0\u00a0  $x^- = \\text{ arg max } J(x_k - x), \\text{ where } x \\in X_k$ \n\u00a0\u00a0\u00a0\u00a0 $if \\; J(x_k - x) > J(x_k - x)$ :   \n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0  $X_k-1 = X_k - x^- $ \n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0  $k = k - 1$    \n\u00a0\u00a0\u00a0\u00a0 Go to Step 1      In step 1, we include the feature from the  feature space  that leads to the best performance increase for our  feature subset  (assessed by the  criterion function ). Then, we go over to step 2  In step 2, we only remove a feature if the resulting subset would gain an increase in performance. We go back to step 1.    Steps 1 and 2 are repeated until the  Termination  criterion is reached.   Termination:  stop when  k  equals the number of desired features",
            "title": "Sequential Floating Forward Selection (SFFS)"
        },
        {
            "location": "/user_guide/feature_selection/SequentialFeatureSelector/#references",
            "text": "Ferri, F., et al.  \"Comparative study of techniques for large-scale feature selection.\"  Pattern Recognition in Practice IV (1994): 403-413.",
            "title": "References"
        },
        {
            "location": "/user_guide/feature_selection/SequentialFeatureSelector/#examples",
            "text": "",
            "title": "Examples"
        },
        {
            "location": "/user_guide/feature_selection/SequentialFeatureSelector/#example-1-a-simple-sequential-forward-selection-example",
            "text": "Initializing a simple classifier from scikit-learn:  from sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.datasets import load_iris\niris = load_iris()\nX = iris.data\ny = iris.target\nknn = KNeighborsClassifier(n_neighbors=4)  We start by selection the \"best\" 3 features from the Iris dataset via Sequential Forward Selection (SFS). Here, we set  forward=True  and  floating=False . By choosing  cv=0 , we don't perform any cross-validation, therefore, the performance (here:  'accuracy' ) is computed entirely on the training set.   from mlxtend.feature_selection import SequentialFeatureSelector as SFS\n\nsfs1 = SFS(knn, \n           k_features=3, \n           forward=True, \n           floating=False, \n           scoring='accuracy',\n           cv=0)\nsfs1 = sfs1.fit(X, y)  Features: 3/3  Via the  subsets_  attribute, we can take a look at the selected feature indices at each step:  sfs1.subsets_  {1: {'avg_score': 0.95999999999999996,\n  'cv_scores': array([ 0.96]),\n  'feature_idx': (3,)},\n 2: {'avg_score': 0.97333333333333338,\n  'cv_scores': array([ 0.97333333]),\n  'feature_idx': (2, 3)},\n 3: {'avg_score': 0.97333333333333338,\n  'cv_scores': array([ 0.97333333]),\n  'feature_idx': (1, 2, 3)}}  Furthermore, we can access the indices of the 3 best features directly via the  k_feature_idx_  attribute:  sfs1.k_feature_idx_  (1, 2, 3)  Finally, the prediction score for these 3 features can be accesses via  k_score_ :  sfs1.k_score_  0.97333333333333338",
            "title": "Example 1 - A simple Sequential Forward Selection example"
        },
        {
            "location": "/user_guide/feature_selection/SequentialFeatureSelector/#example-2-toggling-between-sfs-sbs-sffs-and-sfbs",
            "text": "Using the  forward  and  floating  parameters, we can toggle between SFS, SBS, SFFS, and SFBS as shown below. Note that we are performing (stratified) 4-fold cross-validation for more robust estimates in contrast to Example 1. Via  n_jobs=-1 , we choose to run the cross-validation on all our available CPU cores.  # Sequential Forward Selection\nsfs = SFS(knn, \n          k_features=3, \n          forward=True, \n          floating=False, \n          scoring='accuracy',\n          print_progress=False,\n          cv=4,\n          n_jobs=-1)\nsfs = sfs.fit(X, y)\n\nprint('\\nSequential Forward Selection (k=3):')\nprint(sfs.k_feature_idx_)\nprint('CV Score:')\nprint(sfs.k_score_)\n\n###################################################\n\n# Sequential Backward Selection\nsbs = SFS(knn, \n          k_features=3, \n          forward=False, \n          floating=False, \n          scoring='accuracy',\n          print_progress=False,\n          cv=4,\n          n_jobs=-1)\nsbs = sbs.fit(X, y)\n\nprint('\\nSequential Backward Selection (k=3):')\nprint(sbs.k_feature_idx_)\nprint('CV Score:')\nprint(sbs.k_score_)\n\n###################################################\n\n# Sequential Floating Forward Selection\nsffs = SFS(knn, \n           k_features=3, \n           forward=True, \n           floating=True, \n           scoring='accuracy',\n           print_progress=False,\n           cv=4,\n           n_jobs=-1)\nsffs = sffs.fit(X, y)\n\nprint('\\nSequential Floating Forward Selection (k=3):')\nprint(sffs.k_feature_idx_)\nprint('CV Score:')\nprint(sffs.k_score_)\n\n###################################################\n\n# Sequential Floating Backward Selection\nsfbs = SFS(knn, \n           k_features=3, \n           forward=False, \n           floating=True, \n           scoring='accuracy',\n           print_progress=False,\n           cv=4,\n           n_jobs=-1)\nsfbs = sfbs.fit(X, y)\n\nprint('\\nSequential Floating Backward Selection (k=3):')\nprint(sfbs.k_feature_idx_)\nprint('CV Score:')\nprint(sfbs.k_score_)  Sequential Forward Selection (k=3):\n(1, 2, 3)\nCV Score:\n0.972756410256\n\nSequential Backward Selection (k=3):\n(1, 2, 3)\nCV Score:\n0.972756410256\n\nSequential Floating Forward Selection (k=3):\n(1, 2, 3)\nCV Score:\n0.972756410256\n\nSequential Floating Backward Selection (k=3):\n(1, 2, 3)\nCV Score:\n0.972756410256  In this simple scenario, selecting the best 3 features out of the 4 available features in the Iris set, we end up with similar results regardless of which sequential selection algorithms we used.",
            "title": "Example 2 - Toggling between SFS, SBS, SFFS, and SFBS"
        },
        {
            "location": "/user_guide/feature_selection/SequentialFeatureSelector/#example-3-visualizing-the-results-in-dataframes",
            "text": "For our convenience, we can visualize the output from the feature selection in a pandas DataFrame format using the  get_metric_dict  method of the SequentialFeatureSelector object. The columns  std_dev  and  std_err  represent the standard deviation and standard errors of the cross-validation scores, respectively.  Below, we see the DataFrame of the Sequential Forward Selector from Example 2:  import pandas as pd\npd.DataFrame.from_dict(sfs.get_metric_dict()).T   \n   \n     \n       \n       avg_score \n       ci_bound \n       cv_scores \n       feature_idx \n       std_dev \n       std_err \n     \n   \n   \n     \n       1 \n       0.952991 \n       0.0660624 \n       [0.974358974359, 0.948717948718, 0.88888888888... \n       (3,) \n       0.0412122 \n       0.0237939 \n     \n     \n       2 \n       0.959936 \n       0.0494801 \n       [0.974358974359, 0.948717948718, 0.91666666666... \n       (2, 3) \n       0.0308676 \n       0.0178214 \n     \n     \n       3 \n       0.972756 \n       0.0315204 \n       [0.974358974359, 1.0, 0.944444444444, 0.972222... \n       (1, 2, 3) \n       0.0196636 \n       0.0113528 \n     \n      Now, let's compare it to the Sequential Backward Selector:  pd.DataFrame.from_dict(sbs.get_metric_dict()).T   \n   \n     \n       \n       avg_score \n       ci_bound \n       cv_scores \n       feature_idx \n       std_dev \n       std_err \n     \n   \n   \n     \n       3 \n       0.972756 \n       0.0315204 \n       [0.974358974359, 1.0, 0.944444444444, 0.972222... \n       (1, 2, 3) \n       0.0196636 \n       0.0113528 \n     \n     \n       4 \n       0.952991 \n       0.0372857 \n       [0.974358974359, 0.948717948718, 0.91666666666... \n       (0, 1, 2, 3) \n       0.0232602 \n       0.0134293 \n     \n      We can see that both SFS and SFBS found the same \"best\" 3 features, however, the intermediate steps where obviously different.  The  ci_bound  column in the DataFrames above represents the confidence interval around the computed cross-validation scores. By default, a confidence interval of 95% is used, but we can use different confidence bounds via the  confidence_interval  parameter. E.g., the confidence bounds for a 90% confidence interval can be obtained as follows:  pd.DataFrame.from_dict(sbs.get_metric_dict(confidence_interval=0.90)).T   \n   \n     \n       \n       avg_score \n       ci_bound \n       cv_scores \n       feature_idx \n       std_dev \n       std_err \n     \n   \n   \n     \n       3 \n       0.972756 \n       0.0242024 \n       [0.974358974359, 1.0, 0.944444444444, 0.972222... \n       (1, 2, 3) \n       0.0196636 \n       0.0113528 \n     \n     \n       4 \n       0.952991 \n       0.0286292 \n       [0.974358974359, 0.948717948718, 0.91666666666... \n       (0, 1, 2, 3) \n       0.0232602 \n       0.0134293",
            "title": "Example 3 - Visualizing the results in DataFrames"
        },
        {
            "location": "/user_guide/feature_selection/SequentialFeatureSelector/#example-4-plotting-the-results",
            "text": "After importing the little helper function  plot_sequential_feature_selection , we can also visualize the results using matplotlib figures.  from mlxtend.feature_selection import plot_sequential_feature_selection as plot_sfs\nimport matplotlib.pyplot as plt\n\nsfs = SFS(knn, \n          k_features=4, \n          forward=True, \n          floating=False, \n          scoring='accuracy',\n          cv=5)\n\nsfs = sfs.fit(X, y)\n\nfig1 = plot_sfs(sfs.get_metric_dict(), kind='std_dev')\n\nplt.ylim([0.8, 1])\nplt.title('Sequential Forward Selection (w. StdDev)')\nplt.grid()\nplt.show()  Features: 4/4",
            "title": "Example 4 - Plotting the results"
        },
        {
            "location": "/user_guide/feature_selection/SequentialFeatureSelector/#example-5-sequential-feature-selection-for-regression",
            "text": "Similar to the classification examples above, the  SequentialFeatureSelector  also supports scikit-learn's estimators\nfor regression.  from sklearn.linear_model import LinearRegression\nfrom sklearn.datasets import load_boston\n\nboston = load_boston()\nX, y = boston.data, boston.target\n\nlr = LinearRegression()\n\nsfs = SFS(lr, \n          k_features=13, \n          forward=True, \n          floating=False, \n          scoring='mean_squared_error',\n          cv=10)\n\nsfs = sfs.fit(X, y)\nfig = plot_sfs(sfs.get_metric_dict(), kind='std_err')\n\nplt.title('Sequential Forward Selection (w. StdDev)')\nplt.grid()\nplt.show()  Features: 13/13",
            "title": "Example 5 - Sequential Feature Selection for Regression"
        },
        {
            "location": "/user_guide/feature_selection/SequentialFeatureSelector/#example-6-using-the-selected-feature-subset-for-making-new-predictions",
            "text": "# Initialize the dataset\n\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.datasets import load_iris\nfrom sklearn.cross_validation import train_test_split\n\niris = load_iris()\nX, y = iris.data, iris.target\nX_train, X_test, y_train, y_test = train_test_split(\n         X, y, test_size=0.33, random_state=1)\n\nknn = KNeighborsClassifier(n_neighbors=4)  # Select the \"best\" three features via\n# 5-fold cross-validation on the training set.\n\nfrom mlxtend.feature_selection import SequentialFeatureSelector as SFS\n\nsfs1 = SFS(knn, \n           k_features=3, \n           forward=True, \n           floating=False, \n           scoring='accuracy',\n           cv=5)\nsfs1 = sfs1.fit(X_train, y_train)  Features: 3/3  print('Selected features:', sfs1.k_feature_idx_)  Selected features: (1, 2, 3)  # Generate the new subsets based on the selected features\n# Note that the transform call is equivalent to\n# X_train[:, sfs1.k_feature_idx_]\n\nX_train_sfs = sfs1.transform(X_train)\nX_test_sfs = sfs1.transform(X_test)\n\n# Fit the estimator using the new feature subset\n# and make a prediction on the test data\nknn.fit(X_train_sfs, y_train)\ny_pred = knn.predict(X_test_sfs)\n\n# Compute the accuracy of the prediction\nacc = float((y_test == y_pred).sum()) / y_pred.shape[0]\nprint('Test set accuracy: %.2f %%' % (acc*100))  Test set accuracy: 96.00 %",
            "title": "Example 6 -- Using the Selected Feature Subset For Making New Predictions"
        },
        {
            "location": "/user_guide/feature_selection/SequentialFeatureSelector/#example-7-sequential-feature-selection-and-gridsearch",
            "text": "# Initialize the dataset\n\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.datasets import load_iris\nfrom sklearn.cross_validation import train_test_split\n\niris = load_iris()\nX, y = iris.data, iris.target\nX_train, X_test, y_train, y_test = train_test_split(\n         X, y, test_size=0.33, random_state=1)  Use scikit-learn's  GridSearch  to tune the hyperparameters inside and outside the  SequentialFeatureSelector :  from sklearn.grid_search import GridSearchCV\nfrom sklearn.pipeline import Pipeline\nfrom mlxtend.feature_selection import SequentialFeatureSelector as SFS\nimport mlxtend\n\nknn = KNeighborsClassifier(n_neighbors=2)\n\nsfs1 = SFS(estimator=knn, \n           k_features=3,\n           forward=True, \n           floating=False, \n           scoring='accuracy',\n           print_progress=False,\n           cv=5)\n\npipe = Pipeline([('sfs', sfs1), \n                 ('knn', knn)])\n\nparam_grid = [\n  {'sfs__k_features': [1, 2, 3, 4],\n   'sfs__estimator__n_neighbors': [1, 2, 3, 4]}\n  ]\n\ngs = GridSearchCV(estimator=pipe, \n                  param_grid=param_grid, \n                  scoring='accuracy', \n                  n_jobs=1, \n                  cv=5, \n                  verbose=1, \n                  refit=False)\n\n# run gridearch\ngs = gs.fit(X_train, y_train)  Fitting 5 folds for each of 16 candidates, totalling 80 fits\n\n\n[Parallel(n_jobs=1)]: Done  49 tasks       | elapsed:    2.6s\n[Parallel(n_jobs=1)]: Done  80 out of  80 | elapsed:    4.6s finished  ... and the \"best\" parameters determined by GridSearch are ...  print(\"Best parameters via GridSearch\", gs.best_params_)  Best parameters via GridSearch {'sfs__k_features': 3, 'sfs__estimator__n_neighbors': 1}",
            "title": "Example 7 -- Sequential Feature Selection and GridSearch"
        },
        {
            "location": "/user_guide/feature_selection/SequentialFeatureSelector/#obtaining-the-best-k-feature-indices-after-gridsearch",
            "text": "If we are interested in the best  k  feature indices via  SequentialFeatureSelection.k_feature_idx_ , we have to initialize a  GridSearchCV  object with  refit=True . Now, the grid search object will take the complete training dataset and the best parameters, which it found via cross-validation, to train the estimator pipeline.  gs = GridSearchCV(estimator=pipe, \n                  param_grid=param_grid, \n                  scoring='accuracy', \n                  n_jobs=1, \n                  cv=5, \n                  verbose=1, \n                  refit=True)\ngs = gs.fit(X_train, y_train)  Fitting 5 folds for each of 16 candidates, totalling 80 fits\n\n\n[Parallel(n_jobs=1)]: Done  49 tasks       | elapsed:    2.6s\n[Parallel(n_jobs=1)]: Done  80 out of  80 | elapsed:    4.4s finished  After running the grid search, we can access the individual pipeline objects of the  best_estimator_  via the  steps  attribute.  gs.best_estimator_.steps  [('sfs', SequentialFeatureSelector(clone_estimator=True, cv=5,\n               estimator=KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n             metric_params=None, n_jobs=1, n_neighbors=1, p=2,\n             weights='uniform'),\n               floating=False, forward=True, k_features=3, n_jobs=1,\n               pre_dispatch='2*n_jobs', print_progress=False,\n               scoring='accuracy', skip_if_stuck=True)),\n ('knn',\n  KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n             metric_params=None, n_jobs=1, n_neighbors=2, p=2,\n             weights='uniform'))]  Via sub-indexing, we can then obtain the best-selected feature subset:  print('Best features:', gs.best_estimator_.steps[0][1].k_feature_idx_)  Best features: (0, 1, 3)  During cross-validation, this feature combination had a CV accuracy of:  print('Best score:', gs.best_score_)  Best score: 0.94  gs.best_params_  {'sfs__estimator__n_neighbors': 1, 'sfs__k_features': 3}  Alternatively , if we can set the \"best grid search parameters\" in our pipeline manually if we ran  GridSearchCV  with  refit=False . It should yield the same results:  pipe.set_params(**gs.best_params_).fit(X_train, y_train)\nprint('Best features:', pipe.steps[0][1].k_feature_idx_)  Best features: (0, 1, 3)",
            "title": "Obtaining the best k feature indices after GridSearch"
        },
        {
            "location": "/user_guide/feature_selection/SequentialFeatureSelector/#example-8-selecting-the-best-feature-combination-in-a-k-range",
            "text": "If  k_features  is set to to a tuple  (min_k, max_k)  (new in 0.4.2), the SFS will now select the best feature combination that it discovered by iterating from  k=1  to  max_k  (forward), or  max_k  to  min_k  (backward). The size of the returned feature subset is then within  max_k  to  min_k , depending on which combination scored best during cross validation.  X.shape  (150, 4)  from mlxtend.feature_selection import SequentialFeatureSelector as SFS\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom mlxtend.data import wine_data\nfrom sklearn.cross_validation import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.pipeline import make_pipeline\n\nX, y = wine_data()\nX_train, X_test, y_train, y_test= train_test_split(X, y, \n                                                   stratify=y,\n                                                   test_size=0.3,\n                                                   random_state=1)\n\nknn = KNeighborsClassifier(n_neighbors=2)\n\nsfs1 = SFS(estimator=knn, \n           k_features=(3, 10),\n           forward=True, \n           floating=False, \n           scoring='accuracy',\n           print_progress=False,\n           cv=5)\n\npipe = make_pipeline(StandardScaler(), sfs1)\n\npipe.fit(X_train, y_train)\n\nprint('best combination (ACC: %.3f): %s\\n' % (sfs1.k_score_, sfs1.k_feature_idx_))\nprint('all subsets:\\n', sfs1.subsets_)\nplot_sfs(sfs1.get_metric_dict(), kind='std_err');  best combination (ACC: 0.984): (0, 2, 3, 6, 8, 9, 12)\n\nall subsets:\n {1: {'avg_score': 0.79045484949832778, 'feature_idx': (6,), 'cv_scores': array([ 0.76923077,  0.68      ,  0.84      ,  0.75      ,  0.91304348])}, 2: {'avg_score': 0.92686510590858406, 'feature_idx': (9, 6), 'cv_scores': array([ 0.88461538,  0.96      ,  0.96      ,  0.91666667,  0.91304348])}, 3: {'avg_score': 0.95927870680044602, 'feature_idx': (9, 12, 6), 'cv_scores': array([ 0.96153846,  0.96      ,  0.96      ,  0.95833333,  0.95652174])}, 4: {'avg_score': 0.96791973244147156, 'feature_idx': (0, 9, 12, 6), 'cv_scores': array([ 0.92307692,  0.96      ,  1.        ,  1.        ,  0.95652174])}, 5: {'avg_score': 0.98361204013377923, 'feature_idx': (0, 9, 3, 12, 6), 'cv_scores': array([ 0.96153846,  1.        ,  1.        ,  1.        ,  0.95652174])}, 6: {'avg_score': 0.97561204013377922, 'feature_idx': (0, 2, 3, 6, 9, 12), 'cv_scores': array([ 0.96153846,  1.        ,  0.96      ,  1.        ,  0.95652174])}, 7: {'avg_score': 0.98399999999999999, 'feature_idx': (0, 2, 3, 6, 8, 9, 12), 'cv_scores': array([ 1.  ,  0.92,  1.  ,  1.  ,  1.  ])}, 8: {'avg_score': 0.98399999999999999, 'feature_idx': (0, 2, 3, 6, 8, 9, 10, 12), 'cv_scores': array([ 1.  ,  0.96,  0.96,  1.  ,  1.  ])}, 9: {'avg_score': 0.98399999999999999, 'feature_idx': (0, 2, 3, 6, 8, 9, 10, 11, 12), 'cv_scores': array([ 1.  ,  0.96,  0.96,  1.  ,  1.  ])}, 10: {'avg_score': 0.96830769230769231, 'feature_idx': (0, 2, 3, 6, 7, 8, 9, 10, 11, 12), 'cv_scores': array([ 0.96153846,  0.92      ,  0.96      ,  1.        ,  1.        ])}}",
            "title": "Example 8 -- Selecting the \"best\"  feature combination in a k-range"
        },
        {
            "location": "/user_guide/feature_selection/SequentialFeatureSelector/#api",
            "text": "SequentialFeatureSelector(estimator, k_features='best', forward=True, floating=False, print_progress=True, scoring='accuracy', cv=5, skip_if_stuck=True, n_jobs=1, pre_dispatch='2 n_jobs', clone_estimator=True)*  Sequential Feature Selection for Classification and Regression.  Parameters    estimator  : scikit-learn classifier or regressor    k_features  : int or tuple (new in 0.4.2) (default: 1)  Number of features to select,\nwhere k_features < the full feature set.\nNew in 0.4.2: A tuple containing a min and max value can be provided,\nand the SFS will consider return any feature combination between\nmin and max that scored highest in cross-validtion. For example,\nthe tuple (1, 4) will return any combination from\n1 up to 4 features instead of a fixed number of features k.    forward  : bool (default: True)  Forward selection if True,\nbackward selection otherwise    floating  : bool (default: False)  Adds a conditional exclusion/inclusion if True.    print_progress  : bool (default: True)  Prints progress as the number of epochs\nto stderr.    scoring  : str, (default='accuracy')  Scoring metric in {accuracy, f1, precision, recall, roc_auc}\nfor classifiers,\n{'mean_absolute_error', 'mean_squared_error',\n'median_absolute_error', 'r2'} for regressors,\nor a callable object or function with\nsignature  scorer(estimator, X, y) .    cv  : int (default: 5)  Scikit-learn cross-validation generator or  int .\nIf estimator is a classifier (or y consists of integer class labels),\nstratified k-fold is performed, and regular k-fold cross-validation\notherwise.\nNo cross-validation if cv is None, False, or 0.\nskip_if_stuck: bool (default: True)\nSet to True to skip conditional\nexclusion/inclusion if floating=True and\nalgorithm gets stuck in cycles.    n_jobs  : int (default: 1)  The number of CPUs to use for cross validation. -1 means 'all CPUs'.    pre_dispatch  : int, or string (default: '2*n_jobs')  Controls the number of jobs that get dispatched\nduring parallel execution in cross_val_score.\nReducing this number can be useful to avoid an explosion of\nmemory consumption when more jobs get dispatched than CPUs can process.\nThis parameter can be:\nNone, in which case all the jobs are immediately created and spawned.\nUse this for lightweight and fast-running jobs,\nto avoid delays due to on-demand spawning of the jobs\nAn int, giving the exact number of total jobs that are spawned\nA string, giving an expression as a function\nof n_jobs, as in  2*n_jobs    clone_estimator  : bool (default: True)  Clones estimator if True; works with the original estimator instance\nif False. Set to False if the estimator doesn't\nimplement scikit-learn's set_params and get_params methods.\nIn addition, it is required to set cv=0, and n_jobs=1.    Attributes    k_feature_idx_  : array-like, shape = [n_predictions]  Feature Indices of the selected feature subsets.    k_score_  : float  Cross validation average score of the selected subset.    subsets_  : dict  A dictionary of selected feature subsets during the\nsequential selection, where the dictionary keys are\nthe lengths k of these feature subsets. The dictionary\nvalues are dictionaries themselves with the following\nkeys: 'feature_idx' (tuple of indices of the feature subset)\n'cv_scores' (list individual cross-validation scores)\n'avg_score' (average cross-validation score)",
            "title": "API"
        },
        {
            "location": "/user_guide/feature_selection/SequentialFeatureSelector/#methods",
            "text": "fit(X, y)  Perform feature selection and learn model from training data.  Parameters    X  : {array-like, sparse matrix}, shape = [n_samples, n_features]  Training vectors, where n_samples is the number of samples and\nn_features is the number of features.    y  : array-like, shape = [n_samples]  Target values.    Returns   self  : object    fit_transform(X, y)  Fit to training data then reduce X to its most important features.  Parameters    X  : {array-like, sparse matrix}, shape = [n_samples, n_features]  Training vectors, where n_samples is the number of samples and\nn_features is the number of features.    Returns  Reduced feature subset of X, shape={n_samples, k_features}   get_metric_dict(confidence_interval=0.95)  Return metric dictionary  Parameters    confidence_interval  : float (default: 0.95)  A positive float between 0.0 and 1.0 to compute the confidence\ninterval bounds of the CV score averages.    Returns  Dictionary with items where each dictionary value is a list\n    with the number of iterations (number of feature subsets) as\n    its length. The dictionary keys corresponding to these lists\n    are as follows:\n    'feature_idx': tuple of the indices of the feature subset\n    'cv_scores': list with individual CV scores\n    'avg_score': of CV average scores\n    'std_dev': standard deviation of the CV score average\n    'std_err': standard error of the CV score average\n    'ci_bound': confidence interval bound of the CV score average   get_params(deep=True)  Get parameters for this estimator.  Parameters  deep: boolean, optional\n    If True, will return the parameters for this estimator and\n    contained subobjects that are estimators.  Returns    params  : mapping of string to any  Parameter names mapped to their values.     set_params( params)  Set the parameters of this estimator.  The method works on simple estimators as well as on nested objects\n(such as pipelines). The former have parameters of the form\n``<component>__<parameter>`` so that it's possible to update each\ncomponent of a nested object.  Returns  self   transform(X)  Reduce X to its most important features.  Parameters    X  : {array-like, sparse matrix}, shape = [n_samples, n_features]  Training vectors, where n_samples is the number of samples and\nn_features is the number of features.    Returns  Reduced feature subset of X, shape={n_samples, k_features}   plot_sequential_feature_selection(metric_dict, kind='std_dev', color='blue', bcolor='steelblue', marker='o', alpha=0.2, ylabel='Performance', confidence_interval=0.95)  Plot sequential feature selection.  Parameters    metric_dict  : mlxtend.SequentialFeatureSelector.get_metric_dict() object    kind  : str (default: \"std_dev\")  The kind of error bar or confidence interval in\n{'std_dev', 'std_err', 'ci', None}.    color  : str (default: \"blue\")  Color of the lineplot (accepts any matplotlib color name)    bcolor  : str (default: \"steelblue\").  Color of the error bars / confidence intervals\n(accepts any matplotlib color name).    marker  : str (default: \"o\")  Marker of the line plot\n(accepts any matplotlib marker name).    alpha  : float in [0, 1] (default: 0.2)  Transparency of the error bars / confidence intervals.    ylabel  : str (default: \"Performance\")  Y-axis label.    confidence_interval  : float (default: 0.95)  Confidence level if  kind='ci' .    Returns   fig  : matplotlib.pyplot.figure() object",
            "title": "Methods"
        },
        {
            "location": "/user_guide/feature_selection/ColumnSelector/",
            "text": "ColumnSelector\n\n\nImplementation of a column selector class for scikit-learn pipelines.\n\n\n\n\nfrom mlxtend.feature_selection import ColumnSelector\n\n\n\n\nOverview\n\n\nThe \nColumnSelector\n can be used for \"manual\" feature selection, e.g., as part of a grid search via a scikit-learn pipeline.\n\n\nReferences\n\n\n-\n\n\nExamples\n\n\nExample 1 - Feature Selection via GridSearch\n\n\nLoad a simple benchmark dataset:\n\n\nfrom sklearn.datasets import load_iris\n\niris = load_iris()\nX = iris.data\ny = iris.target\n\n\n\n\nCreate all possible combinations:\n\n\nfrom itertools import combinations\n\nall_comb = []\nfor size in range(1, 5):\n    all_comb += list(combinations(range(X.shape[1]), r=size))\nprint(all_comb)\n\n\n\n\n[(0,), (1,), (2,), (3,), (0, 1), (0, 2), (0, 3), (1, 2), (1, 3), (2, 3), (0, 1, 2), (0, 1, 3), (0, 2, 3), (1, 2, 3), (0, 1, 2, 3)]\n\n\n\nFeature and model selection via grid search:\n\n\nfrom mlxtend.feature_selection import ColumnSelector\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.grid_search import GridSearchCV\nfrom sklearn.pipeline import make_pipeline\n\npipe = make_pipeline(StandardScaler(),\n                     ColumnSelector(),\n                     KNeighborsClassifier())\n\nparam_grid = {'columnselector__cols': all_comb,\n              'kneighborsclassifier__n_neighbors': list(range(1, 11))}\n\ngrid = GridSearchCV(pipe, param_grid, cv=5, n_jobs=-1)\ngrid.fit(X, y)\nprint('Best parameters:', grid.best_params_)\nprint('Best performance:', grid.best_score_)\n\n\n\n\nBest parameters: {'columnselector__cols': (2, 3), 'kneighborsclassifier__n_neighbors': 1}\nBest performance: 0.98\n\n\n\nAPI\n\n\nColumnSelector(cols=None)\n\n\nBase class for all estimators in scikit-learn\n\n\nNotes\n\n\nAll estimators should specify all the parameters that can be set\n    at the class level in their \n__init__\n as explicit keyword\narguments (no \n*args\n or \n**kwargs\n).\n\n\nMethods\n\n\n\n\n\nfit(X, y=None)\n\n\nMock method. Does nothing.\n\n\nParameters\n\n\n\n\n\n\nX\n : {array-like, sparse matrix}, shape = [n_samples, n_features]\n\n\nTraining vectors, where n_samples is the number of samples and\nn_features is the number of features.\n\n\n\n\n\n\ny\n : array-like, shape = [n_samples] (default: None)\n\n\n\n\n\n\nReturns\n\n\nself\n\n\n\n\n\nfit_transform(X, y=None)\n\n\nReturn a slice of the input array.\n\n\nParameters\n\n\n\n\n\n\nX\n : {array-like, sparse matrix}, shape = [n_samples, n_features]\n\n\nTraining vectors, where n_samples is the number of samples and\nn_features is the number of features.\n\n\n\n\n\n\ny\n : array-like, shape = [n_samples] (default: None)\n\n\n\n\n\n\nReturns\n\n\n\n\n\n\nX_slice\n : shape = [n_samples, k_features]\n\n\nSubset of the feature space where k_features <= n_features\n\n\n\n\n\n\n\n\n\nget_params(deep=True)\n\n\nGet parameters for this estimator.\n\n\nParameters\n\n\ndeep: boolean, optional\n    If True, will return the parameters for this estimator and\n    contained subobjects that are estimators.\n\n\nReturns\n\n\n\n\n\n\nparams\n : mapping of string to any\n\n\nParameter names mapped to their values.\n\n\n\n\n\n\n\n\n\nset_params(\nparams)\n\n\nSet the parameters of this estimator.\n\n\nThe method works on simple estimators as well as on nested objects\n(such as pipelines). The former have parameters of the form\n``<component>__<parameter>`` so that it's possible to update each\ncomponent of a nested object.\n\n\n\nReturns\n\n\nself\n\n\n\n\n\ntransform(X, y=None)\n\n\nNone",
            "title": "ColumnSelector"
        },
        {
            "location": "/user_guide/feature_selection/ColumnSelector/#columnselector",
            "text": "Implementation of a column selector class for scikit-learn pipelines.   from mlxtend.feature_selection import ColumnSelector",
            "title": "ColumnSelector"
        },
        {
            "location": "/user_guide/feature_selection/ColumnSelector/#overview",
            "text": "The  ColumnSelector  can be used for \"manual\" feature selection, e.g., as part of a grid search via a scikit-learn pipeline.",
            "title": "Overview"
        },
        {
            "location": "/user_guide/feature_selection/ColumnSelector/#references",
            "text": "-",
            "title": "References"
        },
        {
            "location": "/user_guide/feature_selection/ColumnSelector/#examples",
            "text": "",
            "title": "Examples"
        },
        {
            "location": "/user_guide/feature_selection/ColumnSelector/#example-1-feature-selection-via-gridsearch",
            "text": "Load a simple benchmark dataset:  from sklearn.datasets import load_iris\n\niris = load_iris()\nX = iris.data\ny = iris.target  Create all possible combinations:  from itertools import combinations\n\nall_comb = []\nfor size in range(1, 5):\n    all_comb += list(combinations(range(X.shape[1]), r=size))\nprint(all_comb)  [(0,), (1,), (2,), (3,), (0, 1), (0, 2), (0, 3), (1, 2), (1, 3), (2, 3), (0, 1, 2), (0, 1, 3), (0, 2, 3), (1, 2, 3), (0, 1, 2, 3)]  Feature and model selection via grid search:  from mlxtend.feature_selection import ColumnSelector\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.grid_search import GridSearchCV\nfrom sklearn.pipeline import make_pipeline\n\npipe = make_pipeline(StandardScaler(),\n                     ColumnSelector(),\n                     KNeighborsClassifier())\n\nparam_grid = {'columnselector__cols': all_comb,\n              'kneighborsclassifier__n_neighbors': list(range(1, 11))}\n\ngrid = GridSearchCV(pipe, param_grid, cv=5, n_jobs=-1)\ngrid.fit(X, y)\nprint('Best parameters:', grid.best_params_)\nprint('Best performance:', grid.best_score_)  Best parameters: {'columnselector__cols': (2, 3), 'kneighborsclassifier__n_neighbors': 1}\nBest performance: 0.98",
            "title": "Example 1 - Feature Selection via GridSearch"
        },
        {
            "location": "/user_guide/feature_selection/ColumnSelector/#api",
            "text": "ColumnSelector(cols=None)  Base class for all estimators in scikit-learn  Notes  All estimators should specify all the parameters that can be set\n    at the class level in their  __init__  as explicit keyword\narguments (no  *args  or  **kwargs ).",
            "title": "API"
        },
        {
            "location": "/user_guide/feature_selection/ColumnSelector/#methods",
            "text": "fit(X, y=None)  Mock method. Does nothing.  Parameters    X  : {array-like, sparse matrix}, shape = [n_samples, n_features]  Training vectors, where n_samples is the number of samples and\nn_features is the number of features.    y  : array-like, shape = [n_samples] (default: None)    Returns  self   fit_transform(X, y=None)  Return a slice of the input array.  Parameters    X  : {array-like, sparse matrix}, shape = [n_samples, n_features]  Training vectors, where n_samples is the number of samples and\nn_features is the number of features.    y  : array-like, shape = [n_samples] (default: None)    Returns    X_slice  : shape = [n_samples, k_features]  Subset of the feature space where k_features <= n_features     get_params(deep=True)  Get parameters for this estimator.  Parameters  deep: boolean, optional\n    If True, will return the parameters for this estimator and\n    contained subobjects that are estimators.  Returns    params  : mapping of string to any  Parameter names mapped to their values.     set_params( params)  Set the parameters of this estimator.  The method works on simple estimators as well as on nested objects\n(such as pipelines). The former have parameters of the form\n``<component>__<parameter>`` so that it's possible to update each\ncomponent of a nested object.  Returns  self   transform(X, y=None)  None",
            "title": "Methods"
        },
        {
            "location": "/user_guide/feature_extraction/PrincipalComponentAnalysis/",
            "text": "Principal Component Analysis\n\n\nImplementation of Principal Component Analysis for dimensionality reduction\n\n\n\n\nfrom mlxtend.feature_extraction import PrincipalComponentAnalysis\n\n\n\n\nOverview\n\n\nThe sheer size of data in the modern age is not only a challenge for computer hardware but also a main bottleneck for the performance of many machine learning algorithms. The main goal of a PCA analysis is to identify patterns in data; PCA aims to detect the correlation between variables. If a strong correlation between variables exists, the attempt to reduce the dimensionality only makes sense. In a nutshell, this is what PCA is all about: Finding the directions of maximum variance in high-dimensional data and project it onto a smaller dimensional subspace while retaining most of the information.\n\n\nPCA and Dimensionality Reduction\n\n\nOften, the desired goal is to reduce the dimensions of a \n$d$\n-dimensional dataset by projecting it onto a \n$(k)$\n-dimensional subspace (where \n$k\\;<\\;d$\n) in order to increase the computational efficiency while retaining most of the information. An important question is \"what is the size of \n$k$\n that represents the data 'well'?\"\n\n\nLater, we will compute eigenvectors (the principal components) of a dataset and collect them in a projection matrix. Each of those eigenvectors is associated with an eigenvalue which can be interpreted as the \"length\" or \"magnitude\" of the corresponding eigenvector. If some eigenvalues have a significantly larger magnitude than others that the reduction of the dataset via PCA onto a smaller dimensional subspace by dropping the \"less informative\" eigenpairs is reasonable.\n\n\nA Summary of the PCA Approach\n\n\n\n\nStandardize the data.\n\n\nObtain the Eigenvectors and Eigenvalues from the covariance matrix or correlation matrix, or perform Singular Vector Decomposition.\n\n\nSort eigenvalues in descending order and choose the \n$k$\n eigenvectors that correspond to the \n$k$\n largest eigenvalues where \n$k$\n is the number of dimensions of the new feature subspace (\n$k \\le d$\n).\n\n\nConstruct the projection matrix \n$\\mathbf{W}$\n from the selected \n$k$\n eigenvectors.\n\n\nTransform the original dataset \n$\\mathbf{X}$\n via \n$\\mathbf{W}$\n to obtain a \n$k$\n-dimensional feature subspace \n$\\mathbf{Y}$\n.\n\n\n\n\nReferences\n\n\n\n\nPearson, Karl. \"LIII. \nOn lines and planes of closest fit to systems of points in space.\n\" The London, Edinburgh, and Dublin Philosophical Magazine and Journal of Science 2.11 (1901): 559-572.\n\n\n\n\nExamples\n\n\nExample 1 - PCA on Iris\n\n\nfrom mlxtend.data import iris_data\nfrom mlxtend.preprocessing import standardize\nfrom mlxtend.feature_extraction import PrincipalComponentAnalysis\n\nX, y = iris_data()\nX = standardize(X)\n\npca = PrincipalComponentAnalysis(n_components=2)\npca.fit(X)\nX_pca = pca.transform(X)\n\n\n\n\nimport matplotlib.pyplot as plt\n\nwith plt.style.context('seaborn-whitegrid'):\n    plt.figure(figsize=(6, 4))\n    for lab, col in zip((0, 1, 2),\n                        ('blue', 'red', 'green')):\n        plt.scatter(X_pca[y==lab, 0],\n                    X_pca[y==lab, 1],\n                    label=lab,\n                    c=col)\n    plt.xlabel('Principal Component 1')\n    plt.ylabel('Principal Component 2')\n    plt.legend(loc='lower center')\n    plt.tight_layout()\n    plt.show()\n\n\n\n\n\n\nExample 2 - Plotting the Variance Explained Ratio\n\n\nfrom mlxtend.data import iris_data\nfrom mlxtend.preprocessing import standardize\n\nX, y = iris_data()\nX = standardize(X)\n\npca = PrincipalComponentAnalysis(n_components=None)\npca.fit(X)\nX_pca = pca.transform(X)\n\n\n\n\nimport numpy as np\n\ntot = sum(pca.e_vals_)\nvar_exp = [(i / tot)*100 for i in sorted(pca.e_vals_, reverse=True)]\ncum_var_exp = np.cumsum(var_exp)\n\n\n\n\nwith plt.style.context('seaborn-whitegrid'):\n    fig, ax = plt.subplots(figsize=(6, 4))\n    plt.bar(range(4), var_exp, alpha=0.5, align='center',\n            label='individual explained variance')\n    plt.step(range(4), cum_var_exp, where='mid',\n             label='cumulative explained variance')\n    plt.ylabel('Explained variance ratio')\n    plt.xlabel('Principal components')\n    plt.xticks(range(4))\n    ax.set_xticklabels(np.arange(1, X.shape[1] + 1))\n    plt.legend(loc='best')\n    plt.tight_layout()\n\n\n\n\n\n\nExample 3 - PCA via SVD\n\n\nWhile the eigendecomposition of the covariance or correlation matrix may be more intuitiuve, most PCA implementations perform a Singular Vector Decomposition (SVD) to improve the computational efficiency. Another advantage of using SVD is that the results tend to be more numerically stable, since we can decompose the input matrix directly without the additional covariance-matrix step.\n\n\nfrom mlxtend.data import iris_data\nfrom mlxtend.preprocessing import standardize\nfrom mlxtend.feature_extraction import PrincipalComponentAnalysis\n\nX, y = iris_data()\nX = standardize(X)\n\npca = PrincipalComponentAnalysis(n_components=2,\n                                 solver='svd')\npca.fit(X)\nX_pca = pca.transform(X)\n\n\n\n\nimport matplotlib.pyplot as plt\n\nwith plt.style.context('seaborn-whitegrid'):\n    plt.figure(figsize=(6, 4))\n    for lab, col in zip((0, 1, 2),\n                        ('blue', 'red', 'green')):\n        plt.scatter(X_pca[y==lab, 0],\n                    X_pca[y==lab, 1],\n                    label=lab,\n                    c=col)\n    plt.xlabel('Principal Component 1')\n    plt.ylabel('Principal Component 2')\n    plt.legend(loc='lower center')\n    plt.tight_layout()\n    plt.show()\n\n\n\n\n\n\nIf we compare this PCA projection to the previous plot in example 1, we notice that they are mirror images of each other. Note that this is not due to an error in any of those two implementations, but the reason for this difference is that, depending on the eigensolver, eigenvectors can have either negative or positive signs.\n\n\nFor instance, if \n$v$\n is an eigenvector of a matrix \n$\\Sigma$\n, we have\n\n\n$$\\Sigma v = \\lambda v,$$\n\n\nwhere \n$\\lambda$\n is our eigenvalue\n\n\nthen \n$-v$\n is also an eigenvector that has the same eigenvalue, since\n\n\n$$\\Sigma(-v) = -\\Sigma v = -\\lambda v = \\lambda(-v).$$\n\n\nAPI\n\n\nPrincipalComponentAnalysis(n_components=None, solver='eigen')\n\n\nPrincipal Component Analysis Class\n\n\nParameters\n\n\n\n\n\n\nn_components\n : int (default: None)\n\n\nThe number of principal components for transformation.\nKeeps the original dimensions of the dataset if \nNone\n.\n\n\n\n\n\n\nsolver\n : str (default: 'eigen')\n\n\nMethod for performing the matrix decomposition.\n\n\n\n\n\n\nAttributes\n\n\n\n\n\n\nw_\n : array-like, shape=[n_features, n_components]\n\n\nProjection matrix\n\n\n\n\n\n\ne_vals_\n : array-like, shape=[n_features]\n\n\nEigenvalues in sorted order.\n\n\n\n\n\n\ne_vecs_\n : array-like, shape=[n_features]\n\n\nEigenvectors in sorted order.\n\n\n\n\n\n\nMethods\n\n\n\n\n\nfit(X)\n\n\nLearn model from training data.\n\n\nParameters\n\n\n\n\n\n\nX\n : {array-like, sparse matrix}, shape = [n_samples, n_features]\n\n\nTraining vectors, where n_samples is the number of samples and\nn_features is the number of features.\n\n\n\n\n\n\nReturns\n\n\n\n\nself\n : object\n\n\n\n\n\n\n\ntransform(X)\n\n\nApply the linear transformation on X.\n\n\nParameters\n\n\n\n\n\n\nX\n : {array-like, sparse matrix}, shape = [n_samples, n_features]\n\n\nTraining vectors, where n_samples is the number of samples and\nn_features is the number of features.\n\n\n\n\n\n\nReturns\n\n\n\n\n\n\nX_projected\n : np.ndarray, shape = [n_samples, n_components]\n\n\nProjected training vectors.",
            "title": "PrincipalComponentAnalysis"
        },
        {
            "location": "/user_guide/feature_extraction/PrincipalComponentAnalysis/#principal-component-analysis",
            "text": "Implementation of Principal Component Analysis for dimensionality reduction   from mlxtend.feature_extraction import PrincipalComponentAnalysis",
            "title": "Principal Component Analysis"
        },
        {
            "location": "/user_guide/feature_extraction/PrincipalComponentAnalysis/#overview",
            "text": "The sheer size of data in the modern age is not only a challenge for computer hardware but also a main bottleneck for the performance of many machine learning algorithms. The main goal of a PCA analysis is to identify patterns in data; PCA aims to detect the correlation between variables. If a strong correlation between variables exists, the attempt to reduce the dimensionality only makes sense. In a nutshell, this is what PCA is all about: Finding the directions of maximum variance in high-dimensional data and project it onto a smaller dimensional subspace while retaining most of the information.",
            "title": "Overview"
        },
        {
            "location": "/user_guide/feature_extraction/PrincipalComponentAnalysis/#pca-and-dimensionality-reduction",
            "text": "Often, the desired goal is to reduce the dimensions of a  $d$ -dimensional dataset by projecting it onto a  $(k)$ -dimensional subspace (where  $k\\;<\\;d$ ) in order to increase the computational efficiency while retaining most of the information. An important question is \"what is the size of  $k$  that represents the data 'well'?\"  Later, we will compute eigenvectors (the principal components) of a dataset and collect them in a projection matrix. Each of those eigenvectors is associated with an eigenvalue which can be interpreted as the \"length\" or \"magnitude\" of the corresponding eigenvector. If some eigenvalues have a significantly larger magnitude than others that the reduction of the dataset via PCA onto a smaller dimensional subspace by dropping the \"less informative\" eigenpairs is reasonable.",
            "title": "PCA and Dimensionality Reduction"
        },
        {
            "location": "/user_guide/feature_extraction/PrincipalComponentAnalysis/#a-summary-of-the-pca-approach",
            "text": "Standardize the data.  Obtain the Eigenvectors and Eigenvalues from the covariance matrix or correlation matrix, or perform Singular Vector Decomposition.  Sort eigenvalues in descending order and choose the  $k$  eigenvectors that correspond to the  $k$  largest eigenvalues where  $k$  is the number of dimensions of the new feature subspace ( $k \\le d$ ).  Construct the projection matrix  $\\mathbf{W}$  from the selected  $k$  eigenvectors.  Transform the original dataset  $\\mathbf{X}$  via  $\\mathbf{W}$  to obtain a  $k$ -dimensional feature subspace  $\\mathbf{Y}$ .",
            "title": "A Summary of the PCA Approach"
        },
        {
            "location": "/user_guide/feature_extraction/PrincipalComponentAnalysis/#references",
            "text": "Pearson, Karl. \"LIII.  On lines and planes of closest fit to systems of points in space. \" The London, Edinburgh, and Dublin Philosophical Magazine and Journal of Science 2.11 (1901): 559-572.",
            "title": "References"
        },
        {
            "location": "/user_guide/feature_extraction/PrincipalComponentAnalysis/#examples",
            "text": "",
            "title": "Examples"
        },
        {
            "location": "/user_guide/feature_extraction/PrincipalComponentAnalysis/#example-1-pca-on-iris",
            "text": "from mlxtend.data import iris_data\nfrom mlxtend.preprocessing import standardize\nfrom mlxtend.feature_extraction import PrincipalComponentAnalysis\n\nX, y = iris_data()\nX = standardize(X)\n\npca = PrincipalComponentAnalysis(n_components=2)\npca.fit(X)\nX_pca = pca.transform(X)  import matplotlib.pyplot as plt\n\nwith plt.style.context('seaborn-whitegrid'):\n    plt.figure(figsize=(6, 4))\n    for lab, col in zip((0, 1, 2),\n                        ('blue', 'red', 'green')):\n        plt.scatter(X_pca[y==lab, 0],\n                    X_pca[y==lab, 1],\n                    label=lab,\n                    c=col)\n    plt.xlabel('Principal Component 1')\n    plt.ylabel('Principal Component 2')\n    plt.legend(loc='lower center')\n    plt.tight_layout()\n    plt.show()",
            "title": "Example 1 - PCA on Iris"
        },
        {
            "location": "/user_guide/feature_extraction/PrincipalComponentAnalysis/#example-2-plotting-the-variance-explained-ratio",
            "text": "from mlxtend.data import iris_data\nfrom mlxtend.preprocessing import standardize\n\nX, y = iris_data()\nX = standardize(X)\n\npca = PrincipalComponentAnalysis(n_components=None)\npca.fit(X)\nX_pca = pca.transform(X)  import numpy as np\n\ntot = sum(pca.e_vals_)\nvar_exp = [(i / tot)*100 for i in sorted(pca.e_vals_, reverse=True)]\ncum_var_exp = np.cumsum(var_exp)  with plt.style.context('seaborn-whitegrid'):\n    fig, ax = plt.subplots(figsize=(6, 4))\n    plt.bar(range(4), var_exp, alpha=0.5, align='center',\n            label='individual explained variance')\n    plt.step(range(4), cum_var_exp, where='mid',\n             label='cumulative explained variance')\n    plt.ylabel('Explained variance ratio')\n    plt.xlabel('Principal components')\n    plt.xticks(range(4))\n    ax.set_xticklabels(np.arange(1, X.shape[1] + 1))\n    plt.legend(loc='best')\n    plt.tight_layout()",
            "title": "Example 2 - Plotting the Variance Explained Ratio"
        },
        {
            "location": "/user_guide/feature_extraction/PrincipalComponentAnalysis/#example-3-pca-via-svd",
            "text": "While the eigendecomposition of the covariance or correlation matrix may be more intuitiuve, most PCA implementations perform a Singular Vector Decomposition (SVD) to improve the computational efficiency. Another advantage of using SVD is that the results tend to be more numerically stable, since we can decompose the input matrix directly without the additional covariance-matrix step.  from mlxtend.data import iris_data\nfrom mlxtend.preprocessing import standardize\nfrom mlxtend.feature_extraction import PrincipalComponentAnalysis\n\nX, y = iris_data()\nX = standardize(X)\n\npca = PrincipalComponentAnalysis(n_components=2,\n                                 solver='svd')\npca.fit(X)\nX_pca = pca.transform(X)  import matplotlib.pyplot as plt\n\nwith plt.style.context('seaborn-whitegrid'):\n    plt.figure(figsize=(6, 4))\n    for lab, col in zip((0, 1, 2),\n                        ('blue', 'red', 'green')):\n        plt.scatter(X_pca[y==lab, 0],\n                    X_pca[y==lab, 1],\n                    label=lab,\n                    c=col)\n    plt.xlabel('Principal Component 1')\n    plt.ylabel('Principal Component 2')\n    plt.legend(loc='lower center')\n    plt.tight_layout()\n    plt.show()   If we compare this PCA projection to the previous plot in example 1, we notice that they are mirror images of each other. Note that this is not due to an error in any of those two implementations, but the reason for this difference is that, depending on the eigensolver, eigenvectors can have either negative or positive signs.  For instance, if  $v$  is an eigenvector of a matrix  $\\Sigma$ , we have  $$\\Sigma v = \\lambda v,$$  where  $\\lambda$  is our eigenvalue  then  $-v$  is also an eigenvector that has the same eigenvalue, since  $$\\Sigma(-v) = -\\Sigma v = -\\lambda v = \\lambda(-v).$$",
            "title": "Example 3 - PCA via SVD"
        },
        {
            "location": "/user_guide/feature_extraction/PrincipalComponentAnalysis/#api",
            "text": "PrincipalComponentAnalysis(n_components=None, solver='eigen')  Principal Component Analysis Class  Parameters    n_components  : int (default: None)  The number of principal components for transformation.\nKeeps the original dimensions of the dataset if  None .    solver  : str (default: 'eigen')  Method for performing the matrix decomposition.    Attributes    w_  : array-like, shape=[n_features, n_components]  Projection matrix    e_vals_  : array-like, shape=[n_features]  Eigenvalues in sorted order.    e_vecs_  : array-like, shape=[n_features]  Eigenvectors in sorted order.",
            "title": "API"
        },
        {
            "location": "/user_guide/feature_extraction/PrincipalComponentAnalysis/#methods",
            "text": "fit(X)  Learn model from training data.  Parameters    X  : {array-like, sparse matrix}, shape = [n_samples, n_features]  Training vectors, where n_samples is the number of samples and\nn_features is the number of features.    Returns   self  : object    transform(X)  Apply the linear transformation on X.  Parameters    X  : {array-like, sparse matrix}, shape = [n_samples, n_features]  Training vectors, where n_samples is the number of samples and\nn_features is the number of features.    Returns    X_projected  : np.ndarray, shape = [n_samples, n_components]  Projected training vectors.",
            "title": "Methods"
        },
        {
            "location": "/user_guide/feature_extraction/LinearDiscriminantAnalysis/",
            "text": "Linear Discriminant Analysis\n\n\nImplementation of Linear Discriminant Analysis for dimensionality reduction\n\n\n\n\nfrom mlxtend.feature_extraction import LinearDiscriminantAnalysis\n\n\n\n\nOverview\n\n\nLinear Discriminant Analysis (LDA) is most commonly used as dimensionality reduction technique in the pre-processing step for pattern-classification and machine learning applications.\nThe goal is to project a dataset onto a lower-dimensional space with good class-separability in order avoid overfitting (\"curse of dimensionality\") and also reduce computational costs.\n\n\nRonald A. Fisher formulated the \nLinear Discriminant\n in 1936 (\nThe Use of Multiple  Measurements in Taxonomic Problems\n), and it also has some practical uses as classifier. The original Linear discriminant was described for a 2-class problem, and it was then later generalized as \"multi-class Linear Discriminant Analysis\" or \"Multiple Discriminant Analysis\" by C. R. Rao in 1948 (\nThe utilization of multiple measurements in problems of biological classification\n)\n\n\nThe general LDA approach is very similar to a Principal Component Analysis, but in addition to finding the component axes that maximize the variance of our data (PCA), we are additionally interested in the axes that maximize the separation between multiple classes (LDA).\n\n\nSo, in a nutshell, often the goal of an LDA is to project a feature space (a dataset n-dimensional samples) onto a smaller subspace \n$k$\n (where \n$k \\leq n-1$\n) while maintaining the class-discriminatory information. \n\nIn general, dimensionality reduction does not only help reducing computational costs for a given classification task, but it can also be helpful to avoid overfitting by minimizing the error in parameter estimation (\"curse of dimensionality\").\n\n\nSummarizing the LDA approach in 5 steps\n\n\nListed below are the 5 general steps for performing a linear discriminant analysis.\n\n\n\n\nCompute the \n$d$\n-dimensional mean vectors for the different classes from the dataset.\n\n\nCompute the scatter matrices (in-between-class and within-class scatter matrix).\n\n\nCompute the eigenvectors (\n$\\mathbf{e_1}, \\; \\mathbf{e_2}, \\; ..., \\; \\mathbf{e_d}$\n) and corresponding eigenvalues (\n$\\mathbf{\\lambda_1}, \\; \\mathbf{\\lambda_2}, \\; ..., \\; \\mathbf{\\lambda_d}$\n) for the scatter matrices.\n\n\nSort the eigenvectors by decreasing eigenvalues and choose \n$k$\n eigenvectors with the largest eigenvalues to form a \n$k \\times d$\n dimensional matrix \n$\\mathbf{W}$\n (where every column represents an eigenvector).\n\n\nUse this \n$k \\times d$\n eigenvector matrix to transform the samples onto the new subspace. This can be summarized by the mathematical equation: \n$\\mathbf{Y} = \\mathbf{X} \\times \\mathbf{W}$\n (where \n$\\mathbf{X}$\n is a \n$n \\times d$\n-dimensional matrix representing the \n$n$\n samples, and \n$\\mathbf{y}$\n are the transformed \n$n \\times k$\n-dimensional samples in the new subspace).\n\n\n\n\nReferences\n\n\n\n\nFisher, Ronald A. \"\nThe use of multiple measurements in taxonomic problems.\n\" Annals of eugenics 7.2 (1936): 179-188.\n\n\nRao, C. Radhakrishna. \"\nThe utilization of multiple measurements in problems of biological classification.\n\" Journal of the Royal Statistical Society. Series B (Methodological) 10.2 (1948): 159-203.\n\n\n\n\nExamples\n\n\nExample 1 - LDA on Iris\n\n\nfrom mlxtend.data import iris_data\nfrom mlxtend.preprocessing import standardize\nfrom mlxtend.feature_extraction import LinearDiscriminantAnalysis\n\nX, y = iris_data()\nX = standardize(X)\n\nlda = LinearDiscriminantAnalysis(n_discriminants=2)\nlda.fit(X, y)\nX_lda = lda.transform(X)\n\n\n\n\nimport matplotlib.pyplot as plt\n\nwith plt.style.context('seaborn-whitegrid'):\n    plt.figure(figsize=(6, 4))\n    for lab, col in zip((0, 1, 2),\n                        ('blue', 'red', 'green')):\n        plt.scatter(X_lda[y == lab, 0],\n                    X_lda[y == lab, 1],\n                    label=lab,\n                    c=col)\n    plt.xlabel('Linear Discriminant 1')\n    plt.ylabel('Linear Discriminant 2')\n    plt.legend(loc='lower right')\n    plt.tight_layout()\n    plt.show()\n\n\n\n\n\n\nExample 2 - Plotting the Between-Class Variance Explained Ratio\n\n\nfrom mlxtend.data import iris_data\nfrom mlxtend.preprocessing import standardize\nfrom mlxtend.feature_extraction import LinearDiscriminantAnalysis\n\nX, y = iris_data()\nX = standardize(X)\n\nlda = LinearDiscriminantAnalysis(n_discriminants=None)\nlda.fit(X, y)\nX_lda = lda.transform(X)\n\n\n\n\nimport numpy as np\n\ntot = sum(lda.e_vals_)\nvar_exp = [(i / tot)*100 for i in sorted(lda.e_vals_, reverse=True)]\ncum_var_exp = np.cumsum(var_exp)\n\n\n\n\nwith plt.style.context('seaborn-whitegrid'):\n    fig, ax = plt.subplots(figsize=(6, 4))\n    plt.bar(range(4), var_exp, alpha=0.5, align='center',\n            label='individual explained variance')\n    plt.step(range(4), cum_var_exp, where='mid',\n             label='cumulative explained variance')\n    plt.ylabel('Explained variance ratio')\n    plt.xlabel('Principal components')\n    plt.xticks(range(4))\n    ax.set_xticklabels(np.arange(1, X.shape[1] + 1))\n    plt.legend(loc='best')\n    plt.tight_layout()\n\n\n\n\n\n\nAPI\n\n\nLinearDiscriminantAnalysis(n_discriminants=None)\n\n\nLinear Discriminant Analysis Class\n\n\nParameters\n\n\n\n\n\n\nn_discriminants\n : int (default: None)\n\n\nThe number of discrimants for transformation.\nKeeps the original dimensions of the dataset if \nNone\n.\n\n\n\n\n\n\nAttributes\n\n\n\n\n\n\nw_\n : array-like, shape=[n_features, n_discriminants]\n\n\nProjection matrix\n\n\n\n\n\n\ne_vals_\n : array-like, shape=[n_features]\n\n\nEigenvalues in sorted order.\n\n\n\n\n\n\ne_vecs_\n : array-like, shape=[n_features]\n\n\nEigenvectors in sorted order.\n\n\n\n\n\n\nMethods\n\n\n\n\n\nfit(X, y, n_classes=None)\n\n\nFit the LDA model with X.\n\n\nParameters\n\n\n\n\n\n\nX\n : {array-like, sparse matrix}, shape = [n_samples, n_features]\n\n\nTraining vectors, where n_samples is the number of samples and\nn_features is the number of features.\n\n\n\n\n\n\ny\n : array-like, shape = [n_samples]\n\n\nTarget values.\n\n\n\n\n\n\nn_classes\n : int (default: None)\n\n\nA positive integer to declare the number of class labels\nif not all class labels are present in a partial training set.\nGets the number of class labels automatically if None.\n\n\n\n\n\n\nReturns\n\n\n\n\nself\n : object\n\n\n\n\n\n\n\ntransform(X)\n\n\nApply the linear transformation on X.\n\n\nParameters\n\n\n\n\n\n\nX\n : {array-like, sparse matrix}, shape = [n_samples, n_features]\n\n\nTraining vectors, where n_samples is the number of samples and\nn_features is the number of features.\n\n\n\n\n\n\nReturns\n\n\n\n\n\n\nX_projected\n : np.ndarray, shape = [n_samples, n_discriminants]\n\n\nProjected training vectors.",
            "title": "LinearDiscriminantAnalysis"
        },
        {
            "location": "/user_guide/feature_extraction/LinearDiscriminantAnalysis/#linear-discriminant-analysis",
            "text": "Implementation of Linear Discriminant Analysis for dimensionality reduction   from mlxtend.feature_extraction import LinearDiscriminantAnalysis",
            "title": "Linear Discriminant Analysis"
        },
        {
            "location": "/user_guide/feature_extraction/LinearDiscriminantAnalysis/#overview",
            "text": "Linear Discriminant Analysis (LDA) is most commonly used as dimensionality reduction technique in the pre-processing step for pattern-classification and machine learning applications.\nThe goal is to project a dataset onto a lower-dimensional space with good class-separability in order avoid overfitting (\"curse of dimensionality\") and also reduce computational costs.  Ronald A. Fisher formulated the  Linear Discriminant  in 1936 ( The Use of Multiple  Measurements in Taxonomic Problems ), and it also has some practical uses as classifier. The original Linear discriminant was described for a 2-class problem, and it was then later generalized as \"multi-class Linear Discriminant Analysis\" or \"Multiple Discriminant Analysis\" by C. R. Rao in 1948 ( The utilization of multiple measurements in problems of biological classification )  The general LDA approach is very similar to a Principal Component Analysis, but in addition to finding the component axes that maximize the variance of our data (PCA), we are additionally interested in the axes that maximize the separation between multiple classes (LDA).  So, in a nutshell, often the goal of an LDA is to project a feature space (a dataset n-dimensional samples) onto a smaller subspace  $k$  (where  $k \\leq n-1$ ) while maintaining the class-discriminatory information.  \nIn general, dimensionality reduction does not only help reducing computational costs for a given classification task, but it can also be helpful to avoid overfitting by minimizing the error in parameter estimation (\"curse of dimensionality\").",
            "title": "Overview"
        },
        {
            "location": "/user_guide/feature_extraction/LinearDiscriminantAnalysis/#summarizing-the-lda-approach-in-5-steps",
            "text": "Listed below are the 5 general steps for performing a linear discriminant analysis.   Compute the  $d$ -dimensional mean vectors for the different classes from the dataset.  Compute the scatter matrices (in-between-class and within-class scatter matrix).  Compute the eigenvectors ( $\\mathbf{e_1}, \\; \\mathbf{e_2}, \\; ..., \\; \\mathbf{e_d}$ ) and corresponding eigenvalues ( $\\mathbf{\\lambda_1}, \\; \\mathbf{\\lambda_2}, \\; ..., \\; \\mathbf{\\lambda_d}$ ) for the scatter matrices.  Sort the eigenvectors by decreasing eigenvalues and choose  $k$  eigenvectors with the largest eigenvalues to form a  $k \\times d$  dimensional matrix  $\\mathbf{W}$  (where every column represents an eigenvector).  Use this  $k \\times d$  eigenvector matrix to transform the samples onto the new subspace. This can be summarized by the mathematical equation:  $\\mathbf{Y} = \\mathbf{X} \\times \\mathbf{W}$  (where  $\\mathbf{X}$  is a  $n \\times d$ -dimensional matrix representing the  $n$  samples, and  $\\mathbf{y}$  are the transformed  $n \\times k$ -dimensional samples in the new subspace).",
            "title": "Summarizing the LDA approach in 5 steps"
        },
        {
            "location": "/user_guide/feature_extraction/LinearDiscriminantAnalysis/#references",
            "text": "Fisher, Ronald A. \" The use of multiple measurements in taxonomic problems. \" Annals of eugenics 7.2 (1936): 179-188.  Rao, C. Radhakrishna. \" The utilization of multiple measurements in problems of biological classification. \" Journal of the Royal Statistical Society. Series B (Methodological) 10.2 (1948): 159-203.",
            "title": "References"
        },
        {
            "location": "/user_guide/feature_extraction/LinearDiscriminantAnalysis/#examples",
            "text": "",
            "title": "Examples"
        },
        {
            "location": "/user_guide/feature_extraction/LinearDiscriminantAnalysis/#example-1-lda-on-iris",
            "text": "from mlxtend.data import iris_data\nfrom mlxtend.preprocessing import standardize\nfrom mlxtend.feature_extraction import LinearDiscriminantAnalysis\n\nX, y = iris_data()\nX = standardize(X)\n\nlda = LinearDiscriminantAnalysis(n_discriminants=2)\nlda.fit(X, y)\nX_lda = lda.transform(X)  import matplotlib.pyplot as plt\n\nwith plt.style.context('seaborn-whitegrid'):\n    plt.figure(figsize=(6, 4))\n    for lab, col in zip((0, 1, 2),\n                        ('blue', 'red', 'green')):\n        plt.scatter(X_lda[y == lab, 0],\n                    X_lda[y == lab, 1],\n                    label=lab,\n                    c=col)\n    plt.xlabel('Linear Discriminant 1')\n    plt.ylabel('Linear Discriminant 2')\n    plt.legend(loc='lower right')\n    plt.tight_layout()\n    plt.show()",
            "title": "Example 1 - LDA on Iris"
        },
        {
            "location": "/user_guide/feature_extraction/LinearDiscriminantAnalysis/#example-2-plotting-the-between-class-variance-explained-ratio",
            "text": "from mlxtend.data import iris_data\nfrom mlxtend.preprocessing import standardize\nfrom mlxtend.feature_extraction import LinearDiscriminantAnalysis\n\nX, y = iris_data()\nX = standardize(X)\n\nlda = LinearDiscriminantAnalysis(n_discriminants=None)\nlda.fit(X, y)\nX_lda = lda.transform(X)  import numpy as np\n\ntot = sum(lda.e_vals_)\nvar_exp = [(i / tot)*100 for i in sorted(lda.e_vals_, reverse=True)]\ncum_var_exp = np.cumsum(var_exp)  with plt.style.context('seaborn-whitegrid'):\n    fig, ax = plt.subplots(figsize=(6, 4))\n    plt.bar(range(4), var_exp, alpha=0.5, align='center',\n            label='individual explained variance')\n    plt.step(range(4), cum_var_exp, where='mid',\n             label='cumulative explained variance')\n    plt.ylabel('Explained variance ratio')\n    plt.xlabel('Principal components')\n    plt.xticks(range(4))\n    ax.set_xticklabels(np.arange(1, X.shape[1] + 1))\n    plt.legend(loc='best')\n    plt.tight_layout()",
            "title": "Example 2 - Plotting the Between-Class Variance Explained Ratio"
        },
        {
            "location": "/user_guide/feature_extraction/LinearDiscriminantAnalysis/#api",
            "text": "LinearDiscriminantAnalysis(n_discriminants=None)  Linear Discriminant Analysis Class  Parameters    n_discriminants  : int (default: None)  The number of discrimants for transformation.\nKeeps the original dimensions of the dataset if  None .    Attributes    w_  : array-like, shape=[n_features, n_discriminants]  Projection matrix    e_vals_  : array-like, shape=[n_features]  Eigenvalues in sorted order.    e_vecs_  : array-like, shape=[n_features]  Eigenvectors in sorted order.",
            "title": "API"
        },
        {
            "location": "/user_guide/feature_extraction/LinearDiscriminantAnalysis/#methods",
            "text": "fit(X, y, n_classes=None)  Fit the LDA model with X.  Parameters    X  : {array-like, sparse matrix}, shape = [n_samples, n_features]  Training vectors, where n_samples is the number of samples and\nn_features is the number of features.    y  : array-like, shape = [n_samples]  Target values.    n_classes  : int (default: None)  A positive integer to declare the number of class labels\nif not all class labels are present in a partial training set.\nGets the number of class labels automatically if None.    Returns   self  : object    transform(X)  Apply the linear transformation on X.  Parameters    X  : {array-like, sparse matrix}, shape = [n_samples, n_features]  Training vectors, where n_samples is the number of samples and\nn_features is the number of features.    Returns    X_projected  : np.ndarray, shape = [n_samples, n_discriminants]  Projected training vectors.",
            "title": "Methods"
        },
        {
            "location": "/user_guide/feature_extraction/RBFKernelPCA/",
            "text": "RBF Kernel Principal Component Analysis\n\n\nImplementation of RBF Kernel Principal Component Analysis for non-linear dimensionality reduction\n\n\n\n\nfrom mlxtend.feature_extraction import RBFKernelPCA\n\n\n\n\nOverview\n\n\nMost machine learning algorithms have been developed and statistically validated for linearly separable data. Popular examples are linear classifiers like Support Vector Machines (SVMs) or the (standard) Principal Component Analysis (PCA) for dimensionality reduction. However, most real world data requires nonlinear methods in order to perform tasks that involve the analysis and discovery of patterns successfully.\n\n\nThe focus of this overview is to briefly introduce the idea of kernel methods and to implement a Gaussian radius basis function (RBF) kernel that is used to perform nonlinear dimensionality reduction via BF kernel principal component analysis (kPCA).\n\n\nPrincipal Component Analysis\n\n\nThe main purpose of principal component analysis (PCA) is the analysis of data to identify patterns that represent the data \u201cwell.\u201d The principal components can be understood as new axes of the dataset that maximize the variance along those axes (the eigenvectors of the covariance matrix). In other words, PCA aims to find the axes with maximum variances along which the data is most spread.\n\n\n\n\nFor more details, please see the related article on \nmlxtend.feature_extraction.PrincipalComponentAnalysis\n.\n\n\nNonlinear dimensionality reduction\n\n\nThe \u201cclassic\u201d PCA approach described above is a linear projection technique that works well if the data is linearly separable. However, in the case of linearly inseparable data, a nonlinear technique is required if the task is to reduce the dimensionality of a dataset.\n\n\n\n\nKernel functions and the kernel trick\n\n\nThe basic idea to deal with linearly inseparable data is to project it onto a higher dimensional space where it becomes linearly separable. Let us call this nonlinear mapping function \n$\\phi$\n so that the mapping of a sample \n$\\mathbf{x}$\n can be written as \n$\\mathbf{x} \\rightarrow \\phi (\\mathbf{x})$\n, which is called \"kernel function.\"\n\n\nNow, the term \"kernel\" describes a function that calculates the dot product of the images of the samples \n$\\mathbf{x}$\n under \n$\\phi$\n.\n\n\n$$\\kappa(\\mathbf{x_i, x_j}) =  \\phi (\\mathbf{x_i}) \\phi (\\mathbf{x_j})^T$$\n\n\nMore details about the derivation of this equation are provided in this excellent review article by Quan Wang: \nKernel Principal Component Analysis and its Applications in Face Recognition and Active Shape Models\n.[\n1\n]\n\n\nIn other words, the function \n$\\phi$\n maps the original d-dimensional features into a larger, k-dimensional feature space by creating nononlinear combinations of the original features. For example, if \n$\\mathbf{x}$\n consists of 2 features:\n\n\n$$\n\\mathbf{x} = \\big[x_1 \\quad x_2\\big]^T \\quad \\quad \\mathbf{x} \\in I\\!R^d\n$$\n\n\n$$\n\\Downarrow \\phi\n$$\n\n\n$$\n\\mathbf{x}' = \\big[x_1 \\quad x_2 \\quad x_1 x_2 \\quad x_{1}^2 \\quad x_1 x_{2}^3 \\quad \\dots \\big]^T \\quad \\quad \\mathbf{x} \\in I\\!R^k (k >> d)\n$$\n\n\nOften, the mathematical definition of the RBF kernel is written and implemented as\n\n\n$$\n\\kappa(\\mathbf{x_i, x_j}) = exp\\bigg(- \\gamma \\; \\lVert\\mathbf{x_i - x_j }\\rVert^{2}_{2} \\bigg)\n$$\n\n\nwhere \n$\\textstyle\\gamma = \\tfrac{1}{2\\sigma^2}$\n is a free parameter that is to be optimized.\n\n\nGaussian radial basis function (RBF) Kernel PCA\n\n\nIn the linear PCA approach, we are interested in the principal components that maximize the variance in the dataset. This is done by extracting the eigenvectors (principle components) that correspond to the largest eigenvalues based on the covariance matrix:\n\n\n$$\\text{Cov} = \\frac{1}{N} \\sum_{i=1}^{N} \\mathbf{x_i} \\mathbf{x_i}^T$$\n\n\nBernhard Scholkopf (\nKernel Principal Component Analysis\n [\n2\n]) generalized this approach for data that was mapped onto the higher dimensional space via a kernel function:\n\n\n$$\\text{Cov} = \\frac{1}{N} \\sum_{i=1}^{N} \\phi(\\mathbf{x_i}) \\phi(\\mathbf{x_i})^T$$\n\n\nHowever, in practice the the covariance matrix in the higher dimensional space is not calculated explicitly (kernel trick). Therefore, the implementation of RBF kernel PCA does not yield the principal component axes (in contrast to the standard PCA), but the obtained eigenvectors can be understood as projections of the data onto the principal components.\n\n\nRBF kernel PCA step-by-step\n\n\n1. Computation of the kernel (similarity) matrix.\n\n\nIn this first step, we need to calculate\n\n\n$$\\kappa(\\mathbf{x_i, x_j}) = exp\\bigg(- \\gamma \\; \\lVert\\mathbf{x_i - x_j }\\rVert^{2}_{2} \\bigg)$$\n\n\nfor every pair of points. E.g., if we have a dataset of 100 samples, this step would result in a symmetric 100x100 kernel matrix.\n\n\n2. Eigendecomposition of the kernel matrix.\n\n\nSince it is not guaranteed that the kernel matrix is centered, we can apply the following equation to do so:\n\n\n$$K' = K - \\mathbf{1_N} K - K \\mathbf{1_N} + \\mathbf{1_N} K \\mathbf{1_N}$$\n\n\nwhere \n$\\mathbf{1_N}$\n is (like the kernel matrix) a \n$N\\times N$\n matrix with all values equal to \n$\\frac{1}{N}$\n. [\n3\n]\n\n\nNow, we have to obtain the eigenvectors of the centered kernel matrix that correspond to the largest eigenvalues. Those eigenvectors are the data points already projected onto the respective principal components.\n\n\nProjecting new data\n\n\nSo far, so good, in the sections above, we have been projecting an dataset onto a new feature subspace. However, in a real application, we are usually interested in mapping new data points onto the same new feature subspace (e.g., if are working with a training and a test dataset in pattern classification tasks).\n\n\nRemember, when we computed the eigenvectors \n$\\mathbf{\\alpha}$\n of the centered kernel matrix, those values were actually already the projected datapoints onto the principal component axis \n$\\mathbf{g}$\n.\n\n\nIf we want to project a new data point \n$\\mathbf{x}$\n onto this principal component axis, we'd need to compute \n$\\phi(\\mathbf{x})^T  \\mathbf{g}$\n.\n\n\nFortunately, also here, we don't have to compute \n$\\phi(\\mathbf{x})^T  \\mathbf{g}$\n explicitely but use the kernel trick to calculate the RBF kernel between the new data point and every data point \n$j$\n in the training dataset:\n\n\n$$\\phi(\\mathbf{x})^T  \\mathbf{g}  = \\sum_j \\alpha_{i} \\; \\phi(\\mathbf{x}) \\; \\phi(\\mathbf{x_j})^T$$\n\n\n$$=  \\sum_j \\alpha_{i} \\; \\kappa(\\mathbf{x}, \\mathbf{x_j})$$\n\n\nand the eigenvectors \n$\\alpha$\n and eigenvalues \n$\\lambda$\n of the Kernel matrix \n$\\mathbf{K}$\n satisfy the equation\n\n$\\mathbf{K} \\alpha = \\lambda \\alpha$\n, we just need to normalize the eigenvector by the corresponding eigenvalue.\n\n\nReferences\n\n\n[1] Q. Wang. \nKernel principal component analysis and its applications in face recognition and active shape models\n. CoRR, abs/1207.3538, 2012.\n\n\n[2] B. Scholkopf, A. Smola, and K.-R. Muller. \nKernel principal component analysis\n. pages 583\u2013588, 1997.\n\n\n[3] B. Scholkopf, A. Smola, and K.-R. Muller. \nNonlinear component analysis as a kernel eigenvalue problem\n. Neural computation, 10(5):1299\u20131319, 1998.\n\n\nExamples\n\n\nExample 1 - Half-moon shapes\n\n\nWe will start with a simple example of 2 half-moon shapes generated by the \nmake_moons\n function from scikit-learn.\n\n\nimport matplotlib.pyplot as plt\nfrom sklearn.datasets import make_moons\n\nX, y = make_moons(n_samples=50, random_state=1)\n\nplt.scatter(X[y==0, 0], X[y==0, 1], \n            color='red', marker='o', alpha=0.5)\nplt.scatter(X[y==1, 0], X[y==1, 1], \n            color='blue', marker='^', alpha=0.5)\nplt.ylabel('y coordinate')\nplt.xlabel('x coordinate')\n\nplt.show()\n\n\n\n\n\n\nSince the two half-moon shapes are linearly inseparable, we expect that the \u201cclassic\u201d PCA will fail to give us a \u201cgood\u201d representation of the data in 1D space. Let us use \nPCA\n class to perform the dimensionality reduction.\n\n\nfrom mlxtend.feature_extraction import PrincipalComponentAnalysis as PCA\n\npca = PCA(n_components=2)\nX_pca = pca.fit(X).transform(X)\n\nplt.scatter(X_pca[y==0, 0], X_pca[y==0, 1], \n            color='red', marker='o', alpha=0.5)\nplt.scatter(X_pca[y==1, 0], X_pca[y==1, 1], \n            color='blue', marker='^', alpha=0.5)\n\nplt.xlabel('PC1')\nplt.ylabel('PC2')\nplt.show()\n\n\n\n\n\n\nAs we can see, the resulting principal components do not yield a subspace where the data is linearly separated well. Note that PCA is a unsupervised method and does not \u201cconsider\u201d class labels in order to maximize the variance in contrast to Linear Discriminant Analysis. Here, the colors blue and red are just added for visualization purposes to indicate the degree of separation.\n\n\nNext, we will perform dimensionality reduction via RBF kernel PCA on our half-moon data. The choice of \n$\\gamma$\n\ndepends on the dataset and can be obtained via hyperparameter tuning techniques like Grid Search. Hyperparameter tuning is a broad topic itself, and here I will just use a \n$\\gamma$\n-value that I found to produce \u201cgood\u201d results.\n\n\nfrom mlxtend.data import iris_data\nfrom mlxtend.preprocessing import standardize\nfrom mlxtend.feature_extraction import RBFKernelPCA as KPCA\n\nkpca = KPCA(gamma=15.0, n_components=2)\nkpca.fit(X)\nX_kpca = kpca.X_projected_\n\n\n\n\nPlease note that the components of kernel methods such as RBF kernel PCA already represent the projected data points (in contrast to PCA, where the component axis are the \"top k\" eigenvectors thar are used to contruct a projection matrix, which is then used to transform the training samples). Thus, the projected training set is available after fitting via the \n.X_projected_\n attribute.\n\n\nplt.scatter(X_kpca[y==0, 0], X_kpca[y==0, 1], \n            color='red', marker='o', alpha=0.5)\nplt.scatter(X_kpca[y==1, 0], X_kpca[y==1, 1], \n            color='blue', marker='^', alpha=0.5)\n\nplt.title('First 2 principal components after RBF Kernel PCA')\nplt.xlabel('PC1')\nplt.ylabel('PC2')\nplt.show()\n\n\n\n\n\n\nThe new feature space is linearly separable now. Since we are often interested in dimensionality reduction, let's have a look at the first component only.\n\n\nimport numpy as np\n\nplt.scatter(X_kpca[y==0, 0], np.zeros((25, 1)), \n            color='red', marker='o', alpha=0.5)\nplt.scatter(X_kpca[y==1, 0], np.zeros((25, 1)), \n            color='blue', marker='^', alpha=0.5)\n\nplt.title('First principal component after RBF Kernel PCA')\nplt.xlabel('PC1')\nplt.yticks([])\nplt.show()\n\n\n\n\n\n\nWe can clearly see that the projection via RBF kernel PCA yielded a subspace where the classes are separated well. Such a subspace can then be used as input for generalized linear classification models, e.g.,  logistic regression.\n\n\nProjecting new data\n\n\nFinally, via the transform method, we can project new data onto the new component axes.\n\n\nimport matplotlib.pyplot as plt\nfrom sklearn.datasets import make_moons\n\nX2, y2 = make_moons(n_samples=200, random_state=5)\nX2_kpca = kpca.transform(X2)\n\nplt.scatter(X_kpca[y==0, 0], X_kpca[y==0, 1], \n            color='red', marker='o', alpha=0.5, label='fit data')\nplt.scatter(X_kpca[y==1, 0], X_kpca[y==1, 1], \n            color='blue', marker='^', alpha=0.5, label='fit data')\n\nplt.scatter(X2_kpca[y2==0, 0], X2_kpca[y2==0, 1], \n            color='orange', marker='v', \n            alpha=0.2, label='new data')\nplt.scatter(X2_kpca[y2==1, 0], X2_kpca[y2==1, 1], \n            color='cyan', marker='s', \n            alpha=0.2, label='new data')\n\nplt.legend()\nplt.show()\n\n\n\n\n\n\nExample 2 - Concentric circles\n\n\nFollowing the concepts explained in example 1, let's have a look at another classic case: 2 concentric circles with random noise produced by scikit-learn\u2019s \nmake_circles\n.\n\n\nfrom sklearn.datasets import make_circles\n\nX, y = make_circles(n_samples=1000, random_state=123, \n                    noise=0.1, factor=0.2)\n\nplt.figure(figsize=(8,6))\n\nplt.scatter(X[y==0, 0], X[y==0, 1], color='red', alpha=0.5)\nplt.scatter(X[y==1, 0], X[y==1, 1], color='blue', alpha=0.5)\nplt.title('Concentric circles')\nplt.ylabel('y coordinate')\nplt.xlabel('x coordinate')\nplt.show()\n\n\n\n\n\n\nfrom mlxtend.data import iris_data\nfrom mlxtend.preprocessing import standardize\nfrom mlxtend.feature_extraction import RBFKernelPCA as KPCA\n\nkpca = KPCA(gamma=15.0, n_components=2)\nkpca.fit(X)\nX_kpca = kpca.X_projected_\n\n\n\n\nplt.scatter(X_kpca[y==0, 0], X_kpca[y==0, 1], \n            color='red', marker='o', alpha=0.5)\nplt.scatter(X_kpca[y==1, 0], X_kpca[y==1, 1], \n            color='blue', marker='^', alpha=0.5)\n\nplt.title('First 2 principal components after RBF Kernel PCA')\nplt.xlabel('PC1')\nplt.ylabel('PC2')\nplt.show()\n\n\n\n\n\n\nplt.scatter(X_kpca[y==0, 0], np.zeros((500, 1)), \n            color='red', marker='o', alpha=0.5)\nplt.scatter(X_kpca[y==1, 0], np.zeros((500, 1)), \n            color='blue', marker='^', alpha=0.5)\n\nplt.title('First principal component after RBF Kernel PCA')\nplt.xlabel('PC1')\nplt.yticks([])\nplt.show()\n\n\n\n\n\n\nAPI\n\n\nRBFKernelPrincipalComponentAnalysis(gamma=15.0, n_components=None, copy_X=True)\n\n\nRBF Kernel Principal Component Analysis for dimensionality reduction.\n\n\nParameters\n\n\n\n\n\n\ngamma\n : float (default: 15.0)\n\n\nFree parameter (coefficient) of the RBF kernel.\n\n\n\n\n\n\nn_components\n : int (default: None)\n\n\nThe number of principal components for transformation.\nKeeps the original dimensions of the dataset if \nNone\n.\n\n\n\n\n\n\ncopy_X\n : bool (default: True)\n\n\nCopies training data, which is required to compute the projection\nof new data via the transform method. Uses a reference to X if False.\n\n\n\n\n\n\nAttributes\n\n\n\n\n\n\ne_vals_\n : array-like, shape=[n_features]\n\n\nEigenvalues in sorted order.\n\n\n\n\n\n\ne_vecs_\n : array-like, shape=[n_features]\n\n\nEigenvectors in sorted order.\n\n\n\n\n\n\nX_projected_\n : array-like, shape=[n_samples, n_components]\n\n\nTraining samples projected along the component axes.\n\n\n\n\n\n\nMethods\n\n\n\n\n\nfit(X)\n\n\nFit the PCA model with X.\n\n\nParameters\n\n\n\n\n\n\nX\n : {array-like, sparse matrix}, shape = [n_samples, n_features]\n\n\nTraining vectors, where n_samples is the number of samples and\nn_features is the number of features.\n\n\n\n\n\n\nReturns\n\n\n\n\nself\n : object\n\n\n\n\n\n\n\ntransform(X)\n\n\nApply the non-linear transformation on X.\n\n\nParameters\n\n\n\n\n\n\nX\n : {array-like, sparse matrix}, shape = [n_samples, n_features]\n\n\nTraining vectors, where n_samples is the number of samples and\nn_features is the number of features.\n\n\n\n\n\n\nReturns\n\n\n\n\n\n\nX_projected\n : np.ndarray, shape = [n_samples, n_components]\n\n\nProjected training vectors.",
            "title": "RBFKernelPCA"
        },
        {
            "location": "/user_guide/feature_extraction/RBFKernelPCA/#rbf-kernel-principal-component-analysis",
            "text": "Implementation of RBF Kernel Principal Component Analysis for non-linear dimensionality reduction   from mlxtend.feature_extraction import RBFKernelPCA",
            "title": "RBF Kernel Principal Component Analysis"
        },
        {
            "location": "/user_guide/feature_extraction/RBFKernelPCA/#overview",
            "text": "Most machine learning algorithms have been developed and statistically validated for linearly separable data. Popular examples are linear classifiers like Support Vector Machines (SVMs) or the (standard) Principal Component Analysis (PCA) for dimensionality reduction. However, most real world data requires nonlinear methods in order to perform tasks that involve the analysis and discovery of patterns successfully.  The focus of this overview is to briefly introduce the idea of kernel methods and to implement a Gaussian radius basis function (RBF) kernel that is used to perform nonlinear dimensionality reduction via BF kernel principal component analysis (kPCA).",
            "title": "Overview"
        },
        {
            "location": "/user_guide/feature_extraction/RBFKernelPCA/#principal-component-analysis",
            "text": "The main purpose of principal component analysis (PCA) is the analysis of data to identify patterns that represent the data \u201cwell.\u201d The principal components can be understood as new axes of the dataset that maximize the variance along those axes (the eigenvectors of the covariance matrix). In other words, PCA aims to find the axes with maximum variances along which the data is most spread.   For more details, please see the related article on  mlxtend.feature_extraction.PrincipalComponentAnalysis .",
            "title": "Principal Component Analysis"
        },
        {
            "location": "/user_guide/feature_extraction/RBFKernelPCA/#nonlinear-dimensionality-reduction",
            "text": "The \u201cclassic\u201d PCA approach described above is a linear projection technique that works well if the data is linearly separable. However, in the case of linearly inseparable data, a nonlinear technique is required if the task is to reduce the dimensionality of a dataset.",
            "title": "Nonlinear dimensionality reduction"
        },
        {
            "location": "/user_guide/feature_extraction/RBFKernelPCA/#kernel-functions-and-the-kernel-trick",
            "text": "The basic idea to deal with linearly inseparable data is to project it onto a higher dimensional space where it becomes linearly separable. Let us call this nonlinear mapping function  $\\phi$  so that the mapping of a sample  $\\mathbf{x}$  can be written as  $\\mathbf{x} \\rightarrow \\phi (\\mathbf{x})$ , which is called \"kernel function.\"  Now, the term \"kernel\" describes a function that calculates the dot product of the images of the samples  $\\mathbf{x}$  under  $\\phi$ .  $$\\kappa(\\mathbf{x_i, x_j}) =  \\phi (\\mathbf{x_i}) \\phi (\\mathbf{x_j})^T$$  More details about the derivation of this equation are provided in this excellent review article by Quan Wang:  Kernel Principal Component Analysis and its Applications in Face Recognition and Active Shape Models .[ 1 ]  In other words, the function  $\\phi$  maps the original d-dimensional features into a larger, k-dimensional feature space by creating nononlinear combinations of the original features. For example, if  $\\mathbf{x}$  consists of 2 features:  $$\n\\mathbf{x} = \\big[x_1 \\quad x_2\\big]^T \\quad \\quad \\mathbf{x} \\in I\\!R^d\n$$  $$\n\\Downarrow \\phi\n$$  $$\n\\mathbf{x}' = \\big[x_1 \\quad x_2 \\quad x_1 x_2 \\quad x_{1}^2 \\quad x_1 x_{2}^3 \\quad \\dots \\big]^T \\quad \\quad \\mathbf{x} \\in I\\!R^k (k >> d)\n$$  Often, the mathematical definition of the RBF kernel is written and implemented as  $$\n\\kappa(\\mathbf{x_i, x_j}) = exp\\bigg(- \\gamma \\; \\lVert\\mathbf{x_i - x_j }\\rVert^{2}_{2} \\bigg)\n$$  where  $\\textstyle\\gamma = \\tfrac{1}{2\\sigma^2}$  is a free parameter that is to be optimized.",
            "title": "Kernel functions and the kernel trick"
        },
        {
            "location": "/user_guide/feature_extraction/RBFKernelPCA/#gaussian-radial-basis-function-rbf-kernel-pca",
            "text": "In the linear PCA approach, we are interested in the principal components that maximize the variance in the dataset. This is done by extracting the eigenvectors (principle components) that correspond to the largest eigenvalues based on the covariance matrix:  $$\\text{Cov} = \\frac{1}{N} \\sum_{i=1}^{N} \\mathbf{x_i} \\mathbf{x_i}^T$$  Bernhard Scholkopf ( Kernel Principal Component Analysis  [ 2 ]) generalized this approach for data that was mapped onto the higher dimensional space via a kernel function:  $$\\text{Cov} = \\frac{1}{N} \\sum_{i=1}^{N} \\phi(\\mathbf{x_i}) \\phi(\\mathbf{x_i})^T$$  However, in practice the the covariance matrix in the higher dimensional space is not calculated explicitly (kernel trick). Therefore, the implementation of RBF kernel PCA does not yield the principal component axes (in contrast to the standard PCA), but the obtained eigenvectors can be understood as projections of the data onto the principal components.",
            "title": "Gaussian radial basis function (RBF) Kernel PCA"
        },
        {
            "location": "/user_guide/feature_extraction/RBFKernelPCA/#rbf-kernel-pca-step-by-step",
            "text": "",
            "title": "RBF kernel PCA step-by-step"
        },
        {
            "location": "/user_guide/feature_extraction/RBFKernelPCA/#1-computation-of-the-kernel-similarity-matrix",
            "text": "In this first step, we need to calculate  $$\\kappa(\\mathbf{x_i, x_j}) = exp\\bigg(- \\gamma \\; \\lVert\\mathbf{x_i - x_j }\\rVert^{2}_{2} \\bigg)$$  for every pair of points. E.g., if we have a dataset of 100 samples, this step would result in a symmetric 100x100 kernel matrix.",
            "title": "1. Computation of the kernel (similarity) matrix."
        },
        {
            "location": "/user_guide/feature_extraction/RBFKernelPCA/#2-eigendecomposition-of-the-kernel-matrix",
            "text": "Since it is not guaranteed that the kernel matrix is centered, we can apply the following equation to do so:  $$K' = K - \\mathbf{1_N} K - K \\mathbf{1_N} + \\mathbf{1_N} K \\mathbf{1_N}$$  where  $\\mathbf{1_N}$  is (like the kernel matrix) a  $N\\times N$  matrix with all values equal to  $\\frac{1}{N}$ . [ 3 ]  Now, we have to obtain the eigenvectors of the centered kernel matrix that correspond to the largest eigenvalues. Those eigenvectors are the data points already projected onto the respective principal components.",
            "title": "2. Eigendecomposition of the kernel matrix."
        },
        {
            "location": "/user_guide/feature_extraction/RBFKernelPCA/#projecting-new-data",
            "text": "So far, so good, in the sections above, we have been projecting an dataset onto a new feature subspace. However, in a real application, we are usually interested in mapping new data points onto the same new feature subspace (e.g., if are working with a training and a test dataset in pattern classification tasks).  Remember, when we computed the eigenvectors  $\\mathbf{\\alpha}$  of the centered kernel matrix, those values were actually already the projected datapoints onto the principal component axis  $\\mathbf{g}$ .  If we want to project a new data point  $\\mathbf{x}$  onto this principal component axis, we'd need to compute  $\\phi(\\mathbf{x})^T  \\mathbf{g}$ .  Fortunately, also here, we don't have to compute  $\\phi(\\mathbf{x})^T  \\mathbf{g}$  explicitely but use the kernel trick to calculate the RBF kernel between the new data point and every data point  $j$  in the training dataset:  $$\\phi(\\mathbf{x})^T  \\mathbf{g}  = \\sum_j \\alpha_{i} \\; \\phi(\\mathbf{x}) \\; \\phi(\\mathbf{x_j})^T$$  $$=  \\sum_j \\alpha_{i} \\; \\kappa(\\mathbf{x}, \\mathbf{x_j})$$  and the eigenvectors  $\\alpha$  and eigenvalues  $\\lambda$  of the Kernel matrix  $\\mathbf{K}$  satisfy the equation $\\mathbf{K} \\alpha = \\lambda \\alpha$ , we just need to normalize the eigenvector by the corresponding eigenvalue.",
            "title": "Projecting new data"
        },
        {
            "location": "/user_guide/feature_extraction/RBFKernelPCA/#references",
            "text": "[1] Q. Wang.  Kernel principal component analysis and its applications in face recognition and active shape models . CoRR, abs/1207.3538, 2012.  [2] B. Scholkopf, A. Smola, and K.-R. Muller.  Kernel principal component analysis . pages 583\u2013588, 1997.  [3] B. Scholkopf, A. Smola, and K.-R. Muller.  Nonlinear component analysis as a kernel eigenvalue problem . Neural computation, 10(5):1299\u20131319, 1998.",
            "title": "References"
        },
        {
            "location": "/user_guide/feature_extraction/RBFKernelPCA/#examples",
            "text": "",
            "title": "Examples"
        },
        {
            "location": "/user_guide/feature_extraction/RBFKernelPCA/#example-1-half-moon-shapes",
            "text": "We will start with a simple example of 2 half-moon shapes generated by the  make_moons  function from scikit-learn.  import matplotlib.pyplot as plt\nfrom sklearn.datasets import make_moons\n\nX, y = make_moons(n_samples=50, random_state=1)\n\nplt.scatter(X[y==0, 0], X[y==0, 1], \n            color='red', marker='o', alpha=0.5)\nplt.scatter(X[y==1, 0], X[y==1, 1], \n            color='blue', marker='^', alpha=0.5)\nplt.ylabel('y coordinate')\nplt.xlabel('x coordinate')\n\nplt.show()   Since the two half-moon shapes are linearly inseparable, we expect that the \u201cclassic\u201d PCA will fail to give us a \u201cgood\u201d representation of the data in 1D space. Let us use  PCA  class to perform the dimensionality reduction.  from mlxtend.feature_extraction import PrincipalComponentAnalysis as PCA\n\npca = PCA(n_components=2)\nX_pca = pca.fit(X).transform(X)\n\nplt.scatter(X_pca[y==0, 0], X_pca[y==0, 1], \n            color='red', marker='o', alpha=0.5)\nplt.scatter(X_pca[y==1, 0], X_pca[y==1, 1], \n            color='blue', marker='^', alpha=0.5)\n\nplt.xlabel('PC1')\nplt.ylabel('PC2')\nplt.show()   As we can see, the resulting principal components do not yield a subspace where the data is linearly separated well. Note that PCA is a unsupervised method and does not \u201cconsider\u201d class labels in order to maximize the variance in contrast to Linear Discriminant Analysis. Here, the colors blue and red are just added for visualization purposes to indicate the degree of separation.  Next, we will perform dimensionality reduction via RBF kernel PCA on our half-moon data. The choice of  $\\gamma$ \ndepends on the dataset and can be obtained via hyperparameter tuning techniques like Grid Search. Hyperparameter tuning is a broad topic itself, and here I will just use a  $\\gamma$ -value that I found to produce \u201cgood\u201d results.  from mlxtend.data import iris_data\nfrom mlxtend.preprocessing import standardize\nfrom mlxtend.feature_extraction import RBFKernelPCA as KPCA\n\nkpca = KPCA(gamma=15.0, n_components=2)\nkpca.fit(X)\nX_kpca = kpca.X_projected_  Please note that the components of kernel methods such as RBF kernel PCA already represent the projected data points (in contrast to PCA, where the component axis are the \"top k\" eigenvectors thar are used to contruct a projection matrix, which is then used to transform the training samples). Thus, the projected training set is available after fitting via the  .X_projected_  attribute.  plt.scatter(X_kpca[y==0, 0], X_kpca[y==0, 1], \n            color='red', marker='o', alpha=0.5)\nplt.scatter(X_kpca[y==1, 0], X_kpca[y==1, 1], \n            color='blue', marker='^', alpha=0.5)\n\nplt.title('First 2 principal components after RBF Kernel PCA')\nplt.xlabel('PC1')\nplt.ylabel('PC2')\nplt.show()   The new feature space is linearly separable now. Since we are often interested in dimensionality reduction, let's have a look at the first component only.  import numpy as np\n\nplt.scatter(X_kpca[y==0, 0], np.zeros((25, 1)), \n            color='red', marker='o', alpha=0.5)\nplt.scatter(X_kpca[y==1, 0], np.zeros((25, 1)), \n            color='blue', marker='^', alpha=0.5)\n\nplt.title('First principal component after RBF Kernel PCA')\nplt.xlabel('PC1')\nplt.yticks([])\nplt.show()   We can clearly see that the projection via RBF kernel PCA yielded a subspace where the classes are separated well. Such a subspace can then be used as input for generalized linear classification models, e.g.,  logistic regression.",
            "title": "Example 1 - Half-moon shapes"
        },
        {
            "location": "/user_guide/feature_extraction/RBFKernelPCA/#projecting-new-data_1",
            "text": "Finally, via the transform method, we can project new data onto the new component axes.  import matplotlib.pyplot as plt\nfrom sklearn.datasets import make_moons\n\nX2, y2 = make_moons(n_samples=200, random_state=5)\nX2_kpca = kpca.transform(X2)\n\nplt.scatter(X_kpca[y==0, 0], X_kpca[y==0, 1], \n            color='red', marker='o', alpha=0.5, label='fit data')\nplt.scatter(X_kpca[y==1, 0], X_kpca[y==1, 1], \n            color='blue', marker='^', alpha=0.5, label='fit data')\n\nplt.scatter(X2_kpca[y2==0, 0], X2_kpca[y2==0, 1], \n            color='orange', marker='v', \n            alpha=0.2, label='new data')\nplt.scatter(X2_kpca[y2==1, 0], X2_kpca[y2==1, 1], \n            color='cyan', marker='s', \n            alpha=0.2, label='new data')\n\nplt.legend()\nplt.show()",
            "title": "Projecting new data"
        },
        {
            "location": "/user_guide/feature_extraction/RBFKernelPCA/#example-2-concentric-circles",
            "text": "Following the concepts explained in example 1, let's have a look at another classic case: 2 concentric circles with random noise produced by scikit-learn\u2019s  make_circles .  from sklearn.datasets import make_circles\n\nX, y = make_circles(n_samples=1000, random_state=123, \n                    noise=0.1, factor=0.2)\n\nplt.figure(figsize=(8,6))\n\nplt.scatter(X[y==0, 0], X[y==0, 1], color='red', alpha=0.5)\nplt.scatter(X[y==1, 0], X[y==1, 1], color='blue', alpha=0.5)\nplt.title('Concentric circles')\nplt.ylabel('y coordinate')\nplt.xlabel('x coordinate')\nplt.show()   from mlxtend.data import iris_data\nfrom mlxtend.preprocessing import standardize\nfrom mlxtend.feature_extraction import RBFKernelPCA as KPCA\n\nkpca = KPCA(gamma=15.0, n_components=2)\nkpca.fit(X)\nX_kpca = kpca.X_projected_  plt.scatter(X_kpca[y==0, 0], X_kpca[y==0, 1], \n            color='red', marker='o', alpha=0.5)\nplt.scatter(X_kpca[y==1, 0], X_kpca[y==1, 1], \n            color='blue', marker='^', alpha=0.5)\n\nplt.title('First 2 principal components after RBF Kernel PCA')\nplt.xlabel('PC1')\nplt.ylabel('PC2')\nplt.show()   plt.scatter(X_kpca[y==0, 0], np.zeros((500, 1)), \n            color='red', marker='o', alpha=0.5)\nplt.scatter(X_kpca[y==1, 0], np.zeros((500, 1)), \n            color='blue', marker='^', alpha=0.5)\n\nplt.title('First principal component after RBF Kernel PCA')\nplt.xlabel('PC1')\nplt.yticks([])\nplt.show()",
            "title": "Example 2 - Concentric circles"
        },
        {
            "location": "/user_guide/feature_extraction/RBFKernelPCA/#api",
            "text": "RBFKernelPrincipalComponentAnalysis(gamma=15.0, n_components=None, copy_X=True)  RBF Kernel Principal Component Analysis for dimensionality reduction.  Parameters    gamma  : float (default: 15.0)  Free parameter (coefficient) of the RBF kernel.    n_components  : int (default: None)  The number of principal components for transformation.\nKeeps the original dimensions of the dataset if  None .    copy_X  : bool (default: True)  Copies training data, which is required to compute the projection\nof new data via the transform method. Uses a reference to X if False.    Attributes    e_vals_  : array-like, shape=[n_features]  Eigenvalues in sorted order.    e_vecs_  : array-like, shape=[n_features]  Eigenvectors in sorted order.    X_projected_  : array-like, shape=[n_samples, n_components]  Training samples projected along the component axes.",
            "title": "API"
        },
        {
            "location": "/user_guide/feature_extraction/RBFKernelPCA/#methods",
            "text": "fit(X)  Fit the PCA model with X.  Parameters    X  : {array-like, sparse matrix}, shape = [n_samples, n_features]  Training vectors, where n_samples is the number of samples and\nn_features is the number of features.    Returns   self  : object    transform(X)  Apply the non-linear transformation on X.  Parameters    X  : {array-like, sparse matrix}, shape = [n_samples, n_features]  Training vectors, where n_samples is the number of samples and\nn_features is the number of features.    Returns    X_projected  : np.ndarray, shape = [n_samples, n_components]  Projected training vectors.",
            "title": "Methods"
        },
        {
            "location": "/user_guide/cluster/Kmeans/",
            "text": "Kmeans\n\n\nA implementation of k-means clustering.\n\n\n\n\nfrom mlxtend.cluster import Kmeans\n\n\n\n\nOverview\n\n\nClustering falls into the category of unsupervised learning, a subfield of machine learning where the ground truth labels are not available to us in real-world applications. In clustering, our goal is to group samples by similarity (in k-means: Euclidean distance).\n\n\nThe k-means algorithms can be summarized as follows:\n\n\n\n\nRandomly pick k centroids from the sample points as initial cluster centers.\n\n\nAssign each sample to the nearest centroid \n$\\mu(j), \\; j \\in {1,...,k}$\n.\n\n\nMove the centroids to the center of the samples that were assigned to it.\n\n\nRepeat steps 2 and 3 until the cluster assignments do not change or a user-defined tolerance or a maximum number of iterations is reached.\n\n\n\n\nReferences\n\n\n\n\nMacQueen, J. B. (1967). \nSome Methods for classification and Analysis of Multivariate Observations\n. Proceedings of 5th Berkeley Symposium on Mathematical Statistics and Probability. University of California Press. pp. 281\u2013297. MR 0214227. Zbl 0214.46201. Retrieved 2009-04-07.\n\n\n\n\nExamples\n\n\nExample 1 - Three Blobs\n\n\nLoad some sample data:\n\n\nimport matplotlib.pyplot as plt\nfrom mlxtend.data import three_blobs_data\n\nX, y = three_blobs_data()\nplt.scatter(X[:, 0], X[:, 1], c='white')\nplt.show()\n\n\n\n\n\n\nCompute the cluster centroids:\n\n\nfrom mlxtend.cluster import Kmeans\n\nkm = Kmeans(k=3, \n            max_iter=50, \n            random_seed=1, \n            print_progress=3)\n\nkm.fit(X)\n\nprint('Iterations until convergence:', km.iterations_)\nprint('Final centroids:\\n', km.centroids_)\n\n\n\n\nIteration: 2/50 | Elapsed: 00:00:00 | ETA: 00:00:00\n\nIterations until convergence: 2\nFinal centroids:\n [[-1.5947298   2.92236966]\n [ 2.06521743  0.96137409]\n [ 0.9329651   4.35420713]]\n\n\n\nVisualize the cluster memberships:\n\n\ny_clust = km.predict(X)\n\nplt.scatter(X[y_clust == 0, 0],\n            X[y_clust == 0, 1],\n            s=50,\n            c='lightgreen',\n            marker='s',\n            label='cluster 1')\n\nplt.scatter(X[y_clust == 1,0],\n            X[y_clust == 1,1],\n            s=50,\n            c='orange',\n            marker='o',\n            label='cluster 2')\n\nplt.scatter(X[y_clust == 2,0],\n            X[y_clust == 2,1],\n            s=50,\n            c='lightblue',\n            marker='v',\n            label='cluster 3')\n\n\nplt.scatter(km.centroids_[:,0],\n            km.centroids_[:,1],\n            s=250,\n            marker='*',\n            c='red',\n            label='centroids')\n\nplt.legend(loc='lower left',\n           scatterpoints=1)\nplt.grid()\nplt.show()\n\n\n\n\n\n\nAPI\n\n\nKmeans(k, max_iter=10, convergence_tolerance=1e-05, random_seed=None, print_progress=0)\n\n\nK-means clustering class.\n\n\nAdded in 0.4.1dev\n\n\n\nParameters\n\n\n\n\n\n\nk\n : int\n\n\nNumber of clusters\n\n\n\n\n\n\nmax_iter\n : int (default: 10)\n\n\nNumber of iterations during cluster assignment.\nCluster re-assignment stops automatically when the algorithm\nconverged.\n\n\n\n\n\n\nconvergence_tolerance\n : float (default: 1e-05)\n\n\nCompares current centroids with centroids of the previous iteration\nusing the given tolerance (a small positive float)to determine\nif the algorithm converged early.\n\n\n\n\n\n\nrandom_seed\n : int (default: None)\n\n\nSet random state for the initial centroid assignment.\n\n\n\n\n\n\nprint_progress\n : int (default: 0)\n\n\nPrints progress in fitting to stderr.\n0: No output\n1: Iterations elapsed\n2: 1 plus time elapsed\n3: 2 plus estimated time until completion\n\n\n\n\n\n\nAttributes\n\n\n\n\n\n\ncentroids_\n : 2d-array, shape={k, n_features}\n\n\nFeature values of the k cluster centroids.\n\n\n\n\n\n\ncusters_\n : dictionary\n\n\nThe cluster assignments stored as a Python dictionary;\nthe dictionary keys denote the cluster indeces and the items are\nPython lists of the sample indices that were assigned to each\ncluster.\n\n\n\n\n\n\niterations_\n : int\n\n\nNumber of iterations until convergence.\n\n\n\n\n\n\nMethods\n\n\n\n\n\nfit(X, init_params=True)\n\n\nLearn model from training data.\n\n\nParameters\n\n\n\n\n\n\nX\n : {array-like, sparse matrix}, shape = [n_samples, n_features]\n\n\nTraining vectors, where n_samples is the number of samples and\nn_features is the number of features.\n\n\n\n\n\n\ninit_params\n : bool (default: True)\n\n\nRe-initializes model parameters prior to fitting.\nSet False to continue training with weights from\na previous model fitting.\n\n\n\n\n\n\nReturns\n\n\n\n\nself\n : object\n\n\n\n\n\n\n\npredict(X)\n\n\nPredict targets from X.\n\n\nParameters\n\n\n\n\n\n\nX\n : {array-like, sparse matrix}, shape = [n_samples, n_features]\n\n\nTraining vectors, where n_samples is the number of samples and\nn_features is the number of features.\n\n\n\n\n\n\nReturns\n\n\n\n\n\n\ntarget_values\n : array-like, shape = [n_samples]\n\n\nPredicted target values.",
            "title": "Kmeans"
        },
        {
            "location": "/user_guide/cluster/Kmeans/#kmeans",
            "text": "A implementation of k-means clustering.   from mlxtend.cluster import Kmeans",
            "title": "Kmeans"
        },
        {
            "location": "/user_guide/cluster/Kmeans/#overview",
            "text": "Clustering falls into the category of unsupervised learning, a subfield of machine learning where the ground truth labels are not available to us in real-world applications. In clustering, our goal is to group samples by similarity (in k-means: Euclidean distance).  The k-means algorithms can be summarized as follows:   Randomly pick k centroids from the sample points as initial cluster centers.  Assign each sample to the nearest centroid  $\\mu(j), \\; j \\in {1,...,k}$ .  Move the centroids to the center of the samples that were assigned to it.  Repeat steps 2 and 3 until the cluster assignments do not change or a user-defined tolerance or a maximum number of iterations is reached.",
            "title": "Overview"
        },
        {
            "location": "/user_guide/cluster/Kmeans/#references",
            "text": "MacQueen, J. B. (1967).  Some Methods for classification and Analysis of Multivariate Observations . Proceedings of 5th Berkeley Symposium on Mathematical Statistics and Probability. University of California Press. pp. 281\u2013297. MR 0214227. Zbl 0214.46201. Retrieved 2009-04-07.",
            "title": "References"
        },
        {
            "location": "/user_guide/cluster/Kmeans/#examples",
            "text": "",
            "title": "Examples"
        },
        {
            "location": "/user_guide/cluster/Kmeans/#example-1-three-blobs",
            "text": "",
            "title": "Example 1 - Three Blobs"
        },
        {
            "location": "/user_guide/cluster/Kmeans/#load-some-sample-data",
            "text": "import matplotlib.pyplot as plt\nfrom mlxtend.data import three_blobs_data\n\nX, y = three_blobs_data()\nplt.scatter(X[:, 0], X[:, 1], c='white')\nplt.show()",
            "title": "Load some sample data:"
        },
        {
            "location": "/user_guide/cluster/Kmeans/#compute-the-cluster-centroids",
            "text": "from mlxtend.cluster import Kmeans\n\nkm = Kmeans(k=3, \n            max_iter=50, \n            random_seed=1, \n            print_progress=3)\n\nkm.fit(X)\n\nprint('Iterations until convergence:', km.iterations_)\nprint('Final centroids:\\n', km.centroids_)  Iteration: 2/50 | Elapsed: 00:00:00 | ETA: 00:00:00\n\nIterations until convergence: 2\nFinal centroids:\n [[-1.5947298   2.92236966]\n [ 2.06521743  0.96137409]\n [ 0.9329651   4.35420713]]",
            "title": "Compute the cluster centroids:"
        },
        {
            "location": "/user_guide/cluster/Kmeans/#visualize-the-cluster-memberships",
            "text": "y_clust = km.predict(X)\n\nplt.scatter(X[y_clust == 0, 0],\n            X[y_clust == 0, 1],\n            s=50,\n            c='lightgreen',\n            marker='s',\n            label='cluster 1')\n\nplt.scatter(X[y_clust == 1,0],\n            X[y_clust == 1,1],\n            s=50,\n            c='orange',\n            marker='o',\n            label='cluster 2')\n\nplt.scatter(X[y_clust == 2,0],\n            X[y_clust == 2,1],\n            s=50,\n            c='lightblue',\n            marker='v',\n            label='cluster 3')\n\n\nplt.scatter(km.centroids_[:,0],\n            km.centroids_[:,1],\n            s=250,\n            marker='*',\n            c='red',\n            label='centroids')\n\nplt.legend(loc='lower left',\n           scatterpoints=1)\nplt.grid()\nplt.show()",
            "title": "Visualize the cluster memberships:"
        },
        {
            "location": "/user_guide/cluster/Kmeans/#api",
            "text": "Kmeans(k, max_iter=10, convergence_tolerance=1e-05, random_seed=None, print_progress=0)  K-means clustering class.  Added in 0.4.1dev  Parameters    k  : int  Number of clusters    max_iter  : int (default: 10)  Number of iterations during cluster assignment.\nCluster re-assignment stops automatically when the algorithm\nconverged.    convergence_tolerance  : float (default: 1e-05)  Compares current centroids with centroids of the previous iteration\nusing the given tolerance (a small positive float)to determine\nif the algorithm converged early.    random_seed  : int (default: None)  Set random state for the initial centroid assignment.    print_progress  : int (default: 0)  Prints progress in fitting to stderr.\n0: No output\n1: Iterations elapsed\n2: 1 plus time elapsed\n3: 2 plus estimated time until completion    Attributes    centroids_  : 2d-array, shape={k, n_features}  Feature values of the k cluster centroids.    custers_  : dictionary  The cluster assignments stored as a Python dictionary;\nthe dictionary keys denote the cluster indeces and the items are\nPython lists of the sample indices that were assigned to each\ncluster.    iterations_  : int  Number of iterations until convergence.",
            "title": "API"
        },
        {
            "location": "/user_guide/cluster/Kmeans/#methods",
            "text": "fit(X, init_params=True)  Learn model from training data.  Parameters    X  : {array-like, sparse matrix}, shape = [n_samples, n_features]  Training vectors, where n_samples is the number of samples and\nn_features is the number of features.    init_params  : bool (default: True)  Re-initializes model parameters prior to fitting.\nSet False to continue training with weights from\na previous model fitting.    Returns   self  : object    predict(X)  Predict targets from X.  Parameters    X  : {array-like, sparse matrix}, shape = [n_samples, n_features]  Training vectors, where n_samples is the number of samples and\nn_features is the number of features.    Returns    target_values  : array-like, shape = [n_samples]  Predicted target values.",
            "title": "Methods"
        },
        {
            "location": "/user_guide/tf_cluster/TfKmeans/",
            "text": "TfKmeans\n\n\nA implementation of k-means clustering in TensorFlow.\n\n\n\n\nfrom mlxtend.tf_cluster import TfKmeans\n\n\n\n\nOverview\n\n\nClustering falls into the category of unsupervised learning, a subfield of machine learning where the ground truth labels are not available to us in real-world applications. In clustering, our goal is to group samples by similarity (in k-means: Euclidean distance).\n\n\nThe k-means algorithms can be summarized as follows:\n\n\n\n\nRandomly pick k centroids from the sample points as initial cluster centers.\n\n\nAssign each sample to the nearest centroid \n$\\mu(j), \\; j \\in {1,...,k}$\n.\n\n\nMove the centroids to the center of the samples that were assigned to it.\n\n\nRepeat steps 2 and 3 until the cluster assignments do not change or a user-defined tolerance or a maximum number of iterations is reached.\n\n\n\n\nReferences\n\n\n\n\nMacQueen, J. B. (1967). \nSome Methods for classification and Analysis of Multivariate Observations\n. Proceedings of 5th Berkeley Symposium on Mathematical Statistics and Probability. University of California Press. pp. 281\u2013297. MR 0214227. Zbl 0214.46201. Retrieved 2009-04-07.\n\n\n\n\nExamples\n\n\nExample 1 - Three Blobs\n\n\nLoad some sample data:\n\n\nimport matplotlib.pyplot as plt\nfrom mlxtend.data import three_blobs_data\n\nX, y = three_blobs_data()\nplt.scatter(X[:, 0], X[:, 1], c='white')\nplt.show()\n\n\n\n\n\n\nCompute the cluster centroids:\n\n\nfrom mlxtend.tf_cluster import TfKmeans\n\nkm = TfKmeans(k=3, \n              max_iter=50, \n              random_seed=1, \n              print_progress=3)\n\nkm.fit(X)\n\nprint('Iterations until convergence:', km.iterations_)\nprint('Final centroids:\\n', km.centroids_)\n\n\n\n\nIteration: 2/50 | Elapsed: 00:00:00 | ETA: 00:00:00\n\nIterations until convergence: 2\nFinal centroids:\n [[-1.59473002  2.92236972]\n [ 2.06521749  0.96137404]\n [ 0.93296534  4.35420704]]\n\n\n\nVisualize the cluster memberships:\n\n\ny_clust = km.predict(X)\n\nplt.scatter(X[y_clust == 0, 0],\n            X[y_clust == 0, 1],\n            s=50,\n            c='lightgreen',\n            marker='s',\n            label='cluster 1')\n\nplt.scatter(X[y_clust == 1,0],\n            X[y_clust == 1,1],\n            s=50,\n            c='orange',\n            marker='o',\n            label='cluster 2')\n\nplt.scatter(X[y_clust == 2,0],\n            X[y_clust == 2,1],\n            s=50,\n            c='lightblue',\n            marker='v',\n            label='cluster 3')\n\n\nplt.scatter(km.centroids_[:,0],\n            km.centroids_[:,1],\n            s=250,\n            marker='*',\n            c='red',\n            label='centroids')\n\nplt.legend(loc='lower left', \n           scatterpoints=1)\nplt.grid()\nplt.show()\n\n\n\n\n\n\nAPI\n\n\nTfKmeans(k, max_iter=10, convergence_tolerance=1e-05, random_seed=None, print_progress=0, dtype=None)\n\n\nTensorFlow K-means clustering class.\n\n\nAdded in 0.4.1dev\n\n\n\nParameters\n\n\n\n\n\n\nk\n : int\n\n\nNumber of clusters\n\n\n\n\n\n\nmax_iter\n : int (default: 10)\n\n\nNumber of iterations during cluster assignment.\nCluster re-assignment stops automatically when the algorithm\nconverged.\n\n\n\n\n\n\nconvergence_tolerance\n : float (default: 1e-05)\n\n\nCompares current centroids with centroids of the previous iteration\nusing the given tolerance (a small positive float)to determine\nif the algorithm converged early.\n\n\n\n\n\n\nrandom_seed\n : int (default: None)\n\n\nSet random state for the initial centroid assignment.\n\n\n\n\n\n\nprint_progress\n : int (default: 0)\n\n\nPrints progress in fitting to stderr.\n0: No output\n1: Iterations elapsed\n2: 1 plus time elapsed\n3: 2 plus estimated time until completion\n\n\n\n\n\n\ndtype\n : Array-type (default: None)\n\n\nUses tensorflow.float32 if None.\n\n\n\n\n\n\nAttributes\n\n\n\n\n\n\ncentroids_\n : 2d-array, shape = {k, n_features}\n\n\nFeature values of the k cluster centroids.\n\n\n\n\n\n\ncusters_\n : dictionary\n\n\nThe cluster assignments stored as a Python dictionary;\nthe dictionary keys denote the cluster indeces and the items are\nPython lists of the sample indices that were assigned to each\ncluster.\n\n\n\n\n\n\niterations_\n : int\n\n\nNumber of iterations until convergence.\n\n\n\n\n\n\nMethods\n\n\n\n\n\nfit(X, init_params=True)\n\n\nLearn model from training data.\n\n\nParameters\n\n\n\n\n\n\nX\n : {array-like, sparse matrix}, shape = [n_samples, n_features]\n\n\nTraining vectors, where n_samples is the number of samples and\nn_features is the number of features.\n\n\n\n\n\n\ninit_params\n : bool (default: True)\n\n\nRe-initializes model parameters prior to fitting.\nSet False to continue training with weights from\na previous model fitting.\n\n\n\n\n\n\nReturns\n\n\n\n\nself\n : object\n\n\n\n\n\n\n\npredict(X)\n\n\nPredict targets from X.\n\n\nParameters\n\n\n\n\n\n\nX\n : {array-like, sparse matrix}, shape = [n_samples, n_features]\n\n\nTraining vectors, where n_samples is the number of samples and\nn_features is the number of features.\n\n\n\n\n\n\nReturns\n\n\n\n\n\n\ntarget_values\n : array-like, shape = [n_samples]\n\n\nPredicted target values.",
            "title": "TfKmeans"
        },
        {
            "location": "/user_guide/tf_cluster/TfKmeans/#tfkmeans",
            "text": "A implementation of k-means clustering in TensorFlow.   from mlxtend.tf_cluster import TfKmeans",
            "title": "TfKmeans"
        },
        {
            "location": "/user_guide/tf_cluster/TfKmeans/#overview",
            "text": "Clustering falls into the category of unsupervised learning, a subfield of machine learning where the ground truth labels are not available to us in real-world applications. In clustering, our goal is to group samples by similarity (in k-means: Euclidean distance).  The k-means algorithms can be summarized as follows:   Randomly pick k centroids from the sample points as initial cluster centers.  Assign each sample to the nearest centroid  $\\mu(j), \\; j \\in {1,...,k}$ .  Move the centroids to the center of the samples that were assigned to it.  Repeat steps 2 and 3 until the cluster assignments do not change or a user-defined tolerance or a maximum number of iterations is reached.",
            "title": "Overview"
        },
        {
            "location": "/user_guide/tf_cluster/TfKmeans/#references",
            "text": "MacQueen, J. B. (1967).  Some Methods for classification and Analysis of Multivariate Observations . Proceedings of 5th Berkeley Symposium on Mathematical Statistics and Probability. University of California Press. pp. 281\u2013297. MR 0214227. Zbl 0214.46201. Retrieved 2009-04-07.",
            "title": "References"
        },
        {
            "location": "/user_guide/tf_cluster/TfKmeans/#examples",
            "text": "",
            "title": "Examples"
        },
        {
            "location": "/user_guide/tf_cluster/TfKmeans/#example-1-three-blobs",
            "text": "",
            "title": "Example 1 - Three Blobs"
        },
        {
            "location": "/user_guide/tf_cluster/TfKmeans/#load-some-sample-data",
            "text": "import matplotlib.pyplot as plt\nfrom mlxtend.data import three_blobs_data\n\nX, y = three_blobs_data()\nplt.scatter(X[:, 0], X[:, 1], c='white')\nplt.show()",
            "title": "Load some sample data:"
        },
        {
            "location": "/user_guide/tf_cluster/TfKmeans/#compute-the-cluster-centroids",
            "text": "from mlxtend.tf_cluster import TfKmeans\n\nkm = TfKmeans(k=3, \n              max_iter=50, \n              random_seed=1, \n              print_progress=3)\n\nkm.fit(X)\n\nprint('Iterations until convergence:', km.iterations_)\nprint('Final centroids:\\n', km.centroids_)  Iteration: 2/50 | Elapsed: 00:00:00 | ETA: 00:00:00\n\nIterations until convergence: 2\nFinal centroids:\n [[-1.59473002  2.92236972]\n [ 2.06521749  0.96137404]\n [ 0.93296534  4.35420704]]",
            "title": "Compute the cluster centroids:"
        },
        {
            "location": "/user_guide/tf_cluster/TfKmeans/#visualize-the-cluster-memberships",
            "text": "y_clust = km.predict(X)\n\nplt.scatter(X[y_clust == 0, 0],\n            X[y_clust == 0, 1],\n            s=50,\n            c='lightgreen',\n            marker='s',\n            label='cluster 1')\n\nplt.scatter(X[y_clust == 1,0],\n            X[y_clust == 1,1],\n            s=50,\n            c='orange',\n            marker='o',\n            label='cluster 2')\n\nplt.scatter(X[y_clust == 2,0],\n            X[y_clust == 2,1],\n            s=50,\n            c='lightblue',\n            marker='v',\n            label='cluster 3')\n\n\nplt.scatter(km.centroids_[:,0],\n            km.centroids_[:,1],\n            s=250,\n            marker='*',\n            c='red',\n            label='centroids')\n\nplt.legend(loc='lower left', \n           scatterpoints=1)\nplt.grid()\nplt.show()",
            "title": "Visualize the cluster memberships:"
        },
        {
            "location": "/user_guide/tf_cluster/TfKmeans/#api",
            "text": "TfKmeans(k, max_iter=10, convergence_tolerance=1e-05, random_seed=None, print_progress=0, dtype=None)  TensorFlow K-means clustering class.  Added in 0.4.1dev  Parameters    k  : int  Number of clusters    max_iter  : int (default: 10)  Number of iterations during cluster assignment.\nCluster re-assignment stops automatically when the algorithm\nconverged.    convergence_tolerance  : float (default: 1e-05)  Compares current centroids with centroids of the previous iteration\nusing the given tolerance (a small positive float)to determine\nif the algorithm converged early.    random_seed  : int (default: None)  Set random state for the initial centroid assignment.    print_progress  : int (default: 0)  Prints progress in fitting to stderr.\n0: No output\n1: Iterations elapsed\n2: 1 plus time elapsed\n3: 2 plus estimated time until completion    dtype  : Array-type (default: None)  Uses tensorflow.float32 if None.    Attributes    centroids_  : 2d-array, shape = {k, n_features}  Feature values of the k cluster centroids.    custers_  : dictionary  The cluster assignments stored as a Python dictionary;\nthe dictionary keys denote the cluster indeces and the items are\nPython lists of the sample indices that were assigned to each\ncluster.    iterations_  : int  Number of iterations until convergence.",
            "title": "API"
        },
        {
            "location": "/user_guide/tf_cluster/TfKmeans/#methods",
            "text": "fit(X, init_params=True)  Learn model from training data.  Parameters    X  : {array-like, sparse matrix}, shape = [n_samples, n_features]  Training vectors, where n_samples is the number of samples and\nn_features is the number of features.    init_params  : bool (default: True)  Re-initializes model parameters prior to fitting.\nSet False to continue training with weights from\na previous model fitting.    Returns   self  : object    predict(X)  Predict targets from X.  Parameters    X  : {array-like, sparse matrix}, shape = [n_samples, n_features]  Training vectors, where n_samples is the number of samples and\nn_features is the number of features.    Returns    target_values  : array-like, shape = [n_samples]  Predicted target values.",
            "title": "Methods"
        },
        {
            "location": "/user_guide/evaluate/confusion_matrix/",
            "text": "Confusion Matrix\n\n\nFunctions for generating confusion matrices.\n\n\n\n\nfrom mlxtend.evaluate import confusion_matrix\n  \n\n\nfrom mlxtend.evaluate import plot_confusion_matrix\n\n\n\n\nOverview\n\n\nConfusion Matrix\n\n\nThe \nconfusion matrix\n (or \nerror matrix\n) is one way to summarize the performance of a classifier for binary classification tasks. This square matrix consists of columns and rows that list the number of instances as absolute or relative \"actual class\" vs. \"predicted class\" ratios.\n\n\nLet \n$P$\n be the label of class 1 and \n$N$\n be the label of a second class or the label of all classes that are \nnot class 1\n in a multi-class setting.\n\n\n\n\nReferences\n\n\n\n\n-\n\n\n\n\nExamples\n\n\nExample 1 - Binary classification\n\n\nfrom mlxtend.evaluate import confusion_matrix\n\ny_target =    [0, 0, 1, 0, 0, 1, 1, 1]\ny_predicted = [1, 0, 1, 0, 0, 0, 0, 1]\n\ncm = confusion_matrix(y_target=y_target, \n                      y_predicted=y_predicted)\ncm\n\n\n\n\narray([[3, 1],\n       [2, 2]])\n\n\n\nimport matplotlib.pyplot as plt\nfrom mlxtend.evaluate import plot_confusion_matrix\n\nfig, ax = plot_confusion_matrix(conf_mat=cm)\nplt.show()\n\n\n\n\n\n\nExample 2 - Multi-class classification\n\n\nfrom mlxtend.evaluate import confusion_matrix\n\ny_target =    [1, 1, 1, 0, 0, 2, 0, 3]\ny_predicted = [1, 0, 1, 0, 0, 2, 1, 3]\n\ncm = confusion_matrix(y_target=y_target, \n                      y_predicted=y_predicted, \n                      binary=False)\ncm\n\n\n\n\narray([[2, 1, 0, 0],\n       [1, 2, 0, 0],\n       [0, 0, 1, 0],\n       [0, 0, 0, 1]])\n\n\n\nimport matplotlib.pyplot as plt\nfrom mlxtend.evaluate import confusion_matrix\n\nfig, ax = plot_confusion_matrix(conf_mat=cm)\nplt.show()\n\n\n\n\n\n\nExample 3 - Multi-class to binary\n\n\nBy setting \nbinary=True\n, all class labels that are not the positive class label are being summarized to class 0. The positive class label becomes class 1.\n\n\nimport matplotlib.pyplot as plt\nfrom mlxtend.evaluate import confusion_matrix\n\ny_target =    [1, 1, 1, 0, 0, 2, 0, 3]\ny_predicted = [1, 0, 1, 0, 0, 2, 1, 3]\n\ncm = confusion_matrix(y_target=y_target, \n                      y_predicted=y_predicted, \n                      binary=True, \n                      positive_label=1)\ncm\n\n\n\n\narray([[4, 1],\n       [1, 2]])\n\n\n\nfrom mlxtend.evaluate import plot_confusion_matrix\n\nfig, ax = plot_confusion_matrix(conf_mat=cm)\nplt.show()\n\n\n\n\n\n\nAPI\n\n\nconfusion_matrix(y_target, y_predicted, binary=False, positive_label=1)\n\n\nCompute a confusion matrix/contingency table.\n\n\nParameters\n\n\n\n\n\n\ny_target\n : array-like, shape=[n_samples]\n\n\nTrue class labels.\n\n\n\n\n\n\ny_predicted\n : array-like, shape=[n_samples]\n\n\nPredicted class labels.\n\n\n\n\n\n\nbinary\n : bool (default: False)\n\n\nMaps a multi-class problem onto a\nbinary confusion matrix, where\nthe positive class is 1 and\nall other classes are 0.\n\n\n\n\n\n\npositive_label\n : int (default: 1)\n\n\nClass label of the positive class.\n\n\n\n\n\n\nReturns\n\n\n\n\nmat\n : array-like, shape=[n_classes, n_classes]\n\n\n\n\n\n\nplot_confusion_matrix(conf_mat, hide_spines=False, hide_ticks=False, figsize=(2.5, 2.5), cmap=None, alpha=0.3)\n\n\nPlot a confusion matrix via matplotlib.\n\n\nParameters\n\n\n\n\n\n\nconf_mat\n : array-like, shape = [n_classes, n_classes]\n\n\nConfusion matrix from evaluate.confusion matrix.\n\n\n\n\n\n\nhide_spines\n : bool (default: False)\n\n\nHides axis spines if True.\n\n\n\n\n\n\nhide_ticks\n : bool (default: False)\n\n\nHides axis ticks if True\n\n\n\n\n\n\nfigsize\n : tuple (default: (2.5, 2.5))\n\n\nHeight and width of the figure\n\n\n\n\n\n\ncmap\n : matplotlib colormap (default: \nNone\n)\n\n\nUses matplotlib.pyplot.cm.Blues if \nNone\n\n\n\n\n\n\nReturns\n\n\n\n\n\n\nfig, ax\n : matplotlib.pyplot subplot objects\n\n\nFigure and axis elements of the subplot.",
            "title": "Confusion matrix"
        },
        {
            "location": "/user_guide/evaluate/confusion_matrix/#confusion-matrix",
            "text": "Functions for generating confusion matrices.   from mlxtend.evaluate import confusion_matrix     from mlxtend.evaluate import plot_confusion_matrix",
            "title": "Confusion Matrix"
        },
        {
            "location": "/user_guide/evaluate/confusion_matrix/#overview",
            "text": "",
            "title": "Overview"
        },
        {
            "location": "/user_guide/evaluate/confusion_matrix/#confusion-matrix_1",
            "text": "The  confusion matrix  (or  error matrix ) is one way to summarize the performance of a classifier for binary classification tasks. This square matrix consists of columns and rows that list the number of instances as absolute or relative \"actual class\" vs. \"predicted class\" ratios.  Let  $P$  be the label of class 1 and  $N$  be the label of a second class or the label of all classes that are  not class 1  in a multi-class setting.",
            "title": "Confusion Matrix"
        },
        {
            "location": "/user_guide/evaluate/confusion_matrix/#references",
            "text": "-",
            "title": "References"
        },
        {
            "location": "/user_guide/evaluate/confusion_matrix/#examples",
            "text": "",
            "title": "Examples"
        },
        {
            "location": "/user_guide/evaluate/confusion_matrix/#example-1-binary-classification",
            "text": "from mlxtend.evaluate import confusion_matrix\n\ny_target =    [0, 0, 1, 0, 0, 1, 1, 1]\ny_predicted = [1, 0, 1, 0, 0, 0, 0, 1]\n\ncm = confusion_matrix(y_target=y_target, \n                      y_predicted=y_predicted)\ncm  array([[3, 1],\n       [2, 2]])  import matplotlib.pyplot as plt\nfrom mlxtend.evaluate import plot_confusion_matrix\n\nfig, ax = plot_confusion_matrix(conf_mat=cm)\nplt.show()",
            "title": "Example 1 - Binary classification"
        },
        {
            "location": "/user_guide/evaluate/confusion_matrix/#example-2-multi-class-classification",
            "text": "from mlxtend.evaluate import confusion_matrix\n\ny_target =    [1, 1, 1, 0, 0, 2, 0, 3]\ny_predicted = [1, 0, 1, 0, 0, 2, 1, 3]\n\ncm = confusion_matrix(y_target=y_target, \n                      y_predicted=y_predicted, \n                      binary=False)\ncm  array([[2, 1, 0, 0],\n       [1, 2, 0, 0],\n       [0, 0, 1, 0],\n       [0, 0, 0, 1]])  import matplotlib.pyplot as plt\nfrom mlxtend.evaluate import confusion_matrix\n\nfig, ax = plot_confusion_matrix(conf_mat=cm)\nplt.show()",
            "title": "Example 2 - Multi-class classification"
        },
        {
            "location": "/user_guide/evaluate/confusion_matrix/#example-3-multi-class-to-binary",
            "text": "By setting  binary=True , all class labels that are not the positive class label are being summarized to class 0. The positive class label becomes class 1.  import matplotlib.pyplot as plt\nfrom mlxtend.evaluate import confusion_matrix\n\ny_target =    [1, 1, 1, 0, 0, 2, 0, 3]\ny_predicted = [1, 0, 1, 0, 0, 2, 1, 3]\n\ncm = confusion_matrix(y_target=y_target, \n                      y_predicted=y_predicted, \n                      binary=True, \n                      positive_label=1)\ncm  array([[4, 1],\n       [1, 2]])  from mlxtend.evaluate import plot_confusion_matrix\n\nfig, ax = plot_confusion_matrix(conf_mat=cm)\nplt.show()",
            "title": "Example 3 - Multi-class to binary"
        },
        {
            "location": "/user_guide/evaluate/confusion_matrix/#api",
            "text": "confusion_matrix(y_target, y_predicted, binary=False, positive_label=1)  Compute a confusion matrix/contingency table.  Parameters    y_target  : array-like, shape=[n_samples]  True class labels.    y_predicted  : array-like, shape=[n_samples]  Predicted class labels.    binary  : bool (default: False)  Maps a multi-class problem onto a\nbinary confusion matrix, where\nthe positive class is 1 and\nall other classes are 0.    positive_label  : int (default: 1)  Class label of the positive class.    Returns   mat  : array-like, shape=[n_classes, n_classes]    plot_confusion_matrix(conf_mat, hide_spines=False, hide_ticks=False, figsize=(2.5, 2.5), cmap=None, alpha=0.3)  Plot a confusion matrix via matplotlib.  Parameters    conf_mat  : array-like, shape = [n_classes, n_classes]  Confusion matrix from evaluate.confusion matrix.    hide_spines  : bool (default: False)  Hides axis spines if True.    hide_ticks  : bool (default: False)  Hides axis ticks if True    figsize  : tuple (default: (2.5, 2.5))  Height and width of the figure    cmap  : matplotlib colormap (default:  None )  Uses matplotlib.pyplot.cm.Blues if  None    Returns    fig, ax  : matplotlib.pyplot subplot objects  Figure and axis elements of the subplot.",
            "title": "API"
        },
        {
            "location": "/user_guide/evaluate/plot_decision_regions/",
            "text": "Plotting Decision Regions\n\n\nA function for plotting decision regions of classifiers in 1 or 2 dimensions.\n\n\n\n\nfrom mlxtend.evaluate import plot_decision_regions\n\n\n\n\nReferences\n\n\n\n\n\n\n\n\nExamples\n\n\nExample 1 - Decision regions in 2D\n\n\nfrom mlxtend.evaluate import plot_decision_regions\nimport matplotlib.pyplot as plt\nfrom sklearn import datasets\nfrom sklearn.svm import SVC\n\n# Loading some example data\niris = datasets.load_iris()\nX = iris.data[:, [0,2]]\ny = iris.target\n\n# Training a classifier\nsvm = SVC(C=0.5, kernel='linear')\nsvm.fit(X,y)\n\n\n# Plotting decision regions\nplot_decision_regions(X, y, clf=svm, \n                      res=0.02, legend=2)\n\n# Adding axes annotations\nplt.xlabel('sepal length [cm]')\nplt.ylabel('petal length [cm]')\nplt.title('SVM on Iris')\nplt.show()\n\n\n\n\n\n\nExample 2 - Decision regions in 1D\n\n\nfrom mlxtend.evaluate import plot_decision_regions\nimport matplotlib.pyplot as plt\nfrom sklearn import datasets\nfrom sklearn.svm import SVC\n\n# Loading some example data\niris = datasets.load_iris()\nX = iris.data[:, 2]\nX = X[:, None]\ny = iris.target\n\n# Training a classifier\nsvm = SVC(C=0.5, kernel='linear')\nsvm.fit(X,y)\n\n# Plotting decision regions\nplot_decision_regions(X, y, clf=svm, \n                      res=0.02, legend=2)\n\n# Adding axes annotations\nplt.xlabel('sepal length [cm]')\nplt.title('SVM on Iris')\n\nplt.show()\n\n\n\n\n\n\nExample 3 - Decision Region Grids\n\n\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.naive_bayes import GaussianNB \nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.svm import SVC\nfrom sklearn import datasets\nimport numpy as np\n\n# Initializing Classifiers\nclf1 = LogisticRegression(random_state=1)\nclf2 = RandomForestClassifier(random_state=1)\nclf3 = GaussianNB()\nclf4 = SVC()\n\n# Loading some example data\niris = datasets.load_iris()\nX = iris.data[:, [0,2]]\ny = iris.target\n\n\n\n\nimport matplotlib.pyplot as plt\nfrom mlxtend.evaluate import plot_decision_regions\nimport matplotlib.gridspec as gridspec\nimport itertools\ngs = gridspec.GridSpec(2, 2)\n\nfig = plt.figure(figsize=(10,8))\n\nlabels = ['Logistic Regression', 'Random Forest', 'Naive Bayes', 'SVM']\nfor clf, lab, grd in zip([clf1, clf2, clf3, clf4],\n                         labels,\n                         itertools.product([0, 1], repeat=2)):\n\n    clf.fit(X, y)\n    ax = plt.subplot(gs[grd[0], grd[1]])\n    fig = plot_decision_regions(X=X, y=y, clf=clf, legend=2)\n    plt.title(lab)\n\nplt.show()\n\n\n\n\n\n\nExample 4 - Highlighting Test Data Points\n\n\nfrom mlxtend.evaluate import plot_decision_regions\nfrom mlxtend.preprocessing import shuffle_arrays_unison\nimport matplotlib.pyplot as plt\nfrom sklearn import datasets\nfrom sklearn.svm import SVC\n\n\n# Loading some example data\niris = datasets.load_iris()\nX, y = iris.data[:, [0,2]], iris.target\nX, y = shuffle_arrays_unison(arrays=[X, y], random_seed=3)\n\nX_train, y_train = X[:100], y[:100]\nX_test, y_test = X[100:], y[100:]\n\n# Training a classifier\nsvm = SVC(C=0.5, kernel='linear')\nsvm.fit(X_train, y_train)\n\n# Plotting decision regions\nplot_decision_regions(X, y, clf=svm, res=0.02, \n                      legend=2, X_highlight=X_test)\n\n# Adding axes annotations\nplt.xlabel('sepal length [cm]')\nplt.ylabel('petal length [cm]')\nplt.title('SVM on Iris')\nplt.show()\n\n\n\n\n\n\nExample 5 - Evaluating Classifier Behavior on Non-Linear Problems\n\n\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.naive_bayes import GaussianNB \nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.svm import SVC\n\n# Initializing Classifiers\nclf1 = LogisticRegression(random_state=1)\nclf2 = RandomForestClassifier(n_estimators=100, \n                              random_state=1)\nclf3 = GaussianNB()\nclf4 = SVC()\n\n\n\n\n# Loading Plotting Utilities\nimport matplotlib.pyplot as plt\nimport matplotlib.gridspec as gridspec\nimport itertools\n#from mlxtend.evaluate import plot_decision_regions\nimport numpy as np\n\n\n\n\nXOR\n\n\nxx, yy = np.meshgrid(np.linspace(-3, 3, 50),\n                     np.linspace(-3, 3, 50))\nrng = np.random.RandomState(0)\nX = rng.randn(300, 2)\ny = np.array(np.logical_xor(X[:, 0] > 0, X[:, 1] > 0), \n             dtype=int)\n\n\n\n\ngs = gridspec.GridSpec(2, 2)\n\nfig = plt.figure(figsize=(10,8))\n\nlabels = ['Logistic Regression', 'Random Forest', 'Naive Bayes', 'SVM']\nfor clf, lab, grd in zip([clf1, clf2, clf3, clf4],\n                         labels,\n                         itertools.product([0, 1], repeat=2)):\n\n    clf.fit(X, y)\n    ax = plt.subplot(gs[grd[0], grd[1]])\n    fig = plot_decision_regions(X=X, y=y, clf=clf, legend=2)\n    plt.title(lab)\n\nplt.show()\n\n\n\n\n\n\nHalf-Moons\n\n\nfrom sklearn.datasets import make_moons\nX, y = make_moons(n_samples=100, random_state=123)\n\ngs = gridspec.GridSpec(2, 2)\n\nfig = plt.figure(figsize=(10,8))\n\nlabels = ['Logistic Regression', 'Random Forest', 'Naive Bayes', 'SVM']\nfor clf, lab, grd in zip([clf1, clf2, clf3, clf4],\n                         labels,\n                         itertools.product([0, 1], repeat=2)):\n\n    clf.fit(X, y)\n    ax = plt.subplot(gs[grd[0], grd[1]])\n    fig = plot_decision_regions(X=X, y=y, clf=clf, legend=2)\n    plt.title(lab)\n\nplt.show()\n\n\n\n\n\n\nConcentric Circles\n\n\nfrom sklearn.datasets import make_circles\nX, y = make_circles(n_samples=1000, random_state=123, noise=0.1, factor=0.2)\n\ngs = gridspec.GridSpec(2, 2)\n\nfig = plt.figure(figsize=(10,8))\n\nlabels = ['Logistic Regression', 'Random Forest', 'Naive Bayes', 'SVM']\nfor clf, lab, grd in zip([clf1, clf2, clf3, clf4],\n                         labels,\n                         itertools.product([0, 1], repeat=2)):\n\n    clf.fit(X, y)\n    ax = plt.subplot(gs[grd[0], grd[1]])\n    fig = plot_decision_regions(X=X, y=y, clf=clf, legend=2)\n    plt.title(lab)\n\nplt.show()\n\n\n\n\n\n\nAPI\n\n\nplot_decision_regions(X, y, clf, ax=None, X_highlight=None, res=0.02, legend=1, hide_spines=True, markers='s^oxv<>', colors='red,blue,limegreen,gray,cyan')\n\n\nPlot decision regions of a classifier.\n\n\nParameters\n\n\n\n\n\n\nX\n : array-like, shape = [n_samples, n_features]\n\n\nFeature Matrix.\n\n\n\n\n\n\ny\n : array-like, shape = [n_samples]\n\n\nTrue class labels.\n\n\n\n\n\n\nclf\n : Classifier object.\n\n\nMust have a .predict method.\n\n\n\n\n\n\nax\n : matplotlib.axes.Axes (default: None)\n\n\nAn existing matplotlib Axes. Creates\none if ax=None.\n\n\n\n\n\n\nX_highlight\n : array-like, shape = [n_samples, n_features] (default: None)\n\n\nAn array with data points that are used to highlight samples in \nX\n.\n\n\n\n\n\n\nres\n : float (default: 0.02)\n\n\nGrid width. Lower values increase the resolution but\nslow down the plotting.\n\n\n\n\n\n\nhide_spines\n : bool (default: True)\n\n\nHide axis spines if True.\n\n\n\n\n\n\nlegend\n : int (default: 1)\n\n\nInteger to specify the legend location.\nNo legend if legend is 0.\n\n\n\n\n\n\nmarkers\n : list\n\n\nScatterplot markers.\n\n\n\n\n\n\ncolors\n : str (default 'red,blue,limegreen,gray,cyan')\n\n\nComma separated list of colors.\n\n\n\n\n\n\nReturns\n\n\n\n\nax\n : matplotlib.axes.Axes object",
            "title": "Plot decision regions"
        },
        {
            "location": "/user_guide/evaluate/plot_decision_regions/#plotting-decision-regions",
            "text": "A function for plotting decision regions of classifiers in 1 or 2 dimensions.   from mlxtend.evaluate import plot_decision_regions",
            "title": "Plotting Decision Regions"
        },
        {
            "location": "/user_guide/evaluate/plot_decision_regions/#references",
            "text": "",
            "title": "References"
        },
        {
            "location": "/user_guide/evaluate/plot_decision_regions/#examples",
            "text": "",
            "title": "Examples"
        },
        {
            "location": "/user_guide/evaluate/plot_decision_regions/#example-1-decision-regions-in-2d",
            "text": "from mlxtend.evaluate import plot_decision_regions\nimport matplotlib.pyplot as plt\nfrom sklearn import datasets\nfrom sklearn.svm import SVC\n\n# Loading some example data\niris = datasets.load_iris()\nX = iris.data[:, [0,2]]\ny = iris.target\n\n# Training a classifier\nsvm = SVC(C=0.5, kernel='linear')\nsvm.fit(X,y)\n\n\n# Plotting decision regions\nplot_decision_regions(X, y, clf=svm, \n                      res=0.02, legend=2)\n\n# Adding axes annotations\nplt.xlabel('sepal length [cm]')\nplt.ylabel('petal length [cm]')\nplt.title('SVM on Iris')\nplt.show()",
            "title": "Example 1 - Decision regions in 2D"
        },
        {
            "location": "/user_guide/evaluate/plot_decision_regions/#example-2-decision-regions-in-1d",
            "text": "from mlxtend.evaluate import plot_decision_regions\nimport matplotlib.pyplot as plt\nfrom sklearn import datasets\nfrom sklearn.svm import SVC\n\n# Loading some example data\niris = datasets.load_iris()\nX = iris.data[:, 2]\nX = X[:, None]\ny = iris.target\n\n# Training a classifier\nsvm = SVC(C=0.5, kernel='linear')\nsvm.fit(X,y)\n\n# Plotting decision regions\nplot_decision_regions(X, y, clf=svm, \n                      res=0.02, legend=2)\n\n# Adding axes annotations\nplt.xlabel('sepal length [cm]')\nplt.title('SVM on Iris')\n\nplt.show()",
            "title": "Example 2 - Decision regions in 1D"
        },
        {
            "location": "/user_guide/evaluate/plot_decision_regions/#example-3-decision-region-grids",
            "text": "from sklearn.linear_model import LogisticRegression\nfrom sklearn.naive_bayes import GaussianNB \nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.svm import SVC\nfrom sklearn import datasets\nimport numpy as np\n\n# Initializing Classifiers\nclf1 = LogisticRegression(random_state=1)\nclf2 = RandomForestClassifier(random_state=1)\nclf3 = GaussianNB()\nclf4 = SVC()\n\n# Loading some example data\niris = datasets.load_iris()\nX = iris.data[:, [0,2]]\ny = iris.target  import matplotlib.pyplot as plt\nfrom mlxtend.evaluate import plot_decision_regions\nimport matplotlib.gridspec as gridspec\nimport itertools\ngs = gridspec.GridSpec(2, 2)\n\nfig = plt.figure(figsize=(10,8))\n\nlabels = ['Logistic Regression', 'Random Forest', 'Naive Bayes', 'SVM']\nfor clf, lab, grd in zip([clf1, clf2, clf3, clf4],\n                         labels,\n                         itertools.product([0, 1], repeat=2)):\n\n    clf.fit(X, y)\n    ax = plt.subplot(gs[grd[0], grd[1]])\n    fig = plot_decision_regions(X=X, y=y, clf=clf, legend=2)\n    plt.title(lab)\n\nplt.show()",
            "title": "Example 3 - Decision Region Grids"
        },
        {
            "location": "/user_guide/evaluate/plot_decision_regions/#example-4-highlighting-test-data-points",
            "text": "from mlxtend.evaluate import plot_decision_regions\nfrom mlxtend.preprocessing import shuffle_arrays_unison\nimport matplotlib.pyplot as plt\nfrom sklearn import datasets\nfrom sklearn.svm import SVC\n\n\n# Loading some example data\niris = datasets.load_iris()\nX, y = iris.data[:, [0,2]], iris.target\nX, y = shuffle_arrays_unison(arrays=[X, y], random_seed=3)\n\nX_train, y_train = X[:100], y[:100]\nX_test, y_test = X[100:], y[100:]\n\n# Training a classifier\nsvm = SVC(C=0.5, kernel='linear')\nsvm.fit(X_train, y_train)\n\n# Plotting decision regions\nplot_decision_regions(X, y, clf=svm, res=0.02, \n                      legend=2, X_highlight=X_test)\n\n# Adding axes annotations\nplt.xlabel('sepal length [cm]')\nplt.ylabel('petal length [cm]')\nplt.title('SVM on Iris')\nplt.show()",
            "title": "Example 4 - Highlighting Test Data Points"
        },
        {
            "location": "/user_guide/evaluate/plot_decision_regions/#example-5-evaluating-classifier-behavior-on-non-linear-problems",
            "text": "from sklearn.linear_model import LogisticRegression\nfrom sklearn.naive_bayes import GaussianNB \nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.svm import SVC\n\n# Initializing Classifiers\nclf1 = LogisticRegression(random_state=1)\nclf2 = RandomForestClassifier(n_estimators=100, \n                              random_state=1)\nclf3 = GaussianNB()\nclf4 = SVC()  # Loading Plotting Utilities\nimport matplotlib.pyplot as plt\nimport matplotlib.gridspec as gridspec\nimport itertools\n#from mlxtend.evaluate import plot_decision_regions\nimport numpy as np",
            "title": "Example 5 - Evaluating Classifier Behavior on Non-Linear Problems"
        },
        {
            "location": "/user_guide/evaluate/plot_decision_regions/#xor",
            "text": "xx, yy = np.meshgrid(np.linspace(-3, 3, 50),\n                     np.linspace(-3, 3, 50))\nrng = np.random.RandomState(0)\nX = rng.randn(300, 2)\ny = np.array(np.logical_xor(X[:, 0] > 0, X[:, 1] > 0), \n             dtype=int)  gs = gridspec.GridSpec(2, 2)\n\nfig = plt.figure(figsize=(10,8))\n\nlabels = ['Logistic Regression', 'Random Forest', 'Naive Bayes', 'SVM']\nfor clf, lab, grd in zip([clf1, clf2, clf3, clf4],\n                         labels,\n                         itertools.product([0, 1], repeat=2)):\n\n    clf.fit(X, y)\n    ax = plt.subplot(gs[grd[0], grd[1]])\n    fig = plot_decision_regions(X=X, y=y, clf=clf, legend=2)\n    plt.title(lab)\n\nplt.show()",
            "title": "XOR"
        },
        {
            "location": "/user_guide/evaluate/plot_decision_regions/#half-moons",
            "text": "from sklearn.datasets import make_moons\nX, y = make_moons(n_samples=100, random_state=123)\n\ngs = gridspec.GridSpec(2, 2)\n\nfig = plt.figure(figsize=(10,8))\n\nlabels = ['Logistic Regression', 'Random Forest', 'Naive Bayes', 'SVM']\nfor clf, lab, grd in zip([clf1, clf2, clf3, clf4],\n                         labels,\n                         itertools.product([0, 1], repeat=2)):\n\n    clf.fit(X, y)\n    ax = plt.subplot(gs[grd[0], grd[1]])\n    fig = plot_decision_regions(X=X, y=y, clf=clf, legend=2)\n    plt.title(lab)\n\nplt.show()",
            "title": "Half-Moons"
        },
        {
            "location": "/user_guide/evaluate/plot_decision_regions/#concentric-circles",
            "text": "from sklearn.datasets import make_circles\nX, y = make_circles(n_samples=1000, random_state=123, noise=0.1, factor=0.2)\n\ngs = gridspec.GridSpec(2, 2)\n\nfig = plt.figure(figsize=(10,8))\n\nlabels = ['Logistic Regression', 'Random Forest', 'Naive Bayes', 'SVM']\nfor clf, lab, grd in zip([clf1, clf2, clf3, clf4],\n                         labels,\n                         itertools.product([0, 1], repeat=2)):\n\n    clf.fit(X, y)\n    ax = plt.subplot(gs[grd[0], grd[1]])\n    fig = plot_decision_regions(X=X, y=y, clf=clf, legend=2)\n    plt.title(lab)\n\nplt.show()",
            "title": "Concentric Circles"
        },
        {
            "location": "/user_guide/evaluate/plot_decision_regions/#api",
            "text": "plot_decision_regions(X, y, clf, ax=None, X_highlight=None, res=0.02, legend=1, hide_spines=True, markers='s^oxv<>', colors='red,blue,limegreen,gray,cyan')  Plot decision regions of a classifier.  Parameters    X  : array-like, shape = [n_samples, n_features]  Feature Matrix.    y  : array-like, shape = [n_samples]  True class labels.    clf  : Classifier object.  Must have a .predict method.    ax  : matplotlib.axes.Axes (default: None)  An existing matplotlib Axes. Creates\none if ax=None.    X_highlight  : array-like, shape = [n_samples, n_features] (default: None)  An array with data points that are used to highlight samples in  X .    res  : float (default: 0.02)  Grid width. Lower values increase the resolution but\nslow down the plotting.    hide_spines  : bool (default: True)  Hide axis spines if True.    legend  : int (default: 1)  Integer to specify the legend location.\nNo legend if legend is 0.    markers  : list  Scatterplot markers.    colors  : str (default 'red,blue,limegreen,gray,cyan')  Comma separated list of colors.    Returns   ax  : matplotlib.axes.Axes object",
            "title": "API"
        },
        {
            "location": "/user_guide/evaluate/plot_learning_curves/",
            "text": "Plotting Learning Curves\n\n\nA function to plot learning curves for classifiers. Learning curves are extremely useful to analyze if a model is suffering from over- or under-fitting (high variance or high bias). The function can be imported via\n\n\n\n\nfrom mlxtend.evaluate import plot_learning_curves\n\n\n\n\nReferences\n\n\n-\n\n\nExamples\n\n\nExample 1\n\n\nfrom mlxtend.evaluate import plot_learning_curves\nimport matplotlib.pyplot as plt\nfrom mlxtend.data import iris_data\nfrom mlxtend.preprocessing import shuffle_arrays_unison\nfrom sklearn.neighbors import KNeighborsClassifier\nimport numpy as np\n\n\n# Loading some example data\nX, y = iris_data()\nX, y = shuffle_arrays_unison(arrays=[X, y], random_seed=123)\nX_train, X_test = X[:100], X[100:]\ny_train, y_test = y[:100], y[100:]\n\nclf = KNeighborsClassifier(n_neighbors=5)\n\nplot_learning_curves(X_train, y_train, X_test, y_test, clf)\nplt.show()\n\n\n\n\n\n\nAPI\n\n\nplot_learning_curves(X_train, y_train, X_test, y_test, clf, train_marker='o', test_marker='^', scoring='misclassification error', suppress_plot=False, print_model=True, style='fivethirtyeight', legend_loc='best')\n\n\nPlots learning curves of a classifier.\n\n\nParameters\n\n\n\n\n\n\nX_train\n : array-like, shape = [n_samples, n_features]\n\n\nFeature matrix of the training dataset.\n\n\n\n\n\n\ny_train\n : array-like, shape = [n_samples]\n\n\nTrue class labels of the training dataset.\n\n\n\n\n\n\nX_test\n : array-like, shape = [n_samples, n_features]\n\n\nFeature matrix of the test dataset.\n\n\n\n\n\n\ny_test\n : array-like, shape = [n_samples]\n\n\nTrue class labels of the test dataset.\n\n\n\n\n\n\nclf\n : Classifier object. Must have a .predict .fit method.\n\n\n\n\n\n\ntrain_marker\n : str (default: 'o')\n\n\nMarker for the training set line plot.\n\n\n\n\n\n\ntest_marker\n : str (default: '^')\n\n\nMarker for the test set line plot.\n\n\n\n\n\n\nscoring\n : str (default: 'misclassification error')\n\n\nIf not 'misclassification error', accepts the following metrics\n(from scikit-learn):\n\n\n\n\n\n\nsuppress_plot=False\n : bool (default: False)\n\n\nSuppress matplotlib plots if True. Recommended\nfor testing purposes.\n\n\n\n\n\n\nprint_model\n : bool (default: True)\n\n\nPrint model parameters in plot title if True.\n\n\n\n\n\n\nstyle\n : str (default: 'fivethirtyeight')\n\n\nMatplotlib style\n\n\n\n\n\n\nlegend_loc\n : str (default: 'best')\n\n\nWhere to place the plot legend:\n\n\n\n\n\n\nReturns\n\n\n\n\nerrors\n : (training_error, test_error): tuple of lists",
            "title": "Plot learning curves"
        },
        {
            "location": "/user_guide/evaluate/plot_learning_curves/#plotting-learning-curves",
            "text": "A function to plot learning curves for classifiers. Learning curves are extremely useful to analyze if a model is suffering from over- or under-fitting (high variance or high bias). The function can be imported via   from mlxtend.evaluate import plot_learning_curves",
            "title": "Plotting Learning Curves"
        },
        {
            "location": "/user_guide/evaluate/plot_learning_curves/#references",
            "text": "-",
            "title": "References"
        },
        {
            "location": "/user_guide/evaluate/plot_learning_curves/#examples",
            "text": "",
            "title": "Examples"
        },
        {
            "location": "/user_guide/evaluate/plot_learning_curves/#example-1",
            "text": "from mlxtend.evaluate import plot_learning_curves\nimport matplotlib.pyplot as plt\nfrom mlxtend.data import iris_data\nfrom mlxtend.preprocessing import shuffle_arrays_unison\nfrom sklearn.neighbors import KNeighborsClassifier\nimport numpy as np\n\n\n# Loading some example data\nX, y = iris_data()\nX, y = shuffle_arrays_unison(arrays=[X, y], random_seed=123)\nX_train, X_test = X[:100], X[100:]\ny_train, y_test = y[:100], y[100:]\n\nclf = KNeighborsClassifier(n_neighbors=5)\n\nplot_learning_curves(X_train, y_train, X_test, y_test, clf)\nplt.show()",
            "title": "Example 1"
        },
        {
            "location": "/user_guide/evaluate/plot_learning_curves/#api",
            "text": "plot_learning_curves(X_train, y_train, X_test, y_test, clf, train_marker='o', test_marker='^', scoring='misclassification error', suppress_plot=False, print_model=True, style='fivethirtyeight', legend_loc='best')  Plots learning curves of a classifier.  Parameters    X_train  : array-like, shape = [n_samples, n_features]  Feature matrix of the training dataset.    y_train  : array-like, shape = [n_samples]  True class labels of the training dataset.    X_test  : array-like, shape = [n_samples, n_features]  Feature matrix of the test dataset.    y_test  : array-like, shape = [n_samples]  True class labels of the test dataset.    clf  : Classifier object. Must have a .predict .fit method.    train_marker  : str (default: 'o')  Marker for the training set line plot.    test_marker  : str (default: '^')  Marker for the test set line plot.    scoring  : str (default: 'misclassification error')  If not 'misclassification error', accepts the following metrics\n(from scikit-learn):    suppress_plot=False  : bool (default: False)  Suppress matplotlib plots if True. Recommended\nfor testing purposes.    print_model  : bool (default: True)  Print model parameters in plot title if True.    style  : str (default: 'fivethirtyeight')  Matplotlib style    legend_loc  : str (default: 'best')  Where to place the plot legend:    Returns   errors  : (training_error, test_error): tuple of lists",
            "title": "API"
        },
        {
            "location": "/user_guide/evaluate/scoring/",
            "text": "Scoring\n\n\nA function for computing various different performance metrics.\n\n\n\n\nfrom mlxtend.evaluate import scoring\n\n\n\n\nOverview\n\n\nConfusion Matrix\n\n\nThe \nconfusion matrix\n (or \nerror matrix\n) is one way to summarize the performance of a classifier for binary classification tasks. This square matrix consists of columns and rows that list the number of instances as absolute or relative \"actual class\" vs. \"predicted class\" ratios.\n\n\nLet \n$P$\n be the label of class 1 and \n$N$\n be the label of a second class or the label of all classes that are \nnot class 1\n in a multi-class setting.\n\n\n\n\nError and Accuracy\n\n\nBoth the prediction \nerror\n (ERR) and \naccuracy\n (ACC) provide general information about how many samples are misclassified. The \nerror\n can be understood as the sum of all false predictions divided by the number of total predications, and the the accuracy is calculated as the sum of correct predictions divided by the total number of predictions, respectively. \n\n\n$$ERR = \\frac{FP + FN}{FP+ FN + TP + TN} = 1-ACC$$\n\n\n$$ACC = \\frac{TP + TN}{FP+ FN + TP + TN} = 1-ERR$$\n\n\nTrue and False Positive Rates\n\n\nThe \nTrue Positive Rate\n (TPR) and \nFalse Positive Rate\n (FPR) are performance metrics that are especially useful for imbalanced class problems. In \nspam classification\n, for example, we are of course primarily interested in the detection and filtering out of \nspam\n. However, it is also important to decrease the number of messages that were incorrectly classified as \nspam\n (\nFalse Positives\n): A situation where a person misses an important message is considered as \"worse\" than a situation where a person ends up with a few \nspam\n messages in his e-mail inbox. In contrast to the \nFPR\n, the \nTrue Positive Rate\n provides useful information about the fraction of \npositive\n (or \nrelevant\n) samples that were correctly identified out of the total pool of \nPositives\n.\n\n\n$$FPR = \\frac{FP}{N} =  \\frac{FP}{FP + TN}$$\n\n\n$$TPR = \\frac{TP}{P} =  \\frac{TP}{FN + TP}$$\n\n\nPrecision, Recall, and the F1-Score\n\n\nPrecision\n (PRE) and \nRecall\n (REC) are metrics that are more commonly used in \nInformation Technology\n and related to the \nFalse\n and \nTrue Prositive Rates\n. In fact, \nRecall\n is synonymous to the \nTrue Positive Rate\n and also sometimes called \nSensitivity\n. The F\n$_1$\n-Score can be understood as a combination of both \nPrecision\n and \nRecall\n.\n\n\n$$PRE = \\frac{TP}{TP + FP}$$\n\n\n$$REC = TPR = \\frac{TP}{P} =  \\frac{TP}{FN + TP}$$\n\n\n$$F_1 = 2 \\cdot \\frac{PRE \\cdot REC}{PRE + REC}$$\n\n\nSensitivity and Specificity\n\n\nSensitivity\n (SEN) is synonymous to \nRecall\n and the \nTrue Positive Rate\n whereas \nSpecificity (SPC)\n is synonymous to the \nTrue Negative Rate\n -- \nSensitivity\n measures the recovery rate of the \nPositives\n and complimentary, the \nSpecificity\n measures the recovery rate of the \nNegatives\n.\n\n\n$$SEN = TPR = REC = \\frac{TP}{P} =  \\frac{TP}{FN + TP}$$\n\n\n$$SPC = TNR =\\frac{TN}{N} =  \\frac{TN}{FP + TN}$$\n\n\nMatthews Correlation Coefficient\n\n\nMatthews correlation coefficient\n (MCC) was first formulated by Brian W. Matthews [3] in 1975 to assess the performance of protein secondary structure predictions. The MCC can be understood as a specific case of a linear correlation coefficient (\nPearson's R\n) for a binary classification setting and is considered as especially useful in unbalanced class settings.\nThe previous metrics take values in the range between 0 (worst) and 1 (best), whereas the MCC is bounded between the range 1 (perfect correlation between ground truth and predicted outcome) and -1 (inverse or negative correlation) -- a value of 0 denotes a random prediction. \n\n\n$$MCC = \\frac{ TP \\times TN - FP \\times FN } {\\sqrt{ (TP + FP) ( TP + FN ) ( TN + FP ) ( TN + FN ) } }$$\n\n\nAverage Per-Class Accuracy\n\n\nThe \"overall\" accuracy is defined as the number of correct predictions (\ntrue positives\n TP and \ntrue negatives\n TN) over all samples \nn\n:\n\n\n$$ACC = \\frac{TP + TN}{n}$$\n\n\nin a binary class setting:\n\n\n\n\nIn a multi-class setting, we can generalize the computation of the accuracy as the fraction of all true predictions (the diagonal) over all samples n.\n\n\n$$ACC = \\frac{T}{n}$$\n\n\nConsidering a multi-class problem with 3 classes (C0, C1, C2)\n\n\n\n\nlet's assume our model made the following predictions:\n\n\n\n\nWe compute the accuracy as:\n\n\n$$ACC = \\frac{3 + 50 + 18}{90} \\approx 0.79$$\n\n\nNow, in order to compute the \naverage per-class accuracy\n, we compute the binary accuracy for each class label separately; i.e., if class 1 is the positive class, class 0 and 2 are both considered the negative class.\n\n\n$$APC\\;ACC = \\frac{83/90 + 71/90 + 78/90}{3} \\approx 0.86$$\n\n\nReferences\n\n\n\n\n[1] S. Raschka. \nAn overview of general performance metrics of binary classifier systems\n. Computing Research Repository (CoRR), abs/1410.5330, 2014.\n\n\n[2] Cyril Goutte and Eric Gaussier. \nA probabilistic interpretation of precision, recall and f-score, with implication for evaluation\n. In Advances in Information Retrieval, pages 345\u2013359. Springer, 2005.\n\n\n[3] Brian W Matthews. \nComparison of the predicted and observed secondary structure of T4 phage lysozyme\n. Biochimica et Biophysica Acta (BBA)- Protein Structure, 405(2):442\u2013451, 1975.\n\n\n\n\nExamples\n\n\nExample 1 - Classification Error\n\n\n#from mlxtend.evaluate import scoring\n\ny_targ = [1, 1, 1, 0, 0, 2, 0, 3]\ny_pred = [1, 0, 1, 0, 0, 2, 1, 3]\nres = scoring(y_target=y_targ, y_predicted=y_pred, metric='error')\n\nprint('Error: %s%%' % (res * 100))\n\n\n\n\nError: 25.0%\n\n\n\nAPI\n\n\nscoring(y_target, y_predicted, metric='error', positive_label=1)\n\n\nCompute a scoring metric for supervised learning.\n\n\nParameters\n\n\n\n\n\n\ny_target\n : array-like, shape=[n_values]\n\n\nTrue class labels or target values.\n\n\n\n\n\n\ny_predicted\n : array-like, shape=[n_values]\n\n\nPredicted class labels or target values.\n\n\n\n\n\n\nmetric\n : str (default: 'error')\n\n\nPerformance metric.\n[TP: True positives, TN = True negatives,\n\n\nTN: True negatives, FN = False negatives]\n\n\n'accuracy': (TP + TN)/(FP + FN + TP + TN) = 1-ERR\n\n\n'error': (TP + TN)/(FP+ FN + TP + TN) = 1-ACC\n\n\n'false_positive_rate': FP/N = FP/(FP + TN)\n\n\n'true_positive_rate': TP/P = TP/(FN + TP)\n\n\n'true_negative_rate': TN/N = TN/(FP + TN)\n\n\n'precision': TP/(TP + FP)\n\n\n'recall': equal to 'true_positive_rate'\n\n\n'sensitivity': equal to 'true_positive_rate' or 'recall'\n\n\n'specificity': equal to 'true_negative_rate'\n\n\n'f1': 2 * (PRE * REC)/(PRE + REC)\n\n\n'matthews_corr_coef':  (TP\nTN - FP\nFN)\n/ (sqrt{(TP + FP)( TP + FN )( TN + FP )( TN + FN )})\n\n\n\n\n\n\npositive_label\n : int (default: 1)\n\n\nLabel of the positive class for binary classification\nmetrics.\n\n\n\n\n\n\nReturns\n\n\n\n\nscore\n : float",
            "title": "Scoring"
        },
        {
            "location": "/user_guide/evaluate/scoring/#scoring",
            "text": "A function for computing various different performance metrics.   from mlxtend.evaluate import scoring",
            "title": "Scoring"
        },
        {
            "location": "/user_guide/evaluate/scoring/#overview",
            "text": "",
            "title": "Overview"
        },
        {
            "location": "/user_guide/evaluate/scoring/#confusion-matrix",
            "text": "The  confusion matrix  (or  error matrix ) is one way to summarize the performance of a classifier for binary classification tasks. This square matrix consists of columns and rows that list the number of instances as absolute or relative \"actual class\" vs. \"predicted class\" ratios.  Let  $P$  be the label of class 1 and  $N$  be the label of a second class or the label of all classes that are  not class 1  in a multi-class setting.",
            "title": "Confusion Matrix"
        },
        {
            "location": "/user_guide/evaluate/scoring/#error-and-accuracy",
            "text": "Both the prediction  error  (ERR) and  accuracy  (ACC) provide general information about how many samples are misclassified. The  error  can be understood as the sum of all false predictions divided by the number of total predications, and the the accuracy is calculated as the sum of correct predictions divided by the total number of predictions, respectively.   $$ERR = \\frac{FP + FN}{FP+ FN + TP + TN} = 1-ACC$$  $$ACC = \\frac{TP + TN}{FP+ FN + TP + TN} = 1-ERR$$",
            "title": "Error and Accuracy"
        },
        {
            "location": "/user_guide/evaluate/scoring/#true-and-false-positive-rates",
            "text": "The  True Positive Rate  (TPR) and  False Positive Rate  (FPR) are performance metrics that are especially useful for imbalanced class problems. In  spam classification , for example, we are of course primarily interested in the detection and filtering out of  spam . However, it is also important to decrease the number of messages that were incorrectly classified as  spam  ( False Positives ): A situation where a person misses an important message is considered as \"worse\" than a situation where a person ends up with a few  spam  messages in his e-mail inbox. In contrast to the  FPR , the  True Positive Rate  provides useful information about the fraction of  positive  (or  relevant ) samples that were correctly identified out of the total pool of  Positives .  $$FPR = \\frac{FP}{N} =  \\frac{FP}{FP + TN}$$  $$TPR = \\frac{TP}{P} =  \\frac{TP}{FN + TP}$$",
            "title": "True and False Positive Rates"
        },
        {
            "location": "/user_guide/evaluate/scoring/#precision-recall-and-the-f1-score",
            "text": "Precision  (PRE) and  Recall  (REC) are metrics that are more commonly used in  Information Technology  and related to the  False  and  True Prositive Rates . In fact,  Recall  is synonymous to the  True Positive Rate  and also sometimes called  Sensitivity . The F $_1$ -Score can be understood as a combination of both  Precision  and  Recall .  $$PRE = \\frac{TP}{TP + FP}$$  $$REC = TPR = \\frac{TP}{P} =  \\frac{TP}{FN + TP}$$  $$F_1 = 2 \\cdot \\frac{PRE \\cdot REC}{PRE + REC}$$",
            "title": "Precision, Recall, and the F1-Score"
        },
        {
            "location": "/user_guide/evaluate/scoring/#sensitivity-and-specificity",
            "text": "Sensitivity  (SEN) is synonymous to  Recall  and the  True Positive Rate  whereas  Specificity (SPC)  is synonymous to the  True Negative Rate  --  Sensitivity  measures the recovery rate of the  Positives  and complimentary, the  Specificity  measures the recovery rate of the  Negatives .  $$SEN = TPR = REC = \\frac{TP}{P} =  \\frac{TP}{FN + TP}$$  $$SPC = TNR =\\frac{TN}{N} =  \\frac{TN}{FP + TN}$$",
            "title": "Sensitivity and Specificity"
        },
        {
            "location": "/user_guide/evaluate/scoring/#matthews-correlation-coefficient",
            "text": "Matthews correlation coefficient  (MCC) was first formulated by Brian W. Matthews [3] in 1975 to assess the performance of protein secondary structure predictions. The MCC can be understood as a specific case of a linear correlation coefficient ( Pearson's R ) for a binary classification setting and is considered as especially useful in unbalanced class settings.\nThe previous metrics take values in the range between 0 (worst) and 1 (best), whereas the MCC is bounded between the range 1 (perfect correlation between ground truth and predicted outcome) and -1 (inverse or negative correlation) -- a value of 0 denotes a random prediction.   $$MCC = \\frac{ TP \\times TN - FP \\times FN } {\\sqrt{ (TP + FP) ( TP + FN ) ( TN + FP ) ( TN + FN ) } }$$",
            "title": "Matthews Correlation Coefficient"
        },
        {
            "location": "/user_guide/evaluate/scoring/#average-per-class-accuracy",
            "text": "The \"overall\" accuracy is defined as the number of correct predictions ( true positives  TP and  true negatives  TN) over all samples  n :  $$ACC = \\frac{TP + TN}{n}$$  in a binary class setting:   In a multi-class setting, we can generalize the computation of the accuracy as the fraction of all true predictions (the diagonal) over all samples n.  $$ACC = \\frac{T}{n}$$  Considering a multi-class problem with 3 classes (C0, C1, C2)   let's assume our model made the following predictions:   We compute the accuracy as:  $$ACC = \\frac{3 + 50 + 18}{90} \\approx 0.79$$  Now, in order to compute the  average per-class accuracy , we compute the binary accuracy for each class label separately; i.e., if class 1 is the positive class, class 0 and 2 are both considered the negative class.  $$APC\\;ACC = \\frac{83/90 + 71/90 + 78/90}{3} \\approx 0.86$$",
            "title": "Average Per-Class Accuracy"
        },
        {
            "location": "/user_guide/evaluate/scoring/#references",
            "text": "[1] S. Raschka.  An overview of general performance metrics of binary classifier systems . Computing Research Repository (CoRR), abs/1410.5330, 2014.  [2] Cyril Goutte and Eric Gaussier.  A probabilistic interpretation of precision, recall and f-score, with implication for evaluation . In Advances in Information Retrieval, pages 345\u2013359. Springer, 2005.  [3] Brian W Matthews.  Comparison of the predicted and observed secondary structure of T4 phage lysozyme . Biochimica et Biophysica Acta (BBA)- Protein Structure, 405(2):442\u2013451, 1975.",
            "title": "References"
        },
        {
            "location": "/user_guide/evaluate/scoring/#examples",
            "text": "",
            "title": "Examples"
        },
        {
            "location": "/user_guide/evaluate/scoring/#example-1-classification-error",
            "text": "#from mlxtend.evaluate import scoring\n\ny_targ = [1, 1, 1, 0, 0, 2, 0, 3]\ny_pred = [1, 0, 1, 0, 0, 2, 1, 3]\nres = scoring(y_target=y_targ, y_predicted=y_pred, metric='error')\n\nprint('Error: %s%%' % (res * 100))  Error: 25.0%",
            "title": "Example 1 - Classification Error"
        },
        {
            "location": "/user_guide/evaluate/scoring/#api",
            "text": "scoring(y_target, y_predicted, metric='error', positive_label=1)  Compute a scoring metric for supervised learning.  Parameters    y_target  : array-like, shape=[n_values]  True class labels or target values.    y_predicted  : array-like, shape=[n_values]  Predicted class labels or target values.    metric  : str (default: 'error')  Performance metric.\n[TP: True positives, TN = True negatives,  TN: True negatives, FN = False negatives]  'accuracy': (TP + TN)/(FP + FN + TP + TN) = 1-ERR  'error': (TP + TN)/(FP+ FN + TP + TN) = 1-ACC  'false_positive_rate': FP/N = FP/(FP + TN)  'true_positive_rate': TP/P = TP/(FN + TP)  'true_negative_rate': TN/N = TN/(FP + TN)  'precision': TP/(TP + FP)  'recall': equal to 'true_positive_rate'  'sensitivity': equal to 'true_positive_rate' or 'recall'  'specificity': equal to 'true_negative_rate'  'f1': 2 * (PRE * REC)/(PRE + REC)  'matthews_corr_coef':  (TP TN - FP FN)\n/ (sqrt{(TP + FP)( TP + FN )( TN + FP )( TN + FN )})    positive_label  : int (default: 1)  Label of the positive class for binary classification\nmetrics.    Returns   score  : float",
            "title": "API"
        },
        {
            "location": "/user_guide/preprocessing/MeanCenterer/",
            "text": "Mean Centerer\n\n\nA transformer object that performs column-based mean centering on a NumPy array.\n\n\n\n\nfrom mlxtend.preprocessing import MeanCenterer\n\n\n\n\nExamples\n\n\nExample 1 - Centering a NumPy Array\n\n\nUse the \nfit\n method to fit the column means of a dataset (e.g., the training dataset) to a new \nMeanCenterer\n object. Then, call the \ntransform\n method on the same dataset to center it at the sample mean.\n\n\nimport numpy as np\nfrom mlxtend.preprocessing import MeanCenterer\nX_train = np.array(\n                   [[1, 2, 3],\n                    [4, 5, 6],\n                    [7, 8, 9]])\nmc = MeanCenterer().fit(X_train)\nmc.transform(X_train)\n\n\n\n\narray([[-3., -3., -3.],\n       [ 0.,  0.,  0.],\n       [ 3.,  3.,  3.]])\n\n\n\nAPI\n\n\nMeanCenterer()\n\n\nColumn centering of vectors and matrices.\n\n\nAttributes\n\n\n\n\n\n\ncol_means\n : numpy.ndarray [n_columns]\n\n\nNumPy array storing the mean values for centering after fitting\nthe MeanCenterer object.\n\n\n\n\n\n\nMethods\n\n\n\n\n\nfit(X)\n\n\nGets the column means for mean centering.\n\n\nParameters\n\n\n\n\n\n\nX\n : {array-like, sparse matrix}, shape = [n_samples, n_features]\n\n\nArray of data vectors, where n_samples is the number of samples and\nn_features is the number of features.\n\n\n\n\n\n\nReturns\n\n    self\n\n\n\n\n\nfit_transform(X)\n\n\nFits and transforms an arry.\n\n\nParameters\n\n\n\n\n\n\nX\n : {array-like, sparse matrix}, shape = [n_samples, n_features]\n\n\nArray of data vectors, where n_samples is the number of samples and\nn_features is the number of features.\n\n\n\n\n\n\nReturns\n\n\n\n\n\n\nX_tr\n : {array-like, sparse matrix}, shape = [n_samples, n_features]\n\n\nA copy of the input array with the columns centered.\n\n\n\n\n\n\n\n\n\ntransform(X)\n\n\nCenters a NumPy array.\n\n\nParameters\n\n\n\n\n\n\nX\n : {array-like, sparse matrix}, shape = [n_samples, n_features]\n\n\nArray of data vectors, where n_samples is the number of samples and\nn_features is the number of features.\n\n\n\n\n\n\nReturns\n\n\n\n\n\n\nX_tr\n : {array-like, sparse matrix}, shape = [n_samples, n_features]\n\n\nA copy of the input array with the columns centered.",
            "title": "MeanCenterer"
        },
        {
            "location": "/user_guide/preprocessing/MeanCenterer/#mean-centerer",
            "text": "A transformer object that performs column-based mean centering on a NumPy array.   from mlxtend.preprocessing import MeanCenterer",
            "title": "Mean Centerer"
        },
        {
            "location": "/user_guide/preprocessing/MeanCenterer/#examples",
            "text": "",
            "title": "Examples"
        },
        {
            "location": "/user_guide/preprocessing/MeanCenterer/#example-1-centering-a-numpy-array",
            "text": "Use the  fit  method to fit the column means of a dataset (e.g., the training dataset) to a new  MeanCenterer  object. Then, call the  transform  method on the same dataset to center it at the sample mean.  import numpy as np\nfrom mlxtend.preprocessing import MeanCenterer\nX_train = np.array(\n                   [[1, 2, 3],\n                    [4, 5, 6],\n                    [7, 8, 9]])\nmc = MeanCenterer().fit(X_train)\nmc.transform(X_train)  array([[-3., -3., -3.],\n       [ 0.,  0.,  0.],\n       [ 3.,  3.,  3.]])",
            "title": "Example 1 - Centering a NumPy Array"
        },
        {
            "location": "/user_guide/preprocessing/MeanCenterer/#api",
            "text": "MeanCenterer()  Column centering of vectors and matrices.  Attributes    col_means  : numpy.ndarray [n_columns]  NumPy array storing the mean values for centering after fitting\nthe MeanCenterer object.",
            "title": "API"
        },
        {
            "location": "/user_guide/preprocessing/MeanCenterer/#methods",
            "text": "fit(X)  Gets the column means for mean centering.  Parameters    X  : {array-like, sparse matrix}, shape = [n_samples, n_features]  Array of data vectors, where n_samples is the number of samples and\nn_features is the number of features.    Returns \n    self   fit_transform(X)  Fits and transforms an arry.  Parameters    X  : {array-like, sparse matrix}, shape = [n_samples, n_features]  Array of data vectors, where n_samples is the number of samples and\nn_features is the number of features.    Returns    X_tr  : {array-like, sparse matrix}, shape = [n_samples, n_features]  A copy of the input array with the columns centered.     transform(X)  Centers a NumPy array.  Parameters    X  : {array-like, sparse matrix}, shape = [n_samples, n_features]  Array of data vectors, where n_samples is the number of samples and\nn_features is the number of features.    Returns    X_tr  : {array-like, sparse matrix}, shape = [n_samples, n_features]  A copy of the input array with the columns centered.",
            "title": "Methods"
        },
        {
            "location": "/user_guide/preprocessing/minmax_scaling/",
            "text": "MinMax Scaling\n\n\nA function for min-max scaling of pandas DataFrames or NumPy arrays.\n\n\n\n\nfrom mlxtend.preprocessing import MinMaxScaling\n\n\n\n\nAn alternative approach to Z-score normalization (or standardization) is the so-called Min-Max scaling (often also simply called \"normalization\" - a common cause for ambiguities).\nIn this approach, the data is scaled to a fixed range - usually 0 to 1.\nThe cost of having this bounded range - in contrast to standardization - is that we will end up with smaller standard deviations, which can suppress the effect of outliers.\n\n\nA Min-Max scaling is typically done via the following equation:\n\n\n$$X_{sc} = \\frac{X - X_{min}}{X_{max} - X_{min}}.$$\n\n\nOne family of algorithms that is scale-invariant encompasses tree-based learning algorithms. Let's take the general CART decision tree algorithm. Without going into much depth regarding information gain and impurity measures, we can think of the decision as \"is feature x_i >= some_val?\" Intuitively, we can see that it really doesn't matter on which scale this feature is (centimeters, Fahrenheit, a standardized scale -- it really doesn't matter).\n\n\nSome examples of algorithms where feature scaling matters are:\n\n\n\n\nk-nearest neighbors with an Euclidean distance measure if want all features to contribute equally\n\n\nk-means (see k-nearest neighbors)\n\n\nlogistic regression, SVMs, perceptrons, neural networks etc. if you are using gradient descent/ascent-based optimization, otherwise some weights will update much faster than others\n\n\nlinear discriminant analysis, principal component analysis, kernel principal component analysis since you want to find directions of maximizing the variance (under the constraints that those directions/eigenvectors/principal components are orthogonal); you want to have features on the same scale since you'd emphasize variables on \"larger measurement scales\" more.\n\n\n\n\nThere are many more cases than I can possibly list here ... I always recommend you to think about the algorithm and what it's doing, and then it typically becomes obvious whether we want to scale your features or not.\n\n\nIn addition, we'd also want to think about whether we want to \"standardize\" or \"normalize\" (here: scaling to [0, 1] range) our data. Some algorithms assume that our data is centered at 0. For example, if we initialize the weights of a small multi-layer perceptron with tanh activation units to 0 or small random values centered around zero, we want to update the model weights \"equally.\"\nAs a rule of thumb I'd say: When in doubt, just standardize the data, it shouldn't hurt.   \n\n\nExamples\n\n\nExample 1 - Scaling a Pandas DataFrame\n\n\nimport pandas as pd\n\ns1 = pd.Series([1, 2, 3, 4, 5, 6], index=(range(6)))\ns2 = pd.Series([10, 9, 8, 7, 6, 5], index=(range(6)))\ndf = pd.DataFrame(s1, columns=['s1'])\ndf['s2'] = s2\ndf\n\n\n\n\n\n\n\n  \n\n    \n\n      \n\n      \ns1\n\n      \ns2\n\n    \n\n  \n\n  \n\n    \n\n      \n0\n\n      \n1\n\n      \n10\n\n    \n\n    \n\n      \n1\n\n      \n2\n\n      \n9\n\n    \n\n    \n\n      \n2\n\n      \n3\n\n      \n8\n\n    \n\n    \n\n      \n3\n\n      \n4\n\n      \n7\n\n    \n\n    \n\n      \n4\n\n      \n5\n\n      \n6\n\n    \n\n    \n\n      \n5\n\n      \n6\n\n      \n5\n\n    \n\n  \n\n\n\n\n\n\n\nfrom mlxtend.preprocessing import minmax_scaling\n\nminmax_scaling(df, columns=['s1', 's2'])\n\n\n\n\n\n\n\n  \n\n    \n\n      \n\n      \ns1\n\n      \ns2\n\n    \n\n  \n\n  \n\n    \n\n      \n0\n\n      \n0.0\n\n      \n1.0\n\n    \n\n    \n\n      \n1\n\n      \n0.2\n\n      \n0.8\n\n    \n\n    \n\n      \n2\n\n      \n0.4\n\n      \n0.6\n\n    \n\n    \n\n      \n3\n\n      \n0.6\n\n      \n0.4\n\n    \n\n    \n\n      \n4\n\n      \n0.8\n\n      \n0.2\n\n    \n\n    \n\n      \n5\n\n      \n1.0\n\n      \n0.0\n\n    \n\n  \n\n\n\n\n\n\n\nExample 2 - Scaling a NumPy Array\n\n\nimport numpy as np\n\nX = np.array([[1, 10], [2, 9], [3, 8], \n              [4, 7], [5, 6], [6, 5]])\nX\n\n\n\n\narray([[ 1, 10],\n       [ 2,  9],\n       [ 3,  8],\n       [ 4,  7],\n       [ 5,  6],\n       [ 6,  5]])\n\n\n\nfrom mlxtend.preprocessing import minmax_scaling\n\nminmax_scaling(X, columns=[0, 1])\n\n\n\n\narray([[ 0. ,  1. ],\n       [ 0.2,  0.8],\n       [ 0.4,  0.6],\n       [ 0.6,  0.4],\n       [ 0.8,  0.2],\n       [ 1. ,  0. ]])\n\n\n\nAPI\n\n\nminmax_scaling(array, columns, min_val=0, max_val=1)\n\n\nMin max scaling of pandas' DataFrames.\n\n\nParameters\n\n\n\n\n\n\narray\n : pandas DataFrame or NumPy ndarray, shape = [n_rows, n_columns].\n\n\n\n\n\n\ncolumns\n : array-like, shape = [n_columns]\n\n\nArray-like with column names, e.g., ['col1', 'col2', ...]\nor column indices [0, 2, 4, ...]\n\n\n\n\n\n\nmin_val\n : \nint\n or \nfloat\n, optional (default=\n0\n)\n\n\nminimum value after rescaling.\n\n\n\n\n\n\nmin_val\n : \nint\n or \nfloat\n, optional (default=\n1\n)\n\n\nmaximum value after rescaling.\n\n\n\n\n\n\nReturns\n\n\n\n\n\n\ndf_new\n : pandas DataFrame object.\n\n\nCopy of the array or DataFrame with rescaled columns.",
            "title": "Minmax scaling"
        },
        {
            "location": "/user_guide/preprocessing/minmax_scaling/#minmax-scaling",
            "text": "A function for min-max scaling of pandas DataFrames or NumPy arrays.   from mlxtend.preprocessing import MinMaxScaling   An alternative approach to Z-score normalization (or standardization) is the so-called Min-Max scaling (often also simply called \"normalization\" - a common cause for ambiguities).\nIn this approach, the data is scaled to a fixed range - usually 0 to 1.\nThe cost of having this bounded range - in contrast to standardization - is that we will end up with smaller standard deviations, which can suppress the effect of outliers.  A Min-Max scaling is typically done via the following equation:  $$X_{sc} = \\frac{X - X_{min}}{X_{max} - X_{min}}.$$  One family of algorithms that is scale-invariant encompasses tree-based learning algorithms. Let's take the general CART decision tree algorithm. Without going into much depth regarding information gain and impurity measures, we can think of the decision as \"is feature x_i >= some_val?\" Intuitively, we can see that it really doesn't matter on which scale this feature is (centimeters, Fahrenheit, a standardized scale -- it really doesn't matter).  Some examples of algorithms where feature scaling matters are:   k-nearest neighbors with an Euclidean distance measure if want all features to contribute equally  k-means (see k-nearest neighbors)  logistic regression, SVMs, perceptrons, neural networks etc. if you are using gradient descent/ascent-based optimization, otherwise some weights will update much faster than others  linear discriminant analysis, principal component analysis, kernel principal component analysis since you want to find directions of maximizing the variance (under the constraints that those directions/eigenvectors/principal components are orthogonal); you want to have features on the same scale since you'd emphasize variables on \"larger measurement scales\" more.   There are many more cases than I can possibly list here ... I always recommend you to think about the algorithm and what it's doing, and then it typically becomes obvious whether we want to scale your features or not.  In addition, we'd also want to think about whether we want to \"standardize\" or \"normalize\" (here: scaling to [0, 1] range) our data. Some algorithms assume that our data is centered at 0. For example, if we initialize the weights of a small multi-layer perceptron with tanh activation units to 0 or small random values centered around zero, we want to update the model weights \"equally.\"\nAs a rule of thumb I'd say: When in doubt, just standardize the data, it shouldn't hurt.",
            "title": "MinMax Scaling"
        },
        {
            "location": "/user_guide/preprocessing/minmax_scaling/#examples",
            "text": "",
            "title": "Examples"
        },
        {
            "location": "/user_guide/preprocessing/minmax_scaling/#example-1-scaling-a-pandas-dataframe",
            "text": "import pandas as pd\n\ns1 = pd.Series([1, 2, 3, 4, 5, 6], index=(range(6)))\ns2 = pd.Series([10, 9, 8, 7, 6, 5], index=(range(6)))\ndf = pd.DataFrame(s1, columns=['s1'])\ndf['s2'] = s2\ndf   \n   \n     \n       \n       s1 \n       s2 \n     \n   \n   \n     \n       0 \n       1 \n       10 \n     \n     \n       1 \n       2 \n       9 \n     \n     \n       2 \n       3 \n       8 \n     \n     \n       3 \n       4 \n       7 \n     \n     \n       4 \n       5 \n       6 \n     \n     \n       5 \n       6 \n       5 \n     \n      from mlxtend.preprocessing import minmax_scaling\n\nminmax_scaling(df, columns=['s1', 's2'])   \n   \n     \n       \n       s1 \n       s2 \n     \n   \n   \n     \n       0 \n       0.0 \n       1.0 \n     \n     \n       1 \n       0.2 \n       0.8 \n     \n     \n       2 \n       0.4 \n       0.6 \n     \n     \n       3 \n       0.6 \n       0.4 \n     \n     \n       4 \n       0.8 \n       0.2 \n     \n     \n       5 \n       1.0 \n       0.0",
            "title": "Example 1 - Scaling a Pandas DataFrame"
        },
        {
            "location": "/user_guide/preprocessing/minmax_scaling/#example-2-scaling-a-numpy-array",
            "text": "import numpy as np\n\nX = np.array([[1, 10], [2, 9], [3, 8], \n              [4, 7], [5, 6], [6, 5]])\nX  array([[ 1, 10],\n       [ 2,  9],\n       [ 3,  8],\n       [ 4,  7],\n       [ 5,  6],\n       [ 6,  5]])  from mlxtend.preprocessing import minmax_scaling\n\nminmax_scaling(X, columns=[0, 1])  array([[ 0. ,  1. ],\n       [ 0.2,  0.8],\n       [ 0.4,  0.6],\n       [ 0.6,  0.4],\n       [ 0.8,  0.2],\n       [ 1. ,  0. ]])",
            "title": "Example 2 - Scaling a NumPy Array"
        },
        {
            "location": "/user_guide/preprocessing/minmax_scaling/#api",
            "text": "minmax_scaling(array, columns, min_val=0, max_val=1)  Min max scaling of pandas' DataFrames.  Parameters    array  : pandas DataFrame or NumPy ndarray, shape = [n_rows, n_columns].    columns  : array-like, shape = [n_columns]  Array-like with column names, e.g., ['col1', 'col2', ...]\nor column indices [0, 2, 4, ...]    min_val  :  int  or  float , optional (default= 0 )  minimum value after rescaling.    min_val  :  int  or  float , optional (default= 1 )  maximum value after rescaling.    Returns    df_new  : pandas DataFrame object.  Copy of the array or DataFrame with rescaled columns.",
            "title": "API"
        },
        {
            "location": "/user_guide/preprocessing/shuffle_arrays_unison/",
            "text": "Shuffle Arrays in Unison\n\n\nA function for NumPy arrays in unison.\n\n\n\n\nfrom mlxtend.preprocessing import shuffle_arrays_unison\n\n\n\n\nExamples\n\n\nExample 1 - Scaling a Pandas DataFrame\n\n\nimport numpy as np\nfrom mlxtend.preprocessing import shuffle_arrays_unison\nX = np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]])\ny = np.array([1, 2, 3])\nprint('X:\\n%s' % X)\nprint('y:\\n%s' % y)\n\n\n\n\nX:\n[[1 2 3]\n [4 5 6]\n [7 8 9]]\ny:\n[1 2 3]\n\n\n\nX2, y2 = shuffle_arrays_unison(arrays=[X, y], random_seed=3)\nprint('X2:\\n%s' % X2)\nprint('y2:\\n%s' % y2)\n\n\n\n\nX2:\n[[4 5 6]\n [1 2 3]\n [7 8 9]]\ny2:\n[2 1 3]\n\n\n\nAPI\n\n\nshuffle_arrays_unison(arrays, random_seed=None)\n\n\nShuffle NumPy arrays in unison.\n\n\nParameters\n\n\n\n\n\n\narrays\n : array-like, shape = [n_arrays]\n\n\nA list of NumPy arrays.\n\n\n\n\n\n\nrandom_seed\n : int (default: None)\n\n\nSets the random state.\n\n\n\n\n\n\nReturns\n\n\n\n\nshuffled_arrays\n : A list of NumPy arrays after shuffling.\n\n\n\n\nExamples\n\n\n>>> import numpy as np\n>>> from mlxtend.preprocessing import shuffle_arrays_unison\n>>> X1 = np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]])\n>>> y1 = np.array([1, 2, 3])\n>>> X2, y2 = shuffle_arrays_unison(arrays=[X1, y1], random_seed=3)\n>>> assert(X2.all() == np.array([[4, 5, 6], [1, 2, 3], [7, 8, 9]]).all())\n>>> assert(y2.all() == np.array([2, 1, 3]).all())\n>>>",
            "title": "Shuffle arrays unison"
        },
        {
            "location": "/user_guide/preprocessing/shuffle_arrays_unison/#shuffle-arrays-in-unison",
            "text": "A function for NumPy arrays in unison.   from mlxtend.preprocessing import shuffle_arrays_unison",
            "title": "Shuffle Arrays in Unison"
        },
        {
            "location": "/user_guide/preprocessing/shuffle_arrays_unison/#examples",
            "text": "",
            "title": "Examples"
        },
        {
            "location": "/user_guide/preprocessing/shuffle_arrays_unison/#example-1-scaling-a-pandas-dataframe",
            "text": "import numpy as np\nfrom mlxtend.preprocessing import shuffle_arrays_unison\nX = np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]])\ny = np.array([1, 2, 3])\nprint('X:\\n%s' % X)\nprint('y:\\n%s' % y)  X:\n[[1 2 3]\n [4 5 6]\n [7 8 9]]\ny:\n[1 2 3]  X2, y2 = shuffle_arrays_unison(arrays=[X, y], random_seed=3)\nprint('X2:\\n%s' % X2)\nprint('y2:\\n%s' % y2)  X2:\n[[4 5 6]\n [1 2 3]\n [7 8 9]]\ny2:\n[2 1 3]",
            "title": "Example 1 - Scaling a Pandas DataFrame"
        },
        {
            "location": "/user_guide/preprocessing/shuffle_arrays_unison/#api",
            "text": "shuffle_arrays_unison(arrays, random_seed=None)  Shuffle NumPy arrays in unison.  Parameters    arrays  : array-like, shape = [n_arrays]  A list of NumPy arrays.    random_seed  : int (default: None)  Sets the random state.    Returns   shuffled_arrays  : A list of NumPy arrays after shuffling.   Examples  >>> import numpy as np\n>>> from mlxtend.preprocessing import shuffle_arrays_unison\n>>> X1 = np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]])\n>>> y1 = np.array([1, 2, 3])\n>>> X2, y2 = shuffle_arrays_unison(arrays=[X1, y1], random_seed=3)\n>>> assert(X2.all() == np.array([[4, 5, 6], [1, 2, 3], [7, 8, 9]]).all())\n>>> assert(y2.all() == np.array([2, 1, 3]).all())\n>>>",
            "title": "API"
        },
        {
            "location": "/user_guide/preprocessing/standardize/",
            "text": "Standardize\n\n\nA function that performs column-based standardization on a NumPy array.\n\n\n\n\nfrom mlxtend.preprocessing import standardize\n\n\n\n\nOverview\n\n\nThe result of standardization (or Z-score normalization) is that the features will be rescaled so that they'll have the properties of a standard normal distribution with\n\n\n$\\mu = 0$\n and \n$\\sigma = 1$\n.\n\n\nwhere \n$\\mu$\n is the mean (average) and \n$\\sigma$\n is the standard deviation from the mean; standard scores (also called z scores) of the samples are calculated as\n\n\n$$z=\\frac{x-\\mu}{\\sigma}.$$\n\n\nStandardizing the features so that they are centered around 0 with a standard deviation of 1 is not only important if we are comparing measurements that have different units, but it is also a general requirement for the optimal performance of many machine learning algorithms. \n\n\nOne family of algorithms that is scale-invariant encompasses tree-based learning algorithms. Let's take the general CART decision tree algorithm. Without going into much depth regarding information gain and impurity measures, we can think of the decision as \"is feature x_i >= some_val?\" Intuitively, we can see that it really doesn't matter on which scale this feature is (centimeters, Fahrenheit, a standardized scale -- it really doesn't matter).\n\n\nSome examples of algorithms where feature scaling matters are:\n\n\n\n\nk-nearest neighbors with an Euclidean distance measure if want all features to contribute equally\n\n\nk-means (see k-nearest neighbors)\n\n\nlogistic regression, SVMs, perceptrons, neural networks etc. if you are using gradient descent/ascent-based optimization, otherwise some weights will update much faster than others\n\n\nlinear discriminant analysis, principal component analysis, kernel principal component analysis since you want to find directions of maximizing the variance (under the constraints that those directions/eigenvectors/principal components are orthogonal); you want to have features on the same scale since you'd emphasize variables on \"larger measurement scales\" more.\n\n\n\n\nThere are many more cases than I can possibly list here ... I always recommend you to think about the algorithm and what it's doing, and then it typically becomes obvious whether we want to scale your features or not.\n\n\nIn addition, we'd also want to think about whether we want to \"standardize\" or \"normalize\" (here: scaling to [0, 1] range) our data. Some algorithms assume that our data is centered at 0. For example, if we initialize the weights of a small multi-layer perceptron with tanh activation units to 0 or small random values centered around zero, we want to update the model weights \"equally.\"\nAs a rule of thumb I'd say: When in doubt, just standardize the data, it shouldn't hurt.   \n\n\nExamples\n\n\nExample 1 - Standardize a Pandas DataFrame\n\n\nimport pandas as pd\n\ns1 = pd.Series([1, 2, 3, 4, 5, 6], index=(range(6)))\ns2 = pd.Series([10, 9, 8, 7, 6, 5], index=(range(6)))\ndf = pd.DataFrame(s1, columns=['s1'])\ndf['s2'] = s2\ndf\n\n\n\n\n\n\n\n  \n\n    \n\n      \n\n      \ns1\n\n      \ns2\n\n    \n\n  \n\n  \n\n    \n\n      \n0\n\n      \n1\n\n      \n10\n\n    \n\n    \n\n      \n1\n\n      \n2\n\n      \n9\n\n    \n\n    \n\n      \n2\n\n      \n3\n\n      \n8\n\n    \n\n    \n\n      \n3\n\n      \n4\n\n      \n7\n\n    \n\n    \n\n      \n4\n\n      \n5\n\n      \n6\n\n    \n\n    \n\n      \n5\n\n      \n6\n\n      \n5\n\n    \n\n  \n\n\n\n\n\n\n\nfrom mlxtend.preprocessing import standardize\nstandardize(df, columns=['s1', 's2'])\n\n\n\n\n\n\n\n  \n\n    \n\n      \n\n      \ns1\n\n      \ns2\n\n    \n\n  \n\n  \n\n    \n\n      \n0\n\n      \n-1.46385\n\n      \n1.46385\n\n    \n\n    \n\n      \n1\n\n      \n-0.87831\n\n      \n0.87831\n\n    \n\n    \n\n      \n2\n\n      \n-0.29277\n\n      \n0.29277\n\n    \n\n    \n\n      \n3\n\n      \n0.29277\n\n      \n-0.29277\n\n    \n\n    \n\n      \n4\n\n      \n0.87831\n\n      \n-0.87831\n\n    \n\n    \n\n      \n5\n\n      \n1.46385\n\n      \n-1.46385\n\n    \n\n  \n\n\n\n\n\n\n\nExample 2 - Standardize a NumPy Array\n\n\nimport numpy as np\n\nX = np.array([[1, 10], [2, 9], [3, 8], [4, 7], [5, 6], [6, 5]])\nX\n\n\n\n\narray([[ 1, 10],\n       [ 2,  9],\n       [ 3,  8],\n       [ 4,  7],\n       [ 5,  6],\n       [ 6,  5]])\n\n\n\nfrom mlxtend.preprocessing import standardize\nstandardize(X, columns=[0, 1])\n\n\n\n\narray([[-1.46385011,  1.46385011],\n       [-0.87831007,  0.87831007],\n       [-0.29277002,  0.29277002],\n       [ 0.29277002, -0.29277002],\n       [ 0.87831007, -0.87831007],\n       [ 1.46385011, -1.46385011]])\n\n\n\nExample 3 - Re-using parameters\n\n\nIn machine learning contexts, it is desired to re-use the parameters that have been obtained from a training set to scale new, future data (including the independent test set). By setting \nreturn_params=True\n, the \nstandardize\n function returns a second object, a parameter dictionary containing the column means and standard deviations that can be re-used by feeding it to the \nparams\n parameter upon function call.\n\n\nimport numpy as np\nfrom mlxtend.preprocessing import standardize\n\nX_train = np.array([[1, 10], [4, 7], [3, 8]])\nX_test = np.array([[1, 2], [3, 4], [5, 6]])\n\nX_train_std, params = standardize(X_train, \n                                  columns=[0, 1], \n                                  return_params=True)\nX_train_std\n\n\n\n\narray([[-1.33630621,  1.33630621],\n       [ 1.06904497, -1.06904497],\n       [ 0.26726124, -0.26726124]])\n\n\n\nparams\n\n\n\n\n{'avgs': array([ 2.66666667,  8.33333333]),\n 'stds': array([ 1.24721913,  1.24721913])}\n\n\n\nX_test_std = standardize(X_test, \n                         columns=[0, 1], \n                         params=params)\nX_test_std\n\n\n\n\narray([[-1.33630621, -5.0779636 ],\n       [ 0.26726124, -3.47439614],\n       [ 1.87082869, -1.87082869]])\n\n\n\nAPI\n\n\nstandardize(array, columns, ddof=0, return_params=False, params=None)\n\n\nStandardize columns in pandas DataFrames.\n\n\nParameters\n\n\n\n\n\n\narray\n : pandas DataFrame or NumPy ndarray, shape = [n_rows, n_columns].\n\n\n\n\n\n\ncolumns\n : array-like, shape = [n_columns]\n\n\nArray-like with column names, e.g., ['col1', 'col2', ...]\nor column indices [0, 2, 4, ...]\n\n\n\n\n\n\nddof\n : int (default: 0)\n\n\nDelta Degrees of Freedom. The divisor used in calculations\nis N - ddof, where N represents the number of elements.\n\n\n\n\n\n\nreturn_params\n : dict (default: False)\n\n\nIf set to True, a dictionary is returned in addition to the\nstandardized array. The parameter dictionary contains the\ncolumn means ('avgs') and standard deviations ('stds') of\nthe individual columns.\n\n\n\n\n\n\nparams\n : dict (default: None)\n\n\nA dictionary with column means and standard deviations as\nreturned by the \nstandardize\n function if \nreturn_params\n\nwas set to True. If a \nparams\n dictionary is provided, the\n\nstandardize\n function will use these instead of computing\nthem from the current array.\n\n\n\n\n\n\nNotes\n\n\nIf all values in a given column are the same, these values are all\n    set to \n0.0\n. The standard deviation in the \nparameters\n dictionary\n    is consequently set to \n1.0\n to avoid dividing by zero.\n\n\nReturns\n\n\n\n\n\n\ndf_new\n : pandas DataFrame object.\n\n\nCopy of the array or DataFrame with standardized columns.",
            "title": "Standardize"
        },
        {
            "location": "/user_guide/preprocessing/standardize/#standardize",
            "text": "A function that performs column-based standardization on a NumPy array.   from mlxtend.preprocessing import standardize",
            "title": "Standardize"
        },
        {
            "location": "/user_guide/preprocessing/standardize/#overview",
            "text": "The result of standardization (or Z-score normalization) is that the features will be rescaled so that they'll have the properties of a standard normal distribution with  $\\mu = 0$  and  $\\sigma = 1$ .  where  $\\mu$  is the mean (average) and  $\\sigma$  is the standard deviation from the mean; standard scores (also called z scores) of the samples are calculated as  $$z=\\frac{x-\\mu}{\\sigma}.$$  Standardizing the features so that they are centered around 0 with a standard deviation of 1 is not only important if we are comparing measurements that have different units, but it is also a general requirement for the optimal performance of many machine learning algorithms.   One family of algorithms that is scale-invariant encompasses tree-based learning algorithms. Let's take the general CART decision tree algorithm. Without going into much depth regarding information gain and impurity measures, we can think of the decision as \"is feature x_i >= some_val?\" Intuitively, we can see that it really doesn't matter on which scale this feature is (centimeters, Fahrenheit, a standardized scale -- it really doesn't matter).  Some examples of algorithms where feature scaling matters are:   k-nearest neighbors with an Euclidean distance measure if want all features to contribute equally  k-means (see k-nearest neighbors)  logistic regression, SVMs, perceptrons, neural networks etc. if you are using gradient descent/ascent-based optimization, otherwise some weights will update much faster than others  linear discriminant analysis, principal component analysis, kernel principal component analysis since you want to find directions of maximizing the variance (under the constraints that those directions/eigenvectors/principal components are orthogonal); you want to have features on the same scale since you'd emphasize variables on \"larger measurement scales\" more.   There are many more cases than I can possibly list here ... I always recommend you to think about the algorithm and what it's doing, and then it typically becomes obvious whether we want to scale your features or not.  In addition, we'd also want to think about whether we want to \"standardize\" or \"normalize\" (here: scaling to [0, 1] range) our data. Some algorithms assume that our data is centered at 0. For example, if we initialize the weights of a small multi-layer perceptron with tanh activation units to 0 or small random values centered around zero, we want to update the model weights \"equally.\"\nAs a rule of thumb I'd say: When in doubt, just standardize the data, it shouldn't hurt.",
            "title": "Overview"
        },
        {
            "location": "/user_guide/preprocessing/standardize/#examples",
            "text": "",
            "title": "Examples"
        },
        {
            "location": "/user_guide/preprocessing/standardize/#example-1-standardize-a-pandas-dataframe",
            "text": "import pandas as pd\n\ns1 = pd.Series([1, 2, 3, 4, 5, 6], index=(range(6)))\ns2 = pd.Series([10, 9, 8, 7, 6, 5], index=(range(6)))\ndf = pd.DataFrame(s1, columns=['s1'])\ndf['s2'] = s2\ndf   \n   \n     \n       \n       s1 \n       s2 \n     \n   \n   \n     \n       0 \n       1 \n       10 \n     \n     \n       1 \n       2 \n       9 \n     \n     \n       2 \n       3 \n       8 \n     \n     \n       3 \n       4 \n       7 \n     \n     \n       4 \n       5 \n       6 \n     \n     \n       5 \n       6 \n       5 \n     \n      from mlxtend.preprocessing import standardize\nstandardize(df, columns=['s1', 's2'])   \n   \n     \n       \n       s1 \n       s2 \n     \n   \n   \n     \n       0 \n       -1.46385 \n       1.46385 \n     \n     \n       1 \n       -0.87831 \n       0.87831 \n     \n     \n       2 \n       -0.29277 \n       0.29277 \n     \n     \n       3 \n       0.29277 \n       -0.29277 \n     \n     \n       4 \n       0.87831 \n       -0.87831 \n     \n     \n       5 \n       1.46385 \n       -1.46385",
            "title": "Example 1 - Standardize a Pandas DataFrame"
        },
        {
            "location": "/user_guide/preprocessing/standardize/#example-2-standardize-a-numpy-array",
            "text": "import numpy as np\n\nX = np.array([[1, 10], [2, 9], [3, 8], [4, 7], [5, 6], [6, 5]])\nX  array([[ 1, 10],\n       [ 2,  9],\n       [ 3,  8],\n       [ 4,  7],\n       [ 5,  6],\n       [ 6,  5]])  from mlxtend.preprocessing import standardize\nstandardize(X, columns=[0, 1])  array([[-1.46385011,  1.46385011],\n       [-0.87831007,  0.87831007],\n       [-0.29277002,  0.29277002],\n       [ 0.29277002, -0.29277002],\n       [ 0.87831007, -0.87831007],\n       [ 1.46385011, -1.46385011]])",
            "title": "Example 2 - Standardize a NumPy Array"
        },
        {
            "location": "/user_guide/preprocessing/standardize/#example-3-re-using-parameters",
            "text": "In machine learning contexts, it is desired to re-use the parameters that have been obtained from a training set to scale new, future data (including the independent test set). By setting  return_params=True , the  standardize  function returns a second object, a parameter dictionary containing the column means and standard deviations that can be re-used by feeding it to the  params  parameter upon function call.  import numpy as np\nfrom mlxtend.preprocessing import standardize\n\nX_train = np.array([[1, 10], [4, 7], [3, 8]])\nX_test = np.array([[1, 2], [3, 4], [5, 6]])\n\nX_train_std, params = standardize(X_train, \n                                  columns=[0, 1], \n                                  return_params=True)\nX_train_std  array([[-1.33630621,  1.33630621],\n       [ 1.06904497, -1.06904497],\n       [ 0.26726124, -0.26726124]])  params  {'avgs': array([ 2.66666667,  8.33333333]),\n 'stds': array([ 1.24721913,  1.24721913])}  X_test_std = standardize(X_test, \n                         columns=[0, 1], \n                         params=params)\nX_test_std  array([[-1.33630621, -5.0779636 ],\n       [ 0.26726124, -3.47439614],\n       [ 1.87082869, -1.87082869]])",
            "title": "Example 3 - Re-using parameters"
        },
        {
            "location": "/user_guide/preprocessing/standardize/#api",
            "text": "standardize(array, columns, ddof=0, return_params=False, params=None)  Standardize columns in pandas DataFrames.  Parameters    array  : pandas DataFrame or NumPy ndarray, shape = [n_rows, n_columns].    columns  : array-like, shape = [n_columns]  Array-like with column names, e.g., ['col1', 'col2', ...]\nor column indices [0, 2, 4, ...]    ddof  : int (default: 0)  Delta Degrees of Freedom. The divisor used in calculations\nis N - ddof, where N represents the number of elements.    return_params  : dict (default: False)  If set to True, a dictionary is returned in addition to the\nstandardized array. The parameter dictionary contains the\ncolumn means ('avgs') and standard deviations ('stds') of\nthe individual columns.    params  : dict (default: None)  A dictionary with column means and standard deviations as\nreturned by the  standardize  function if  return_params \nwas set to True. If a  params  dictionary is provided, the standardize  function will use these instead of computing\nthem from the current array.    Notes  If all values in a given column are the same, these values are all\n    set to  0.0 . The standard deviation in the  parameters  dictionary\n    is consequently set to  1.0  to avoid dividing by zero.  Returns    df_new  : pandas DataFrame object.  Copy of the array or DataFrame with standardized columns.",
            "title": "API"
        },
        {
            "location": "/user_guide/preprocessing/one-hot_encoding/",
            "text": "One-Hot Encoding\n\n\nA function that performs one-hot encoding for class labels.\n\n\n\n\nfrom mlxtend.preprocessing import one_hot\n\n\n\n\nOverview\n\n\nTypical supervised machine learning algorithms for classifications assume that the class labels are \nnominal\n (a special case of \ncategorical\n where no order is implied). A typical example of an nominal feature would be \"color\" since we can't say (in most applications) that \"orange > blue > red\".\n\n\nThe \none_hot\n function provides a simple interface to convert class label integers into a so-called one-hot array, where each unique label is represented as a column in the new array.\n\n\nFor example, let's assume we have 5 data points from 3 different classes: 0, 1, and 2.\n\n\ny = [0, # sample 1, class 0 \n     1, # sample 2, class 1 \n     0, # sample 3, class 0\n     2, # sample 4, class 2\n     2] # sample 5, class 2\n\n\n\nAfter one-hot encoding, we then obtain the following array (note that the index position of the \"1\" in each row denotes the class label of this sample):\n\n\ny = [[1,  0,  0], # sample 1, class 0 \n     [0,  1,  0], # sample 2, class 1  \n     [1,  0,  0], # sample 3, class 0\n     [0,  0,  1], # sample 4, class 2\n     [0,  0,  1]  # sample 5, class 2\n     ])\n\n\n\nExamples\n\n\nExample 1 - Defaults\n\n\nfrom mlxtend.preprocessing import one_hot\nimport numpy as np\n\ny = np.array([0, 1, 2, 1, 2])\none_hot(y)\n\n\n\n\narray([[ 1.,  0.,  0.],\n       [ 0.,  1.,  0.],\n       [ 0.,  0.,  1.],\n       [ 0.,  1.,  0.],\n       [ 0.,  0.,  1.]])\n\n\n\nExample 2 - Python Lists\n\n\nfrom mlxtend.preprocessing import one_hot\n\ny = [0, 1, 2, 1, 2]\none_hot(y)\n\n\n\n\narray([[ 1.,  0.,  0.],\n       [ 0.,  1.,  0.],\n       [ 0.,  0.,  1.],\n       [ 0.,  1.,  0.],\n       [ 0.,  0.,  1.]])\n\n\n\nExample 3 - Integer Arrays\n\n\nfrom mlxtend.preprocessing import one_hot\n\ny = [0, 1, 2, 1, 2]\none_hot(y, dtype='int')\n\n\n\n\narray([[1, 0, 0],\n       [0, 1, 0],\n       [0, 0, 1],\n       [0, 1, 0],\n       [0, 0, 1]])\n\n\n\nExample 4 - Arbitrary Numbers of Class Labels\n\n\nfrom mlxtend.preprocessing import one_hot\n\ny = [0, 1, 2, 1, 2]\none_hot(y, num_labels=10)\n\n\n\n\narray([[ 1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n       [ 0.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n       [ 0.,  0.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n       [ 0.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n       [ 0.,  0.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.]])\n\n\n\nAPI\n\n\none_hot(y, num_labels='auto', dtype='float')\n\n\nOne-hot encoding of class labels\n\n\nParameters\n\n\n\n\n\n\ny\n : array-like, shape = [n_classlabels]\n\n\nPython list or numpy array consisting of class labels.\n\n\n\n\n\n\nnum_labels\n : int or 'auto'\n\n\nNumber of unique labels in the class label array. Infers the number\nof unique labels from the input array if set to 'auto'.\n\n\n\n\n\n\ndtype\n : str\n\n\nNumPy array type (float, float32, float64) of the output array.\n\n\n\n\n\n\nReturns\n\n\n\n\n\n\nonehot\n : numpy.ndarray, shape = [n_classlabels]\n\n\nOne-hot encoded array, where each sample is represented as\na row vector in the returned array.",
            "title": "One hot encoding"
        },
        {
            "location": "/user_guide/preprocessing/one-hot_encoding/#one-hot-encoding",
            "text": "A function that performs one-hot encoding for class labels.   from mlxtend.preprocessing import one_hot",
            "title": "One-Hot Encoding"
        },
        {
            "location": "/user_guide/preprocessing/one-hot_encoding/#overview",
            "text": "Typical supervised machine learning algorithms for classifications assume that the class labels are  nominal  (a special case of  categorical  where no order is implied). A typical example of an nominal feature would be \"color\" since we can't say (in most applications) that \"orange > blue > red\".  The  one_hot  function provides a simple interface to convert class label integers into a so-called one-hot array, where each unique label is represented as a column in the new array.  For example, let's assume we have 5 data points from 3 different classes: 0, 1, and 2.  y = [0, # sample 1, class 0 \n     1, # sample 2, class 1 \n     0, # sample 3, class 0\n     2, # sample 4, class 2\n     2] # sample 5, class 2  After one-hot encoding, we then obtain the following array (note that the index position of the \"1\" in each row denotes the class label of this sample):  y = [[1,  0,  0], # sample 1, class 0 \n     [0,  1,  0], # sample 2, class 1  \n     [1,  0,  0], # sample 3, class 0\n     [0,  0,  1], # sample 4, class 2\n     [0,  0,  1]  # sample 5, class 2\n     ])",
            "title": "Overview"
        },
        {
            "location": "/user_guide/preprocessing/one-hot_encoding/#examples",
            "text": "",
            "title": "Examples"
        },
        {
            "location": "/user_guide/preprocessing/one-hot_encoding/#example-1-defaults",
            "text": "from mlxtend.preprocessing import one_hot\nimport numpy as np\n\ny = np.array([0, 1, 2, 1, 2])\none_hot(y)  array([[ 1.,  0.,  0.],\n       [ 0.,  1.,  0.],\n       [ 0.,  0.,  1.],\n       [ 0.,  1.,  0.],\n       [ 0.,  0.,  1.]])",
            "title": "Example 1 - Defaults"
        },
        {
            "location": "/user_guide/preprocessing/one-hot_encoding/#example-2-python-lists",
            "text": "from mlxtend.preprocessing import one_hot\n\ny = [0, 1, 2, 1, 2]\none_hot(y)  array([[ 1.,  0.,  0.],\n       [ 0.,  1.,  0.],\n       [ 0.,  0.,  1.],\n       [ 0.,  1.,  0.],\n       [ 0.,  0.,  1.]])",
            "title": "Example 2 - Python Lists"
        },
        {
            "location": "/user_guide/preprocessing/one-hot_encoding/#example-3-integer-arrays",
            "text": "from mlxtend.preprocessing import one_hot\n\ny = [0, 1, 2, 1, 2]\none_hot(y, dtype='int')  array([[1, 0, 0],\n       [0, 1, 0],\n       [0, 0, 1],\n       [0, 1, 0],\n       [0, 0, 1]])",
            "title": "Example 3 - Integer Arrays"
        },
        {
            "location": "/user_guide/preprocessing/one-hot_encoding/#example-4-arbitrary-numbers-of-class-labels",
            "text": "from mlxtend.preprocessing import one_hot\n\ny = [0, 1, 2, 1, 2]\none_hot(y, num_labels=10)  array([[ 1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n       [ 0.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n       [ 0.,  0.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n       [ 0.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n       [ 0.,  0.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.]])",
            "title": "Example 4 - Arbitrary Numbers of Class Labels"
        },
        {
            "location": "/user_guide/preprocessing/one-hot_encoding/#api",
            "text": "one_hot(y, num_labels='auto', dtype='float')  One-hot encoding of class labels  Parameters    y  : array-like, shape = [n_classlabels]  Python list or numpy array consisting of class labels.    num_labels  : int or 'auto'  Number of unique labels in the class label array. Infers the number\nof unique labels from the input array if set to 'auto'.    dtype  : str  NumPy array type (float, float32, float64) of the output array.    Returns    onehot  : numpy.ndarray, shape = [n_classlabels]  One-hot encoded array, where each sample is represented as\na row vector in the returned array.",
            "title": "API"
        },
        {
            "location": "/user_guide/preprocessing/DenseTransformer/",
            "text": "DenseTransformer\n\n\nA simple transformer that converts a sparse into a dense numpy array, e.g., required for scikit-learn's \nPipeline\n when, for example, \nCountVectorizers\n are used in combination with estimators that are not compatible with sparse matrices.\n\n\n\n\nfrom mlxtend.preprocessing import DenseTransformer\n\n\n\n\nExamples\n\n\nExample 1\n\n\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.grid_search import GridSearchCV\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom mlxtend.preprocessing import DenseTransformer\nimport re\nimport numpy as np\n\nX_train = np.array(['abc def ghi', 'this is a test',\n                    'this is a test', 'this is a test'])\ny_train = np.array([0, 0, 1, 1])\n\npipe_1 = Pipeline([\n    ('vect', CountVectorizer()),\n    ('to_dense', DenseTransformer()),\n    ('clf', RandomForestClassifier())\n])\n\nparameters_1 = dict(\n    clf__n_estimators=[50, 100, 200],\n    clf__max_features=['sqrt', 'log2', None],)\n\ngrid_search_1 = GridSearchCV(pipe_1, \n                             parameters_1, \n                             n_jobs=1, \n                             verbose=1,\n                             scoring='accuracy',\n                             cv=2)\n\n\nprint(\"Performing grid search...\")\nprint(\"pipeline:\", [name for name, _ in pipe_1.steps])\nprint(\"parameters:\")\ngrid_search_1.fit(X_train, y_train)\nprint(\"Best score: %0.3f\" % grid_search_1.best_score_)\nprint(\"Best parameters set:\")\nbest_parameters_1 = grid_search_1.best_estimator_.get_params()\nfor param_name in sorted(parameters_1.keys()):\n    print(\"\\t%s: %r\" % (param_name, best_parameters_1[param_name]))\n\n\n\n\nPerforming grid search...\npipeline: ['vect', 'to_dense', 'clf']\nparameters:\nFitting 2 folds for each of 9 candidates, totalling 18 fits\nBest score: 0.500\nBest parameters set:\n    clf__max_features: 'sqrt'\n    clf__n_estimators: 50\n\n\n[Parallel(n_jobs=1)]: Done  18 out of  18 | elapsed:    2.4s finished\n\n\n\nAPI\n\n\nDenseTransformer(some_param=True)\n\n\nConvert a sparse matrix into a dense matrix.\n\n\nMethods\n\n\n\n\n\nfit(X, y=None)\n\n\nNone\n\n\n\n\n\nfit_transform(X, y=None)\n\n\nNone\n\n\n\n\n\nget_params(deep=True)\n\n\nNone\n\n\n\n\n\ntransform(X, y=None)\n\n\nNone",
            "title": "DenseTransformer"
        },
        {
            "location": "/user_guide/preprocessing/DenseTransformer/#densetransformer",
            "text": "A simple transformer that converts a sparse into a dense numpy array, e.g., required for scikit-learn's  Pipeline  when, for example,  CountVectorizers  are used in combination with estimators that are not compatible with sparse matrices.   from mlxtend.preprocessing import DenseTransformer",
            "title": "DenseTransformer"
        },
        {
            "location": "/user_guide/preprocessing/DenseTransformer/#examples",
            "text": "",
            "title": "Examples"
        },
        {
            "location": "/user_guide/preprocessing/DenseTransformer/#example-1",
            "text": "from sklearn.pipeline import Pipeline\nfrom sklearn.grid_search import GridSearchCV\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom mlxtend.preprocessing import DenseTransformer\nimport re\nimport numpy as np\n\nX_train = np.array(['abc def ghi', 'this is a test',\n                    'this is a test', 'this is a test'])\ny_train = np.array([0, 0, 1, 1])\n\npipe_1 = Pipeline([\n    ('vect', CountVectorizer()),\n    ('to_dense', DenseTransformer()),\n    ('clf', RandomForestClassifier())\n])\n\nparameters_1 = dict(\n    clf__n_estimators=[50, 100, 200],\n    clf__max_features=['sqrt', 'log2', None],)\n\ngrid_search_1 = GridSearchCV(pipe_1, \n                             parameters_1, \n                             n_jobs=1, \n                             verbose=1,\n                             scoring='accuracy',\n                             cv=2)\n\n\nprint(\"Performing grid search...\")\nprint(\"pipeline:\", [name for name, _ in pipe_1.steps])\nprint(\"parameters:\")\ngrid_search_1.fit(X_train, y_train)\nprint(\"Best score: %0.3f\" % grid_search_1.best_score_)\nprint(\"Best parameters set:\")\nbest_parameters_1 = grid_search_1.best_estimator_.get_params()\nfor param_name in sorted(parameters_1.keys()):\n    print(\"\\t%s: %r\" % (param_name, best_parameters_1[param_name]))  Performing grid search...\npipeline: ['vect', 'to_dense', 'clf']\nparameters:\nFitting 2 folds for each of 9 candidates, totalling 18 fits\nBest score: 0.500\nBest parameters set:\n    clf__max_features: 'sqrt'\n    clf__n_estimators: 50\n\n\n[Parallel(n_jobs=1)]: Done  18 out of  18 | elapsed:    2.4s finished",
            "title": "Example 1"
        },
        {
            "location": "/user_guide/preprocessing/DenseTransformer/#api",
            "text": "DenseTransformer(some_param=True)  Convert a sparse matrix into a dense matrix.",
            "title": "API"
        },
        {
            "location": "/user_guide/preprocessing/DenseTransformer/#methods",
            "text": "fit(X, y=None)  None   fit_transform(X, y=None)  None   get_params(deep=True)  None   transform(X, y=None)  None",
            "title": "Methods"
        },
        {
            "location": "/user_guide/preprocessing/CopyTransformer/",
            "text": "CopyTransformer\n\n\nA simple transformer that returns a copy of the input array, for example, as part of a scikit-learn pipeline.\n\n\n\n\nfrom mlxtend.preprocessing import CopyTransformer\n\n\n\n\nExamples\n\n\nExample 1\n\n\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.grid_search import GridSearchCV\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom mlxtend.preprocessing import CopyTransformer\nimport re\nimport numpy as np\n\nX_train = np.array(['abc def ghi', 'this is a test',\n                    'this is a test', 'this is a test'])\ny_train = np.array([0, 0, 1, 1])\n\npipe_1 = Pipeline([\n    ('vect', CountVectorizer()),\n    ('to_dense', CopyTransformer()),\n    ('clf', RandomForestClassifier())\n])\n\nparameters_1 = dict(\n    clf__n_estimators=[50, 100, 200],\n    clf__max_features=['sqrt', 'log2', None],)\n\ngrid_search_1 = GridSearchCV(pipe_1, \n                             parameters_1, \n                             n_jobs=1, \n                             verbose=1,\n                             scoring='accuracy',\n                             cv=2)\n\n\nprint(\"Performing grid search...\")\nprint(\"pipeline:\", [name for name, _ in pipe_1.steps])\nprint(\"parameters:\")\ngrid_search_1.fit(X_train, y_train)\nprint(\"Best score: %0.3f\" % grid_search_1.best_score_)\nprint(\"Best parameters set:\")\nbest_parameters_1 = grid_search_1.best_estimator_.get_params()\nfor param_name in sorted(parameters_1.keys()):\n    print(\"\\t%s: %r\" % (param_name, best_parameters_1[param_name]))\n\n\n\n\nPerforming grid search...\npipeline: ['vect', 'to_dense', 'clf']\nparameters:\nFitting 2 folds for each of 9 candidates, totalling 18 fits\nBest score: 0.500\nBest parameters set:\n    clf__max_features: 'sqrt'\n    clf__n_estimators: 50\n\n\n[Parallel(n_jobs=1)]: Done  18 out of  18 | elapsed:    2.1s finished\n\n\n\nAPI\n\n\nCopyTransformer()\n\n\nTransformer that returns a copy of the input array\n\n\nMethods\n\n\n\n\n\nfit(X, y=None)\n\n\nMock method. Does nothing.\n\n\nParameters\n\n\n\n\n\n\nX\n : {array-like, sparse matrix}, shape = [n_samples, n_features]\n\n\nTraining vectors, where n_samples is the number of samples and\nn_features is the number of features.\n\n\n\n\n\n\ny\n : array-like, shape = [n_samples] (default: None)\n\n\n\n\n\n\nReturns\n\n\nself\n\n\n\n\n\nfit_transform(X, y=None)\n\n\nReturn a copy of the input array.\n\n\nParameters\n\n\n\n\n\n\nX\n : {array-like, sparse matrix}, shape = [n_samples, n_features]\n\n\nTraining vectors, where n_samples is the number of samples and\nn_features is the number of features.\n\n\n\n\n\n\ny\n : array-like, shape = [n_samples] (default: None)\n\n\n\n\n\n\nReturns\n\n\n\n\nX_copy\n : copy of the input X array.\n\n\n\n\n\n\n\nget_params(deep=True)\n\n\nGet parameters for this estimator.\n\n\nParameters\n\n\ndeep: boolean, optional\n    If True, will return the parameters for this estimator and\n    contained subobjects that are estimators.\n\n\nReturns\n\n\n\n\n\n\nparams\n : mapping of string to any\n\n\nParameter names mapped to their values.\n\n\n\n\n\n\n\n\n\nset_params(\nparams)\n\n\nSet the parameters of this estimator.\n\n\nThe method works on simple estimators as well as on nested objects\n(such as pipelines). The former have parameters of the form\n``<component>__<parameter>`` so that it's possible to update each\ncomponent of a nested object.\n\n\n\nReturns\n\n\nself\n\n\n\n\n\ntransform(X, y=None)\n\n\nReturn a copy of the input array.\n\n\nParameters\n\n\n\n\n\n\nX\n : {array-like, sparse matrix}, shape = [n_samples, n_features]\n\n\nTraining vectors, where n_samples is the number of samples and\nn_features is the number of features.\n\n\n\n\n\n\ny\n : array-like, shape = [n_samples] (default: None)\n\n\n\n\n\n\nReturns\n\n\n\n\nX_copy\n : copy of the input X array.",
            "title": "CopyTransformer"
        },
        {
            "location": "/user_guide/preprocessing/CopyTransformer/#copytransformer",
            "text": "A simple transformer that returns a copy of the input array, for example, as part of a scikit-learn pipeline.   from mlxtend.preprocessing import CopyTransformer",
            "title": "CopyTransformer"
        },
        {
            "location": "/user_guide/preprocessing/CopyTransformer/#examples",
            "text": "",
            "title": "Examples"
        },
        {
            "location": "/user_guide/preprocessing/CopyTransformer/#example-1",
            "text": "from sklearn.pipeline import Pipeline\nfrom sklearn.grid_search import GridSearchCV\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom mlxtend.preprocessing import CopyTransformer\nimport re\nimport numpy as np\n\nX_train = np.array(['abc def ghi', 'this is a test',\n                    'this is a test', 'this is a test'])\ny_train = np.array([0, 0, 1, 1])\n\npipe_1 = Pipeline([\n    ('vect', CountVectorizer()),\n    ('to_dense', CopyTransformer()),\n    ('clf', RandomForestClassifier())\n])\n\nparameters_1 = dict(\n    clf__n_estimators=[50, 100, 200],\n    clf__max_features=['sqrt', 'log2', None],)\n\ngrid_search_1 = GridSearchCV(pipe_1, \n                             parameters_1, \n                             n_jobs=1, \n                             verbose=1,\n                             scoring='accuracy',\n                             cv=2)\n\n\nprint(\"Performing grid search...\")\nprint(\"pipeline:\", [name for name, _ in pipe_1.steps])\nprint(\"parameters:\")\ngrid_search_1.fit(X_train, y_train)\nprint(\"Best score: %0.3f\" % grid_search_1.best_score_)\nprint(\"Best parameters set:\")\nbest_parameters_1 = grid_search_1.best_estimator_.get_params()\nfor param_name in sorted(parameters_1.keys()):\n    print(\"\\t%s: %r\" % (param_name, best_parameters_1[param_name]))  Performing grid search...\npipeline: ['vect', 'to_dense', 'clf']\nparameters:\nFitting 2 folds for each of 9 candidates, totalling 18 fits\nBest score: 0.500\nBest parameters set:\n    clf__max_features: 'sqrt'\n    clf__n_estimators: 50\n\n\n[Parallel(n_jobs=1)]: Done  18 out of  18 | elapsed:    2.1s finished",
            "title": "Example 1"
        },
        {
            "location": "/user_guide/preprocessing/CopyTransformer/#api",
            "text": "CopyTransformer()  Transformer that returns a copy of the input array",
            "title": "API"
        },
        {
            "location": "/user_guide/preprocessing/CopyTransformer/#methods",
            "text": "fit(X, y=None)  Mock method. Does nothing.  Parameters    X  : {array-like, sparse matrix}, shape = [n_samples, n_features]  Training vectors, where n_samples is the number of samples and\nn_features is the number of features.    y  : array-like, shape = [n_samples] (default: None)    Returns  self   fit_transform(X, y=None)  Return a copy of the input array.  Parameters    X  : {array-like, sparse matrix}, shape = [n_samples, n_features]  Training vectors, where n_samples is the number of samples and\nn_features is the number of features.    y  : array-like, shape = [n_samples] (default: None)    Returns   X_copy  : copy of the input X array.    get_params(deep=True)  Get parameters for this estimator.  Parameters  deep: boolean, optional\n    If True, will return the parameters for this estimator and\n    contained subobjects that are estimators.  Returns    params  : mapping of string to any  Parameter names mapped to their values.     set_params( params)  Set the parameters of this estimator.  The method works on simple estimators as well as on nested objects\n(such as pipelines). The former have parameters of the form\n``<component>__<parameter>`` so that it's possible to update each\ncomponent of a nested object.  Returns  self   transform(X, y=None)  Return a copy of the input array.  Parameters    X  : {array-like, sparse matrix}, shape = [n_samples, n_features]  Training vectors, where n_samples is the number of samples and\nn_features is the number of features.    y  : array-like, shape = [n_samples] (default: None)    Returns   X_copy  : copy of the input X array.",
            "title": "Methods"
        },
        {
            "location": "/user_guide/data/autompg_data/",
            "text": "Auto MPG\n\n\nA function that loads the \nautompg\n dataset into NumPy arrays.\n\n\n\n\nfrom mlxtend.data import autompg_data\n\n\n\n\nOverview\n\n\nThe Auto-MPG dataset for regression analysis. The target (\ny\n) is defined as the miles per gallon (mpg) for 392 automobiles (6 rows containing \"NaN\"s have been removed. The 8 feature columns are:\n\n\nFeatures\n\n\n\n\ncylinders: multi-valued discrete \n\n\ndisplacement: continuous \n\n\nhorsepower: continuous \n\n\nweight: continuous \n\n\nacceleration: continuous \n\n\nmodel year: multi-valued discrete \n\n\norigin: multi-valued discrete \n\n\n\n\ncar name: string (unique for each instance)\n\n\n\n\n\n\nNumber of samples: 392\n\n\n\n\n\n\nTarget variable (continuous): mpg\n\n\n\n\n\n\nReferences\n\n\n\n\nSource: \nhttps://archive.ics.uci.edu/ml/datasets/Auto+MPG\n\n\nQuinlan,R. (1993). Combining Instance-Based and Model-Based Learning. In Proceedings on the Tenth International Conference of Machine Learning, 236-243, University of Massachusetts, Amherst. Morgan Kaufmann.\n\n\n\n\nExamples\n\n\nExample - Dataset overview\n\n\nfrom mlxtend.data import autompg_data\nX, y = autompg_data()\n\nprint('Dimensions: %s x %s' % (X.shape[0], X.shape[1]))\nprint('\\nHeader: %s' % ['cylinders', 'displacement', \n                           'horsepower weight', 'acceleration',\n                           'model year', 'origin', 'car name'])\nprint('1st row', X[0])\n\n\n\n\nDimensions: 392 x 8\n\nHeader: ['cylinders', 'displacement', 'horsepower weight', 'acceleration', 'model year', 'origin', 'car name']\n1st row ['8' '307.0' '130.0' '3504.0' '12.0' '70' '1' 'chevrolet chevelle malibu']\n\n\n\nNote that the feature array contains a \nstr\n column (\"car name\"), thus it is recommended to pick the features as needed and convert it into a \nfloat\n array for further analysis. The example below shows how to get rid of the \ncar name\n column and cast the NumPy array as a \nfloat\n array.\n\n\nX[:, :-1].astype(float)\n\n\n\n\narray([[   8. ,  307. ,  130. , ...,   12. ,   70. ,    1. ],\n       [   8. ,  350. ,  165. , ...,   11.5,   70. ,    1. ],\n       [   8. ,  318. ,  150. , ...,   11. ,   70. ,    1. ],\n       ..., \n       [   4. ,  135. ,   84. , ...,   11.6,   82. ,    1. ],\n       [   4. ,  120. ,   79. , ...,   18.6,   82. ,    1. ],\n       [   4. ,  119. ,   82. , ...,   19.4,   82. ,    1. ]])\n\n\n\nAPI\n\n\nautompg_data()\n\n\nAuto MPG dataset.\n\n\n\n\n\n\nSource\n : https://archive.ics.uci.edu/ml/datasets/Auto+MPG\n\n\n\n\n\n\nNumber of samples\n : 392\n\n\n\n\n\n\nContinuous target variable\n : mpg\n\n\nDataset Attributes:\n\n\n\n\n1) cylinders:  multi-valued discrete\n\n\n2) displacement: continuous\n\n\n3) horsepower: continuous\n\n\n4) weight: continuous\n\n\n5) acceleration: continuous\n\n\n6) model year: multi-valued discrete\n\n\n7) origin: multi-valued discrete\n\n\n8) car name: string (unique for each instance)\n\n\n\n\n\n\n\n\nReturns\n\n\n\n\n\n\nX, y\n : [n_samples, n_features], [n_targets]\n\n\nX is the feature matrix with 392 auto samples as rows\nand 8 feature columns (6 rows with NaNs removed).\ny is a 1-dimensional array of the target MPG values.",
            "title": "Autompg data"
        },
        {
            "location": "/user_guide/data/autompg_data/#auto-mpg",
            "text": "A function that loads the  autompg  dataset into NumPy arrays.   from mlxtend.data import autompg_data",
            "title": "Auto MPG"
        },
        {
            "location": "/user_guide/data/autompg_data/#overview",
            "text": "The Auto-MPG dataset for regression analysis. The target ( y ) is defined as the miles per gallon (mpg) for 392 automobiles (6 rows containing \"NaN\"s have been removed. The 8 feature columns are:  Features   cylinders: multi-valued discrete   displacement: continuous   horsepower: continuous   weight: continuous   acceleration: continuous   model year: multi-valued discrete   origin: multi-valued discrete    car name: string (unique for each instance)    Number of samples: 392    Target variable (continuous): mpg",
            "title": "Overview"
        },
        {
            "location": "/user_guide/data/autompg_data/#references",
            "text": "Source:  https://archive.ics.uci.edu/ml/datasets/Auto+MPG  Quinlan,R. (1993). Combining Instance-Based and Model-Based Learning. In Proceedings on the Tenth International Conference of Machine Learning, 236-243, University of Massachusetts, Amherst. Morgan Kaufmann.",
            "title": "References"
        },
        {
            "location": "/user_guide/data/autompg_data/#examples",
            "text": "",
            "title": "Examples"
        },
        {
            "location": "/user_guide/data/autompg_data/#example-dataset-overview",
            "text": "from mlxtend.data import autompg_data\nX, y = autompg_data()\n\nprint('Dimensions: %s x %s' % (X.shape[0], X.shape[1]))\nprint('\\nHeader: %s' % ['cylinders', 'displacement', \n                           'horsepower weight', 'acceleration',\n                           'model year', 'origin', 'car name'])\nprint('1st row', X[0])  Dimensions: 392 x 8\n\nHeader: ['cylinders', 'displacement', 'horsepower weight', 'acceleration', 'model year', 'origin', 'car name']\n1st row ['8' '307.0' '130.0' '3504.0' '12.0' '70' '1' 'chevrolet chevelle malibu']  Note that the feature array contains a  str  column (\"car name\"), thus it is recommended to pick the features as needed and convert it into a  float  array for further analysis. The example below shows how to get rid of the  car name  column and cast the NumPy array as a  float  array.  X[:, :-1].astype(float)  array([[   8. ,  307. ,  130. , ...,   12. ,   70. ,    1. ],\n       [   8. ,  350. ,  165. , ...,   11.5,   70. ,    1. ],\n       [   8. ,  318. ,  150. , ...,   11. ,   70. ,    1. ],\n       ..., \n       [   4. ,  135. ,   84. , ...,   11.6,   82. ,    1. ],\n       [   4. ,  120. ,   79. , ...,   18.6,   82. ,    1. ],\n       [   4. ,  119. ,   82. , ...,   19.4,   82. ,    1. ]])",
            "title": "Example - Dataset overview"
        },
        {
            "location": "/user_guide/data/autompg_data/#api",
            "text": "autompg_data()  Auto MPG dataset.    Source  : https://archive.ics.uci.edu/ml/datasets/Auto+MPG    Number of samples  : 392    Continuous target variable  : mpg  Dataset Attributes:   1) cylinders:  multi-valued discrete  2) displacement: continuous  3) horsepower: continuous  4) weight: continuous  5) acceleration: continuous  6) model year: multi-valued discrete  7) origin: multi-valued discrete  8) car name: string (unique for each instance)     Returns    X, y  : [n_samples, n_features], [n_targets]  X is the feature matrix with 392 auto samples as rows\nand 8 feature columns (6 rows with NaNs removed).\ny is a 1-dimensional array of the target MPG values.",
            "title": "API"
        },
        {
            "location": "/user_guide/data/boston_housing_data/",
            "text": "Boston Housing Data\n\n\nA function that loads the \nboston_housing_data\n dataset into NumPy arrays.\n\n\n\n\nfrom mlxtend.data import boston_housing_data\n\n\n\n\nOverview\n\n\nThe Boston Housing dataset for regression analysis.\n\n\nFeatures\n\n\n\n\nCRIM:      per capita crime rate by town\n\n\nZN:        proportion of residential land zoned for lots over 25,000 sq.ft.\n\n\nINDUS:     proportion of non-retail business acres per town\n\n\nCHAS:      Charles River dummy variable (= 1 if tract bounds river; 0 otherwise)\n\n\nNOX:       nitric oxides concentration (parts per 10 million)\n\n\nRM:        average number of rooms per dwelling\n\n\nAGE:       proportion of owner-occupied units built prior to 1940\n\n\nDIS:       weighted distances to five Boston employment centres\n\n\nRAD:       index of accessibility to radial highways\n\n\nTAX:      full-value property-tax rate per $10,000\n\n\nPTRATIO:  pupil-teacher ratio by town\n\n\nB:        1000(Bk - 0.63)^2 where Bk is the proportion of b. by town\n\n\n\n\nLSTAT:    % lower status of the population\n\n\n\n\n\n\nNumber of samples: 506\n\n\n\n\n\n\nTarget variable (continuous): MEDV, Median value of owner-occupied homes in $1000's\n\n\n\n\n\n\nReferences\n\n\n\n\nSource: \nhttps://archive.ics.uci.edu/ml/datasets/Wine\n\n\nHarrison, D. and Rubinfeld, D.L. \n'Hedonic prices and the demand for clean air', J. Environ. Economics & Management, vol.5, 81-102, 1978.\n\n\n\n\nExamples\n\n\nExample - Dataset overview\n\n\nfrom mlxtend.data import boston_housing_data\nX, y = boston_housing_data()\n\nprint('Dimensions: %s x %s' % (X.shape[0], X.shape[1]))\nprint('1st row', X[0])\n\n\n\n\nDimensions: 506 x 13\n1st row [  6.32000000e-03   1.80000000e+01   2.31000000e+00   0.00000000e+00\n   5.38000000e-01   6.57500000e+00   6.52000000e+01   4.09000000e+00\n   1.00000000e+00   2.96000000e+02   1.53000000e+01   3.96900000e+02\n   4.98000000e+00]\n\n\n\nAPI\n\n\nboston_housing_data()\n\n\nBoston Housing dataset.\n\n\n\n\n\n\nSource\n : https://archive.ics.uci.edu/ml/datasets/Housing\n\n\n\n\n\n\nNumber of samples\n : 506\n\n\n\n\n\n\nContinuous target variable\n : MEDV\n\n\nMEDV = Median value of owner-occupied homes in $1000's\n\n\nDataset Attributes:\n\n\n\n\n1) CRIM      per capita crime rate by town\n\n\n2) ZN        proportion of residential land zoned for lots over\n25,000 sq.ft.\n\n\n3) INDUS     proportion of non-retail business acres per town\n\n\n4) CHAS      Charles River dummy variable (= 1 if tract bounds\nriver; 0 otherwise)\n\n\n5) NOX       nitric oxides concentration (parts per 10 million)\n\n\n6) RM        average number of rooms per dwelling\n\n\n7) AGE       proportion of owner-occupied units built prior to 1940\n\n\n8) DIS       weighted distances to five Boston employment centres\n\n\n9) RAD       index of accessibility to radial highways\n\n\n10) TAX      full-value property-tax rate per $10,000\n\n\n11) PTRATIO  pupil-teacher ratio by town\n\n\n12) B        1000(Bk - 0.63)^2 where Bk is the proportion of b. by town\n\n\n13) LSTAT    % lower status of the population\n\n\n\n\n\n\n\n\nReturns\n\n\n\n\n\n\nX, y\n : [n_samples, n_features], [n_class_labels]\n\n\nX is the feature matrix with 506 housing samples as rows\nand 13 feature columns.\ny is a 1-dimensional array of the continuous target variable MEDV",
            "title": "Boston housing data"
        },
        {
            "location": "/user_guide/data/boston_housing_data/#boston-housing-data",
            "text": "A function that loads the  boston_housing_data  dataset into NumPy arrays.   from mlxtend.data import boston_housing_data",
            "title": "Boston Housing Data"
        },
        {
            "location": "/user_guide/data/boston_housing_data/#overview",
            "text": "The Boston Housing dataset for regression analysis.  Features   CRIM:      per capita crime rate by town  ZN:        proportion of residential land zoned for lots over 25,000 sq.ft.  INDUS:     proportion of non-retail business acres per town  CHAS:      Charles River dummy variable (= 1 if tract bounds river; 0 otherwise)  NOX:       nitric oxides concentration (parts per 10 million)  RM:        average number of rooms per dwelling  AGE:       proportion of owner-occupied units built prior to 1940  DIS:       weighted distances to five Boston employment centres  RAD:       index of accessibility to radial highways  TAX:      full-value property-tax rate per $10,000  PTRATIO:  pupil-teacher ratio by town  B:        1000(Bk - 0.63)^2 where Bk is the proportion of b. by town   LSTAT:    % lower status of the population    Number of samples: 506    Target variable (continuous): MEDV, Median value of owner-occupied homes in $1000's",
            "title": "Overview"
        },
        {
            "location": "/user_guide/data/boston_housing_data/#references",
            "text": "Source:  https://archive.ics.uci.edu/ml/datasets/Wine  Harrison, D. and Rubinfeld, D.L. \n'Hedonic prices and the demand for clean air', J. Environ. Economics & Management, vol.5, 81-102, 1978.",
            "title": "References"
        },
        {
            "location": "/user_guide/data/boston_housing_data/#examples",
            "text": "",
            "title": "Examples"
        },
        {
            "location": "/user_guide/data/boston_housing_data/#example-dataset-overview",
            "text": "from mlxtend.data import boston_housing_data\nX, y = boston_housing_data()\n\nprint('Dimensions: %s x %s' % (X.shape[0], X.shape[1]))\nprint('1st row', X[0])  Dimensions: 506 x 13\n1st row [  6.32000000e-03   1.80000000e+01   2.31000000e+00   0.00000000e+00\n   5.38000000e-01   6.57500000e+00   6.52000000e+01   4.09000000e+00\n   1.00000000e+00   2.96000000e+02   1.53000000e+01   3.96900000e+02\n   4.98000000e+00]",
            "title": "Example - Dataset overview"
        },
        {
            "location": "/user_guide/data/boston_housing_data/#api",
            "text": "boston_housing_data()  Boston Housing dataset.    Source  : https://archive.ics.uci.edu/ml/datasets/Housing    Number of samples  : 506    Continuous target variable  : MEDV  MEDV = Median value of owner-occupied homes in $1000's  Dataset Attributes:   1) CRIM      per capita crime rate by town  2) ZN        proportion of residential land zoned for lots over\n25,000 sq.ft.  3) INDUS     proportion of non-retail business acres per town  4) CHAS      Charles River dummy variable (= 1 if tract bounds\nriver; 0 otherwise)  5) NOX       nitric oxides concentration (parts per 10 million)  6) RM        average number of rooms per dwelling  7) AGE       proportion of owner-occupied units built prior to 1940  8) DIS       weighted distances to five Boston employment centres  9) RAD       index of accessibility to radial highways  10) TAX      full-value property-tax rate per $10,000  11) PTRATIO  pupil-teacher ratio by town  12) B        1000(Bk - 0.63)^2 where Bk is the proportion of b. by town  13) LSTAT    % lower status of the population     Returns    X, y  : [n_samples, n_features], [n_class_labels]  X is the feature matrix with 506 housing samples as rows\nand 13 feature columns.\ny is a 1-dimensional array of the continuous target variable MEDV",
            "title": "API"
        },
        {
            "location": "/user_guide/data/iris_data/",
            "text": "Iris Dataset\n\n\nA function that loads the \niris\n dataset into NumPy arrays.\n\n\n\n\nfrom mlxtend.data import iris_data\n\n\n\n\nOverview\n\n\nThe Iris dataset for classification.\n\n\nFeatures\n\n\n\n\nSepal length\n\n\nSepal width\n\n\nPetal length\n\n\n\n\nPetal width\n\n\n\n\n\n\nNumber of samples: 150\n\n\n\n\n\n\nTarget variable (discrete): {50x Setosa, 50x Versicolor, 50x Virginica}\n\n\n\n\n\n\nReferences\n\n\n\n\nSource: \nhttps://archive.ics.uci.edu/ml/datasets/Iris\n \n\n\nBache, K. & Lichman, M. (2013). UCI Machine Learning Repository. Irvine, CA: University of California, School of Information and Computer Science.\n\n\n\n\nExamples\n\n\nExample - Dataset overview\n\n\nfrom mlxtend.data import iris_data\nX, y = iris_data()\n\nprint('Dimensions: %s x %s' % (X.shape[0], X.shape[1]))\nprint('\\nHeader: %s' % ['sepal length', 'sepal width',\n                        'petal length', 'petal width'])\nprint('1st row', X[0])\n\n\n\n\nDimensions: 150 x 4\n\nHeader: ['sepal length', 'sepal width', 'petal length', 'petal width']\n1st row [ 5.1  3.5  1.4  0.2]\n\n\n\nimport numpy as np\nprint('Classes: Setosa, Versicolor, Virginica')\nprint(np.unique(y))\nprint('Class distribution: %s' % np.bincount(y))\n\n\n\n\nClasses: Setosa, Versicolor, Virginica\n[0 1 2]\nClass distribution: [50 50 50]\n\n\n\nAPI\n\n\niris_data()\n\n\nIris flower dataset.\n\n\n\n\n\n\nSource\n : https://archive.ics.uci.edu/ml/datasets/Iris\n\n\n\n\n\n\nNumber of samples\n : 150\n\n\n\n\n\n\nClass labels\n : {0, 1, 2}, distribution: [50, 50, 50]\n\n\n0 = setosa, 1 = versicolor, 2 = virginica.\n\n\nDataset Attributes:\n\n\n\n\n1) sepal length [cm]\n\n\n2) sepal width [cm]\n\n\n3) petal length [cm]\n\n\n4) petal width [cm]\n\n\n\n\n\n\n\n\nReturns\n\n\n\n\n\n\nX, y\n : [n_samples, n_features], [n_class_labels]\n\n\nX is the feature matrix with 150 flower samples as rows,\nand 4 feature columns sepal length, sepal width,\npetal length, and petal width.\ny is a 1-dimensional array of the class labels {0, 1, 2}",
            "title": "Iris data"
        },
        {
            "location": "/user_guide/data/iris_data/#iris-dataset",
            "text": "A function that loads the  iris  dataset into NumPy arrays.   from mlxtend.data import iris_data",
            "title": "Iris Dataset"
        },
        {
            "location": "/user_guide/data/iris_data/#overview",
            "text": "The Iris dataset for classification.  Features   Sepal length  Sepal width  Petal length   Petal width    Number of samples: 150    Target variable (discrete): {50x Setosa, 50x Versicolor, 50x Virginica}",
            "title": "Overview"
        },
        {
            "location": "/user_guide/data/iris_data/#references",
            "text": "Source:  https://archive.ics.uci.edu/ml/datasets/Iris    Bache, K. & Lichman, M. (2013). UCI Machine Learning Repository. Irvine, CA: University of California, School of Information and Computer Science.",
            "title": "References"
        },
        {
            "location": "/user_guide/data/iris_data/#examples",
            "text": "",
            "title": "Examples"
        },
        {
            "location": "/user_guide/data/iris_data/#example-dataset-overview",
            "text": "from mlxtend.data import iris_data\nX, y = iris_data()\n\nprint('Dimensions: %s x %s' % (X.shape[0], X.shape[1]))\nprint('\\nHeader: %s' % ['sepal length', 'sepal width',\n                        'petal length', 'petal width'])\nprint('1st row', X[0])  Dimensions: 150 x 4\n\nHeader: ['sepal length', 'sepal width', 'petal length', 'petal width']\n1st row [ 5.1  3.5  1.4  0.2]  import numpy as np\nprint('Classes: Setosa, Versicolor, Virginica')\nprint(np.unique(y))\nprint('Class distribution: %s' % np.bincount(y))  Classes: Setosa, Versicolor, Virginica\n[0 1 2]\nClass distribution: [50 50 50]",
            "title": "Example - Dataset overview"
        },
        {
            "location": "/user_guide/data/iris_data/#api",
            "text": "iris_data()  Iris flower dataset.    Source  : https://archive.ics.uci.edu/ml/datasets/Iris    Number of samples  : 150    Class labels  : {0, 1, 2}, distribution: [50, 50, 50]  0 = setosa, 1 = versicolor, 2 = virginica.  Dataset Attributes:   1) sepal length [cm]  2) sepal width [cm]  3) petal length [cm]  4) petal width [cm]     Returns    X, y  : [n_samples, n_features], [n_class_labels]  X is the feature matrix with 150 flower samples as rows,\nand 4 feature columns sepal length, sepal width,\npetal length, and petal width.\ny is a 1-dimensional array of the class labels {0, 1, 2}",
            "title": "API"
        },
        {
            "location": "/user_guide/data/mnist_data/",
            "text": "MNIST Dataset\n\n\nA function that loads the \nMNIST\n dataset into NumPy arrays.\n\n\n\n\nfrom mlxtend.data import mnist_data\n\n\n\n\nOverview\n\n\nThe MNIST dataset was constructed from two datasets of the US National Institute of Standards and Technology (NIST). The training set consists of handwritten digits from 250 different people, 50 percent high school students, and 50 percent employees from the Census Bureau. Note that the test set contains handwritten digits from different people following the same split.\n\n\nFeatures\n\n\nEach feature vector (row in the feature matrix) consists of 784 pixels (intensities) -- unrolled from the original 28x28 pixels images.\n\n\n\n\n\n\nNumber of samples: A subset of 5000 images (the first 500 digits of each class)\n\n\n\n\n\n\nTarget variable (discrete): {500x 0, ..., 500x 9}\n\n\n\n\n\n\nReferences\n\n\n\n\nSource: \nhttp://yann.lecun.com/exdb/mnist/\n\n\nY. LeCun and C. Cortes. Mnist handwritten digit database. AT&T Labs [Online]. Available: \nhttp://yann.lecun.com/exdb/mnist\n, 2010.\n\n\n\n\nExamples\n\n\nExample - Dataset overview\n\n\nfrom mlxtend.data import mnist_data\nX, y = mnist_data()\n\nprint('Dimensions: %s x %s' % (X.shape[0], X.shape[1]))\nprint('1st row', X[0])\n\n\n\n\nDimensions: 5000 x 784\n1st row [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n   0  51 159 253 159  50   0   0   0   0   0   0   0   0   0   0   0   0\n   0   0   0   0   0   0   0   0   0   0  48 238 252 252 252 237   0   0\n   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n   0  54 227 253 252 239 233 252  57   6   0   0   0   0   0   0   0   0\n   0   0   0   0   0   0   0   0   0  10  60 224 252 253 252 202  84 252\n 253 122   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n   0 163 252 252 252 253 252 252  96 189 253 167   0   0   0   0   0   0\n   0   0   0   0   0   0   0   0   0   0  51 238 253 253 190 114 253 228\n  47  79 255 168   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n   0  48 238 252 252 179  12  75 121  21   0   0 253 243  50   0   0   0\n   0   0   0   0   0   0   0   0   0   0  38 165 253 233 208  84   0   0\n   0   0   0   0 253 252 165   0   0   0   0   0   0   0   0   0   0   0\n   0   7 178 252 240  71  19  28   0   0   0   0   0   0 253 252 195   0\n   0   0   0   0   0   0   0   0   0   0   0  57 252 252  63   0   0   0\n   0   0   0   0   0   0 253 252 195   0   0   0   0   0   0   0   0   0\n   0   0   0 198 253 190   0   0   0   0   0   0   0   0   0   0 255 253\n 196   0   0   0   0   0   0   0   0   0   0   0  76 246 252 112   0   0\n   0   0   0   0   0   0   0   0 253 252 148   0   0   0   0   0   0   0\n   0   0   0   0  85 252 230  25   0   0   0   0   0   0   0   0   7 135\n 253 186  12   0   0   0   0   0   0   0   0   0   0   0  85 252 223   0\n   0   0   0   0   0   0   0   7 131 252 225  71   0   0   0   0   0   0\n   0   0   0   0   0   0  85 252 145   0   0   0   0   0   0   0  48 165\n 252 173   0   0   0   0   0   0   0   0   0   0   0   0   0   0  86 253\n 225   0   0   0   0   0   0 114 238 253 162   0   0   0   0   0   0   0\n   0   0   0   0   0   0   0   0  85 252 249 146  48  29  85 178 225 253\n 223 167  56   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n  85 252 252 252 229 215 252 252 252 196 130   0   0   0   0   0   0   0\n   0   0   0   0   0   0   0   0   0   0  28 199 252 252 253 252 252 233\n 145   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n   0   0   0  25 128 252 253 252 141  37   0   0   0   0   0   0   0   0\n   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n   0   0   0   0   0   0   0   0   0   0]\n\n\n\nimport numpy as np\nprint('Classes: Setosa, Versicolor, Virginica')\nprint(np.unique(y))\nprint('Class distribution: %s' % np.bincount(y))\n\n\n\n\nClasses: Setosa, Versicolor, Virginica\n[0 1 2 3 4 5 6 7 8 9]\nClass distribution: [500 500 500 500 500 500 500 500 500 500]\n\n\n\nExample - Visualize MNIST\n\n\n%matplotlib inline\nimport matplotlib.pyplot as plt\ndef plot_digit(X, y, idx):\n    img = X[idx].reshape(28,28)\n    plt.imshow(img, cmap='Greys',  interpolation='nearest')\n    plt.title('true label: %d' % y[idx])\n    plt.show()\nplot_digit(X, y, 4)       \n\n\n\n\n\n\nAPI\n\n\nmnist_data()\n\n\n5000 samples from the MNIST handwritten digits dataset.\n\n\n\n\nData Source\n : http://yann.lecun.com/exdb/mnist/\n\n\n\n\nReturns\n\n\n\n\n\n\nX, y\n : [n_samples, n_features], [n_class_labels]\n\n\nX is the feature matrix with 5000 image samples as rows,\neach row consists of 28x28 pixels that were unrolled into\n784 pixel feature vectors.\ny contains the 10 unique class labels 0-9.",
            "title": "Mnist data"
        },
        {
            "location": "/user_guide/data/mnist_data/#mnist-dataset",
            "text": "A function that loads the  MNIST  dataset into NumPy arrays.   from mlxtend.data import mnist_data",
            "title": "MNIST Dataset"
        },
        {
            "location": "/user_guide/data/mnist_data/#overview",
            "text": "The MNIST dataset was constructed from two datasets of the US National Institute of Standards and Technology (NIST). The training set consists of handwritten digits from 250 different people, 50 percent high school students, and 50 percent employees from the Census Bureau. Note that the test set contains handwritten digits from different people following the same split.  Features  Each feature vector (row in the feature matrix) consists of 784 pixels (intensities) -- unrolled from the original 28x28 pixels images.    Number of samples: A subset of 5000 images (the first 500 digits of each class)    Target variable (discrete): {500x 0, ..., 500x 9}",
            "title": "Overview"
        },
        {
            "location": "/user_guide/data/mnist_data/#references",
            "text": "Source:  http://yann.lecun.com/exdb/mnist/  Y. LeCun and C. Cortes. Mnist handwritten digit database. AT&T Labs [Online]. Available:  http://yann.lecun.com/exdb/mnist , 2010.",
            "title": "References"
        },
        {
            "location": "/user_guide/data/mnist_data/#examples",
            "text": "",
            "title": "Examples"
        },
        {
            "location": "/user_guide/data/mnist_data/#example-dataset-overview",
            "text": "from mlxtend.data import mnist_data\nX, y = mnist_data()\n\nprint('Dimensions: %s x %s' % (X.shape[0], X.shape[1]))\nprint('1st row', X[0])  Dimensions: 5000 x 784\n1st row [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n   0  51 159 253 159  50   0   0   0   0   0   0   0   0   0   0   0   0\n   0   0   0   0   0   0   0   0   0   0  48 238 252 252 252 237   0   0\n   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n   0  54 227 253 252 239 233 252  57   6   0   0   0   0   0   0   0   0\n   0   0   0   0   0   0   0   0   0  10  60 224 252 253 252 202  84 252\n 253 122   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n   0 163 252 252 252 253 252 252  96 189 253 167   0   0   0   0   0   0\n   0   0   0   0   0   0   0   0   0   0  51 238 253 253 190 114 253 228\n  47  79 255 168   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n   0  48 238 252 252 179  12  75 121  21   0   0 253 243  50   0   0   0\n   0   0   0   0   0   0   0   0   0   0  38 165 253 233 208  84   0   0\n   0   0   0   0 253 252 165   0   0   0   0   0   0   0   0   0   0   0\n   0   7 178 252 240  71  19  28   0   0   0   0   0   0 253 252 195   0\n   0   0   0   0   0   0   0   0   0   0   0  57 252 252  63   0   0   0\n   0   0   0   0   0   0 253 252 195   0   0   0   0   0   0   0   0   0\n   0   0   0 198 253 190   0   0   0   0   0   0   0   0   0   0 255 253\n 196   0   0   0   0   0   0   0   0   0   0   0  76 246 252 112   0   0\n   0   0   0   0   0   0   0   0 253 252 148   0   0   0   0   0   0   0\n   0   0   0   0  85 252 230  25   0   0   0   0   0   0   0   0   7 135\n 253 186  12   0   0   0   0   0   0   0   0   0   0   0  85 252 223   0\n   0   0   0   0   0   0   0   7 131 252 225  71   0   0   0   0   0   0\n   0   0   0   0   0   0  85 252 145   0   0   0   0   0   0   0  48 165\n 252 173   0   0   0   0   0   0   0   0   0   0   0   0   0   0  86 253\n 225   0   0   0   0   0   0 114 238 253 162   0   0   0   0   0   0   0\n   0   0   0   0   0   0   0   0  85 252 249 146  48  29  85 178 225 253\n 223 167  56   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n  85 252 252 252 229 215 252 252 252 196 130   0   0   0   0   0   0   0\n   0   0   0   0   0   0   0   0   0   0  28 199 252 252 253 252 252 233\n 145   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n   0   0   0  25 128 252 253 252 141  37   0   0   0   0   0   0   0   0\n   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n   0   0   0   0   0   0   0   0   0   0]  import numpy as np\nprint('Classes: Setosa, Versicolor, Virginica')\nprint(np.unique(y))\nprint('Class distribution: %s' % np.bincount(y))  Classes: Setosa, Versicolor, Virginica\n[0 1 2 3 4 5 6 7 8 9]\nClass distribution: [500 500 500 500 500 500 500 500 500 500]",
            "title": "Example - Dataset overview"
        },
        {
            "location": "/user_guide/data/mnist_data/#example-visualize-mnist",
            "text": "%matplotlib inline\nimport matplotlib.pyplot as plt\ndef plot_digit(X, y, idx):\n    img = X[idx].reshape(28,28)\n    plt.imshow(img, cmap='Greys',  interpolation='nearest')\n    plt.title('true label: %d' % y[idx])\n    plt.show()\nplot_digit(X, y, 4)",
            "title": "Example - Visualize MNIST"
        },
        {
            "location": "/user_guide/data/mnist_data/#api",
            "text": "mnist_data()  5000 samples from the MNIST handwritten digits dataset.   Data Source  : http://yann.lecun.com/exdb/mnist/   Returns    X, y  : [n_samples, n_features], [n_class_labels]  X is the feature matrix with 5000 image samples as rows,\neach row consists of 28x28 pixels that were unrolled into\n784 pixel feature vectors.\ny contains the 10 unique class labels 0-9.",
            "title": "API"
        },
        {
            "location": "/user_guide/data/load_mnist/",
            "text": "Load MNIST Dataset\n\n\nA utility function that loads the \nMNIST\n dataset from byte-form into NumPy arrays.\n\n\n\n\nfrom mlxtend.data_utils import load_mnist_data\n\n\n\n\nOverview\n\n\nThe MNIST dataset was constructed from two datasets of the US National Institute of Standards and Technology (NIST). The training set consists of handwritten digits from 250 different people, 50 percent high school students, and 50 percent employees from the Census Bureau. Note that the test set contains handwritten digits from different people following the same split.\n\n\nThe MNIST dataset is publicly available at http://yann.lecun.com/exdb/mnist/ and consists of the following four parts:\n- Training set images: train-images-idx3-ubyte.gz (9.9 MB, 47 MB unzipped, and 60,000 samples)\n- Training set labels: train-labels-idx1-ubyte.gz (29 KB, 60 KB unzipped, and 60,000 labels)\n- Test set images: t10k-images-idx3-ubyte.gz (1.6 MB, 7.8 MB, unzipped and 10,000 samples)\n- Test set labels: t10k-labels-idx1-ubyte.gz (5 KB, 10 KB unzipped, and 10,000 labels)\n\n\nFeatures\n\n\nEach feature vector (row in the feature matrix) consists of 784 pixels (intensities) -- unrolled from the original 28x28 pixels images.\n\n\n\n\n\n\nNumber of samples: 50000 images\n\n\n\n\n\n\nTarget variable (discrete): {50x Setosa, 50x Versicolor, 50x Virginica}\n\n\n\n\n\n\nReferences\n\n\n\n\nSource: \nhttp://yann.lecun.com/exdb/mnist/\n\n\nY. LeCun and C. Cortes. Mnist handwritten digit database. AT&T Labs [Online]. Available: http://yann. lecun. com/exdb/mnist, 2010.\n\n\n\n\nExamples\n\n\nDownloading the MNIST dataset\n\n\n1) Download the MNIST files from Y. LeCun's website\n\n\n\n\nhttp://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n\n\nhttp://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n\n\nhttp://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n\n\nhttp://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n\n\n\n\nfor example, via\n\n\ncurl -O http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n\n\n\n2) Unzip the downloaded gzip archives\n\n\nfor example, via\n\n\ngunzip t*-ubyte.gz\n\n\n\nExample 1 - Loading MNIST into NumPy Arrays\n\n\nfrom mlxtend.data import loadlocal_mnist\n\n\n\n\nX, y = loadlocal_mnist(\n        images_path='/Users/Sebastian/Desktop/train-images-idx3-ubyte', \n        labels_path='/Users/Sebastian/Desktop/train-labels-idx1-ubyte')\n\n\n\n\n\nprint('Dimensions: %s x %s' % (X.shape[0], X.shape[1]))\nprint('\\n1st row', X[0])\n\n\n\n\nDimensions: 60000 x 784\n\n1st row [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n   0   0   0   0   0   0   0   0   3  18  18  18 126 136 175  26 166 255\n 247 127   0   0   0   0   0   0   0   0   0   0   0   0  30  36  94 154\n 170 253 253 253 253 253 225 172 253 242 195  64   0   0   0   0   0   0\n   0   0   0   0   0  49 238 253 253 253 253 253 253 253 253 251  93  82\n  82  56  39   0   0   0   0   0   0   0   0   0   0   0   0  18 219 253\n 253 253 253 253 198 182 247 241   0   0   0   0   0   0   0   0   0   0\n   0   0   0   0   0   0   0   0  80 156 107 253 253 205  11   0  43 154\n   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n   0  14   1 154 253  90   0   0   0   0   0   0   0   0   0   0   0   0\n   0   0   0   0   0   0   0   0   0   0   0   0   0 139 253 190   2   0\n   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n   0   0   0   0   0  11 190 253  70   0   0   0   0   0   0   0   0   0\n   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0  35 241\n 225 160 108   1   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n   0   0   0   0   0   0   0   0   0  81 240 253 253 119  25   0   0   0\n   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n   0   0  45 186 253 253 150  27   0   0   0   0   0   0   0   0   0   0\n   0   0   0   0   0   0   0   0   0   0   0   0   0  16  93 252 253 187\n   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n   0   0   0   0   0   0   0 249 253 249  64   0   0   0   0   0   0   0\n   0   0   0   0   0   0   0   0   0   0   0   0   0   0  46 130 183 253\n 253 207   2   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n   0   0   0   0  39 148 229 253 253 253 250 182   0   0   0   0   0   0\n   0   0   0   0   0   0   0   0   0   0   0   0  24 114 221 253 253 253\n 253 201  78   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n   0   0  23  66 213 253 253 253 253 198  81   2   0   0   0   0   0   0\n   0   0   0   0   0   0   0   0   0   0  18 171 219 253 253 253 253 195\n  80   9   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n  55 172 226 253 253 253 253 244 133  11   0   0   0   0   0   0   0   0\n   0   0   0   0   0   0   0   0   0   0 136 253 253 253 212 135 132  16\n   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n   0   0   0   0   0   0   0   0   0   0]\n\n\n\nimport numpy as np\nprint('Digits:  0 1 2 3 4 5 6 7 8 9')\nprint('labels: %s' % np.unique(y))\nprint('Class distribution: %s' % np.bincount(y))\n\n\n\n\nDigits:  0 1 2 3 4 5 6 7 8 9\nlabels: [0 1 2 3 4 5 6 7 8 9]\nClass distribution: [5923 6742 5958 6131 5842 5421 5918 6265 5851 5949]\n\n\n\nStore as CSV Files\n\n\nnp.savetxt(fname='/Users/Sebastian/Desktop/images.csv', \n           X=X, delimiter=',', fmt='%d')\nnp.savetxt(fname='/Users/Sebastian/Desktop/labels.csv', \n           X=y, delimiter=',', fmt='%d')\n\n\n\n\nAPI\n\n\nloadlocal_mnist(images_path, labels_path)\n\n\nRead MNIST from ubyte files.\n\n\nParameters\n\n\n\n\n\n\nimages_path\n : str\n\n\npath to the test or train MNIST ubyte file\n\n\n\n\n\n\nlabels_path\n : str\n\n\npath to the test or train MNIST class labels file\n\n\n\n\n\n\nReturns\n\n\n\n\n\n\nimages\n : [n_samples, n_pixels] numpy.array\n\n\nPixel values of the images.\n\n\n\n\n\n\nlabels\n : [n_samples] numpy array\n\n\nTarget class labels",
            "title": "Load mnist"
        },
        {
            "location": "/user_guide/data/load_mnist/#load-mnist-dataset",
            "text": "A utility function that loads the  MNIST  dataset from byte-form into NumPy arrays.   from mlxtend.data_utils import load_mnist_data",
            "title": "Load MNIST Dataset"
        },
        {
            "location": "/user_guide/data/load_mnist/#overview",
            "text": "The MNIST dataset was constructed from two datasets of the US National Institute of Standards and Technology (NIST). The training set consists of handwritten digits from 250 different people, 50 percent high school students, and 50 percent employees from the Census Bureau. Note that the test set contains handwritten digits from different people following the same split.  The MNIST dataset is publicly available at http://yann.lecun.com/exdb/mnist/ and consists of the following four parts:\n- Training set images: train-images-idx3-ubyte.gz (9.9 MB, 47 MB unzipped, and 60,000 samples)\n- Training set labels: train-labels-idx1-ubyte.gz (29 KB, 60 KB unzipped, and 60,000 labels)\n- Test set images: t10k-images-idx3-ubyte.gz (1.6 MB, 7.8 MB, unzipped and 10,000 samples)\n- Test set labels: t10k-labels-idx1-ubyte.gz (5 KB, 10 KB unzipped, and 10,000 labels)  Features  Each feature vector (row in the feature matrix) consists of 784 pixels (intensities) -- unrolled from the original 28x28 pixels images.    Number of samples: 50000 images    Target variable (discrete): {50x Setosa, 50x Versicolor, 50x Virginica}",
            "title": "Overview"
        },
        {
            "location": "/user_guide/data/load_mnist/#references",
            "text": "Source:  http://yann.lecun.com/exdb/mnist/  Y. LeCun and C. Cortes. Mnist handwritten digit database. AT&T Labs [Online]. Available: http://yann. lecun. com/exdb/mnist, 2010.",
            "title": "References"
        },
        {
            "location": "/user_guide/data/load_mnist/#examples",
            "text": "",
            "title": "Examples"
        },
        {
            "location": "/user_guide/data/load_mnist/#downloading-the-mnist-dataset",
            "text": "1) Download the MNIST files from Y. LeCun's website   http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz  http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz  http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz  http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz   for example, via  curl -O http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz  2) Unzip the downloaded gzip archives  for example, via  gunzip t*-ubyte.gz",
            "title": "Downloading the MNIST dataset"
        },
        {
            "location": "/user_guide/data/load_mnist/#example-1-loading-mnist-into-numpy-arrays",
            "text": "from mlxtend.data import loadlocal_mnist  X, y = loadlocal_mnist(\n        images_path='/Users/Sebastian/Desktop/train-images-idx3-ubyte', \n        labels_path='/Users/Sebastian/Desktop/train-labels-idx1-ubyte')  print('Dimensions: %s x %s' % (X.shape[0], X.shape[1]))\nprint('\\n1st row', X[0])  Dimensions: 60000 x 784\n\n1st row [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n   0   0   0   0   0   0   0   0   3  18  18  18 126 136 175  26 166 255\n 247 127   0   0   0   0   0   0   0   0   0   0   0   0  30  36  94 154\n 170 253 253 253 253 253 225 172 253 242 195  64   0   0   0   0   0   0\n   0   0   0   0   0  49 238 253 253 253 253 253 253 253 253 251  93  82\n  82  56  39   0   0   0   0   0   0   0   0   0   0   0   0  18 219 253\n 253 253 253 253 198 182 247 241   0   0   0   0   0   0   0   0   0   0\n   0   0   0   0   0   0   0   0  80 156 107 253 253 205  11   0  43 154\n   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n   0  14   1 154 253  90   0   0   0   0   0   0   0   0   0   0   0   0\n   0   0   0   0   0   0   0   0   0   0   0   0   0 139 253 190   2   0\n   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n   0   0   0   0   0  11 190 253  70   0   0   0   0   0   0   0   0   0\n   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0  35 241\n 225 160 108   1   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n   0   0   0   0   0   0   0   0   0  81 240 253 253 119  25   0   0   0\n   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n   0   0  45 186 253 253 150  27   0   0   0   0   0   0   0   0   0   0\n   0   0   0   0   0   0   0   0   0   0   0   0   0  16  93 252 253 187\n   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n   0   0   0   0   0   0   0 249 253 249  64   0   0   0   0   0   0   0\n   0   0   0   0   0   0   0   0   0   0   0   0   0   0  46 130 183 253\n 253 207   2   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n   0   0   0   0  39 148 229 253 253 253 250 182   0   0   0   0   0   0\n   0   0   0   0   0   0   0   0   0   0   0   0  24 114 221 253 253 253\n 253 201  78   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n   0   0  23  66 213 253 253 253 253 198  81   2   0   0   0   0   0   0\n   0   0   0   0   0   0   0   0   0   0  18 171 219 253 253 253 253 195\n  80   9   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n  55 172 226 253 253 253 253 244 133  11   0   0   0   0   0   0   0   0\n   0   0   0   0   0   0   0   0   0   0 136 253 253 253 212 135 132  16\n   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n   0   0   0   0   0   0   0   0   0   0]  import numpy as np\nprint('Digits:  0 1 2 3 4 5 6 7 8 9')\nprint('labels: %s' % np.unique(y))\nprint('Class distribution: %s' % np.bincount(y))  Digits:  0 1 2 3 4 5 6 7 8 9\nlabels: [0 1 2 3 4 5 6 7 8 9]\nClass distribution: [5923 6742 5958 6131 5842 5421 5918 6265 5851 5949]",
            "title": "Example 1 - Loading MNIST into NumPy Arrays"
        },
        {
            "location": "/user_guide/data/load_mnist/#store-as-csv-files",
            "text": "np.savetxt(fname='/Users/Sebastian/Desktop/images.csv', \n           X=X, delimiter=',', fmt='%d')\nnp.savetxt(fname='/Users/Sebastian/Desktop/labels.csv', \n           X=y, delimiter=',', fmt='%d')",
            "title": "Store as CSV Files"
        },
        {
            "location": "/user_guide/data/load_mnist/#api",
            "text": "loadlocal_mnist(images_path, labels_path)  Read MNIST from ubyte files.  Parameters    images_path  : str  path to the test or train MNIST ubyte file    labels_path  : str  path to the test or train MNIST class labels file    Returns    images  : [n_samples, n_pixels] numpy.array  Pixel values of the images.    labels  : [n_samples] numpy array  Target class labels",
            "title": "API"
        },
        {
            "location": "/user_guide/data/wine_data/",
            "text": "Wine Dataset\n\n\nA function that loads the \nWine\n dataset into NumPy arrays.\n\n\n\n\nfrom mlxtend.data import wine_data\n\n\n\n\nOverview\n\n\nThe Wine dataset for classification.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSamples\n\n\n178\n\n\n\n\n\n\nFeatures\n\n\n13\n\n\n\n\n\n\nClasses\n\n\n3\n\n\n\n\n\n\nData Set Characteristics:\n\n\nMultivariate\n\n\n\n\n\n\nAttribute Characteristics:\n\n\nInteger, Real\n\n\n\n\n\n\nAssociated Tasks:\n\n\nClassification\n\n\n\n\n\n\nMissing Values\n\n\nNone\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ncolumn\n\n\nattribute\n\n\n\n\n\n\n\n\n\n\n1)\n\n\nClass Label\n\n\n\n\n\n\n2)\n\n\nAlcohol\n\n\n\n\n\n\n3)\n\n\nMalic acid\n\n\n\n\n\n\n4)\n\n\nAsh\n\n\n\n\n\n\n5)\n\n\nAlcalinity of ash\n\n\n\n\n\n\n6)\n\n\nMagnesium\n\n\n\n\n\n\n7)\n\n\nTotal phenols\n\n\n\n\n\n\n8)\n\n\nFlavanoids\n\n\n\n\n\n\n9)\n\n\nNonflavanoid phenols\n\n\n\n\n\n\n10)\n\n\nProanthocyanins\n\n\n\n\n\n\n11)\n\n\nintensity\n\n\n\n\n\n\n12)\n\n\nHue\n\n\n\n\n\n\n13)\n\n\nOD280/OD315 of diluted wines\n\n\n\n\n\n\n14)\n\n\nProline\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nclass\n\n\nsamples\n\n\n\n\n\n\n\n\n\n\n0\n\n\n59\n\n\n\n\n\n\n1\n\n\n71\n\n\n\n\n\n\n2\n\n\n48\n\n\n\n\n\n\n\n\nReferences\n\n\n\n\nForina, M. et al, PARVUS - \nAn Extendible Package for Data Exploration, Classification and Correlation. \nInstitute of Pharmaceutical and Food Analysis and Technologies, Via Brigata Salerno, \n16147 Genoa, Italy. \n\n\nSource: \nhttps://archive.ics.uci.edu/ml/datasets/Wine\n\n\nBache, K. & Lichman, M. (2013). UCI Machine Learning Repository. Irvine, CA: University of California, School of Information and Computer Science.\n\n\n\n\nExamples\n\n\nExample - Dataset overview\n\n\nfrom mlxtend.data import wine_data\nX, y = wine_data()\n\nprint('Dimensions: %s x %s' % (X.shape[0], X.shape[1]))\nprint('\\nHeader: %s' % ['sepal length', 'sepal width',\n                        'petal length', 'petal width'])\nprint('1st row', X[0])\n\n\n\n\nDimensions: 178 x 13\n\nHeader: ['sepal length', 'sepal width', 'petal length', 'petal width']\n1st row [  1.42300000e+01   1.71000000e+00   2.43000000e+00   1.56000000e+01\n   1.27000000e+02   2.80000000e+00   3.06000000e+00   2.80000000e-01\n   2.29000000e+00   5.64000000e+00   1.04000000e+00   3.92000000e+00\n   1.06500000e+03]\n\n\n\nimport numpy as np\nprint('Classes: %s' % np.unique(y))\nprint('Class distribution: %s' % np.bincount(y))\n\n\n\n\nClasses: [0 1 2]\nClass distribution: [59 71 48]\n\n\n\nAPI\n\n\nwine_data()\n\n\nWine dataset.\n\n\n\n\n\n\nSource\n : https://archive.ics.uci.edu/ml/datasets/Wine\n\n\n\n\n\n\nNumber of samples\n : 178\n\n\n\n\n\n\nClass labels\n : {0, 1, 2}, distribution: [59, 71, 48]\n\n\nDataset Attributes:\n\n\n\n\n1) Alcohol\n\n\n2) Malic acid\n\n\n3) Ash\n\n\n4) Alcalinity of ash\n\n\n5) Magnesium\n\n\n6) Total phenols\n\n\n7) Flavanoids\n\n\n8) Nonflavanoid phenols\n\n\n9) Proanthocyanins\n\n\n10) Color intensity\n\n\n11) Hue\n\n\n12) OD280/OD315 of diluted wines\n\n\n13) Proline\n\n\n\n\n\n\n\n\nReturns\n\n\n\n\n\n\nX, y\n : [n_samples, n_features], [n_class_labels]\n\n\nX is the feature matrix with 178 wine samples as rows\nand 13 feature columns.\ny is a 1-dimensional array of the 3 class labels 0, 1, 2",
            "title": "Wine data"
        },
        {
            "location": "/user_guide/data/wine_data/#wine-dataset",
            "text": "A function that loads the  Wine  dataset into NumPy arrays.   from mlxtend.data import wine_data",
            "title": "Wine Dataset"
        },
        {
            "location": "/user_guide/data/wine_data/#overview",
            "text": "The Wine dataset for classification.           Samples  178    Features  13    Classes  3    Data Set Characteristics:  Multivariate    Attribute Characteristics:  Integer, Real    Associated Tasks:  Classification    Missing Values  None        column  attribute      1)  Class Label    2)  Alcohol    3)  Malic acid    4)  Ash    5)  Alcalinity of ash    6)  Magnesium    7)  Total phenols    8)  Flavanoids    9)  Nonflavanoid phenols    10)  Proanthocyanins    11)  intensity    12)  Hue    13)  OD280/OD315 of diluted wines    14)  Proline        class  samples      0  59    1  71    2  48",
            "title": "Overview"
        },
        {
            "location": "/user_guide/data/wine_data/#references",
            "text": "Forina, M. et al, PARVUS - \nAn Extendible Package for Data Exploration, Classification and Correlation. \nInstitute of Pharmaceutical and Food Analysis and Technologies, Via Brigata Salerno, \n16147 Genoa, Italy.   Source:  https://archive.ics.uci.edu/ml/datasets/Wine  Bache, K. & Lichman, M. (2013). UCI Machine Learning Repository. Irvine, CA: University of California, School of Information and Computer Science.",
            "title": "References"
        },
        {
            "location": "/user_guide/data/wine_data/#examples",
            "text": "",
            "title": "Examples"
        },
        {
            "location": "/user_guide/data/wine_data/#example-dataset-overview",
            "text": "from mlxtend.data import wine_data\nX, y = wine_data()\n\nprint('Dimensions: %s x %s' % (X.shape[0], X.shape[1]))\nprint('\\nHeader: %s' % ['sepal length', 'sepal width',\n                        'petal length', 'petal width'])\nprint('1st row', X[0])  Dimensions: 178 x 13\n\nHeader: ['sepal length', 'sepal width', 'petal length', 'petal width']\n1st row [  1.42300000e+01   1.71000000e+00   2.43000000e+00   1.56000000e+01\n   1.27000000e+02   2.80000000e+00   3.06000000e+00   2.80000000e-01\n   2.29000000e+00   5.64000000e+00   1.04000000e+00   3.92000000e+00\n   1.06500000e+03]  import numpy as np\nprint('Classes: %s' % np.unique(y))\nprint('Class distribution: %s' % np.bincount(y))  Classes: [0 1 2]\nClass distribution: [59 71 48]",
            "title": "Example - Dataset overview"
        },
        {
            "location": "/user_guide/data/wine_data/#api",
            "text": "wine_data()  Wine dataset.    Source  : https://archive.ics.uci.edu/ml/datasets/Wine    Number of samples  : 178    Class labels  : {0, 1, 2}, distribution: [59, 71, 48]  Dataset Attributes:   1) Alcohol  2) Malic acid  3) Ash  4) Alcalinity of ash  5) Magnesium  6) Total phenols  7) Flavanoids  8) Nonflavanoid phenols  9) Proanthocyanins  10) Color intensity  11) Hue  12) OD280/OD315 of diluted wines  13) Proline     Returns    X, y  : [n_samples, n_features], [n_class_labels]  X is the feature matrix with 178 wine samples as rows\nand 13 feature columns.\ny is a 1-dimensional array of the 3 class labels 0, 1, 2",
            "title": "API"
        },
        {
            "location": "/user_guide/data/three_blobs_data/",
            "text": "Three Blobs Dataset\n\n\nA function that loads the \nthree_blobs\n dataset into NumPy arrays.\n\n\n\n\nfrom mlxtend.data import three_blobs_data\n\n\n\n\nOverview\n\n\nA random dataset of 3 2D blobs for clustering.\n\n\n\n\nNumber of samples : 150\n\n\nSuggested labels \n$\\in$\n {0, 1, 2}, distribution: [50, 50, 50]\n\n\n\n\nReferences\n\n\n\n\n\n\n\n\nExamples\n\n\nExample - Dataset overview\n\n\nfrom mlxtend.data import three_blobs_data\nX, y = three_blobs_data()\n\nprint('Dimensions: %s x %s' % (X.shape[0], X.shape[1]))\n\nprint('1st row', X[0])\n\n\n\n\nDimensions: 150 x 2\n1st row [ 2.60509732  1.22529553]\n\n\n\nimport numpy as np\n\nprint('Suggested cluster labels')\nprint(np.unique(y))\nprint('Label distribution: %s' % np.bincount(y))\n\n\n\n\nSuggested cluster labels\n[0 1 2]\nLabel distribution: [50 50 50]\n\n\n\nimport matplotlib.pyplot as plt\n\nplt.scatter(X[:,0], X[:,1],\n            c='white',\n            marker='o',\n            s=50)\n\nplt.grid()\nplt.show()\n\n\n\n\n\n\nplt.scatter(X[y == 0, 0],\n            X[y == 0, 1],\n            s=50,\n            c='lightgreen',\n            marker='s',\n            label='cluster 1')\n\nplt.scatter(X[y == 1,0],\n            X[y == 1,1],\n            s=50,\n            c='orange',\n            marker='o',\n            label='cluster 2')\n\nplt.scatter(X[y == 2,0],\n            X[y == 2,1],\n            s=50,\n            c='lightblue',\n            marker='v',\n            label='cluster 3')\n\nplt.legend(loc='lower left')\nplt.grid()\nplt.show()\n\n\n\n\n\n\nAPI\n\n\nthree_blobs_data()\n\n\nA random dataset of 3 2D blobs for clustering.\n\n\n\n\n\n\nNumber of samples\n : 150\n\n\n\n\n\n\nSuggested labels\n : {0, 1, 2}, distribution: [50, 50, 50]\n\n\n\n\n\n\nReturns\n\n\n\n\n\n\nX, y\n : [n_samples, n_features], [n_cluster_labels]\n\n\nX is the feature matrix with 159 samples as rows\nand 2 feature columns.\ny is a 1-dimensional array of the 3 suggested cluster labels 0, 1, 2",
            "title": "Three blobs data"
        },
        {
            "location": "/user_guide/data/three_blobs_data/#three-blobs-dataset",
            "text": "A function that loads the  three_blobs  dataset into NumPy arrays.   from mlxtend.data import three_blobs_data",
            "title": "Three Blobs Dataset"
        },
        {
            "location": "/user_guide/data/three_blobs_data/#overview",
            "text": "A random dataset of 3 2D blobs for clustering.   Number of samples : 150  Suggested labels  $\\in$  {0, 1, 2}, distribution: [50, 50, 50]",
            "title": "Overview"
        },
        {
            "location": "/user_guide/data/three_blobs_data/#references",
            "text": "",
            "title": "References"
        },
        {
            "location": "/user_guide/data/three_blobs_data/#examples",
            "text": "",
            "title": "Examples"
        },
        {
            "location": "/user_guide/data/three_blobs_data/#example-dataset-overview",
            "text": "from mlxtend.data import three_blobs_data\nX, y = three_blobs_data()\n\nprint('Dimensions: %s x %s' % (X.shape[0], X.shape[1]))\n\nprint('1st row', X[0])  Dimensions: 150 x 2\n1st row [ 2.60509732  1.22529553]  import numpy as np\n\nprint('Suggested cluster labels')\nprint(np.unique(y))\nprint('Label distribution: %s' % np.bincount(y))  Suggested cluster labels\n[0 1 2]\nLabel distribution: [50 50 50]  import matplotlib.pyplot as plt\n\nplt.scatter(X[:,0], X[:,1],\n            c='white',\n            marker='o',\n            s=50)\n\nplt.grid()\nplt.show()   plt.scatter(X[y == 0, 0],\n            X[y == 0, 1],\n            s=50,\n            c='lightgreen',\n            marker='s',\n            label='cluster 1')\n\nplt.scatter(X[y == 1,0],\n            X[y == 1,1],\n            s=50,\n            c='orange',\n            marker='o',\n            label='cluster 2')\n\nplt.scatter(X[y == 2,0],\n            X[y == 2,1],\n            s=50,\n            c='lightblue',\n            marker='v',\n            label='cluster 3')\n\nplt.legend(loc='lower left')\nplt.grid()\nplt.show()",
            "title": "Example - Dataset overview"
        },
        {
            "location": "/user_guide/data/three_blobs_data/#api",
            "text": "three_blobs_data()  A random dataset of 3 2D blobs for clustering.    Number of samples  : 150    Suggested labels  : {0, 1, 2}, distribution: [50, 50, 50]    Returns    X, y  : [n_samples, n_features], [n_cluster_labels]  X is the feature matrix with 159 samples as rows\nand 2 feature columns.\ny is a 1-dimensional array of the 3 suggested cluster labels 0, 1, 2",
            "title": "API"
        },
        {
            "location": "/user_guide/file_io/find_filegroups/",
            "text": "Find Filegroups\n\n\nA function that finds files that belong together (i.e., differ only by file extension) in different directories and collects them in a Python dictionary for further processing tasks. \n\n\n\n\nfrom mlxtend.file_io import find_filegroups\n\n\n\n\nOverview\n\n\nThis function finds files that are related to each other based on their file names. This can be useful for parsing collections files that have been stored in different subdirectories, for examples:\n\n\ninput_dir/\n    task01.txt\n    task02.txt\n    ...\nlog_dir/\n    task01.log\n    task02.log\n    ...\noutput_dir/\n    task01.dat\n    task02.dat\n    ...\n\n\n\nReferences\n\n\n\n\n-\n\n\n\n\nExamples\n\n\nExample 1 - Grouping related files in a dictionary\n\n\nGiven the following directory and file structure\n\n\ndir_1/\n    file_1.log\n    file_2.log\n    file_3.log\ndir_2/\n    file_1.csv\n    file_2.csv\n    file_3.csv\ndir_3/\n    file_1.txt\n    file_2.txt\n    file_3.txt\n\n\n\nwe can use \nfind_filegroups\n to group related files as items of a dictionary as shown below:\n\n\nfrom mlxtend.file_io import find_filegroups\n\nfind_filegroups(paths=['./data_find_filegroups/dir_1', \n                       './data_find_filegroups/dir_2', \n                       './data_find_filegroups/dir_3'], \n                substring='file_')\n\n\n\n\n{'file_1': ['./data_find_filegroups/dir_1/file_1.log',\n  './data_find_filegroups/dir_2/file_1.csv',\n  './data_find_filegroups/dir_3/file_1.txt'],\n 'file_2': ['./data_find_filegroups/dir_1/file_2.log',\n  './data_find_filegroups/dir_2/file_2.csv',\n  './data_find_filegroups/dir_3/file_2.txt'],\n 'file_3': ['./data_find_filegroups/dir_1/file_3.log',\n  './data_find_filegroups/dir_2/file_3.csv',\n  './data_find_filegroups/dir_3/file_3.txt']}\n\n\n\nAPI\n\n\nfind_filegroups(paths, substring='', extensions=None, validity_check=True, ignore_invisible=True, rstrip='', ignore_substring=None)\n\n\nFind and collect files from different directories in a python dictionary.\n\n\nParameters\n\n\n\n\n\n\npaths\n : \nlist\n\n\nPaths of the directories to be searched. Dictionary keys are build from\nthe first directory.\n\n\n\n\n\n\nsubstring\n : \nstr\n (default: '')\n\n\nSubstring that all files have to contain to be considered.\n\n\n\n\n\n\nextensions\n : \nlist\n (default: None)\n\n\nNone\n or \nlist\n of allowed file extensions for each path.\nIf provided, the number of extensions must match the number of \npaths\n.\n\n\n\n\n\n\nvalidity_check\n : \nbool\n (default: None)\n\n\nIf \nTrue\n, checks if all dictionary values\nhave the same number of file paths. Prints\na warning and returns an empty dictionary if the validity check failed.\n\n\n\n\n\n\nignore_invisible\n : \nbool\n (default: True)\n\n\nIf \nTrue\n, ignores invisible files\n(i.e., files starting with a period).\n\n\n\n\n\n\nrstrip\n : \nstr\n (default: '')\n\n\nIf provided, strips characters from right side of the file\nbase names after splitting the extension.\nUseful to trim different filenames to a common stem.\nE.g,. \"abc_d.txt\" and \"abc_d_.csv\" would share\nthe stem \"abc_d\" if rstrip is set to \"_\".\n\n\n\n\n\n\nignore_substring\n : \nstr\n (default: None)\n\n\nIgnores files that contain the specified substring.\n\n\n\n\n\n\nReturns\n\n\n\n\n\n\ngroups\n : \ndict\n\n\nDictionary of files paths. Keys are the file names\nfound in the first directory listed\nin \npaths\n (without file extension).",
            "title": "Find filegroups"
        },
        {
            "location": "/user_guide/file_io/find_filegroups/#find-filegroups",
            "text": "A function that finds files that belong together (i.e., differ only by file extension) in different directories and collects them in a Python dictionary for further processing tasks.    from mlxtend.file_io import find_filegroups",
            "title": "Find Filegroups"
        },
        {
            "location": "/user_guide/file_io/find_filegroups/#overview",
            "text": "This function finds files that are related to each other based on their file names. This can be useful for parsing collections files that have been stored in different subdirectories, for examples:  input_dir/\n    task01.txt\n    task02.txt\n    ...\nlog_dir/\n    task01.log\n    task02.log\n    ...\noutput_dir/\n    task01.dat\n    task02.dat\n    ...",
            "title": "Overview"
        },
        {
            "location": "/user_guide/file_io/find_filegroups/#references",
            "text": "-",
            "title": "References"
        },
        {
            "location": "/user_guide/file_io/find_filegroups/#examples",
            "text": "",
            "title": "Examples"
        },
        {
            "location": "/user_guide/file_io/find_filegroups/#example-1-grouping-related-files-in-a-dictionary",
            "text": "Given the following directory and file structure  dir_1/\n    file_1.log\n    file_2.log\n    file_3.log\ndir_2/\n    file_1.csv\n    file_2.csv\n    file_3.csv\ndir_3/\n    file_1.txt\n    file_2.txt\n    file_3.txt  we can use  find_filegroups  to group related files as items of a dictionary as shown below:  from mlxtend.file_io import find_filegroups\n\nfind_filegroups(paths=['./data_find_filegroups/dir_1', \n                       './data_find_filegroups/dir_2', \n                       './data_find_filegroups/dir_3'], \n                substring='file_')  {'file_1': ['./data_find_filegroups/dir_1/file_1.log',\n  './data_find_filegroups/dir_2/file_1.csv',\n  './data_find_filegroups/dir_3/file_1.txt'],\n 'file_2': ['./data_find_filegroups/dir_1/file_2.log',\n  './data_find_filegroups/dir_2/file_2.csv',\n  './data_find_filegroups/dir_3/file_2.txt'],\n 'file_3': ['./data_find_filegroups/dir_1/file_3.log',\n  './data_find_filegroups/dir_2/file_3.csv',\n  './data_find_filegroups/dir_3/file_3.txt']}",
            "title": "Example 1 - Grouping related files in a dictionary"
        },
        {
            "location": "/user_guide/file_io/find_filegroups/#api",
            "text": "find_filegroups(paths, substring='', extensions=None, validity_check=True, ignore_invisible=True, rstrip='', ignore_substring=None)  Find and collect files from different directories in a python dictionary.  Parameters    paths  :  list  Paths of the directories to be searched. Dictionary keys are build from\nthe first directory.    substring  :  str  (default: '')  Substring that all files have to contain to be considered.    extensions  :  list  (default: None)  None  or  list  of allowed file extensions for each path.\nIf provided, the number of extensions must match the number of  paths .    validity_check  :  bool  (default: None)  If  True , checks if all dictionary values\nhave the same number of file paths. Prints\na warning and returns an empty dictionary if the validity check failed.    ignore_invisible  :  bool  (default: True)  If  True , ignores invisible files\n(i.e., files starting with a period).    rstrip  :  str  (default: '')  If provided, strips characters from right side of the file\nbase names after splitting the extension.\nUseful to trim different filenames to a common stem.\nE.g,. \"abc_d.txt\" and \"abc_d_.csv\" would share\nthe stem \"abc_d\" if rstrip is set to \"_\".    ignore_substring  :  str  (default: None)  Ignores files that contain the specified substring.    Returns    groups  :  dict  Dictionary of files paths. Keys are the file names\nfound in the first directory listed\nin  paths  (without file extension).",
            "title": "API"
        },
        {
            "location": "/user_guide/file_io/find_files/",
            "text": "Find Files\n\n\nA function that finds files in a given directory based on substring matches and returns a list of the file names found.\n\n\n\n\nfrom mlxtend.file_io import find_files\n\n\n\n\nOverview\n\n\nThis function finds files based on substring search. This is especially useful if we want to find specific files in a directory tree and return their absolute paths for further processing in Python.\n\n\nReferences\n\n\n\n\n-\n\n\n\n\nExamples\n\n\nExample 1 - Grouping related files in a dictionary\n\n\nGiven the following directory and file structure\n\n\ndir_1/\n    file_1.log\n    file_2.log\n    file_3.log\ndir_2/\n    file_1.csv\n    file_2.csv\n    file_3.csv\ndir_3/\n    file_1.txt\n    file_2.txt\n    file_3.txt\n\n\n\nwe can use \nfind_files\n to return the paths to all files that contain the substring \n_2\n as follows: \n\n\nfrom mlxtend.file_io import find_files\n\nfind_files(substring='_2', path='./data_find_filegroups/', recursive=True)\n\n\n\n\n['./data_find_filegroups/dir_1/file_2.log',\n './data_find_filegroups/dir_2/file_2.csv',\n './data_find_filegroups/dir_3/file_2.txt']\n\n\n\nAPI\n\n\nfind_files(substring, path, recursive=False, check_ext=None, ignore_invisible=True, ignore_substring=None)\n\n\nFind files in a directory based on substring matching.\n\n\nParameters\n\n\n\n\n\n\nsubstring\n : \nstr\n\n\nSubstring of the file to be matched.\n\n\n\n\n\n\npath\n : \nstr\n\n\nPath where to look.\nrecursive: \nbool\n\nIf true, searches subdirectories recursively.\ncheck_ext: \nstr\n\nIf string (e.g., '.txt'), only returns files that\nmatch the specified file extension.\n\n\n\n\n\n\nignore_invisible\n : \nbool\n\n\nIf \nTrue\n, ignores invisible files\n(i.e., files starting with a period).\n\n\n\n\n\n\nignore_substring\n : \nstr\n\n\nIgnores files that contain the specified substring.\n\n\n\n\n\n\nReturns\n\n\n\n\n\n\nresults\n : \nlist\n\n\nList of the matched files.",
            "title": "Find files"
        },
        {
            "location": "/user_guide/file_io/find_files/#find-files",
            "text": "A function that finds files in a given directory based on substring matches and returns a list of the file names found.   from mlxtend.file_io import find_files",
            "title": "Find Files"
        },
        {
            "location": "/user_guide/file_io/find_files/#overview",
            "text": "This function finds files based on substring search. This is especially useful if we want to find specific files in a directory tree and return their absolute paths for further processing in Python.",
            "title": "Overview"
        },
        {
            "location": "/user_guide/file_io/find_files/#references",
            "text": "-",
            "title": "References"
        },
        {
            "location": "/user_guide/file_io/find_files/#examples",
            "text": "",
            "title": "Examples"
        },
        {
            "location": "/user_guide/file_io/find_files/#example-1-grouping-related-files-in-a-dictionary",
            "text": "Given the following directory and file structure  dir_1/\n    file_1.log\n    file_2.log\n    file_3.log\ndir_2/\n    file_1.csv\n    file_2.csv\n    file_3.csv\ndir_3/\n    file_1.txt\n    file_2.txt\n    file_3.txt  we can use  find_files  to return the paths to all files that contain the substring  _2  as follows:   from mlxtend.file_io import find_files\n\nfind_files(substring='_2', path='./data_find_filegroups/', recursive=True)  ['./data_find_filegroups/dir_1/file_2.log',\n './data_find_filegroups/dir_2/file_2.csv',\n './data_find_filegroups/dir_3/file_2.txt']",
            "title": "Example 1 - Grouping related files in a dictionary"
        },
        {
            "location": "/user_guide/file_io/find_files/#api",
            "text": "find_files(substring, path, recursive=False, check_ext=None, ignore_invisible=True, ignore_substring=None)  Find files in a directory based on substring matching.  Parameters    substring  :  str  Substring of the file to be matched.    path  :  str  Path where to look.\nrecursive:  bool \nIf true, searches subdirectories recursively.\ncheck_ext:  str \nIf string (e.g., '.txt'), only returns files that\nmatch the specified file extension.    ignore_invisible  :  bool  If  True , ignores invisible files\n(i.e., files starting with a period).    ignore_substring  :  str  Ignores files that contain the specified substring.    Returns    results  :  list  List of the matched files.",
            "title": "API"
        },
        {
            "location": "/user_guide/general_plotting/category_scatter/",
            "text": "Scatterplot with Categories\n\n\nA function to quickly produce a scatter plot colored by categories from a pandas \nDataFrame\n or NumPy \nndarray\n object.\n\n\n\n\nfrom mlxtend.general_plotting import category_scatter\n\n\n\n\nOverview\n\n\nReferences\n\n\n\n\n-\n\n\n\n\nExamples\n\n\nExample 1 - Category Scatter from Pandas DataFrames\n\n\nimport pandas as pd\nfrom io import StringIO\n\ncsvfile = \"\"\"label,x,y\nclass1,10.0,8.04\nclass1,10.5,7.30\nclass2,8.3,5.5\nclass2,8.1,5.9\nclass3,3.5,3.5\nclass3,3.8,5.1\"\"\"\n\ndf = pd.read_csv(StringIO(csvfile))\ndf\n\n\n\n\n\n\n\n  \n\n    \n\n      \n\n      \nlabel\n\n      \nx\n\n      \ny\n\n    \n\n  \n\n  \n\n    \n\n      \n0\n\n      \nclass1\n\n      \n10.0\n\n      \n8.04\n\n    \n\n    \n\n      \n1\n\n      \nclass1\n\n      \n10.5\n\n      \n7.30\n\n    \n\n    \n\n      \n2\n\n      \nclass2\n\n      \n8.3\n\n      \n5.50\n\n    \n\n    \n\n      \n3\n\n      \nclass2\n\n      \n8.1\n\n      \n5.90\n\n    \n\n    \n\n      \n4\n\n      \nclass3\n\n      \n3.5\n\n      \n3.50\n\n    \n\n    \n\n      \n5\n\n      \nclass3\n\n      \n3.8\n\n      \n5.10\n\n    \n\n  \n\n\n\n\n\n\n\nPlotting the data where the categories are determined by the unique values in the label column \nlabel_col\n. The \nx\n and \ny\n values are simply the column names of the DataFrame that we want to plot.\n\n\n%matplotlib inline\nimport matplotlib.pyplot as plt\nfrom mlxtend.general_plotting import category_scatter\n\nfig = category_scatter(x='x', y='y', label_col='label', \n                       data=df, legend_loc='upper left')\n\n\n\n\n\n\nExample 2 - Category Scatter from NumPy Arrays\n\n\nimport numpy as np\nfrom io import BytesIO\n\ncsvfile = \"\"\"1,10.0,8.04\n1,10.5,7.30\n2,8.3,5.5\n2,8.1,5.9\n3,3.5,3.5\n3,3.8,5.1\"\"\"\n\nary = np.genfromtxt(BytesIO(csvfile.encode()), delimiter=',')\nary\n\n\n\n\narray([[  1.  ,  10.  ,   8.04],\n       [  1.  ,  10.5 ,   7.3 ],\n       [  2.  ,   8.3 ,   5.5 ],\n       [  2.  ,   8.1 ,   5.9 ],\n       [  3.  ,   3.5 ,   3.5 ],\n       [  3.  ,   3.8 ,   5.1 ]])\n\n\n\nNow, pretending that the first column represents the labels, and the second and third column represent the \nx\n and \ny\n values, respectively.\n\n\n%matplotlib inline\nimport matplotlib.pyplot as plt\nfrom mlxtend.general_plotting import category_scatter\n\nfix = category_scatter(x=1, y=2, label_col=0, \n                       data=ary, legend_loc='upper left')\n\n\n\n\n\n\nAPI\n\n\ncategory_scatter(x, y, label_col, data, markers='sxo^v', colors=('blue', 'green', 'red', 'purple', 'gray', 'cyan'), alpha=0.7, markersize=20.0, legend_loc='best')\n\n\nScatter plot to plot categories in different colors/markerstyles.\n\n\nParameters\n\n\n\n\n\n\nx\n : str or int\n\n\nDataFrame column name of the x-axis values or\ninteger for the numpy ndarray column index.\n\n\n\n\n\n\ny\n : str\n\n\nDataFrame column name of the y-axis values or\ninteger for the numpy ndarray column index\n\n\n\n\n\n\ndata\n : Pandas DataFrame object or NumPy ndarray.\n\n\n\n\n\n\nmarkers\n : str\n\n\nMarkers that are cycled through the label category.\n\n\n\n\n\n\ncolors\n : tuple\n\n\nColors that are cycled through the label category.\n\n\n\n\n\n\nalpha\n : float (default: 0.7)\n\n\nParameter to control the transparency.\n\n\n\n\n\n\nmarkersize\n : float (default` : 20.0)\n\n\nParameter to control the marker size.\n\n\n\n\n\n\nlegend_loc\n : str (default: 'best')\n\n\nLocation of the plot legend\n{best, upper left, upper right, lower left, lower right}\nNo legend if legend_loc=False\n\n\n\n\n\n\nReturns\n\n\n\n\nfig\n : matplotlig.pyplot figure object",
            "title": "Category scatter"
        },
        {
            "location": "/user_guide/general_plotting/category_scatter/#scatterplot-with-categories",
            "text": "A function to quickly produce a scatter plot colored by categories from a pandas  DataFrame  or NumPy  ndarray  object.   from mlxtend.general_plotting import category_scatter",
            "title": "Scatterplot with Categories"
        },
        {
            "location": "/user_guide/general_plotting/category_scatter/#overview",
            "text": "",
            "title": "Overview"
        },
        {
            "location": "/user_guide/general_plotting/category_scatter/#references",
            "text": "-",
            "title": "References"
        },
        {
            "location": "/user_guide/general_plotting/category_scatter/#examples",
            "text": "",
            "title": "Examples"
        },
        {
            "location": "/user_guide/general_plotting/category_scatter/#example-1-category-scatter-from-pandas-dataframes",
            "text": "import pandas as pd\nfrom io import StringIO\n\ncsvfile = \"\"\"label,x,y\nclass1,10.0,8.04\nclass1,10.5,7.30\nclass2,8.3,5.5\nclass2,8.1,5.9\nclass3,3.5,3.5\nclass3,3.8,5.1\"\"\"\n\ndf = pd.read_csv(StringIO(csvfile))\ndf   \n   \n     \n       \n       label \n       x \n       y \n     \n   \n   \n     \n       0 \n       class1 \n       10.0 \n       8.04 \n     \n     \n       1 \n       class1 \n       10.5 \n       7.30 \n     \n     \n       2 \n       class2 \n       8.3 \n       5.50 \n     \n     \n       3 \n       class2 \n       8.1 \n       5.90 \n     \n     \n       4 \n       class3 \n       3.5 \n       3.50 \n     \n     \n       5 \n       class3 \n       3.8 \n       5.10 \n     \n      Plotting the data where the categories are determined by the unique values in the label column  label_col . The  x  and  y  values are simply the column names of the DataFrame that we want to plot.  %matplotlib inline\nimport matplotlib.pyplot as plt\nfrom mlxtend.general_plotting import category_scatter\n\nfig = category_scatter(x='x', y='y', label_col='label', \n                       data=df, legend_loc='upper left')",
            "title": "Example 1 - Category Scatter from Pandas DataFrames"
        },
        {
            "location": "/user_guide/general_plotting/category_scatter/#example-2-category-scatter-from-numpy-arrays",
            "text": "import numpy as np\nfrom io import BytesIO\n\ncsvfile = \"\"\"1,10.0,8.04\n1,10.5,7.30\n2,8.3,5.5\n2,8.1,5.9\n3,3.5,3.5\n3,3.8,5.1\"\"\"\n\nary = np.genfromtxt(BytesIO(csvfile.encode()), delimiter=',')\nary  array([[  1.  ,  10.  ,   8.04],\n       [  1.  ,  10.5 ,   7.3 ],\n       [  2.  ,   8.3 ,   5.5 ],\n       [  2.  ,   8.1 ,   5.9 ],\n       [  3.  ,   3.5 ,   3.5 ],\n       [  3.  ,   3.8 ,   5.1 ]])  Now, pretending that the first column represents the labels, and the second and third column represent the  x  and  y  values, respectively.  %matplotlib inline\nimport matplotlib.pyplot as plt\nfrom mlxtend.general_plotting import category_scatter\n\nfix = category_scatter(x=1, y=2, label_col=0, \n                       data=ary, legend_loc='upper left')",
            "title": "Example 2 - Category Scatter from NumPy Arrays"
        },
        {
            "location": "/user_guide/general_plotting/category_scatter/#api",
            "text": "category_scatter(x, y, label_col, data, markers='sxo^v', colors=('blue', 'green', 'red', 'purple', 'gray', 'cyan'), alpha=0.7, markersize=20.0, legend_loc='best')  Scatter plot to plot categories in different colors/markerstyles.  Parameters    x  : str or int  DataFrame column name of the x-axis values or\ninteger for the numpy ndarray column index.    y  : str  DataFrame column name of the y-axis values or\ninteger for the numpy ndarray column index    data  : Pandas DataFrame object or NumPy ndarray.    markers  : str  Markers that are cycled through the label category.    colors  : tuple  Colors that are cycled through the label category.    alpha  : float (default: 0.7)  Parameter to control the transparency.    markersize  : float (default` : 20.0)  Parameter to control the marker size.    legend_loc  : str (default: 'best')  Location of the plot legend\n{best, upper left, upper right, lower left, lower right}\nNo legend if legend_loc=False    Returns   fig  : matplotlig.pyplot figure object",
            "title": "API"
        },
        {
            "location": "/user_guide/general_plotting/enrichment_plot/",
            "text": "Enrichment Plot\n\n\nA function to plot step plots of cumulative counts.\n\n\n\n\nfrom mlxtend.general_plotting import category_scatter\n\n\n\n\nOverview\n\n\nIn enrichment plots, the y-axis can be interpreted as \"how many samples are less or equal to the corresponding x-axis label.\"\n\n\nReferences\n\n\n\n\n-\n\n\n\n\nExamples\n\n\nExample 1 - Enrichment Plots from Pandas DataFrames\n\n\nimport pandas as pd\ns1 = [1.1, 1.5]\ns2 = [2.1, 1.8]\ns3 = [3.1, 2.1]\ns4 = [3.9, 2.5]\ndata = [s1, s2, s3, s4]\ndf = pd.DataFrame(data, columns=['X1', 'X2'])\ndf\n\n\n\n\n\n\n\n  \n\n    \n\n      \n\n      \nX1\n\n      \nX2\n\n    \n\n  \n\n  \n\n    \n\n      \n0\n\n      \n1.1\n\n      \n1.5\n\n    \n\n    \n\n      \n1\n\n      \n2.1\n\n      \n1.8\n\n    \n\n    \n\n      \n2\n\n      \n3.1\n\n      \n2.1\n\n    \n\n    \n\n      \n3\n\n      \n3.9\n\n      \n2.5\n\n    \n\n  \n\n\n\n\n\n\n\nPlotting the data where the categories are determined by the unique values in the label column \nlabel_col\n. The \nx\n and \ny\n values are simply the column names of the DataFrame that we want to plot.\n\n\n%matplotlib inline\nimport matplotlib.pyplot as plt\nfrom mlxtend.general_plotting import enrichment_plot\n\nax = enrichment_plot(df, legend_loc='upper left')\n\n\n\n\n\n\nAPI\n\n\nenrichment_plot(df, colors='bgrkcy', markers=' ', linestyles='-', alpha=0.5, lw=2, legend=True, where='post', grid=True, count_label='Count', xlim='auto', ylim='auto', invert_axes=False, legend_loc='best', ax=None)\n\n\nPlot stacked barplots\n\n\nParameters\n\n\n\n\n\n\ndf\n : pandas.DataFrame\n\n\nA pandas DataFrame where columns represent the different categories.\ncolors: str (default: 'bgrcky')\nThe colors of the bars.\n\n\n\n\n\n\nmarkers\n : str (default: ' ')\n\n\nMatplotlib markerstyles, e.g,\n'sov' for square,circle, and triangle markers.\n\n\n\n\n\n\nlinestyles\n : str (default: '-')\n\n\nMatplotlib linestyles, e.g.,\n'-,--' to cycle normal and dashed lines. Note\nthat the different linestyles need to be separated by commas.\n\n\n\n\n\n\nalpha\n : float (default: 0.5)\n\n\nTransparency level from 0.0 to 1.0.\n\n\n\n\n\n\nlw\n : int or float (default: 2)\n\n\nLinewidth parameter.\n\n\n\n\n\n\nlegend\n : bool (default: True)\n\n\nPlots legend if True.\n\n\n\n\n\n\nwhere\n : {'post', 'pre', 'mid'} (default: 'post')\n\n\nStarting location of the steps.\n\n\n\n\n\n\ngrid\n : bool (default: \nTrue\n)\n\n\nPlots a grid if True.\n\n\n\n\n\n\ncount_label\n : str (default: 'Count')\n\n\nLabel for the \"Count\"-axis.\n\n\n\n\n\n\nxlim\n : 'auto' or array-like [min, max] (default: 'auto')\n\n\nMin and maximum position of the x-axis range.\n\n\n\n\n\n\nylim\n : 'auto' or array-like [min, max] (default: 'auto')\n\n\nMin and maximum position of the y-axis range.\n\n\n\n\n\n\ninvert_axes\n : bool (default: False)\n\n\nPlots count on the x-axis if True.\n\n\n\n\n\n\nlegend_loc\n : str (default: 'best')\n\n\nLocation of the plot legend\n{best, upper left, upper right, lower left, lower right}\nNo legend if legend_loc=False\n\n\n\n\n\n\nax\n : matplotlib axis, optional (default: None)\n\n\nUse this axis for plotting or make a new one otherwise\n\n\n\n\n\n\nReturns\n\n\n\n\nax\n : matplotlib axis",
            "title": "Enrichment plot"
        },
        {
            "location": "/user_guide/general_plotting/enrichment_plot/#enrichment-plot",
            "text": "A function to plot step plots of cumulative counts.   from mlxtend.general_plotting import category_scatter",
            "title": "Enrichment Plot"
        },
        {
            "location": "/user_guide/general_plotting/enrichment_plot/#overview",
            "text": "In enrichment plots, the y-axis can be interpreted as \"how many samples are less or equal to the corresponding x-axis label.\"",
            "title": "Overview"
        },
        {
            "location": "/user_guide/general_plotting/enrichment_plot/#references",
            "text": "-",
            "title": "References"
        },
        {
            "location": "/user_guide/general_plotting/enrichment_plot/#examples",
            "text": "",
            "title": "Examples"
        },
        {
            "location": "/user_guide/general_plotting/enrichment_plot/#example-1-enrichment-plots-from-pandas-dataframes",
            "text": "import pandas as pd\ns1 = [1.1, 1.5]\ns2 = [2.1, 1.8]\ns3 = [3.1, 2.1]\ns4 = [3.9, 2.5]\ndata = [s1, s2, s3, s4]\ndf = pd.DataFrame(data, columns=['X1', 'X2'])\ndf   \n   \n     \n       \n       X1 \n       X2 \n     \n   \n   \n     \n       0 \n       1.1 \n       1.5 \n     \n     \n       1 \n       2.1 \n       1.8 \n     \n     \n       2 \n       3.1 \n       2.1 \n     \n     \n       3 \n       3.9 \n       2.5 \n     \n      Plotting the data where the categories are determined by the unique values in the label column  label_col . The  x  and  y  values are simply the column names of the DataFrame that we want to plot.  %matplotlib inline\nimport matplotlib.pyplot as plt\nfrom mlxtend.general_plotting import enrichment_plot\n\nax = enrichment_plot(df, legend_loc='upper left')",
            "title": "Example 1 - Enrichment Plots from Pandas DataFrames"
        },
        {
            "location": "/user_guide/general_plotting/enrichment_plot/#api",
            "text": "enrichment_plot(df, colors='bgrkcy', markers=' ', linestyles='-', alpha=0.5, lw=2, legend=True, where='post', grid=True, count_label='Count', xlim='auto', ylim='auto', invert_axes=False, legend_loc='best', ax=None)  Plot stacked barplots  Parameters    df  : pandas.DataFrame  A pandas DataFrame where columns represent the different categories.\ncolors: str (default: 'bgrcky')\nThe colors of the bars.    markers  : str (default: ' ')  Matplotlib markerstyles, e.g,\n'sov' for square,circle, and triangle markers.    linestyles  : str (default: '-')  Matplotlib linestyles, e.g.,\n'-,--' to cycle normal and dashed lines. Note\nthat the different linestyles need to be separated by commas.    alpha  : float (default: 0.5)  Transparency level from 0.0 to 1.0.    lw  : int or float (default: 2)  Linewidth parameter.    legend  : bool (default: True)  Plots legend if True.    where  : {'post', 'pre', 'mid'} (default: 'post')  Starting location of the steps.    grid  : bool (default:  True )  Plots a grid if True.    count_label  : str (default: 'Count')  Label for the \"Count\"-axis.    xlim  : 'auto' or array-like [min, max] (default: 'auto')  Min and maximum position of the x-axis range.    ylim  : 'auto' or array-like [min, max] (default: 'auto')  Min and maximum position of the y-axis range.    invert_axes  : bool (default: False)  Plots count on the x-axis if True.    legend_loc  : str (default: 'best')  Location of the plot legend\n{best, upper left, upper right, lower left, lower right}\nNo legend if legend_loc=False    ax  : matplotlib axis, optional (default: None)  Use this axis for plotting or make a new one otherwise    Returns   ax  : matplotlib axis",
            "title": "API"
        },
        {
            "location": "/user_guide/general_plotting/stacked_barplot/",
            "text": "Stacked Barplot\n\n\nA function to conveniently plot stacked bar plots in matplotlib using pandas \nDataFrame\ns. \n\n\n\n\nfrom mlxtend.general_plotting import category_scatter\n\n\n\n\nOverview\n\n\nA matplotlib convenience function for creating barplots from DataFrames where each sample is associated with several categories.\n\n\nReferences\n\n\n\n\n-\n\n\n\n\nExamples\n\n\nExample 1 - Stacked Barplot from Pandas DataFrames\n\n\nimport pandas as pd\n\ns1 = [1.0, 2.0, 3.0, 4.0]\ns2 = [1.4, 2.1, 2.9, 5.1]\ns3 = [1.9, 2.2, 3.5, 4.1]\ns4 = [1.4, 2.5, 3.5, 4.2]\ndata = [s1, s2, s3, s4]\n\ndf = pd.DataFrame(data, columns=['X1', 'X2', 'X3', 'X4'])\ndf.columns = ['X1', 'X2', 'X3', 'X4']\ndf.index = ['Sample1', 'Sample2', 'Sample3', 'Sample4']\ndf\n\n\n\n\n\n\n\n  \n\n    \n\n      \n\n      \nX1\n\n      \nX2\n\n      \nX3\n\n      \nX4\n\n    \n\n  \n\n  \n\n    \n\n      \nSample1\n\n      \n1.0\n\n      \n2.0\n\n      \n3.0\n\n      \n4.0\n\n    \n\n    \n\n      \nSample2\n\n      \n1.4\n\n      \n2.1\n\n      \n2.9\n\n      \n5.1\n\n    \n\n    \n\n      \nSample3\n\n      \n1.9\n\n      \n2.2\n\n      \n3.5\n\n      \n4.1\n\n    \n\n    \n\n      \nSample4\n\n      \n1.4\n\n      \n2.5\n\n      \n3.5\n\n      \n4.2\n\n    \n\n  \n\n\n\n\n\n\n\nBy default, the index of the \nDataFrame\n is used as column labels, and the \nDataFrame\n columns are used for the plot legend.\n\n\nimport matplotlib.pyplot as plt\nfrom mlxtend.general_plotting import stacked_barplot\n\nfig = stacked_barplot(df, rotation=45, legend_loc='best')\n\n\n\n\n\n\nAPI\n\n\nstacked_barplot(df, bar_width='auto', colors='bgrcky', labels='index', rotation=90, legend_loc='best')\n\n\nFunction to plot stacked barplots\n\n\nParameters\n\n\n\n\n\n\ndf\n : pandas.DataFrame\n\n\nA pandas DataFrame where the index denotes the\nx-axis labels, and the columns contain the different\nmeasurements for each row.\nbar_width: 'auto' or float (default: 'auto')\nParameter to set the widths of the bars. if\n'auto', the width is automatically determined by\nthe number of columns in the dataset.\ncolors: str (default: 'bgrcky')\nThe colors of the bars.\nlabels: 'index' or iterable (default: 'index')\nIf 'index', the DataFrame index will be used as\nx-tick labels.\nrotation: int (default: 90)\nParameter to rotate the x-axis labels.\n\n\n\n\n\n\nlegend_loc\n : str (default: 'best')\n\n\nLocation of the plot legend\n{best, upper left, upper right, lower left, lower right}\nNo legend if legend_loc=False\n\n\n\n\n\n\nReturns\n\n\n\n\nfig\n : matplotlib.pyplot figure object",
            "title": "Stacked barplot"
        },
        {
            "location": "/user_guide/general_plotting/stacked_barplot/#stacked-barplot",
            "text": "A function to conveniently plot stacked bar plots in matplotlib using pandas  DataFrame s.    from mlxtend.general_plotting import category_scatter",
            "title": "Stacked Barplot"
        },
        {
            "location": "/user_guide/general_plotting/stacked_barplot/#overview",
            "text": "A matplotlib convenience function for creating barplots from DataFrames where each sample is associated with several categories.",
            "title": "Overview"
        },
        {
            "location": "/user_guide/general_plotting/stacked_barplot/#references",
            "text": "-",
            "title": "References"
        },
        {
            "location": "/user_guide/general_plotting/stacked_barplot/#examples",
            "text": "",
            "title": "Examples"
        },
        {
            "location": "/user_guide/general_plotting/stacked_barplot/#example-1-stacked-barplot-from-pandas-dataframes",
            "text": "import pandas as pd\n\ns1 = [1.0, 2.0, 3.0, 4.0]\ns2 = [1.4, 2.1, 2.9, 5.1]\ns3 = [1.9, 2.2, 3.5, 4.1]\ns4 = [1.4, 2.5, 3.5, 4.2]\ndata = [s1, s2, s3, s4]\n\ndf = pd.DataFrame(data, columns=['X1', 'X2', 'X3', 'X4'])\ndf.columns = ['X1', 'X2', 'X3', 'X4']\ndf.index = ['Sample1', 'Sample2', 'Sample3', 'Sample4']\ndf   \n   \n     \n       \n       X1 \n       X2 \n       X3 \n       X4 \n     \n   \n   \n     \n       Sample1 \n       1.0 \n       2.0 \n       3.0 \n       4.0 \n     \n     \n       Sample2 \n       1.4 \n       2.1 \n       2.9 \n       5.1 \n     \n     \n       Sample3 \n       1.9 \n       2.2 \n       3.5 \n       4.1 \n     \n     \n       Sample4 \n       1.4 \n       2.5 \n       3.5 \n       4.2 \n     \n      By default, the index of the  DataFrame  is used as column labels, and the  DataFrame  columns are used for the plot legend.  import matplotlib.pyplot as plt\nfrom mlxtend.general_plotting import stacked_barplot\n\nfig = stacked_barplot(df, rotation=45, legend_loc='best')",
            "title": "Example 1 - Stacked Barplot from Pandas DataFrames"
        },
        {
            "location": "/user_guide/general_plotting/stacked_barplot/#api",
            "text": "stacked_barplot(df, bar_width='auto', colors='bgrcky', labels='index', rotation=90, legend_loc='best')  Function to plot stacked barplots  Parameters    df  : pandas.DataFrame  A pandas DataFrame where the index denotes the\nx-axis labels, and the columns contain the different\nmeasurements for each row.\nbar_width: 'auto' or float (default: 'auto')\nParameter to set the widths of the bars. if\n'auto', the width is automatically determined by\nthe number of columns in the dataset.\ncolors: str (default: 'bgrcky')\nThe colors of the bars.\nlabels: 'index' or iterable (default: 'index')\nIf 'index', the DataFrame index will be used as\nx-tick labels.\nrotation: int (default: 90)\nParameter to rotate the x-axis labels.    legend_loc  : str (default: 'best')  Location of the plot legend\n{best, upper left, upper right, lower left, lower right}\nNo legend if legend_loc=False    Returns   fig  : matplotlib.pyplot figure object",
            "title": "API"
        },
        {
            "location": "/user_guide/math/num_combinations/",
            "text": "Compute the Number of Combinations\n\n\nA function to calculate the number of combinations for creating subsequences of \nk\n elements out of a sequence with \nn\n elements.\n\n\n\n\nfrom mlxtend.math import num_combinations\n\n\n\n\nOverview\n\n\nCombinations are selections of items from a collection regardless of the order in which they appear (in contrast to permutations). For example, let's consider a combination of 3 elements (k=3) from a collection of 5 elements (n=5): \n\n\n\n\ncollection: {1, 2, 3, 4, 5}\n\n\ncombination 1a: {1, 3, 5} \n\n\ncombination 1b: {1, 5, 3}\n\n\ncombination 1c: {3, 5, 1}\n\n\n...\n\n\ncombination 2: {1, 3, 4}\n\n\n\n\nIn the example above the combinations 1a, 1b, and 1c, are the \"same combination\" and counted as \"1 possible way to combine items 1, 3, and 5\" -- in combinations, the order does not matter.\n\n\nThe number of ways to combine elements (\nwithout replacement\n)  from a collection with size \nn\n into subsets of size \nk\n is computed via the binomial coefficient (\"\nn\n choose \nk\n\"):\n\n\n$$\n\\begin{pmatrix} \nn  \\\\\nk \n\\end{pmatrix} = \\frac{n(n-1)\\ldots(n-k+1)}{k(k-1)\\dots1} = \\frac{n!}{k!(n-k)!}\n$$\n\n\nTo compute the number of combinations \nwith replacement\n, the following, alternative equation \nis used (\"\nn\n multichoose \nk\n\"):\n\n\n$$\\begin{pmatrix} \nn  \\\\\nk \n\\end{pmatrix} = \\begin{pmatrix} \nn + k -1  \\\\\nk \n\\end{pmatrix}$$\n\n\nReferences\n\n\n\n\nhttps://en.wikipedia.org/wiki/Combination\n\n\n\n\nExamples\n\n\nExample 1 - Compute the number of combinations\n\n\nfrom mlxtend.math import num_combinations\n\nc = num_combinations(n=20, k=8, with_replacement=False)\nprint('Number of ways to combine 20 elements'\n      ' into 8 subelements: %d' % c)\n\n\n\n\nNumber of ways to combine 20 elements into 8 subelements: 125970\n\n\n\nfrom mlxtend.math import num_combinations\n\nc = num_combinations(n=20, k=8, with_replacement=True)\nprint('Number of ways to combine 20 elements'\n      ' into 8 subelements (with replacement): %d' % c)\n\n\n\n\nNumber of ways to combine 20 elements into 8 subelements (with replacement): 2220075\n\n\n\nExample 2 - A progress tracking use-case\n\n\nIt is often quite useful to track the progress of a computational expensive tasks to estimate its runtime. Here, the \nnum_combination\n function can be used to compute the maximum number of loops of a \ncombinations\n iterable from itertools:\n\n\nimport itertools\nimport sys\nimport time\nfrom mlxtend.math import num_combinations\n\nitems = {1, 2, 3, 4, 5, 6, 7, 8}\nmax_iter = num_combinations(n=len(items), k=3, \n                            with_replacement=False)\n\nfor idx, i in enumerate(itertools.combinations(items, r=3)):\n    # do some computation with itemset i\n    time.sleep(0.1)\n    sys.stdout.write('\\rProgress: %d/%d' % (idx + 1, max_iter))\n    sys.stdout.flush()\n\n\n\n\nProgress: 56/56\n\n\n\nAPI\n\n\nnum_combinations(n, k, with_replacement=False)\n\n\nFunction to calculate the number of possible combinations.\n\n\nParameters\n\n\n\n\n\n\nn\n : \nint\n\n\nTotal number of items.\n\n\n\n\n\n\nk\n : \nint\n\n\nNumber of elements of the target itemset.\n\n\n\n\n\n\nwith_replacement\n : \nbool\n (default: False)\n\n\nAllows repeated elements if True.\n\n\n\n\n\n\nReturns\n\n\n\n\n\n\ncomb\n : \nint\n\n\nNumber of possible combinations.",
            "title": "Num combinations"
        },
        {
            "location": "/user_guide/math/num_combinations/#compute-the-number-of-combinations",
            "text": "A function to calculate the number of combinations for creating subsequences of  k  elements out of a sequence with  n  elements.   from mlxtend.math import num_combinations",
            "title": "Compute the Number of Combinations"
        },
        {
            "location": "/user_guide/math/num_combinations/#overview",
            "text": "Combinations are selections of items from a collection regardless of the order in which they appear (in contrast to permutations). For example, let's consider a combination of 3 elements (k=3) from a collection of 5 elements (n=5):    collection: {1, 2, 3, 4, 5}  combination 1a: {1, 3, 5}   combination 1b: {1, 5, 3}  combination 1c: {3, 5, 1}  ...  combination 2: {1, 3, 4}   In the example above the combinations 1a, 1b, and 1c, are the \"same combination\" and counted as \"1 possible way to combine items 1, 3, and 5\" -- in combinations, the order does not matter.  The number of ways to combine elements ( without replacement )  from a collection with size  n  into subsets of size  k  is computed via the binomial coefficient (\" n  choose  k \"):  $$\n\\begin{pmatrix} \nn  \\\\\nk \n\\end{pmatrix} = \\frac{n(n-1)\\ldots(n-k+1)}{k(k-1)\\dots1} = \\frac{n!}{k!(n-k)!}\n$$  To compute the number of combinations  with replacement , the following, alternative equation \nis used (\" n  multichoose  k \"):  $$\\begin{pmatrix} \nn  \\\\\nk \n\\end{pmatrix} = \\begin{pmatrix} \nn + k -1  \\\\\nk \n\\end{pmatrix}$$",
            "title": "Overview"
        },
        {
            "location": "/user_guide/math/num_combinations/#references",
            "text": "https://en.wikipedia.org/wiki/Combination",
            "title": "References"
        },
        {
            "location": "/user_guide/math/num_combinations/#examples",
            "text": "",
            "title": "Examples"
        },
        {
            "location": "/user_guide/math/num_combinations/#example-1-compute-the-number-of-combinations",
            "text": "from mlxtend.math import num_combinations\n\nc = num_combinations(n=20, k=8, with_replacement=False)\nprint('Number of ways to combine 20 elements'\n      ' into 8 subelements: %d' % c)  Number of ways to combine 20 elements into 8 subelements: 125970  from mlxtend.math import num_combinations\n\nc = num_combinations(n=20, k=8, with_replacement=True)\nprint('Number of ways to combine 20 elements'\n      ' into 8 subelements (with replacement): %d' % c)  Number of ways to combine 20 elements into 8 subelements (with replacement): 2220075",
            "title": "Example 1 - Compute the number of combinations"
        },
        {
            "location": "/user_guide/math/num_combinations/#example-2-a-progress-tracking-use-case",
            "text": "It is often quite useful to track the progress of a computational expensive tasks to estimate its runtime. Here, the  num_combination  function can be used to compute the maximum number of loops of a  combinations  iterable from itertools:  import itertools\nimport sys\nimport time\nfrom mlxtend.math import num_combinations\n\nitems = {1, 2, 3, 4, 5, 6, 7, 8}\nmax_iter = num_combinations(n=len(items), k=3, \n                            with_replacement=False)\n\nfor idx, i in enumerate(itertools.combinations(items, r=3)):\n    # do some computation with itemset i\n    time.sleep(0.1)\n    sys.stdout.write('\\rProgress: %d/%d' % (idx + 1, max_iter))\n    sys.stdout.flush()  Progress: 56/56",
            "title": "Example 2 - A progress tracking use-case"
        },
        {
            "location": "/user_guide/math/num_combinations/#api",
            "text": "num_combinations(n, k, with_replacement=False)  Function to calculate the number of possible combinations.  Parameters    n  :  int  Total number of items.    k  :  int  Number of elements of the target itemset.    with_replacement  :  bool  (default: False)  Allows repeated elements if True.    Returns    comb  :  int  Number of possible combinations.",
            "title": "API"
        },
        {
            "location": "/user_guide/math/num_permutations/",
            "text": "Compute the Number of Permutations\n\n\nA function to calculate the number of permutations for creating subsequences of \nk\n elements out of a sequence with \nn\n elements.\n\n\n\n\nfrom mlxtend.math import num_permutations\n\n\n\n\nOverview\n\n\nPermutations are selections of items from a collection with regard to the order in which they appear (in contrast to combinations). For example, let's consider a permutation of 3 elements (k=3) from a collection of 5 elements (n=5): \n\n\n\n\ncollection: {1, 2, 3, 4, 5}\n\n\ncombination 1a: {1, 3, 5} \n\n\ncombination 1b: {1, 5, 3}\n\n\ncombination 1c: {3, 5, 1}\n\n\n...\n\n\ncombination 2: {1, 3, 4}\n\n\n\n\nIn the example above the permutations 1a, 1b, and 1c, are the \"same combination\" but distinct permutations -- in combinations, the order does not matter, but in permutation it does matter.\n\n\nThe number of ways to combine elements (\nwithout replacement\n) from a collection with size \nn\n into subsets of size \nk\n is computed via the binomial coefficient (\"\nn\n choose \nk\n\"):\n\n\n$$\nk!\\begin{pmatrix} \nn  \\\\\nk \n\\end{pmatrix} = k! \\cdot \\frac{n!}{k!(n-k)!} = \\frac{n!}{(n-k)!}\n$$\n\n\nTo compute the number of permutations \nwith replacement\n, we simply need to compute \n$n^k$\n.\n\n\nReferences\n\n\n\n\nhttps://en.wikipedia.org/wiki/Permutation\n\n\n\n\nExamples\n\n\nExample 1 - Compute the number of permutations\n\n\nfrom mlxtend.math import num_permutations\n\nc = num_permutations(n=20, k=8, with_replacement=False)\nprint('Number of ways to permute 20 elements'\n      ' into 8 subelements: %d' % c)\n\n\n\n\nNumber of ways to permute 20 elements into 8 subelements: 5079110400\n\n\n\nfrom mlxtend.math import num_permutations\n\nc = num_permutations(n=20, k=8, with_replacement=True)\nprint('Number of ways to combine 20 elements'\n      ' into 8 subelements (with replacement): %d' % c)\n\n\n\n\nNumber of ways to combine 20 elements into 8 subelements (with replacement): 25600000000\n\n\n\nExample 2 - A progress tracking use-case\n\n\nIt is often quite useful to track the progress of a computational expensive tasks to estimate its runtime. Here, the \nnum_combination\n function can be used to compute the maximum number of loops of a \npermutations\n iterable from itertools:\n\n\nimport itertools\nimport sys\nimport time\nfrom mlxtend.math import num_permutations\n\nitems = {1, 2, 3, 4, 5, 6, 7, 8}\nmax_iter = num_permutations(n=len(items), k=3, \n                            with_replacement=False)\n\nfor idx, i in enumerate(itertools.permutations(items, r=3)):\n    # do some computation with itemset i\n    time.sleep(0.01)\n    sys.stdout.write('\\rProgress: %d/%d' % (idx + 1, max_iter))\n    sys.stdout.flush()\n\n\n\n\nProgress: 336/336\n\n\n\nAPI\n\n\nnum_permutations(n, k, with_replacement=False)\n\n\nFunction to calculate the number of possible permutations.\n\n\nParameters\n\n\n\n\n\n\nn\n : \nint\n\n\nTotal number of items.\n\n\n\n\n\n\nk\n : \nint\n\n\nNumber of elements of the target itemset.\n\n\n\n\n\n\nwith_replacement\n : \nbool\n\n\nAllows repeated elements if True.\n\n\n\n\n\n\nReturns\n\n\n\n\n\n\npermut\n : \nint\n\n\nNumber of possible permutations.",
            "title": "Num permutations"
        },
        {
            "location": "/user_guide/math/num_permutations/#compute-the-number-of-permutations",
            "text": "A function to calculate the number of permutations for creating subsequences of  k  elements out of a sequence with  n  elements.   from mlxtend.math import num_permutations",
            "title": "Compute the Number of Permutations"
        },
        {
            "location": "/user_guide/math/num_permutations/#overview",
            "text": "Permutations are selections of items from a collection with regard to the order in which they appear (in contrast to combinations). For example, let's consider a permutation of 3 elements (k=3) from a collection of 5 elements (n=5):    collection: {1, 2, 3, 4, 5}  combination 1a: {1, 3, 5}   combination 1b: {1, 5, 3}  combination 1c: {3, 5, 1}  ...  combination 2: {1, 3, 4}   In the example above the permutations 1a, 1b, and 1c, are the \"same combination\" but distinct permutations -- in combinations, the order does not matter, but in permutation it does matter.  The number of ways to combine elements ( without replacement ) from a collection with size  n  into subsets of size  k  is computed via the binomial coefficient (\" n  choose  k \"):  $$\nk!\\begin{pmatrix} \nn  \\\\\nk \n\\end{pmatrix} = k! \\cdot \\frac{n!}{k!(n-k)!} = \\frac{n!}{(n-k)!}\n$$  To compute the number of permutations  with replacement , we simply need to compute  $n^k$ .",
            "title": "Overview"
        },
        {
            "location": "/user_guide/math/num_permutations/#references",
            "text": "https://en.wikipedia.org/wiki/Permutation",
            "title": "References"
        },
        {
            "location": "/user_guide/math/num_permutations/#examples",
            "text": "",
            "title": "Examples"
        },
        {
            "location": "/user_guide/math/num_permutations/#example-1-compute-the-number-of-permutations",
            "text": "from mlxtend.math import num_permutations\n\nc = num_permutations(n=20, k=8, with_replacement=False)\nprint('Number of ways to permute 20 elements'\n      ' into 8 subelements: %d' % c)  Number of ways to permute 20 elements into 8 subelements: 5079110400  from mlxtend.math import num_permutations\n\nc = num_permutations(n=20, k=8, with_replacement=True)\nprint('Number of ways to combine 20 elements'\n      ' into 8 subelements (with replacement): %d' % c)  Number of ways to combine 20 elements into 8 subelements (with replacement): 25600000000",
            "title": "Example 1 - Compute the number of permutations"
        },
        {
            "location": "/user_guide/math/num_permutations/#example-2-a-progress-tracking-use-case",
            "text": "It is often quite useful to track the progress of a computational expensive tasks to estimate its runtime. Here, the  num_combination  function can be used to compute the maximum number of loops of a  permutations  iterable from itertools:  import itertools\nimport sys\nimport time\nfrom mlxtend.math import num_permutations\n\nitems = {1, 2, 3, 4, 5, 6, 7, 8}\nmax_iter = num_permutations(n=len(items), k=3, \n                            with_replacement=False)\n\nfor idx, i in enumerate(itertools.permutations(items, r=3)):\n    # do some computation with itemset i\n    time.sleep(0.01)\n    sys.stdout.write('\\rProgress: %d/%d' % (idx + 1, max_iter))\n    sys.stdout.flush()  Progress: 336/336",
            "title": "Example 2 - A progress tracking use-case"
        },
        {
            "location": "/user_guide/math/num_permutations/#api",
            "text": "num_permutations(n, k, with_replacement=False)  Function to calculate the number of possible permutations.  Parameters    n  :  int  Total number of items.    k  :  int  Number of elements of the target itemset.    with_replacement  :  bool  Allows repeated elements if True.    Returns    permut  :  int  Number of possible permutations.",
            "title": "API"
        },
        {
            "location": "/user_guide/text/generalize_names/",
            "text": "Generalize Names\n\n\nA function that converts a name into a general format \n<last_name><separator><firstname letter(s)> (all lowercase)\n.\n\n\n\n\nfrom mlxtend.text import generalize_names\n\n\n\n\nOverview\n\n\nA function that converts a name into a general format \n<last_name><separator><firstname letter(s)> (all lowercase)\n, which is useful if data is collected from different sources and is supposed to be compared or merged based on name identifiers. E.g., if names are stored in a pandas \nDataFrame\n column, the apply function can be used to generalize names: \ndf['name'] = df['name'].apply(generalize_names)\n\n\nReferences\n\n\n\n\n-\n\n\n\n\nExamples\n\n\nExample 1 - Defaults\n\n\nfrom mlxtend.text import generalize_names\n\n\n\n\ngeneralize_names('Pozo, Jos\u00e9 \u00c1ngel')\n\n\n\n\n'pozo j'\n\n\n\ngeneralize_names('Jos\u00e9 Pozo')\n\n\n\n\n'pozo j'\n\n\n\ngeneralize_names('Jos\u00e9 \u00c1ngel Pozo')\n\n\n\n\n'pozo j'\n\n\n\nExample 2 - Optional Parameters\n\n\nfrom mlxtend.text import generalize_names\n\n\n\n\ngeneralize_names(\"Eto'o, Samuel\", firstname_output_letters=2)\n\n\n\n\n'etoo sa'\n\n\n\ngeneralize_names(\"Eto'o, Samuel\", firstname_output_letters=0)\n\n\n\n\n'etoo'\n\n\n\ngeneralize_names(\"Eto'o, Samuel\", output_sep=', ')\n\n\n\n\n'etoo, s'\n\n\n\nAPI",
            "title": "Generalize names"
        },
        {
            "location": "/user_guide/text/generalize_names/#generalize-names",
            "text": "A function that converts a name into a general format  <last_name><separator><firstname letter(s)> (all lowercase) .   from mlxtend.text import generalize_names",
            "title": "Generalize Names"
        },
        {
            "location": "/user_guide/text/generalize_names/#overview",
            "text": "A function that converts a name into a general format  <last_name><separator><firstname letter(s)> (all lowercase) , which is useful if data is collected from different sources and is supposed to be compared or merged based on name identifiers. E.g., if names are stored in a pandas  DataFrame  column, the apply function can be used to generalize names:  df['name'] = df['name'].apply(generalize_names)",
            "title": "Overview"
        },
        {
            "location": "/user_guide/text/generalize_names/#references",
            "text": "-",
            "title": "References"
        },
        {
            "location": "/user_guide/text/generalize_names/#examples",
            "text": "",
            "title": "Examples"
        },
        {
            "location": "/user_guide/text/generalize_names/#example-1-defaults",
            "text": "from mlxtend.text import generalize_names  generalize_names('Pozo, Jos\u00e9 \u00c1ngel')  'pozo j'  generalize_names('Jos\u00e9 Pozo')  'pozo j'  generalize_names('Jos\u00e9 \u00c1ngel Pozo')  'pozo j'",
            "title": "Example 1 - Defaults"
        },
        {
            "location": "/user_guide/text/generalize_names/#example-2-optional-parameters",
            "text": "from mlxtend.text import generalize_names  generalize_names(\"Eto'o, Samuel\", firstname_output_letters=2)  'etoo sa'  generalize_names(\"Eto'o, Samuel\", firstname_output_letters=0)  'etoo'  generalize_names(\"Eto'o, Samuel\", output_sep=', ')  'etoo, s'",
            "title": "Example 2 - Optional Parameters"
        },
        {
            "location": "/user_guide/text/generalize_names/#api",
            "text": "",
            "title": "API"
        },
        {
            "location": "/user_guide/text/generalize_names_duplcheck/",
            "text": "Generalize Names & Duplicate Checking\n\n\nA function that converts a name into a general format \n<last_name><separator><firstname letter(s)> (all lowercase)\n in a \npandas DataFrame\n while avoiding duplicate entries.\n\n\n\n\nfrom mlxtend.text import generalize_names_duplcheck\n\n\n\n\nOverview\n\n\nNote\n that using \nmlxtend.text.generalize_names\n with few \nfirstname_output_letters\n can result in duplicate entries. E.g., if your dataset contains the names \"Adam Johnson\" and \"Andrew Johnson\", the default setting (i.e., 1 first name letter) will produce the generalized name \"johnson a\" in both cases.\n\n\nOne solution is to increase the number of first name letters in the output by setting the parameter \nfirstname_output_letters\n to a value larger than 1. \n\n\nAn alternative solution is to use the \ngeneralize_names_duplcheck\n function if you are working with pandas DataFrames. \n\n\nBy default,  \ngeneralize_names_duplcheck\n will apply  \ngeneralize_names\n to a pandas DataFrame column with the minimum number of first name letters and append as many first name letters as necessary until no duplicates are present in the given DataFrame column. An example dataset column that contains the names  \n\n\nReferences\n\n\n\n\n-\n\n\n\n\nExamples\n\n\nExample 1 - Defaults\n\n\nReading in a CSV file that has column \nName\n for which we want to generalize the names:\n\n\n\n\nSamuel Eto'o\n\n\nAdam Johnson\n\n\nAndrew Johnson\n\n\n\n\nimport pandas as pd\nfrom io import StringIO\n\nsimulated_csv = \"name,some_value\\n\"\\\n                \"Samuel Eto'o,1\\n\"\\\n                \"Adam Johnson,1\\n\"\\\n                \"Andrew Johnson,1\\n\"\n\ndf = pd.read_csv(StringIO(simulated_csv))\ndf\n\n\n\n\n\n\n\n  \n\n    \n\n      \n\n      \nname\n\n      \nsome_value\n\n    \n\n  \n\n  \n\n    \n\n      \n0\n\n      \nSamuel Eto'o\n\n      \n1\n\n    \n\n    \n\n      \n1\n\n      \nAdam Johnson\n\n      \n1\n\n    \n\n    \n\n      \n2\n\n      \nAndrew Johnson\n\n      \n1\n\n    \n\n  \n\n\n\n\n\n\n\nApplying \ngeneralize_names_duplcheck\n to generate a new DataFrame with the generalized names without duplicates: \n\n\nfrom mlxtend.text import generalize_names_duplcheck\ndf_new = generalize_names_duplcheck(df=df, col_name='name')\ndf_new\n\n\n\n\n\n\n\n  \n\n    \n\n      \n\n      \nname\n\n      \nsome_value\n\n    \n\n  \n\n  \n\n    \n\n      \n0\n\n      \netoo s\n\n      \n1\n\n    \n\n    \n\n      \n1\n\n      \njohnson ad\n\n      \n1\n\n    \n\n    \n\n      \n2\n\n      \njohnson an\n\n      \n1\n\n    \n\n  \n\n\n\n\n\n\n\nAPI\n\n\ngeneralize_names_duplcheck(df, col_name)\n\n\nGeneralizes names and removes duplicates.\n\n\nApplies mlxtend.text.generalize_names to a DataFrame with 1 first name letter\n    by default and uses more first name letters if duplicates are detected.\n\n\nParameters\n\n\n\n\n\n\ndf\n : \npandas.DataFrame\n\n\nDataFrame that contains a column where generalize_names should be applied.\n\n\n\n\n\n\ncol_name\n : \nstr\n\n\nName of the DataFrame column where \ngeneralize_names\n function should be applied to.\n\n\n\n\n\n\nReturns\n\n\n\n\n\n\ndf_new\n : \nstr\n\n\nNew DataFrame object where generalize_names function has been applied without duplicates.",
            "title": "Generalize names duplcheck"
        },
        {
            "location": "/user_guide/text/generalize_names_duplcheck/#generalize-names-duplicate-checking",
            "text": "A function that converts a name into a general format  <last_name><separator><firstname letter(s)> (all lowercase)  in a  pandas DataFrame  while avoiding duplicate entries.   from mlxtend.text import generalize_names_duplcheck",
            "title": "Generalize Names &amp; Duplicate Checking"
        },
        {
            "location": "/user_guide/text/generalize_names_duplcheck/#overview",
            "text": "Note  that using  mlxtend.text.generalize_names  with few  firstname_output_letters  can result in duplicate entries. E.g., if your dataset contains the names \"Adam Johnson\" and \"Andrew Johnson\", the default setting (i.e., 1 first name letter) will produce the generalized name \"johnson a\" in both cases.  One solution is to increase the number of first name letters in the output by setting the parameter  firstname_output_letters  to a value larger than 1.   An alternative solution is to use the  generalize_names_duplcheck  function if you are working with pandas DataFrames.   By default,   generalize_names_duplcheck  will apply   generalize_names  to a pandas DataFrame column with the minimum number of first name letters and append as many first name letters as necessary until no duplicates are present in the given DataFrame column. An example dataset column that contains the names",
            "title": "Overview"
        },
        {
            "location": "/user_guide/text/generalize_names_duplcheck/#references",
            "text": "-",
            "title": "References"
        },
        {
            "location": "/user_guide/text/generalize_names_duplcheck/#examples",
            "text": "",
            "title": "Examples"
        },
        {
            "location": "/user_guide/text/generalize_names_duplcheck/#example-1-defaults",
            "text": "Reading in a CSV file that has column  Name  for which we want to generalize the names:   Samuel Eto'o  Adam Johnson  Andrew Johnson   import pandas as pd\nfrom io import StringIO\n\nsimulated_csv = \"name,some_value\\n\"\\\n                \"Samuel Eto'o,1\\n\"\\\n                \"Adam Johnson,1\\n\"\\\n                \"Andrew Johnson,1\\n\"\n\ndf = pd.read_csv(StringIO(simulated_csv))\ndf   \n   \n     \n       \n       name \n       some_value \n     \n   \n   \n     \n       0 \n       Samuel Eto'o \n       1 \n     \n     \n       1 \n       Adam Johnson \n       1 \n     \n     \n       2 \n       Andrew Johnson \n       1 \n     \n      Applying  generalize_names_duplcheck  to generate a new DataFrame with the generalized names without duplicates:   from mlxtend.text import generalize_names_duplcheck\ndf_new = generalize_names_duplcheck(df=df, col_name='name')\ndf_new   \n   \n     \n       \n       name \n       some_value \n     \n   \n   \n     \n       0 \n       etoo s \n       1 \n     \n     \n       1 \n       johnson ad \n       1 \n     \n     \n       2 \n       johnson an \n       1",
            "title": "Example 1 - Defaults"
        },
        {
            "location": "/user_guide/text/generalize_names_duplcheck/#api",
            "text": "generalize_names_duplcheck(df, col_name)  Generalizes names and removes duplicates.  Applies mlxtend.text.generalize_names to a DataFrame with 1 first name letter\n    by default and uses more first name letters if duplicates are detected.  Parameters    df  :  pandas.DataFrame  DataFrame that contains a column where generalize_names should be applied.    col_name  :  str  Name of the DataFrame column where  generalize_names  function should be applied to.    Returns    df_new  :  str  New DataFrame object where generalize_names function has been applied without duplicates.",
            "title": "API"
        },
        {
            "location": "/user_guide/text/tokenizer/",
            "text": "Tokenizer\n\n\nDifferent functions to tokenize text.\n\n\n\n\nfrom mlxtend.text import tokenizer_[type]\n\n\n\n\nOverview\n\n\nDifferent functions to tokenize text for natural language processing tasks, for example such as building a bag-of-words model for text classification.\n\n\nReferences\n\n\n\n\n-\n\n\n\n\nExamples\n\n\nExample 1 - Extract Emoticons\n\n\nfrom mlxtend.text import tokenizer_emoticons\n\n\n\n\ntokenizer_emoticons('</a>This :) is :( a test :-)!')\n\n\n\n\n[':)', ':(', ':-)']\n\n\n\nExample 2 - Extract Words and Emoticons\n\n\nfrom mlxtend.text import tokenizer_words_and_emoticons\n\n\n\n\ntokenizer_words_and_emoticons('</a>This :) is :( a test :-)!')\n\n\n\n\n['this', 'is', 'a', 'test', ':)', ':(', ':-)']\n\n\n\nAPI\n\n\ntokenizer_emoticons(text)\n\n\nReturn emoticons from text\n\n\nExample:\n>>> tokenizer_emoticons('</a>This :) is :( a test :-)!')\n[':)', ':(', ':-)']\n\n\n\n\n\ntokenizer_words_and_emoticons(text)\n\n\nConvert text to lowercase words and emoticons.\n\n\nExample:\n>>> tokenizer_words_and_emoticons('</a>This :) is :( a test :-)!')\n['this', 'is', 'a', 'test', ':)', ':(', ':-)']",
            "title": "Tokenizer"
        },
        {
            "location": "/user_guide/text/tokenizer/#tokenizer",
            "text": "Different functions to tokenize text.   from mlxtend.text import tokenizer_[type]",
            "title": "Tokenizer"
        },
        {
            "location": "/user_guide/text/tokenizer/#overview",
            "text": "Different functions to tokenize text for natural language processing tasks, for example such as building a bag-of-words model for text classification.",
            "title": "Overview"
        },
        {
            "location": "/user_guide/text/tokenizer/#references",
            "text": "-",
            "title": "References"
        },
        {
            "location": "/user_guide/text/tokenizer/#examples",
            "text": "",
            "title": "Examples"
        },
        {
            "location": "/user_guide/text/tokenizer/#example-1-extract-emoticons",
            "text": "from mlxtend.text import tokenizer_emoticons  tokenizer_emoticons('</a>This :) is :( a test :-)!')  [':)', ':(', ':-)']",
            "title": "Example 1 - Extract Emoticons"
        },
        {
            "location": "/user_guide/text/tokenizer/#example-2-extract-words-and-emoticons",
            "text": "from mlxtend.text import tokenizer_words_and_emoticons  tokenizer_words_and_emoticons('</a>This :) is :( a test :-)!')  ['this', 'is', 'a', 'test', ':)', ':(', ':-)']",
            "title": "Example 2 - Extract Words and Emoticons"
        },
        {
            "location": "/user_guide/text/tokenizer/#api",
            "text": "tokenizer_emoticons(text)  Return emoticons from text  Example:\n>>> tokenizer_emoticons('</a>This :) is :( a test :-)!')\n[':)', ':(', ':-)']   tokenizer_words_and_emoticons(text)  Convert text to lowercase words and emoticons.  Example:\n>>> tokenizer_words_and_emoticons('</a>This :) is :( a test :-)!')\n['this', 'is', 'a', 'test', ':)', ':(', ':-)']",
            "title": "API"
        },
        {
            "location": "/user_guide/utils/Counter/",
            "text": "Counter\n\n\nA simple progress counter to print the number of iterations and time elapsed in a for-loop execution.\n\n\n\n\nfrom mlxtend.utils import Counter\n\n\n\n\nOverview\n\n\nThe \nCounter\n class implements an object for displaying the number of iterations and time elapsed in a for-loop. Please note that the \nCounter\n was implemented for efficiency; thus, the \nCounter\n offers only very basic functionality in order to avoid relatively expensive evaluations (of if-else statements).\n\n\nReferences\n\n\n\n\n-\n\n\n\n\nExamples\n\n\nExample 1 - Counting the iterations in a for-loop\n\n\nfrom mlxtend.utils import Counter\n\n\n\n\nimport time\n\ncnt = Counter()\nfor i in range(20):\n    # do some computation\n    time.sleep(0.1)\n    cnt.update()\n\n\n\n\n20 iter | 2 sec\n\n\n\nNote that the first number displays the current iteration, and the second number shows the time elapsed after initializing the \nCounter\n.\n\n\nAPI\n\n\nCounter(stderr=False, start_newline=True)\n\n\nClass to display the progress of for-loop iterators.\n\n\nParameters\n\n\n\n\n\n\nstderr\n : bool (default: True)\n\n\nPrints output to sys.stderr if True; uses sys.stdout otherwise.\n\n\n\n\n\n\nstart_newline\n : bool (default: True)\n\n\nPrepends a new line to the counter, which prevents overwriting counters\nif multiple counters are printed in succession.\n\n\n\n\n\n\nAttributes\n\n\n\n\n\n\ncurr_iter\n : int\n\n\nThe current iteration.\n\n\n\n\n\n\nstart_time\n : int\n\n\nThe system's time in seconds when the Counter was initialized.\n\n\n\n\n\n\nMethods\n\n\n\n\n\nupdate()\n\n\nPrint current iteration and time elapsed.",
            "title": "Counter"
        },
        {
            "location": "/user_guide/utils/Counter/#counter",
            "text": "A simple progress counter to print the number of iterations and time elapsed in a for-loop execution.   from mlxtend.utils import Counter",
            "title": "Counter"
        },
        {
            "location": "/user_guide/utils/Counter/#overview",
            "text": "The  Counter  class implements an object for displaying the number of iterations and time elapsed in a for-loop. Please note that the  Counter  was implemented for efficiency; thus, the  Counter  offers only very basic functionality in order to avoid relatively expensive evaluations (of if-else statements).",
            "title": "Overview"
        },
        {
            "location": "/user_guide/utils/Counter/#references",
            "text": "-",
            "title": "References"
        },
        {
            "location": "/user_guide/utils/Counter/#examples",
            "text": "",
            "title": "Examples"
        },
        {
            "location": "/user_guide/utils/Counter/#example-1-counting-the-iterations-in-a-for-loop",
            "text": "from mlxtend.utils import Counter  import time\n\ncnt = Counter()\nfor i in range(20):\n    # do some computation\n    time.sleep(0.1)\n    cnt.update()  20 iter | 2 sec  Note that the first number displays the current iteration, and the second number shows the time elapsed after initializing the  Counter .",
            "title": "Example 1 - Counting the iterations in a for-loop"
        },
        {
            "location": "/user_guide/utils/Counter/#api",
            "text": "Counter(stderr=False, start_newline=True)  Class to display the progress of for-loop iterators.  Parameters    stderr  : bool (default: True)  Prints output to sys.stderr if True; uses sys.stdout otherwise.    start_newline  : bool (default: True)  Prepends a new line to the counter, which prevents overwriting counters\nif multiple counters are printed in succession.    Attributes    curr_iter  : int  The current iteration.    start_time  : int  The system's time in seconds when the Counter was initialized.",
            "title": "API"
        },
        {
            "location": "/user_guide/utils/Counter/#methods",
            "text": "update()  Print current iteration and time elapsed.",
            "title": "Methods"
        },
        {
            "location": "/user_guide/general_concepts/activation-functions/",
            "text": "Activation Functions for Artificial Neural Networks",
            "title": "Activation functions"
        },
        {
            "location": "/user_guide/general_concepts/activation-functions/#activation-functions-for-artificial-neural-networks",
            "text": "",
            "title": "Activation Functions for Artificial Neural Networks"
        },
        {
            "location": "/user_guide/general_concepts/gradient-optimization/",
            "text": "Gradient Descent and Stochastic Gradient Descent\n\n\nGradient Descent (GD) Optimization\n\n\nUsing the Gradient Decent optimization algorithm, the weights are updated incrementally after each epoch (= pass over the training dataset).\n\n\nCompatible cost functions \n$J(\\cdot)$\n\n\n\n\n\n\nSum of squared errors (SSE) [ \nmlxtend.regressor.LinearRegression\n, \nmlxtend.classfier.Adaline\n ]:\n\n$$J(\\mathbf{w}) = \\frac{1}{2} \\sum_i (\\text{target}^{(i)} - \\text{output}^{(i)})^2$$\n\n\n\n\n\n\nLogistic Cost (cross-entropy) [ \nmlxtend.classfier.LogisticRegression\n ]:\n...\n\n\n\n\n\n\nThe magnitude and direction of the weight update is computed by taking a step in the opposite direction of the cost gradient\n\n\n$$\\Delta w_j = -\\eta \\frac{\\partial J}{\\partial w_j},$$\n\n\nwhere \n$\\eta$\n is the learning rate. The weights are then updated after each epoch via the following update rule:\n\n\n$$\\mathbf{w} := \\mathbf{w} + \\Delta\\mathbf{w},$$\n\n\nwhere \n$\\Delta\\mathbf{w}$\n is a vector that contains the weight updates of each weight coefficient \n${w}$\n, which are computed as follows:\n\n\n$$\\Delta w_j = -\\eta \\frac{\\partial J}{\\partial w_j}\\\\\n= -\\eta \\sum_i (\\text{target}^{(i)} - \\text{output}^{(i)})(-x_{j}^{(i)})\\\\\n= \\eta \\sum_i (\\text{target}^{(i)} - \\text{output}^{(i)})x_{j}^{(i)}.$$\n\n\nEssentially, we can picture Gradient Descent optimization as a hiker (the weight coefficient) who wants to climb down a mountain (cost function) into valley (cost minimum), and each step is determined by the steepness of the slope (gradient) and the leg length of the hiker (learning rate). Considering a cost function with only a single weight coefficient, we can illustrate this concept as follows:\n\n\n\n\nStochastic Gradient Descent (SGD)\n\n\nIn Gradient Descent optimization, we compute the cost gradient based on the complete training set; hence, we sometimes also call it \nbatch gradient descent\n. In case of very large datasets, using Gradient Descent can be quite costly since we are only taking a single step for one pass over the training set -- thus, the larger the training set, the slower our algorithm updates the weights and the longer it may take until it converges to the global cost minimum (note that the SSE cost function is convex).\n\n\nIn Stochastic Gradient Descent (sometimes also referred to as \niterative\n or \non-line\n gradient descent), we \ndon't\n accumulate the weight updates as we've seen above for Gradient Descent:\n\n\n\n\nfor one or more epochs:\n\n\nfor each weight \n$j$\n\n\n$w_j := w + \\Delta w_j$\n,   where:   \n$\\Delta w_j= \\eta \\sum_i (\\text{target}^{(i)} - \\text{output}^{(i)})x_{j}^{(i)}$\n\n\n\n\n\n\n\n\n\n\n\n\nInstead, we update the weights after each training sample:\n\n\n\n\nfor one or more epochs, or until approx. cost minimum is reached:\n\n\nfor training sample \n$i$\n:\n\n\nfor each weight \n$j$\n\n\n$w_j := w + \\Delta w_j$\n,   where:   \n$\\Delta w_j= \\eta (\\text{target}^{(i)} - \\text{output}^{(i)})x_{j}^{(i)}$\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nHere, the term \"stochastic\" comes from the fact that the gradient based on a single training sample is a \"stochastic approximation\" of the \"true\" cost gradient. Due to its stochastic nature, the path towards the global cost minimum is not \"direct\" as in Gradient Descent, but may go \"zig-zag\" if we are visuallizing the cost surface in a 2D space. However, it has been shown that Stochastic Gradient Descent almost surely converges to the global cost minimum if the cost function is convex (or pseudo-convex)[1].\n\n\nStochastic Gradient Descent Shuffling\n\n\nThere are several different flavors of stochastic gradient descent, which can be all seen throughout the literature. Let's take a look at the three most common variants:\n\n\nA)\n\n\n\n\nrandomly shuffle samples in the training set\n\n\nfor one or more epochs, or until approx. cost minimum is reached\n\n\nfor training sample \ni\n\n\ncompute gradients and perform weight updates\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nB)\n\n\n\n\nfor one or more epochs, or until approx. cost minimum is reached\n\n\nrandomly shuffle samples in the training set\n\n\nfor training sample \ni\n\n\ncompute gradients and perform weight updates\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nC)\n\n\n\n\nfor iterations \nt\n, or until approx. cost minimum is reached:\n\n\ndraw random sample from the training set\n\n\ncompute gradients and perform weight updates\n\n\n\n\n\n\n\n\n\n\n\n\nIn scenario A [3], we shuffle the training set only one time in the beginning; whereas in scenario B, we shuffle the training set after each epoch to prevent repeating update cycles. In both scenario A and scenario B, each training sample is only used once per epoch to update the model weights.\n\n\nIn scenario C, we draw the training samples randomly with replacement from the training set [2]. If the number of iterations \nt\n is equal to the number of training samples, we learn the model based on a \nbootstrap sample\n of the training set.\n\n\nMini-Batch Gradient Descent (MB-GD)\n\n\nMini-Batch Gradient Descent (MB-GD) a compromise between batch GD and SGD. In MB-GD, we update the model based on smaller groups of training samples; instead of computing the gradient from 1 sample (SGD) or all \nn\n training samples (GD), we compute the gradient from \n$1 < k < n$\n training samples (a common mini-batch size is \n$k=50$\n).\n\n\nMB-GD converges in fewer iterations than GD because we update the weights more frequently; however, MB-GD let's us utilize vectorized operation, which typically results in a computational performance gain over SGD.\n\n\nLearning Rates\n\n\n\n\n\n\nAn adaptive learning rate \n$\\eta$\n: Choosing a decrease constant \nd\n that shrinks the learning rate over time:  \n$\\eta(t+1) := \\eta(t) / (1 + t \\times d)$\n\n\n\n\n\n\nMomentum learning by adding a factor of the previous gradient to the weight update for faster updates: \n$\\Delta \\mathbf{w}_{t+1} := \\eta \\nabla J(\\mathbf{w}_{t+1}) + \\alpha \\Delta {w}_{t}$\n\n\n\n\n\n\nReferences\n\n\n\n\n[1] Bottou, L\u00e9on (1998). \n\"Online Algorithms and Stochastic Approximations\"\n. Online Learning and Neural Networks. Cambridge University Press. ISBN 978-0-521-65263-6\n\n\n[2] Bottou, L\u00e9on. \n\"Large-scale machine learning with stochastic gradient descent.\"\n Proceedings of COMPSTAT'2010. Physica-Verlag HD, 2010. 177-186.\n\n\n[3] Bottou, L\u00e9on. \n\"Stochastic gradient descent tricks.\"\n Neural Networks: Tricks of the Trade. Springer Berlin Heidelberg, 2012. 421-436.",
            "title": "Gradient optimization"
        },
        {
            "location": "/user_guide/general_concepts/gradient-optimization/#gradient-descent-and-stochastic-gradient-descent",
            "text": "",
            "title": "Gradient Descent and Stochastic Gradient Descent"
        },
        {
            "location": "/user_guide/general_concepts/gradient-optimization/#gradient-descent-gd-optimization",
            "text": "Using the Gradient Decent optimization algorithm, the weights are updated incrementally after each epoch (= pass over the training dataset).  Compatible cost functions  $J(\\cdot)$    Sum of squared errors (SSE) [  mlxtend.regressor.LinearRegression ,  mlxtend.classfier.Adaline  ]: $$J(\\mathbf{w}) = \\frac{1}{2} \\sum_i (\\text{target}^{(i)} - \\text{output}^{(i)})^2$$    Logistic Cost (cross-entropy) [  mlxtend.classfier.LogisticRegression  ]:\n...    The magnitude and direction of the weight update is computed by taking a step in the opposite direction of the cost gradient  $$\\Delta w_j = -\\eta \\frac{\\partial J}{\\partial w_j},$$  where  $\\eta$  is the learning rate. The weights are then updated after each epoch via the following update rule:  $$\\mathbf{w} := \\mathbf{w} + \\Delta\\mathbf{w},$$  where  $\\Delta\\mathbf{w}$  is a vector that contains the weight updates of each weight coefficient  ${w}$ , which are computed as follows:  $$\\Delta w_j = -\\eta \\frac{\\partial J}{\\partial w_j}\\\\\n= -\\eta \\sum_i (\\text{target}^{(i)} - \\text{output}^{(i)})(-x_{j}^{(i)})\\\\\n= \\eta \\sum_i (\\text{target}^{(i)} - \\text{output}^{(i)})x_{j}^{(i)}.$$  Essentially, we can picture Gradient Descent optimization as a hiker (the weight coefficient) who wants to climb down a mountain (cost function) into valley (cost minimum), and each step is determined by the steepness of the slope (gradient) and the leg length of the hiker (learning rate). Considering a cost function with only a single weight coefficient, we can illustrate this concept as follows:",
            "title": "Gradient Descent (GD) Optimization"
        },
        {
            "location": "/user_guide/general_concepts/gradient-optimization/#stochastic-gradient-descent-sgd",
            "text": "In Gradient Descent optimization, we compute the cost gradient based on the complete training set; hence, we sometimes also call it  batch gradient descent . In case of very large datasets, using Gradient Descent can be quite costly since we are only taking a single step for one pass over the training set -- thus, the larger the training set, the slower our algorithm updates the weights and the longer it may take until it converges to the global cost minimum (note that the SSE cost function is convex).  In Stochastic Gradient Descent (sometimes also referred to as  iterative  or  on-line  gradient descent), we  don't  accumulate the weight updates as we've seen above for Gradient Descent:   for one or more epochs:  for each weight  $j$  $w_j := w + \\Delta w_j$ ,   where:    $\\Delta w_j= \\eta \\sum_i (\\text{target}^{(i)} - \\text{output}^{(i)})x_{j}^{(i)}$       Instead, we update the weights after each training sample:   for one or more epochs, or until approx. cost minimum is reached:  for training sample  $i$ :  for each weight  $j$  $w_j := w + \\Delta w_j$ ,   where:    $\\Delta w_j= \\eta (\\text{target}^{(i)} - \\text{output}^{(i)})x_{j}^{(i)}$         Here, the term \"stochastic\" comes from the fact that the gradient based on a single training sample is a \"stochastic approximation\" of the \"true\" cost gradient. Due to its stochastic nature, the path towards the global cost minimum is not \"direct\" as in Gradient Descent, but may go \"zig-zag\" if we are visuallizing the cost surface in a 2D space. However, it has been shown that Stochastic Gradient Descent almost surely converges to the global cost minimum if the cost function is convex (or pseudo-convex)[1].",
            "title": "Stochastic Gradient Descent (SGD)"
        },
        {
            "location": "/user_guide/general_concepts/gradient-optimization/#stochastic-gradient-descent-shuffling",
            "text": "There are several different flavors of stochastic gradient descent, which can be all seen throughout the literature. Let's take a look at the three most common variants:",
            "title": "Stochastic Gradient Descent Shuffling"
        },
        {
            "location": "/user_guide/general_concepts/gradient-optimization/#a",
            "text": "randomly shuffle samples in the training set  for one or more epochs, or until approx. cost minimum is reached  for training sample  i  compute gradients and perform weight updates",
            "title": "A)"
        },
        {
            "location": "/user_guide/general_concepts/gradient-optimization/#b",
            "text": "for one or more epochs, or until approx. cost minimum is reached  randomly shuffle samples in the training set  for training sample  i  compute gradients and perform weight updates",
            "title": "B)"
        },
        {
            "location": "/user_guide/general_concepts/gradient-optimization/#c",
            "text": "for iterations  t , or until approx. cost minimum is reached:  draw random sample from the training set  compute gradients and perform weight updates       In scenario A [3], we shuffle the training set only one time in the beginning; whereas in scenario B, we shuffle the training set after each epoch to prevent repeating update cycles. In both scenario A and scenario B, each training sample is only used once per epoch to update the model weights.  In scenario C, we draw the training samples randomly with replacement from the training set [2]. If the number of iterations  t  is equal to the number of training samples, we learn the model based on a  bootstrap sample  of the training set.",
            "title": "C)"
        },
        {
            "location": "/user_guide/general_concepts/gradient-optimization/#mini-batch-gradient-descent-mb-gd",
            "text": "Mini-Batch Gradient Descent (MB-GD) a compromise between batch GD and SGD. In MB-GD, we update the model based on smaller groups of training samples; instead of computing the gradient from 1 sample (SGD) or all  n  training samples (GD), we compute the gradient from  $1 < k < n$  training samples (a common mini-batch size is  $k=50$ ).  MB-GD converges in fewer iterations than GD because we update the weights more frequently; however, MB-GD let's us utilize vectorized operation, which typically results in a computational performance gain over SGD.",
            "title": "Mini-Batch Gradient Descent (MB-GD)"
        },
        {
            "location": "/user_guide/general_concepts/gradient-optimization/#learning-rates",
            "text": "An adaptive learning rate  $\\eta$ : Choosing a decrease constant  d  that shrinks the learning rate over time:   $\\eta(t+1) := \\eta(t) / (1 + t \\times d)$    Momentum learning by adding a factor of the previous gradient to the weight update for faster updates:  $\\Delta \\mathbf{w}_{t+1} := \\eta \\nabla J(\\mathbf{w}_{t+1}) + \\alpha \\Delta {w}_{t}$",
            "title": "Learning Rates"
        },
        {
            "location": "/user_guide/general_concepts/gradient-optimization/#references",
            "text": "[1] Bottou, L\u00e9on (1998).  \"Online Algorithms and Stochastic Approximations\" . Online Learning and Neural Networks. Cambridge University Press. ISBN 978-0-521-65263-6  [2] Bottou, L\u00e9on.  \"Large-scale machine learning with stochastic gradient descent.\"  Proceedings of COMPSTAT'2010. Physica-Verlag HD, 2010. 177-186.  [3] Bottou, L\u00e9on.  \"Stochastic gradient descent tricks.\"  Neural Networks: Tricks of the Trade. Springer Berlin Heidelberg, 2012. 421-436.",
            "title": "References"
        },
        {
            "location": "/user_guide/general_concepts/linear-gradient-derivative/",
            "text": "Deriving the Gradient Descent Rule for Linear Regression and Adaline\n\n\nLinear Regression and Adaptive Linear Neurons (Adalines) are closely related to each other. In fact, the Adaline algorithm is a identical to linear regression except for a threshold function \n$\\phi(\\cdot)_T$\n that converts the continuous output into a categorical class label\n\n\n$$\\phi(z)_T = \\begin{cases} \n      1 & if \\; z \\geq 0 \\\\\n      0 & if \\; z < 0 \n   \\end{cases},$$\n\n\nwhere \n$z$\n is the net input, which is computed as the sum of the input features \n$\\mathbf{x}$\n multiplied by the model weights \n$\\mathbf{w}$\n:\n\n\n$$z = w_0x_0 + w_1x_1 \\dots w_mx_m = \\sum_{j=0}^{m} x_j w_j = \\mathbf{w}^T \\mathbf{x}$$\n\n\n(Note that \n$x_0$\n refers to the bias unit so that \n$x_0=1$\n.)\n\n\nIn the case of linear regression and Adaline, the activation function \n$\\phi(\\cdot)_A$\n is simply the identity function so that \n$\\phi(z)_A = z$\n.\n\n\n\n\nNow, in order to learn the optimal model weights \n$\\mathbf{w}$\n, we need to define a cost function that we can optimize. Here, our cost function \n$J({\\cdot})$\n is the sum of squared errors (SSE), which we multiply by \n$\\frac{1}{2}$\n to make the derivation easier:\n\n\n$$J({\\mathbf{w}}) = \\frac{1}{2} \\sum_i \\big(y^{(i)} - \\phi(z)_{A}^{(i)}\\big)^2,$$\n\n\nwhere \n$y^{(i)}$\n is the label or target label of the \n$i$\nth training point \n$x^{(i)}$\n.\n\n\n(Note that the SSE cost function is convex and therefore differentiable.)\n\n\nIn simple words, we can summarize the gradient descent learning as follows:\n\n\n\n\nInitialize the weights to 0 or small random numbers.\n\n\nFor \n$k$\n epochs (passes over the training set)\n\n\nFor each training sample \n$x^{(i)}$\n\n\nCompute the predicted output value \n$\\hat{y}^{(i)}$\n\n\nCompare \n$\\hat{y}^{(i)}$\n to the actual output \n$y^{(i)}$\n and Compute the \"weight update\" value\n\n\nUpdate the \"weight update\" value\n\n\n\n\n\n\nUpdate the weight coefficients by the accumulated \"weight update\" values\n\n\n\n\n\n\n\n\nWhich we can translate into a more mathematical notation:\n\n\n\n\nInitialize the weights to 0 or small random numbers.\n\n\n\n\nFor \n$k$\n epochs\n\n\n\n\n\n\nFor each training sample \n$x^{(i)}$\n\n\n\n\n$\\phi(z^{(i)})_A = \\hat{y}^{(i)}$\n\n\n$\\Delta w_{(t+1), \\; j} = \\eta (y^{(i)} - \\hat{y}^{(i)}) x_{j}^{(i)}\\;$\n  (where \n$\\eta$\n is the learning rate); \n\n\n$\\Delta w_{j} :=  \\Delta w_j\\; + \\Delta w_{(t+1), \\;j}$\n \n\n\n\n\n\n\n\n\n$\\mathbf{w} := \\mathbf{w} + \\Delta \\mathbf{w}$\n\n\n\n\n\n\n\n\n\n\nPerforming this global weight update\n\n\n$$\\mathbf{w} := \\mathbf{w} + \\Delta \\mathbf{w},$$\n\n\ncan be understood as \"updating the model weights by taking an opposite step towards the cost gradient scaled by the learning rate \n$\\eta$\n\" \n\n\n$$\\Delta \\mathbf{w} = - \\eta \\nabla J(\\mathbf{w}),$$\n\n\nwhere the partial derivative with respect to each \n$w_j$\n can be written as\n\n\n$$\\frac{\\partial J}{\\partial w_j} = - \\sum_i \\big(y^{(i)} - \\phi(z)_{A}^{(i)}\\big) x_{j}^{(i)}.$$\n\n\nTo summarize: in order to use gradient descent to learn the model coefficients, we simply update the weights \n$\\mathbf{w}$\n by taking a step into the opposite direction of the gradient for each pass over the training set -- that's basically it. But how do we get to the equation\n\n\n$$\\frac{\\partial J}{\\partial w_j} = - \\sum_i \\big(y^{(i)} - \\phi(z)_{A}^{(i)}\\big) x_{j}^{(i)}?$$\n\n\nLet's walk through the derivation step by step.\n\n\n$$\\begin{aligned}\n& \\frac{\\partial J}{\\partial w_j} \\\\\n& = \\frac{\\partial}{\\partial w_j} \\frac{1}{2} \\sum_i \\big(y^{(i)} - \\phi(z)_{A}^{(i)}\\big)^2 \\\\\n& = \\frac{1}{2} \\frac{\\partial}{\\partial w_j} \\sum_i \\big(y^{(i)} - \\phi(z)_{A}^{(i)}\\big)^2 \\\\\n& = \\frac{1}{2} \\sum_i  \\big(y^{(i)} - \\phi(z)_{A}^{(i)}\\big) \\frac{\\partial}{\\partial w_j}  \\big(y^{(i)} - \\phi(z)_{A}^{(i)}\\big) \\\\\n& = \\sum_i  \\big(y^{(i)} - \\phi(z)_{A}^{(i)}\\big) \\frac{\\partial}{\\partial w_j} \\bigg(y^{(i)} - \\sum_i \\big(w_{j}^{(i)} x_{j}^{(i)} \\big) \\bigg) \\\\\n& = \\sum_i \\big(y^{(i)} - \\phi(z)_{A}^{(i)}\\big)(-x_{j}^{(i)}) \\\\\n& = - \\sum_i \\big(y^{(i)} - \\phi(z)_{A}^{(i)}\\big)x_{j}^{(i)} \n\\end{aligned}$$",
            "title": "Linear gradient derivative"
        },
        {
            "location": "/user_guide/general_concepts/linear-gradient-derivative/#deriving-the-gradient-descent-rule-for-linear-regression-and-adaline",
            "text": "Linear Regression and Adaptive Linear Neurons (Adalines) are closely related to each other. In fact, the Adaline algorithm is a identical to linear regression except for a threshold function  $\\phi(\\cdot)_T$  that converts the continuous output into a categorical class label  $$\\phi(z)_T = \\begin{cases} \n      1 & if \\; z \\geq 0 \\\\\n      0 & if \\; z < 0 \n   \\end{cases},$$  where  $z$  is the net input, which is computed as the sum of the input features  $\\mathbf{x}$  multiplied by the model weights  $\\mathbf{w}$ :  $$z = w_0x_0 + w_1x_1 \\dots w_mx_m = \\sum_{j=0}^{m} x_j w_j = \\mathbf{w}^T \\mathbf{x}$$  (Note that  $x_0$  refers to the bias unit so that  $x_0=1$ .)  In the case of linear regression and Adaline, the activation function  $\\phi(\\cdot)_A$  is simply the identity function so that  $\\phi(z)_A = z$ .   Now, in order to learn the optimal model weights  $\\mathbf{w}$ , we need to define a cost function that we can optimize. Here, our cost function  $J({\\cdot})$  is the sum of squared errors (SSE), which we multiply by  $\\frac{1}{2}$  to make the derivation easier:  $$J({\\mathbf{w}}) = \\frac{1}{2} \\sum_i \\big(y^{(i)} - \\phi(z)_{A}^{(i)}\\big)^2,$$  where  $y^{(i)}$  is the label or target label of the  $i$ th training point  $x^{(i)}$ .  (Note that the SSE cost function is convex and therefore differentiable.)  In simple words, we can summarize the gradient descent learning as follows:   Initialize the weights to 0 or small random numbers.  For  $k$  epochs (passes over the training set)  For each training sample  $x^{(i)}$  Compute the predicted output value  $\\hat{y}^{(i)}$  Compare  $\\hat{y}^{(i)}$  to the actual output  $y^{(i)}$  and Compute the \"weight update\" value  Update the \"weight update\" value    Update the weight coefficients by the accumulated \"weight update\" values     Which we can translate into a more mathematical notation:   Initialize the weights to 0 or small random numbers.   For  $k$  epochs    For each training sample  $x^{(i)}$   $\\phi(z^{(i)})_A = \\hat{y}^{(i)}$  $\\Delta w_{(t+1), \\; j} = \\eta (y^{(i)} - \\hat{y}^{(i)}) x_{j}^{(i)}\\;$   (where  $\\eta$  is the learning rate);   $\\Delta w_{j} :=  \\Delta w_j\\; + \\Delta w_{(t+1), \\;j}$       $\\mathbf{w} := \\mathbf{w} + \\Delta \\mathbf{w}$      Performing this global weight update  $$\\mathbf{w} := \\mathbf{w} + \\Delta \\mathbf{w},$$  can be understood as \"updating the model weights by taking an opposite step towards the cost gradient scaled by the learning rate  $\\eta$ \"   $$\\Delta \\mathbf{w} = - \\eta \\nabla J(\\mathbf{w}),$$  where the partial derivative with respect to each  $w_j$  can be written as  $$\\frac{\\partial J}{\\partial w_j} = - \\sum_i \\big(y^{(i)} - \\phi(z)_{A}^{(i)}\\big) x_{j}^{(i)}.$$  To summarize: in order to use gradient descent to learn the model coefficients, we simply update the weights  $\\mathbf{w}$  by taking a step into the opposite direction of the gradient for each pass over the training set -- that's basically it. But how do we get to the equation  $$\\frac{\\partial J}{\\partial w_j} = - \\sum_i \\big(y^{(i)} - \\phi(z)_{A}^{(i)}\\big) x_{j}^{(i)}?$$  Let's walk through the derivation step by step.  $$\\begin{aligned}\n& \\frac{\\partial J}{\\partial w_j} \\\\\n& = \\frac{\\partial}{\\partial w_j} \\frac{1}{2} \\sum_i \\big(y^{(i)} - \\phi(z)_{A}^{(i)}\\big)^2 \\\\\n& = \\frac{1}{2} \\frac{\\partial}{\\partial w_j} \\sum_i \\big(y^{(i)} - \\phi(z)_{A}^{(i)}\\big)^2 \\\\\n& = \\frac{1}{2} \\sum_i  \\big(y^{(i)} - \\phi(z)_{A}^{(i)}\\big) \\frac{\\partial}{\\partial w_j}  \\big(y^{(i)} - \\phi(z)_{A}^{(i)}\\big) \\\\\n& = \\sum_i  \\big(y^{(i)} - \\phi(z)_{A}^{(i)}\\big) \\frac{\\partial}{\\partial w_j} \\bigg(y^{(i)} - \\sum_i \\big(w_{j}^{(i)} x_{j}^{(i)} \\big) \\bigg) \\\\\n& = \\sum_i \\big(y^{(i)} - \\phi(z)_{A}^{(i)}\\big)(-x_{j}^{(i)}) \\\\\n& = - \\sum_i \\big(y^{(i)} - \\phi(z)_{A}^{(i)}\\big)x_{j}^{(i)} \n\\end{aligned}$$",
            "title": "Deriving the Gradient Descent Rule for Linear Regression and Adaline"
        },
        {
            "location": "/user_guide/general_concepts/regularization-linear/",
            "text": "Regularization of Generalized Linear Models\n\n\nOverview\n\n\nWe can understand regularization as an approach of adding an additional bias to a model to reduce the degree of overfitting in models that suffer from high variance. By adding regularization terms to the cost function, we penalize large model coefficients (weights); effectively, we are reducing the complexity of the model.\n\n\nL2 regularization\n\n\nIn L2 regularization, we shrink the weights by computing the Euclidean norm of the weight coefficients (the weight vector \n$\\mathbf{w}$\n); \n$\\lambda$\n is the regularization parameter to be optimized.\n\n\n$$L2: \\lambda\\; \\lVert \\mathbf{w} \\lVert_2 = \\lambda \\sum_{j=1}^{m} w_j^2$$\n\n\nFor example, we can regularize the sum of squared errors cost function (SSE) as follows:\n\n$$SSE =  \\sum^{n}_{i=1} \\big(\\text{target}^{(i)} - \\text{output}^{(i)}\\big)^2 + L2$$\n\n\nIntuitively, we can think of regression as an additional penalty term or constraint as shown in the figure below. Without regularization, our objective is to find the global cost minimum. By adding a regularization penalty, our objective becomes to minimize the cost function under the constraint that we have to stay within our \"budget\" (the gray-shaded ball).\n\n\n\n\nIn addition, we can control the regularization strength via the regularization\nparameter \n$\\lambda$\n. The larger the value of \n$\\lambda$\n, the stronger the regularization of the model. The weight coefficients approach 0 when \n$\\lambda$\n goes towards infinity.\n\n\nL1 regularization\n\n\nIn L1 regularization, we shrink the weights using the absolute values of the weight coefficients (the weight vector \n$\\mathbf{w}$\n); \n$\\lambda$\n is the regularization parameter to be optimized.\n\n\n$$L1: \\lambda \\; \\lVert\\mathbf{w}\\rVert_1 = \\lambda \\sum_{j=1}^{m} |w_j|$$\n\n\nFor example, we can regularize the sum of squared errors cost function (SSE) as follows:\n\n$$SSE =  \\sum^{n}_{i=1} \\big(\\text{target}^{(i)} - \\text{output}^{(i)}\\big)^2 + L1$$\n\n\nAt its core, L1-regularization is very similar to L2 regularization. However, instead of a quadratic penalty term as in L2, we penalize the model by the absolute weight coefficients. As we can see in the figure below, our \"budget\" has \"sharp edges,\" which is the geometric interpretation of why the L1 model induces sparsity.\n\n\n\n\nReferences\n\n\n\n\n[1] M. Y. Park and T. Hastie. \n\"L1-regularization path algorithm for generalized linear models\"\n. Journal of the Royal Statistical Society: Series B (Statistical Methodology), 69(4):659\u2013677, 2007.\n\n\n[2] A. Y. Ng. \n\"Feature selection, L1 vs. L2 regularization, and rotational invariance\"\n. In Proceedings of the twenty-first international conference on Machine learning, page 78. ACM, 2004.",
            "title": "Regularization linear"
        },
        {
            "location": "/user_guide/general_concepts/regularization-linear/#regularization-of-generalized-linear-models",
            "text": "",
            "title": "Regularization of Generalized Linear Models"
        },
        {
            "location": "/user_guide/general_concepts/regularization-linear/#overview",
            "text": "We can understand regularization as an approach of adding an additional bias to a model to reduce the degree of overfitting in models that suffer from high variance. By adding regularization terms to the cost function, we penalize large model coefficients (weights); effectively, we are reducing the complexity of the model.",
            "title": "Overview"
        },
        {
            "location": "/user_guide/general_concepts/regularization-linear/#l2-regularization",
            "text": "In L2 regularization, we shrink the weights by computing the Euclidean norm of the weight coefficients (the weight vector  $\\mathbf{w}$ );  $\\lambda$  is the regularization parameter to be optimized.  $$L2: \\lambda\\; \\lVert \\mathbf{w} \\lVert_2 = \\lambda \\sum_{j=1}^{m} w_j^2$$  For example, we can regularize the sum of squared errors cost function (SSE) as follows: $$SSE =  \\sum^{n}_{i=1} \\big(\\text{target}^{(i)} - \\text{output}^{(i)}\\big)^2 + L2$$  Intuitively, we can think of regression as an additional penalty term or constraint as shown in the figure below. Without regularization, our objective is to find the global cost minimum. By adding a regularization penalty, our objective becomes to minimize the cost function under the constraint that we have to stay within our \"budget\" (the gray-shaded ball).   In addition, we can control the regularization strength via the regularization\nparameter  $\\lambda$ . The larger the value of  $\\lambda$ , the stronger the regularization of the model. The weight coefficients approach 0 when  $\\lambda$  goes towards infinity.",
            "title": "L2 regularization"
        },
        {
            "location": "/user_guide/general_concepts/regularization-linear/#l1-regularization",
            "text": "In L1 regularization, we shrink the weights using the absolute values of the weight coefficients (the weight vector  $\\mathbf{w}$ );  $\\lambda$  is the regularization parameter to be optimized.  $$L1: \\lambda \\; \\lVert\\mathbf{w}\\rVert_1 = \\lambda \\sum_{j=1}^{m} |w_j|$$  For example, we can regularize the sum of squared errors cost function (SSE) as follows: $$SSE =  \\sum^{n}_{i=1} \\big(\\text{target}^{(i)} - \\text{output}^{(i)}\\big)^2 + L1$$  At its core, L1-regularization is very similar to L2 regularization. However, instead of a quadratic penalty term as in L2, we penalize the model by the absolute weight coefficients. As we can see in the figure below, our \"budget\" has \"sharp edges,\" which is the geometric interpretation of why the L1 model induces sparsity.",
            "title": "L1 regularization"
        },
        {
            "location": "/user_guide/general_concepts/regularization-linear/#references",
            "text": "[1] M. Y. Park and T. Hastie.  \"L1-regularization path algorithm for generalized linear models\" . Journal of the Royal Statistical Society: Series B (Statistical Methodology), 69(4):659\u2013677, 2007.  [2] A. Y. Ng.  \"Feature selection, L1 vs. L2 regularization, and rotational invariance\" . In Proceedings of the twenty-first international conference on Machine learning, page 78. ACM, 2004.",
            "title": "References"
        },
        {
            "location": "/api_subpackages/mlxtend.classifier/",
            "text": "mlxtend version: 0.4.2dev0 \n\n\nAdaline\n\n\nAdaline(eta=0.01, epochs=50, minibatches=None, random_seed=None, print_progress=0)\n\n\nADAptive LInear NEuron classifier.\n\n\nNote that this implementation of Adaline expects binary class labels\nin {0, 1}.\n\n\n\nParameters\n\n\n\n\n\n\neta\n : float (default: 0.01)\n\n\nsolver rate (between 0.0 and 1.0)\n\n\n\n\n\n\nepochs\n : int (default: 50)\n\n\nPasses over the training dataset.\nPrior to each epoch, the dataset is shuffled\nif \nminibatches > 1\n to prevent cycles in stochastic gradient descent.\n\n\n\n\n\n\nminibatches\n : int (default: None)\n\n\nThe number of minibatches for gradient-based optimization.\nIf None: Normal Equations (closed-form solution)\nIf 1: Gradient Descent learning\nIf len(y): Stochastic Gradient Descent (SGD) online learning\nIf 1 < minibatches < len(y): SGD Minibatch learning\n\n\n\n\n\n\nrandom_seed\n : int (default: None)\n\n\nSet random state for shuffling and initializing the weights.\n\n\n\n\n\n\nprint_progress\n : int (default: 0)\n\n\nPrints progress in fitting to stderr if not solver='normal equation'\n0: No output\n1: Epochs elapsed and cost\n2: 1 plus time elapsed\n3: 2 plus estimated time until completion\n\n\n\n\n\n\nAttributes\n\n\n\n\n\n\nw_\n : 2d-array, shape={n_features, 1}\n\n\nModel weights after fitting.\n\n\n\n\n\n\nb_\n : 1d-array, shape={1,}\n\n\nBias unit after fitting.\n\n\n\n\n\n\ncost_\n : list\n\n\nSum of squared errors after each epoch.\n\n\n\n\n\n\nMethods\n\n\n\n\n\nfit(X, y, init_params=True)\n\n\nLearn model from training data.\n\n\nParameters\n\n\n\n\n\n\nX\n : {array-like, sparse matrix}, shape = [n_samples, n_features]\n\n\nTraining vectors, where n_samples is the number of samples and\nn_features is the number of features.\n\n\n\n\n\n\ny\n : array-like, shape = [n_samples]\n\n\nTarget values.\n\n\n\n\n\n\ninit_params\n : bool (default: True)\n\n\nRe-initializes model parameters prior to fitting.\nSet False to continue training with weights from\na previous model fitting.\n\n\n\n\n\n\nReturns\n\n\n\n\nself\n : object\n\n\n\n\n\n\n\npredict(X)\n\n\nPredict targets from X.\n\n\nParameters\n\n\n\n\n\n\nX\n : {array-like, sparse matrix}, shape = [n_samples, n_features]\n\n\nTraining vectors, where n_samples is the number of samples and\nn_features is the number of features.\n\n\n\n\n\n\nReturns\n\n\n\n\n\n\ntarget_values\n : array-like, shape = [n_samples]\n\n\nPredicted target values.\n\n\n\n\n\n\n\n\n\nscore(X, y)\n\n\nCompute the prediction accuracy\n\n\nParameters\n\n\n\n\n\n\nX\n : {array-like, sparse matrix}, shape = [n_samples, n_features]\n\n\nTraining vectors, where n_samples is the number of samples and\nn_features is the number of features.\n\n\n\n\n\n\ny\n : array-like, shape = [n_samples]\n\n\nTarget values (true class labels).\n\n\n\n\n\n\nReturns\n\n\n\n\n\n\nacc\n : float\n\n\nThe prediction accuracy as a float\nbetween 0.0 and 1.0 (perfect score).\n\n\n\n\n\n\nEnsembleVoteClassifier\n\n\nEnsembleVoteClassifier(clfs, voting='hard', weights=None, verbose=0)\n\n\nSoft Voting/Majority Rule classifier for scikit-learn estimators.\n\n\nParameters\n\n\n\n\n\n\nclfs\n : array-like, shape = [n_classifiers]\n\n\nA list of classifiers.\nInvoking the \nfit\n method on the \nVotingClassifier\n will fit clones\nof those original classifiers that will\nbe stored in the class attribute\n\nself.clfs_\n.\n\n\n\n\n\n\nvoting\n : str, {'hard', 'soft'} (default='hard')\n\n\nIf 'hard', uses predicted class labels for majority rule voting.\nElse if 'soft', predicts the class label based on the argmax of\nthe sums of the predicted probalities, which is recommended for\nan ensemble of well-calibrated classifiers.\n\n\n\n\n\n\nweights\n : array-like, shape = [n_classifiers], optional (default=\nNone\n)\n\n\nSequence of weights (\nfloat\n or \nint\n) to weight the occurances of\npredicted class labels (\nhard\n voting) or class probabilities\nbefore averaging (\nsoft\n voting). Uses uniform weights if \nNone\n.\n\n\n\n\n\n\nverbose\n : int, optional (default=0)\n\n\nControls the verbosity of the building process.\n- \nverbose=0\n (default): Prints nothing\n- \nverbose=1\n: Prints the number & name of the clf being fitted\n- \nverbose=2\n: Prints info about the parameters of the clf being fitted\n- \nverbose>2\n: Changes \nverbose\n param of the underlying clf to\nself.verbose - 2\n\n\n\n\n\n\nAttributes\n\n\n\n\n\n\nclasses_\n : array-like, shape = [n_predictions]\n\n\n\n\n\n\nclf\n : array-like, shape = [n_predictions]\n\n\nThe unmodified input classifiers\n\n\n\n\n\n\nclf_\n : array-like, shape = [n_predictions]\n\n\nFitted clones of the input classifiers\n\n\n\n\n\n\nExamples\n\n\n>>> import numpy as np\n>>> from sklearn.linear_model import LogisticRegression\n>>> from sklearn.naive_bayes import GaussianNB\n>>> from sklearn.ensemble import RandomForestClassifier\n>>> from mlxtend.sklearn import EnsembleVoteClassifier\n>>> clf1 = LogisticRegression(random_seed=1)\n>>> clf2 = RandomForestClassifier(random_seed=1)\n>>> clf3 = GaussianNB()\n>>> X = np.array([[-1, -1], [-2, -1], [-3, -2], [1, 1], [2, 1], [3, 2]])\n>>> y = np.array([1, 1, 1, 2, 2, 2])\n>>> eclf1 = EnsembleVoteClassifier(clfs=[clf1, clf2, clf3],\n... voting='hard', verbose=1)\n>>> eclf1 = eclf1.fit(X, y)\n>>> print(eclf1.predict(X))\n[1 1 1 2 2 2]\n>>> eclf2 = EnsembleVoteClassifier(clfs=[clf1, clf2, clf3], voting='soft')\n>>> eclf2 = eclf2.fit(X, y)\n>>> print(eclf2.predict(X))\n[1 1 1 2 2 2]\n>>> eclf3 = EnsembleVoteClassifier(clfs=[clf1, clf2, clf3],\n...                          voting='soft', weights=[2,1,1])\n>>> eclf3 = eclf3.fit(X, y)\n>>> print(eclf3.predict(X))\n[1 1 1 2 2 2]\n>>>\n\n\n\nMethods\n\n\n\n\n\nfit(X, y)\n\n\nLearn weight coefficients from training data for each classifier.\n\n\nParameters\n\n\n\n\n\n\nX\n : {array-like, sparse matrix}, shape = [n_samples, n_features]\n\n\nTraining vectors, where n_samples is the number of samples and\nn_features is the number of features.\n\n\n\n\n\n\ny\n : array-like, shape = [n_samples]\n\n\nTarget values.\n\n\n\n\n\n\nReturns\n\n\n\n\nself\n : object\n\n\n\n\n\n\n\nfit_transform(X, y=None, \nfit_params)\n\n\nFit to data, then transform it.\n\n\nFits transformer to X and y with optional parameters fit_params\nand returns a transformed version of X.\n\n\n\nParameters\n\n\n\n\n\n\nX\n : numpy array of shape [n_samples, n_features]\n\n\nTraining set.\n\n\n\n\n\n\ny\n : numpy array of shape [n_samples]\n\n\nTarget values.\n\n\n\n\n\n\nReturns\n\n\n\n\n\n\nX_new\n : numpy array of shape [n_samples, n_features_new]\n\n\nTransformed array.\n\n\n\n\n\n\n\n\n\nget_params(deep=True)\n\n\nReturn estimator parameter names for GridSearch support.\n\n\n\n\n\npredict(X)\n\n\nPredict class labels for X.\n\n\nParameters\n\n\n\n\n\n\nX\n : {array-like, sparse matrix}, shape = [n_samples, n_features]\n\n\nTraining vectors, where n_samples is the number of samples and\nn_features is the number of features.\n\n\n\n\n\n\nReturns\n\n\n\n\n\n\nmaj\n : array-like, shape = [n_samples]\n\n\nPredicted class labels.\n\n\n\n\n\n\n\n\n\npredict_proba(X)\n\n\nPredict class probabilities for X.\n\n\nParameters\n\n\n\n\n\n\nX\n : {array-like, sparse matrix}, shape = [n_samples, n_features]\n\n\nTraining vectors, where n_samples is the number of samples and\nn_features is the number of features.\n\n\n\n\n\n\nReturns\n\n\n\n\n\n\navg\n : array-like, shape = [n_samples, n_classes]\n\n\nWeighted average probability for each class per sample.\n\n\n\n\n\n\n\n\n\nscore(X, y, sample_weight=None)\n\n\nReturns the mean accuracy on the given test data and labels.\n\n\nIn multi-label classification, this is the subset accuracy\nwhich is a harsh metric since you require for each sample that\neach label set be correctly predicted.\n\n\n\nParameters\n\n\n\n\n\n\nX\n : array-like, shape = (n_samples, n_features)\n\n\nTest samples.\n\n\n\n\n\n\ny\n : array-like, shape = (n_samples) or (n_samples, n_outputs)\n\n\nTrue labels for X.\n\n\n\n\n\n\nsample_weight\n : array-like, shape = [n_samples], optional\n\n\nSample weights.\n\n\n\n\n\n\nReturns\n\n\n\n\n\n\nscore\n : float\n\n\nMean accuracy of self.predict(X) wrt. y.\n\n\n\n\n\n\n\n\n\nset_params(\nparams)\n\n\nSet the parameters of this estimator.\n\n\nThe method works on simple estimators as well as on nested objects\n(such as pipelines). The former have parameters of the form\n``<component>__<parameter>`` so that it's possible to update each\ncomponent of a nested object.\n\n\n\nReturns\n\n\nself\n\n\n\n\n\ntransform(X)\n\n\nReturn class labels or probabilities for X for each estimator.\n\n\nParameters\n\n\n\n\n\n\nX\n : {array-like, sparse matrix}, shape = [n_samples, n_features]\n\n\nTraining vectors, where n_samples is the number of samples and\nn_features is the number of features.\n\n\n\n\n\n\nReturns\n\n\n\n\n\n\nIf\nvoting='soft'`` : array-like = [n_classifiers, n_samples, n_classes]\n\n\nClass probabilties calculated by each classifier.\n\n\n\n\n\n\nIf\nvoting='hard'`` : array-like = [n_classifiers, n_samples]\n\n\nClass labels predicted by each classifier.\n\n\n\n\n\n\nLogisticRegression\n\n\nLogisticRegression(eta=0.01, epochs=50, l2_lambda=0.0, minibatches=1, random_seed=None, print_progress=0)\n\n\nLogistic regression classifier.\n\n\nNote that this implementation of Logistic Regression\nexpects binary class labels in {0, 1}.\n\n\n\nParameters\n\n\n\n\n\n\neta\n : float (default: 0.01)\n\n\nLearning rate (between 0.0 and 1.0)\n\n\n\n\n\n\nepochs\n : int (default: 50)\n\n\nPasses over the training dataset.\nPrior to each epoch, the dataset is shuffled\nif \nminibatches > 1\n to prevent cycles in stochastic gradient descent.\n\n\n\n\n\n\nl2_lambda\n : float\n\n\nRegularization parameter for L2 regularization.\nNo regularization if l2_lambda=0.0.\n\n\n\n\n\n\nminibatches\n : int (default: 1)\n\n\nThe number of minibatches for gradient-based optimization.\nIf 1: Gradient Descent learning\nIf len(y): Stochastic Gradient Descent (SGD) online learning\nIf 1 < minibatches < len(y): SGD Minibatch learning\n\n\n\n\n\n\nrandom_seed\n : int (default: None)\n\n\nSet random state for shuffling and initializing the weights.\n\n\n\n\n\n\nprint_progress\n : int (default: 0)\n\n\nPrints progress in fitting to stderr.\n0: No output\n1: Epochs elapsed and cost\n2: 1 plus time elapsed\n3: 2 plus estimated time until completion\n\n\n\n\n\n\nAttributes\n\n\n\n\n\n\nw_\n : 2d-array, shape={n_features, 1}\n\n\nModel weights after fitting.\n\n\n\n\n\n\nb_\n : 1d-array, shape={1,}\n\n\nBias unit after fitting.\n\n\n\n\n\n\ncost_\n : list\n\n\nList of floats with cross_entropy cost (sgd or gd) for every\nepoch.\n\n\n\n\n\n\nMethods\n\n\n\n\n\nfit(X, y, init_params=True)\n\n\nLearn model from training data.\n\n\nParameters\n\n\n\n\n\n\nX\n : {array-like, sparse matrix}, shape = [n_samples, n_features]\n\n\nTraining vectors, where n_samples is the number of samples and\nn_features is the number of features.\n\n\n\n\n\n\ny\n : array-like, shape = [n_samples]\n\n\nTarget values.\n\n\n\n\n\n\ninit_params\n : bool (default: True)\n\n\nRe-initializes model parameters prior to fitting.\nSet False to continue training with weights from\na previous model fitting.\n\n\n\n\n\n\nReturns\n\n\n\n\nself\n : object\n\n\n\n\n\n\n\npredict(X)\n\n\nPredict targets from X.\n\n\nParameters\n\n\n\n\n\n\nX\n : {array-like, sparse matrix}, shape = [n_samples, n_features]\n\n\nTraining vectors, where n_samples is the number of samples and\nn_features is the number of features.\n\n\n\n\n\n\nReturns\n\n\n\n\n\n\ntarget_values\n : array-like, shape = [n_samples]\n\n\nPredicted target values.\n\n\n\n\n\n\n\n\n\npredict_proba(X)\n\n\nPredict class probabilities of X from the net input.\n\n\nParameters\n\n\n\n\n\n\nX\n : {array-like, sparse matrix}, shape = [n_samples, n_features]\n\n\nTraining vectors, where n_samples is the number of samples and\nn_features is the number of features.\n\n\n\n\n\n\nReturns\n\n\n\n\nClass 1 probability\n : float\n\n\n\n\n\n\n\nscore(X, y)\n\n\nCompute the prediction accuracy\n\n\nParameters\n\n\n\n\n\n\nX\n : {array-like, sparse matrix}, shape = [n_samples, n_features]\n\n\nTraining vectors, where n_samples is the number of samples and\nn_features is the number of features.\n\n\n\n\n\n\ny\n : array-like, shape = [n_samples]\n\n\nTarget values (true class labels).\n\n\n\n\n\n\nReturns\n\n\n\n\n\n\nacc\n : float\n\n\nThe prediction accuracy as a float\nbetween 0.0 and 1.0 (perfect score).\n\n\n\n\n\n\nMultiLayerPerceptron\n\n\nMultiLayerPerceptron(eta=0.5, epochs=50, hidden_layers=[50], n_classes=None, momentum=0.0, l1=0.0, l2=0.0, dropout=1.0, decrease_const=0.0, minibatches=1, random_seed=None, print_progress=0)\n\n\nMulti-layer perceptron classifier with logistic sigmoid activations\n\n\nParameters\n\n\n\n\n\n\neta\n : float (default: 0.5)\n\n\nLearning rate (between 0.0 and 1.0)\n\n\n\n\n\n\nepochs\n : int (default: 50)\n\n\nPasses over the training dataset.\nPrior to each epoch, the dataset is shuffled\nif \nminibatches > 1\n to prevent cycles in stochastic gradient descent.\n\n\n\n\n\n\nhidden_layers\n : list (default: [50])\n\n\nNumber of units per hidden layer. By default 50 units in the\nfirst hidden layer. At the moment only 1 hidden layer is supported\n\n\n\n\n\n\nn_classes\n : int (default: None)\n\n\nA positive integer to declare the number of class labels\nif not all class labels are present in a partial training set.\nGets the number of class labels automatically if None.\n\n\n\n\n\n\nl1\n : float (default: 0.0)\n\n\nL1 regularization strength\n\n\n\n\n\n\nl2\n : float (default: 0.0)\n\n\nL2 regularization strength\n\n\n\n\n\n\nmomentum\n : float (default: 0.0)\n\n\nMomentum constant. Factor multiplied with the\ngradient of the previous epoch t-1 to improve\nlearning speed\nw(t) := w(t) - (grad(t) + momentum * grad(t-1))\n\n\n\n\n\n\ndecrease_const\n : float (default: 0.0)\n\n\nDecrease constant. Shrinks the learning rate\nafter each epoch via eta / (1 + epoch*decrease_const)\n\n\n\n\n\n\nminibatches\n : int (default: 1)\n\n\nDivide the training data into \nk\n minibatches\nfor accelerated stochastic gradient descent learning.\nGradient Descent Learning if \nminibatches\n = 1\nStochastic Gradient Descent learning if \nminibatches\n = len(y)\nMinibatch learning if \nminibatches\n > 1\n\n\n\n\n\n\nrandom_seed\n : int (default: None)\n\n\nSet random state for shuffling and initializing the weights.\n\n\n\n\n\n\nprint_progress\n : int (default: 0)\n\n\nPrints progress in fitting to stderr.\n0: No output\n1: Epochs elapsed and cost\n2: 1 plus time elapsed\n3: 2 plus estimated time until completion\n\n\n\n\n\n\nAttributes\n\n\n\n\n\n\nw_\n : 2d-array, shape=[n_features, n_classes]\n\n\nWeights after fitting.\n\n\n\n\n\n\nb_\n : 1D-array, shape=[n_classes]\n\n\nBias units after fitting.\n\n\n\n\n\n\ncost_\n : list\n\n\nList of floats; the mean categorical cross entropy\ncost after each epoch.\n\n\n\n\n\n\nMethods\n\n\n\n\n\nfit(X, y, init_params=True)\n\n\nLearn model from training data.\n\n\nParameters\n\n\n\n\n\n\nX\n : {array-like, sparse matrix}, shape = [n_samples, n_features]\n\n\nTraining vectors, where n_samples is the number of samples and\nn_features is the number of features.\n\n\n\n\n\n\ny\n : array-like, shape = [n_samples]\n\n\nTarget values.\n\n\n\n\n\n\ninit_params\n : bool (default: True)\n\n\nRe-initializes model parameters prior to fitting.\nSet False to continue training with weights from\na previous model fitting.\n\n\n\n\n\n\nReturns\n\n\n\n\nself\n : object\n\n\n\n\n\n\n\npredict(X)\n\n\nPredict targets from X.\n\n\nParameters\n\n\n\n\n\n\nX\n : {array-like, sparse matrix}, shape = [n_samples, n_features]\n\n\nTraining vectors, where n_samples is the number of samples and\nn_features is the number of features.\n\n\n\n\n\n\nReturns\n\n\n\n\n\n\ntarget_values\n : array-like, shape = [n_samples]\n\n\nPredicted target values.\n\n\n\n\n\n\n\n\n\npredict_proba(X)\n\n\nPredict class probabilities of X from the net input.\n\n\nParameters\n\n\n\n\n\n\nX\n : {array-like, sparse matrix}, shape = [n_samples, n_features]\n\n\nTraining vectors, where n_samples is the number of samples and\nn_features is the number of features.\n\n\n\n\n\n\nReturns\n\n\n\n\nClass probabilties\n : array-like, shape= [n_samples, n_classes]\n\n\n\n\n\n\n\nscore(X, y)\n\n\nCompute the prediction accuracy\n\n\nParameters\n\n\n\n\n\n\nX\n : {array-like, sparse matrix}, shape = [n_samples, n_features]\n\n\nTraining vectors, where n_samples is the number of samples and\nn_features is the number of features.\n\n\n\n\n\n\ny\n : array-like, shape = [n_samples]\n\n\nTarget values (true class labels).\n\n\n\n\n\n\nReturns\n\n\n\n\n\n\nacc\n : float\n\n\nThe prediction accuracy as a float\nbetween 0.0 and 1.0 (perfect score).\n\n\n\n\n\n\nNeuralNetMLP\n\n\nNeuralNetMLP(n_output, n_features, n_hidden=30, l1=0.0, l2=0.0, epochs=500, eta=0.001, alpha=0.0, decrease_const=0.0, shuffle_init=True, shuffle_epoch=True, minibatches=1, zero_init_weight=False, random_seed=None, print_progress=0)\n\n\nFeedforward neural network / Multi-layer perceptron classifier.\n\n\nParameters\n\n\n\n\n\n\nn_output\n : int\n\n\nNumber of output units, should be equal to the\nnumber of unique class labels.\n\n\n\n\n\n\nn_features\n : int\n\n\nNumber of features (dimensions) in the target dataset.\nShould be equal to the number of columns in the X array.\n\n\n\n\n\n\nn_hidden\n : int (default: 30)\n\n\nNumber of hidden units.\n\n\n\n\n\n\nl1\n : float (default: 0.0)\n\n\nLambda value for L1-regularization.\nNo regularization if l1=0.0 (default)\n\n\n\n\n\n\nl2\n : float (default: 0.0)\n\n\nLambda value for L2-regularization.\nNo regularization if l2=0.0 (default)\n\n\n\n\n\n\nepochs\n : int (default: 500)\n\n\nNumber of passes over the training set.\n\n\n\n\n\n\neta\n : float (default: 0.001)\n\n\nLearning rate.\n\n\n\n\n\n\nalpha\n : float (default: 0.0)\n\n\nMomentum constant. Factor multiplied with the\ngradient of the previous epoch t-1 to improve\nlearning speed\nw(t) := w(t) - (grad(t) + alpha*grad(t-1))\n\n\n\n\n\n\ndecrease_const\n : float (default: 0.0)\n\n\nDecrease constant. Shrinks the learning rate\nafter each epoch via eta / (1 + epoch*decrease_const)\n\n\n\n\n\n\nrandom_weights\n : list (default: [-1.0, 1.0])\n\n\nMin and max values for initializing the random weights.\nInitializes weights to 0 if None or False.\n\n\n\n\n\n\nshuffle_init\n : bool (default: True)\n\n\nShuffles (a copy of the) training data before training.\n\n\n\n\n\n\nshuffle_epoch\n : bool (default: True)\n\n\nShuffles training data before every epoch if True to prevent circles.\n\n\n\n\n\n\nminibatches\n : int (default: 1)\n\n\nDivides training data into k minibatches for efficiency.\nNormal gradient descent learning if k=1 (default).\n\n\n\n\n\n\nrandom_seed\n : int (default: None)\n\n\nSet random seed for shuffling and initializing the weights.\n\n\n\n\n\n\nzero_init_weight\n : bool (default: False)\n\n\nIf True, weights are initialized to zero instead of small random\nnumbers following a standard normal distribution with mean=0 and\nstddev=1.\n\n\n\n\n\n\nprint_progress\n : int (default: 0)\n\n\nPrints progress in fitting to stderr.\n0: No output\n1: Epochs elapsed and cost\n2: 1 plus time elapsed\n3: 2 plus estimated time until completion\n\n\n\n\n\n\nAttributes\n\n\n\n\n\n\ncost_\n : list\n\n\nSum of squared errors after each epoch.\n\n\n\n\n\n\nMethods\n\n\n\n\n\nfit(X, y)\n\n\nLearn weight coefficients from training data.\n\n\nParameters\n\n\n\n\n\n\nX\n : array, shape = [n_samples, n_features]\n\n\nInput layer with original features.\n\n\n\n\n\n\ny\n : array, shape = [n_samples]\n\n\nTarget class labels.\n\n\n\n\n\n\nReturns:\n\n\nself\n\n\n\n\n\npredict(X)\n\n\nPredict class labels of X.\n\n\nParameters\n\n\n\n\n\n\nX\n : {array-like, sparse matrix}, shape = [n_samples, n_features]\n\n\nTraining vectors, where n_samples is the number of samples and\nn_features is the number of features.\n\n\n\n\n\n\nReturns\n\n\n\n\n\n\nclass_labels\n : array-like, shape = [n_samples]\n\n\nPredicted class labels.\n\n\n\n\n\n\n\n\n\nscore(X, y)\n\n\nCompute the prediction accuracy\n\n\nParameters\n\n\n\n\n\n\nX\n : {array-like, sparse matrix}, shape = [n_samples, n_features]\n\n\nTraining vectors, where n_samples is the number of samples and\nn_features is the number of features.\n\n\n\n\n\n\ny\n : array-like, shape = [n_samples]\n\n\nTarget values (true class labels).\n\n\n\n\n\n\nReturns\n\n\n\n\n\n\nacc\n : float\n\n\nThe prediction accuracy as a float\nbetween 0.0 and 1.0 (perfect score).\n\n\n\n\n\n\nPerceptron\n\n\nPerceptron(eta=0.1, epochs=50, random_seed=None, print_progress=0)\n\n\nPerceptron classifier.\n\n\nNote that this implementation of the Perceptron expects binary class labels\nin {0, 1}.\n\n\n\nParameters\n\n\n\n\n\n\neta\n : float (default: 0.1)\n\n\nLearning rate (between 0.0 and 1.0)\n\n\n\n\n\n\nepochs\n : int (default: 50)\n\n\nNumber of passes over the training dataset.\nPrior to each epoch, the dataset is shuffled to prevent cycles.\n\n\n\n\n\n\nrandom_seed\n : int\n\n\nRandom state for initializing random weights and shuffling.\n\n\n\n\n\n\nprint_progress\n : int (default: 0)\n\n\nPrints progress in fitting to stderr.\n0: No output\n1: Epochs elapsed and cost\n2: 1 plus time elapsed\n3: 2 plus estimated time until completion\n\n\n\n\n\n\nAttributes\n\n\n\n\n\n\nw_\n : 2d-array, shape={n_features, 1}\n\n\nModel weights after fitting.\n\n\n\n\n\n\nb_\n : 1d-array, shape={1,}\n\n\nBias unit after fitting.\n\n\n\n\n\n\ncost_\n : list\n\n\nNumber of misclassifications in every epoch.\n\n\n\n\n\n\nMethods\n\n\n\n\n\nfit(X, y, init_params=True)\n\n\nLearn model from training data.\n\n\nParameters\n\n\n\n\n\n\nX\n : {array-like, sparse matrix}, shape = [n_samples, n_features]\n\n\nTraining vectors, where n_samples is the number of samples and\nn_features is the number of features.\n\n\n\n\n\n\ny\n : array-like, shape = [n_samples]\n\n\nTarget values.\n\n\n\n\n\n\ninit_params\n : bool (default: True)\n\n\nRe-initializes model parameters prior to fitting.\nSet False to continue training with weights from\na previous model fitting.\n\n\n\n\n\n\nReturns\n\n\n\n\nself\n : object\n\n\n\n\n\n\n\npredict(X)\n\n\nPredict targets from X.\n\n\nParameters\n\n\n\n\n\n\nX\n : {array-like, sparse matrix}, shape = [n_samples, n_features]\n\n\nTraining vectors, where n_samples is the number of samples and\nn_features is the number of features.\n\n\n\n\n\n\nReturns\n\n\n\n\n\n\ntarget_values\n : array-like, shape = [n_samples]\n\n\nPredicted target values.\n\n\n\n\n\n\n\n\n\nscore(X, y)\n\n\nCompute the prediction accuracy\n\n\nParameters\n\n\n\n\n\n\nX\n : {array-like, sparse matrix}, shape = [n_samples, n_features]\n\n\nTraining vectors, where n_samples is the number of samples and\nn_features is the number of features.\n\n\n\n\n\n\ny\n : array-like, shape = [n_samples]\n\n\nTarget values (true class labels).\n\n\n\n\n\n\nReturns\n\n\n\n\n\n\nacc\n : float\n\n\nThe prediction accuracy as a float\nbetween 0.0 and 1.0 (perfect score).\n\n\n\n\n\n\nSoftmaxRegression\n\n\nSoftmaxRegression(eta=0.01, epochs=50, l2=0.0, minibatches=1, n_classes=None, random_seed=None, print_progress=0)\n\n\nSoftmax regression classifier.\n\n\nParameters\n\n\n\n\n\n\neta\n : float (default: 0.01)\n\n\nLearning rate (between 0.0 and 1.0)\n\n\n\n\n\n\nepochs\n : int (default: 50)\n\n\nPasses over the training dataset.\nPrior to each epoch, the dataset is shuffled\nif \nminibatches > 1\n to prevent cycles in stochastic gradient descent.\n\n\n\n\n\n\nl2\n : float\n\n\nRegularization parameter for L2 regularization.\nNo regularization if l2=0.0.\n\n\n\n\n\n\nminibatches\n : int (default: 1)\n\n\nThe number of minibatches for gradient-based optimization.\nIf 1: Gradient Descent learning\nIf len(y): Stochastic Gradient Descent (SGD) online learning\nIf 1 < minibatches < len(y): SGD Minibatch learning\n\n\n\n\n\n\nn_classes\n : int (default: None)\n\n\nA positive integer to declare the number of class labels\nif not all class labels are present in a partial training set.\nGets the number of class labels automatically if None.\n\n\n\n\n\n\nrandom_seed\n : int (default: None)\n\n\nSet random state for shuffling and initializing the weights.\n\n\n\n\n\n\nprint_progress\n : int (default: 0)\n\n\nPrints progress in fitting to stderr.\n0: No output\n1: Epochs elapsed and cost\n2: 1 plus time elapsed\n3: 2 plus estimated time until completion\n\n\n\n\n\n\nAttributes\n\n\n\n\n\n\nw_\n : 2d-array, shape={n_features, 1}\n\n\nModel weights after fitting.\n\n\n\n\n\n\nb_\n : 1d-array, shape={1,}\n\n\nBias unit after fitting.\n\n\n\n\n\n\ncost_\n : list\n\n\nList of floats, the average cross_entropy for each epoch.\n\n\n\n\n\n\nMethods\n\n\n\n\n\nfit(X, y, init_params=True)\n\n\nLearn model from training data.\n\n\nParameters\n\n\n\n\n\n\nX\n : {array-like, sparse matrix}, shape = [n_samples, n_features]\n\n\nTraining vectors, where n_samples is the number of samples and\nn_features is the number of features.\n\n\n\n\n\n\ny\n : array-like, shape = [n_samples]\n\n\nTarget values.\n\n\n\n\n\n\ninit_params\n : bool (default: True)\n\n\nRe-initializes model parameters prior to fitting.\nSet False to continue training with weights from\na previous model fitting.\n\n\n\n\n\n\nReturns\n\n\n\n\nself\n : object\n\n\n\n\n\n\n\npredict(X)\n\n\nPredict targets from X.\n\n\nParameters\n\n\n\n\n\n\nX\n : {array-like, sparse matrix}, shape = [n_samples, n_features]\n\n\nTraining vectors, where n_samples is the number of samples and\nn_features is the number of features.\n\n\n\n\n\n\nReturns\n\n\n\n\n\n\ntarget_values\n : array-like, shape = [n_samples]\n\n\nPredicted target values.\n\n\n\n\n\n\n\n\n\npredict_proba(X)\n\n\nPredict class probabilities of X from the net input.\n\n\nParameters\n\n\n\n\n\n\nX\n : {array-like, sparse matrix}, shape = [n_samples, n_features]\n\n\nTraining vectors, where n_samples is the number of samples and\nn_features is the number of features.\n\n\n\n\n\n\nReturns\n\n\n\n\nClass probabilties\n : array-like, shape= [n_samples, n_classes]\n\n\n\n\n\n\n\nscore(X, y)\n\n\nCompute the prediction accuracy\n\n\nParameters\n\n\n\n\n\n\nX\n : {array-like, sparse matrix}, shape = [n_samples, n_features]\n\n\nTraining vectors, where n_samples is the number of samples and\nn_features is the number of features.\n\n\n\n\n\n\ny\n : array-like, shape = [n_samples]\n\n\nTarget values (true class labels).\n\n\n\n\n\n\nReturns\n\n\n\n\n\n\nacc\n : float\n\n\nThe prediction accuracy as a float\nbetween 0.0 and 1.0 (perfect score).\n\n\n\n\n\n\nStackingClassifier\n\n\nStackingClassifier(classifiers, meta_classifier, use_probas=False, verbose=0)\n\n\nA Stacking classifier for scikit-learn estimators for classification.\n\n\nParameters\n\n\n\n\n\n\nclassifiers\n : array-like, shape = [n_regressors]\n\n\nA list of classifiers.\nInvoking the \nfit\n method on the \nStackingClassifer\n will fit clones\nof these original classifiers that will\nbe stored in the class attribute\n\nself.clfs_\n.\n\n\n\n\n\n\nmeta_classifier\n : object\n\n\nThe meta-classifier to be fitted on the ensemble of\nclassifiers\n\n\n\n\n\n\nuse_probas\n : bool (default: False)\n\n\nIf True, trains meta-classifier based on predicted probabilities\ninstead of class labels.\n\n\n\n\n\n\nverbose\n : int, optional (default=0)\n\n\nControls the verbosity of the building process.\n- \nverbose=0\n (default): Prints nothing\n- \nverbose=1\n: Prints the number & name of the regressor being fitted\n- \nverbose=2\n: Prints info about the parameters of the\nregressor being fitted\n- \nverbose>2\n: Changes \nverbose\n param of the underlying regressor to\nself.verbose - 2\n\n\n\n\n\n\nAttributes\n\n\n\n\n\n\nclfs_\n : list, shape=[n_classifiers]\n\n\nFitted classifiers (clones of the original classifiers)\n\n\n\n\n\n\nmeta_clf_\n : estimator\n\n\nFitted meta-classifier (clone of the original meta-estimator)\n\n\n\n\n\n\nMethods\n\n\n\n\n\nfit(X, y)\n\n\nFit ensemble classifers and the meta-classifier.\n\n\nParameters\n\n\n\n\n\n\nX\n : {array-like, sparse matrix}, shape = [n_samples, n_features]\n\n\nTraining vectors, where n_samples is the number of samples and\nn_features is the number of features.\n\n\n\n\n\n\ny\n : array-like, shape = [n_samples]\n\n\nTarget values.\n\n\n\n\n\n\nReturns\n\n\n\n\nself\n : object\n\n\n\n\n\n\n\nfit_transform(X, y=None, \nfit_params)\n\n\nFit to data, then transform it.\n\n\nFits transformer to X and y with optional parameters fit_params\nand returns a transformed version of X.\n\n\n\nParameters\n\n\n\n\n\n\nX\n : numpy array of shape [n_samples, n_features]\n\n\nTraining set.\n\n\n\n\n\n\ny\n : numpy array of shape [n_samples]\n\n\nTarget values.\n\n\n\n\n\n\nReturns\n\n\n\n\n\n\nX_new\n : numpy array of shape [n_samples, n_features_new]\n\n\nTransformed array.\n\n\n\n\n\n\n\n\n\nget_params(deep=True)\n\n\nReturn estimator parameter names for GridSearch support.\n\n\n\n\n\npredict(X)\n\n\nPredict target values for X.\n\n\nParameters\n\n\n\n\n\n\nX\n : {array-like, sparse matrix}, shape = [n_samples, n_features]\n\n\nTraining vectors, where n_samples is the number of samples and\nn_features is the number of features.\n\n\n\n\n\n\nReturns\n\n\n\n\n\n\nlabels\n : array-like, shape = [n_samples]\n\n\nPredicted class labels.\n\n\n\n\n\n\n\n\n\npredict_proba(X)\n\n\nPredict class probabilities for X.\n\n\nParameters\n\n\n\n\n\n\nX\n : {array-like, sparse matrix}, shape = [n_samples, n_features]\n\n\nTraining vectors, where n_samples is the number of samples and\nn_features is the number of features.\n\n\n\n\n\n\nReturns\n\n\n\n\n\n\nproba\n : array-like, shape = [n_samples, n_classes]\n\n\nProbability for each class per sample.\n\n\n\n\n\n\n\n\n\nscore(X, y, sample_weight=None)\n\n\nReturns the mean accuracy on the given test data and labels.\n\n\nIn multi-label classification, this is the subset accuracy\nwhich is a harsh metric since you require for each sample that\neach label set be correctly predicted.\n\n\n\nParameters\n\n\n\n\n\n\nX\n : array-like, shape = (n_samples, n_features)\n\n\nTest samples.\n\n\n\n\n\n\ny\n : array-like, shape = (n_samples) or (n_samples, n_outputs)\n\n\nTrue labels for X.\n\n\n\n\n\n\nsample_weight\n : array-like, shape = [n_samples], optional\n\n\nSample weights.\n\n\n\n\n\n\nReturns\n\n\n\n\n\n\nscore\n : float\n\n\nMean accuracy of self.predict(X) wrt. y.\n\n\n\n\n\n\n\n\n\nset_params(\nparams)\n\n\nSet the parameters of this estimator.\n\n\nThe method works on simple estimators as well as on nested objects\n(such as pipelines). The former have parameters of the form\n``<component>__<parameter>`` so that it's possible to update each\ncomponent of a nested object.\n\n\n\nReturns\n\n\nself",
            "title": "Mlxtend.classifier"
        },
        {
            "location": "/api_subpackages/mlxtend.classifier/#adaline",
            "text": "Adaline(eta=0.01, epochs=50, minibatches=None, random_seed=None, print_progress=0)  ADAptive LInear NEuron classifier.  Note that this implementation of Adaline expects binary class labels\nin {0, 1}.  Parameters    eta  : float (default: 0.01)  solver rate (between 0.0 and 1.0)    epochs  : int (default: 50)  Passes over the training dataset.\nPrior to each epoch, the dataset is shuffled\nif  minibatches > 1  to prevent cycles in stochastic gradient descent.    minibatches  : int (default: None)  The number of minibatches for gradient-based optimization.\nIf None: Normal Equations (closed-form solution)\nIf 1: Gradient Descent learning\nIf len(y): Stochastic Gradient Descent (SGD) online learning\nIf 1 < minibatches < len(y): SGD Minibatch learning    random_seed  : int (default: None)  Set random state for shuffling and initializing the weights.    print_progress  : int (default: 0)  Prints progress in fitting to stderr if not solver='normal equation'\n0: No output\n1: Epochs elapsed and cost\n2: 1 plus time elapsed\n3: 2 plus estimated time until completion    Attributes    w_  : 2d-array, shape={n_features, 1}  Model weights after fitting.    b_  : 1d-array, shape={1,}  Bias unit after fitting.    cost_  : list  Sum of squared errors after each epoch.",
            "title": "Adaline"
        },
        {
            "location": "/api_subpackages/mlxtend.classifier/#methods",
            "text": "fit(X, y, init_params=True)  Learn model from training data.  Parameters    X  : {array-like, sparse matrix}, shape = [n_samples, n_features]  Training vectors, where n_samples is the number of samples and\nn_features is the number of features.    y  : array-like, shape = [n_samples]  Target values.    init_params  : bool (default: True)  Re-initializes model parameters prior to fitting.\nSet False to continue training with weights from\na previous model fitting.    Returns   self  : object    predict(X)  Predict targets from X.  Parameters    X  : {array-like, sparse matrix}, shape = [n_samples, n_features]  Training vectors, where n_samples is the number of samples and\nn_features is the number of features.    Returns    target_values  : array-like, shape = [n_samples]  Predicted target values.     score(X, y)  Compute the prediction accuracy  Parameters    X  : {array-like, sparse matrix}, shape = [n_samples, n_features]  Training vectors, where n_samples is the number of samples and\nn_features is the number of features.    y  : array-like, shape = [n_samples]  Target values (true class labels).    Returns    acc  : float  The prediction accuracy as a float\nbetween 0.0 and 1.0 (perfect score).",
            "title": "Methods"
        },
        {
            "location": "/api_subpackages/mlxtend.classifier/#ensemblevoteclassifier",
            "text": "EnsembleVoteClassifier(clfs, voting='hard', weights=None, verbose=0)  Soft Voting/Majority Rule classifier for scikit-learn estimators.  Parameters    clfs  : array-like, shape = [n_classifiers]  A list of classifiers.\nInvoking the  fit  method on the  VotingClassifier  will fit clones\nof those original classifiers that will\nbe stored in the class attribute self.clfs_ .    voting  : str, {'hard', 'soft'} (default='hard')  If 'hard', uses predicted class labels for majority rule voting.\nElse if 'soft', predicts the class label based on the argmax of\nthe sums of the predicted probalities, which is recommended for\nan ensemble of well-calibrated classifiers.    weights  : array-like, shape = [n_classifiers], optional (default= None )  Sequence of weights ( float  or  int ) to weight the occurances of\npredicted class labels ( hard  voting) or class probabilities\nbefore averaging ( soft  voting). Uses uniform weights if  None .    verbose  : int, optional (default=0)  Controls the verbosity of the building process.\n-  verbose=0  (default): Prints nothing\n-  verbose=1 : Prints the number & name of the clf being fitted\n-  verbose=2 : Prints info about the parameters of the clf being fitted\n-  verbose>2 : Changes  verbose  param of the underlying clf to\nself.verbose - 2    Attributes    classes_  : array-like, shape = [n_predictions]    clf  : array-like, shape = [n_predictions]  The unmodified input classifiers    clf_  : array-like, shape = [n_predictions]  Fitted clones of the input classifiers    Examples  >>> import numpy as np\n>>> from sklearn.linear_model import LogisticRegression\n>>> from sklearn.naive_bayes import GaussianNB\n>>> from sklearn.ensemble import RandomForestClassifier\n>>> from mlxtend.sklearn import EnsembleVoteClassifier\n>>> clf1 = LogisticRegression(random_seed=1)\n>>> clf2 = RandomForestClassifier(random_seed=1)\n>>> clf3 = GaussianNB()\n>>> X = np.array([[-1, -1], [-2, -1], [-3, -2], [1, 1], [2, 1], [3, 2]])\n>>> y = np.array([1, 1, 1, 2, 2, 2])\n>>> eclf1 = EnsembleVoteClassifier(clfs=[clf1, clf2, clf3],\n... voting='hard', verbose=1)\n>>> eclf1 = eclf1.fit(X, y)\n>>> print(eclf1.predict(X))\n[1 1 1 2 2 2]\n>>> eclf2 = EnsembleVoteClassifier(clfs=[clf1, clf2, clf3], voting='soft')\n>>> eclf2 = eclf2.fit(X, y)\n>>> print(eclf2.predict(X))\n[1 1 1 2 2 2]\n>>> eclf3 = EnsembleVoteClassifier(clfs=[clf1, clf2, clf3],\n...                          voting='soft', weights=[2,1,1])\n>>> eclf3 = eclf3.fit(X, y)\n>>> print(eclf3.predict(X))\n[1 1 1 2 2 2]\n>>>",
            "title": "EnsembleVoteClassifier"
        },
        {
            "location": "/api_subpackages/mlxtend.classifier/#methods_1",
            "text": "fit(X, y)  Learn weight coefficients from training data for each classifier.  Parameters    X  : {array-like, sparse matrix}, shape = [n_samples, n_features]  Training vectors, where n_samples is the number of samples and\nn_features is the number of features.    y  : array-like, shape = [n_samples]  Target values.    Returns   self  : object    fit_transform(X, y=None,  fit_params)  Fit to data, then transform it.  Fits transformer to X and y with optional parameters fit_params\nand returns a transformed version of X.  Parameters    X  : numpy array of shape [n_samples, n_features]  Training set.    y  : numpy array of shape [n_samples]  Target values.    Returns    X_new  : numpy array of shape [n_samples, n_features_new]  Transformed array.     get_params(deep=True)  Return estimator parameter names for GridSearch support.   predict(X)  Predict class labels for X.  Parameters    X  : {array-like, sparse matrix}, shape = [n_samples, n_features]  Training vectors, where n_samples is the number of samples and\nn_features is the number of features.    Returns    maj  : array-like, shape = [n_samples]  Predicted class labels.     predict_proba(X)  Predict class probabilities for X.  Parameters    X  : {array-like, sparse matrix}, shape = [n_samples, n_features]  Training vectors, where n_samples is the number of samples and\nn_features is the number of features.    Returns    avg  : array-like, shape = [n_samples, n_classes]  Weighted average probability for each class per sample.     score(X, y, sample_weight=None)  Returns the mean accuracy on the given test data and labels.  In multi-label classification, this is the subset accuracy\nwhich is a harsh metric since you require for each sample that\neach label set be correctly predicted.  Parameters    X  : array-like, shape = (n_samples, n_features)  Test samples.    y  : array-like, shape = (n_samples) or (n_samples, n_outputs)  True labels for X.    sample_weight  : array-like, shape = [n_samples], optional  Sample weights.    Returns    score  : float  Mean accuracy of self.predict(X) wrt. y.     set_params( params)  Set the parameters of this estimator.  The method works on simple estimators as well as on nested objects\n(such as pipelines). The former have parameters of the form\n``<component>__<parameter>`` so that it's possible to update each\ncomponent of a nested object.  Returns  self   transform(X)  Return class labels or probabilities for X for each estimator.  Parameters    X  : {array-like, sparse matrix}, shape = [n_samples, n_features]  Training vectors, where n_samples is the number of samples and\nn_features is the number of features.    Returns    If voting='soft'`` : array-like = [n_classifiers, n_samples, n_classes]  Class probabilties calculated by each classifier.    If voting='hard'`` : array-like = [n_classifiers, n_samples]  Class labels predicted by each classifier.",
            "title": "Methods"
        },
        {
            "location": "/api_subpackages/mlxtend.classifier/#logisticregression",
            "text": "LogisticRegression(eta=0.01, epochs=50, l2_lambda=0.0, minibatches=1, random_seed=None, print_progress=0)  Logistic regression classifier.  Note that this implementation of Logistic Regression\nexpects binary class labels in {0, 1}.  Parameters    eta  : float (default: 0.01)  Learning rate (between 0.0 and 1.0)    epochs  : int (default: 50)  Passes over the training dataset.\nPrior to each epoch, the dataset is shuffled\nif  minibatches > 1  to prevent cycles in stochastic gradient descent.    l2_lambda  : float  Regularization parameter for L2 regularization.\nNo regularization if l2_lambda=0.0.    minibatches  : int (default: 1)  The number of minibatches for gradient-based optimization.\nIf 1: Gradient Descent learning\nIf len(y): Stochastic Gradient Descent (SGD) online learning\nIf 1 < minibatches < len(y): SGD Minibatch learning    random_seed  : int (default: None)  Set random state for shuffling and initializing the weights.    print_progress  : int (default: 0)  Prints progress in fitting to stderr.\n0: No output\n1: Epochs elapsed and cost\n2: 1 plus time elapsed\n3: 2 plus estimated time until completion    Attributes    w_  : 2d-array, shape={n_features, 1}  Model weights after fitting.    b_  : 1d-array, shape={1,}  Bias unit after fitting.    cost_  : list  List of floats with cross_entropy cost (sgd or gd) for every\nepoch.",
            "title": "LogisticRegression"
        },
        {
            "location": "/api_subpackages/mlxtend.classifier/#methods_2",
            "text": "fit(X, y, init_params=True)  Learn model from training data.  Parameters    X  : {array-like, sparse matrix}, shape = [n_samples, n_features]  Training vectors, where n_samples is the number of samples and\nn_features is the number of features.    y  : array-like, shape = [n_samples]  Target values.    init_params  : bool (default: True)  Re-initializes model parameters prior to fitting.\nSet False to continue training with weights from\na previous model fitting.    Returns   self  : object    predict(X)  Predict targets from X.  Parameters    X  : {array-like, sparse matrix}, shape = [n_samples, n_features]  Training vectors, where n_samples is the number of samples and\nn_features is the number of features.    Returns    target_values  : array-like, shape = [n_samples]  Predicted target values.     predict_proba(X)  Predict class probabilities of X from the net input.  Parameters    X  : {array-like, sparse matrix}, shape = [n_samples, n_features]  Training vectors, where n_samples is the number of samples and\nn_features is the number of features.    Returns   Class 1 probability  : float    score(X, y)  Compute the prediction accuracy  Parameters    X  : {array-like, sparse matrix}, shape = [n_samples, n_features]  Training vectors, where n_samples is the number of samples and\nn_features is the number of features.    y  : array-like, shape = [n_samples]  Target values (true class labels).    Returns    acc  : float  The prediction accuracy as a float\nbetween 0.0 and 1.0 (perfect score).",
            "title": "Methods"
        },
        {
            "location": "/api_subpackages/mlxtend.classifier/#multilayerperceptron",
            "text": "MultiLayerPerceptron(eta=0.5, epochs=50, hidden_layers=[50], n_classes=None, momentum=0.0, l1=0.0, l2=0.0, dropout=1.0, decrease_const=0.0, minibatches=1, random_seed=None, print_progress=0)  Multi-layer perceptron classifier with logistic sigmoid activations  Parameters    eta  : float (default: 0.5)  Learning rate (between 0.0 and 1.0)    epochs  : int (default: 50)  Passes over the training dataset.\nPrior to each epoch, the dataset is shuffled\nif  minibatches > 1  to prevent cycles in stochastic gradient descent.    hidden_layers  : list (default: [50])  Number of units per hidden layer. By default 50 units in the\nfirst hidden layer. At the moment only 1 hidden layer is supported    n_classes  : int (default: None)  A positive integer to declare the number of class labels\nif not all class labels are present in a partial training set.\nGets the number of class labels automatically if None.    l1  : float (default: 0.0)  L1 regularization strength    l2  : float (default: 0.0)  L2 regularization strength    momentum  : float (default: 0.0)  Momentum constant. Factor multiplied with the\ngradient of the previous epoch t-1 to improve\nlearning speed\nw(t) := w(t) - (grad(t) + momentum * grad(t-1))    decrease_const  : float (default: 0.0)  Decrease constant. Shrinks the learning rate\nafter each epoch via eta / (1 + epoch*decrease_const)    minibatches  : int (default: 1)  Divide the training data into  k  minibatches\nfor accelerated stochastic gradient descent learning.\nGradient Descent Learning if  minibatches  = 1\nStochastic Gradient Descent learning if  minibatches  = len(y)\nMinibatch learning if  minibatches  > 1    random_seed  : int (default: None)  Set random state for shuffling and initializing the weights.    print_progress  : int (default: 0)  Prints progress in fitting to stderr.\n0: No output\n1: Epochs elapsed and cost\n2: 1 plus time elapsed\n3: 2 plus estimated time until completion    Attributes    w_  : 2d-array, shape=[n_features, n_classes]  Weights after fitting.    b_  : 1D-array, shape=[n_classes]  Bias units after fitting.    cost_  : list  List of floats; the mean categorical cross entropy\ncost after each epoch.",
            "title": "MultiLayerPerceptron"
        },
        {
            "location": "/api_subpackages/mlxtend.classifier/#methods_3",
            "text": "fit(X, y, init_params=True)  Learn model from training data.  Parameters    X  : {array-like, sparse matrix}, shape = [n_samples, n_features]  Training vectors, where n_samples is the number of samples and\nn_features is the number of features.    y  : array-like, shape = [n_samples]  Target values.    init_params  : bool (default: True)  Re-initializes model parameters prior to fitting.\nSet False to continue training with weights from\na previous model fitting.    Returns   self  : object    predict(X)  Predict targets from X.  Parameters    X  : {array-like, sparse matrix}, shape = [n_samples, n_features]  Training vectors, where n_samples is the number of samples and\nn_features is the number of features.    Returns    target_values  : array-like, shape = [n_samples]  Predicted target values.     predict_proba(X)  Predict class probabilities of X from the net input.  Parameters    X  : {array-like, sparse matrix}, shape = [n_samples, n_features]  Training vectors, where n_samples is the number of samples and\nn_features is the number of features.    Returns   Class probabilties  : array-like, shape= [n_samples, n_classes]    score(X, y)  Compute the prediction accuracy  Parameters    X  : {array-like, sparse matrix}, shape = [n_samples, n_features]  Training vectors, where n_samples is the number of samples and\nn_features is the number of features.    y  : array-like, shape = [n_samples]  Target values (true class labels).    Returns    acc  : float  The prediction accuracy as a float\nbetween 0.0 and 1.0 (perfect score).",
            "title": "Methods"
        },
        {
            "location": "/api_subpackages/mlxtend.classifier/#neuralnetmlp",
            "text": "NeuralNetMLP(n_output, n_features, n_hidden=30, l1=0.0, l2=0.0, epochs=500, eta=0.001, alpha=0.0, decrease_const=0.0, shuffle_init=True, shuffle_epoch=True, minibatches=1, zero_init_weight=False, random_seed=None, print_progress=0)  Feedforward neural network / Multi-layer perceptron classifier.  Parameters    n_output  : int  Number of output units, should be equal to the\nnumber of unique class labels.    n_features  : int  Number of features (dimensions) in the target dataset.\nShould be equal to the number of columns in the X array.    n_hidden  : int (default: 30)  Number of hidden units.    l1  : float (default: 0.0)  Lambda value for L1-regularization.\nNo regularization if l1=0.0 (default)    l2  : float (default: 0.0)  Lambda value for L2-regularization.\nNo regularization if l2=0.0 (default)    epochs  : int (default: 500)  Number of passes over the training set.    eta  : float (default: 0.001)  Learning rate.    alpha  : float (default: 0.0)  Momentum constant. Factor multiplied with the\ngradient of the previous epoch t-1 to improve\nlearning speed\nw(t) := w(t) - (grad(t) + alpha*grad(t-1))    decrease_const  : float (default: 0.0)  Decrease constant. Shrinks the learning rate\nafter each epoch via eta / (1 + epoch*decrease_const)    random_weights  : list (default: [-1.0, 1.0])  Min and max values for initializing the random weights.\nInitializes weights to 0 if None or False.    shuffle_init  : bool (default: True)  Shuffles (a copy of the) training data before training.    shuffle_epoch  : bool (default: True)  Shuffles training data before every epoch if True to prevent circles.    minibatches  : int (default: 1)  Divides training data into k minibatches for efficiency.\nNormal gradient descent learning if k=1 (default).    random_seed  : int (default: None)  Set random seed for shuffling and initializing the weights.    zero_init_weight  : bool (default: False)  If True, weights are initialized to zero instead of small random\nnumbers following a standard normal distribution with mean=0 and\nstddev=1.    print_progress  : int (default: 0)  Prints progress in fitting to stderr.\n0: No output\n1: Epochs elapsed and cost\n2: 1 plus time elapsed\n3: 2 plus estimated time until completion    Attributes    cost_  : list  Sum of squared errors after each epoch.",
            "title": "NeuralNetMLP"
        },
        {
            "location": "/api_subpackages/mlxtend.classifier/#methods_4",
            "text": "fit(X, y)  Learn weight coefficients from training data.  Parameters    X  : array, shape = [n_samples, n_features]  Input layer with original features.    y  : array, shape = [n_samples]  Target class labels.    Returns:  self   predict(X)  Predict class labels of X.  Parameters    X  : {array-like, sparse matrix}, shape = [n_samples, n_features]  Training vectors, where n_samples is the number of samples and\nn_features is the number of features.    Returns    class_labels  : array-like, shape = [n_samples]  Predicted class labels.     score(X, y)  Compute the prediction accuracy  Parameters    X  : {array-like, sparse matrix}, shape = [n_samples, n_features]  Training vectors, where n_samples is the number of samples and\nn_features is the number of features.    y  : array-like, shape = [n_samples]  Target values (true class labels).    Returns    acc  : float  The prediction accuracy as a float\nbetween 0.0 and 1.0 (perfect score).",
            "title": "Methods"
        },
        {
            "location": "/api_subpackages/mlxtend.classifier/#perceptron",
            "text": "Perceptron(eta=0.1, epochs=50, random_seed=None, print_progress=0)  Perceptron classifier.  Note that this implementation of the Perceptron expects binary class labels\nin {0, 1}.  Parameters    eta  : float (default: 0.1)  Learning rate (between 0.0 and 1.0)    epochs  : int (default: 50)  Number of passes over the training dataset.\nPrior to each epoch, the dataset is shuffled to prevent cycles.    random_seed  : int  Random state for initializing random weights and shuffling.    print_progress  : int (default: 0)  Prints progress in fitting to stderr.\n0: No output\n1: Epochs elapsed and cost\n2: 1 plus time elapsed\n3: 2 plus estimated time until completion    Attributes    w_  : 2d-array, shape={n_features, 1}  Model weights after fitting.    b_  : 1d-array, shape={1,}  Bias unit after fitting.    cost_  : list  Number of misclassifications in every epoch.",
            "title": "Perceptron"
        },
        {
            "location": "/api_subpackages/mlxtend.classifier/#methods_5",
            "text": "fit(X, y, init_params=True)  Learn model from training data.  Parameters    X  : {array-like, sparse matrix}, shape = [n_samples, n_features]  Training vectors, where n_samples is the number of samples and\nn_features is the number of features.    y  : array-like, shape = [n_samples]  Target values.    init_params  : bool (default: True)  Re-initializes model parameters prior to fitting.\nSet False to continue training with weights from\na previous model fitting.    Returns   self  : object    predict(X)  Predict targets from X.  Parameters    X  : {array-like, sparse matrix}, shape = [n_samples, n_features]  Training vectors, where n_samples is the number of samples and\nn_features is the number of features.    Returns    target_values  : array-like, shape = [n_samples]  Predicted target values.     score(X, y)  Compute the prediction accuracy  Parameters    X  : {array-like, sparse matrix}, shape = [n_samples, n_features]  Training vectors, where n_samples is the number of samples and\nn_features is the number of features.    y  : array-like, shape = [n_samples]  Target values (true class labels).    Returns    acc  : float  The prediction accuracy as a float\nbetween 0.0 and 1.0 (perfect score).",
            "title": "Methods"
        },
        {
            "location": "/api_subpackages/mlxtend.classifier/#softmaxregression",
            "text": "SoftmaxRegression(eta=0.01, epochs=50, l2=0.0, minibatches=1, n_classes=None, random_seed=None, print_progress=0)  Softmax regression classifier.  Parameters    eta  : float (default: 0.01)  Learning rate (between 0.0 and 1.0)    epochs  : int (default: 50)  Passes over the training dataset.\nPrior to each epoch, the dataset is shuffled\nif  minibatches > 1  to prevent cycles in stochastic gradient descent.    l2  : float  Regularization parameter for L2 regularization.\nNo regularization if l2=0.0.    minibatches  : int (default: 1)  The number of minibatches for gradient-based optimization.\nIf 1: Gradient Descent learning\nIf len(y): Stochastic Gradient Descent (SGD) online learning\nIf 1 < minibatches < len(y): SGD Minibatch learning    n_classes  : int (default: None)  A positive integer to declare the number of class labels\nif not all class labels are present in a partial training set.\nGets the number of class labels automatically if None.    random_seed  : int (default: None)  Set random state for shuffling and initializing the weights.    print_progress  : int (default: 0)  Prints progress in fitting to stderr.\n0: No output\n1: Epochs elapsed and cost\n2: 1 plus time elapsed\n3: 2 plus estimated time until completion    Attributes    w_  : 2d-array, shape={n_features, 1}  Model weights after fitting.    b_  : 1d-array, shape={1,}  Bias unit after fitting.    cost_  : list  List of floats, the average cross_entropy for each epoch.",
            "title": "SoftmaxRegression"
        },
        {
            "location": "/api_subpackages/mlxtend.classifier/#methods_6",
            "text": "fit(X, y, init_params=True)  Learn model from training data.  Parameters    X  : {array-like, sparse matrix}, shape = [n_samples, n_features]  Training vectors, where n_samples is the number of samples and\nn_features is the number of features.    y  : array-like, shape = [n_samples]  Target values.    init_params  : bool (default: True)  Re-initializes model parameters prior to fitting.\nSet False to continue training with weights from\na previous model fitting.    Returns   self  : object    predict(X)  Predict targets from X.  Parameters    X  : {array-like, sparse matrix}, shape = [n_samples, n_features]  Training vectors, where n_samples is the number of samples and\nn_features is the number of features.    Returns    target_values  : array-like, shape = [n_samples]  Predicted target values.     predict_proba(X)  Predict class probabilities of X from the net input.  Parameters    X  : {array-like, sparse matrix}, shape = [n_samples, n_features]  Training vectors, where n_samples is the number of samples and\nn_features is the number of features.    Returns   Class probabilties  : array-like, shape= [n_samples, n_classes]    score(X, y)  Compute the prediction accuracy  Parameters    X  : {array-like, sparse matrix}, shape = [n_samples, n_features]  Training vectors, where n_samples is the number of samples and\nn_features is the number of features.    y  : array-like, shape = [n_samples]  Target values (true class labels).    Returns    acc  : float  The prediction accuracy as a float\nbetween 0.0 and 1.0 (perfect score).",
            "title": "Methods"
        },
        {
            "location": "/api_subpackages/mlxtend.classifier/#stackingclassifier",
            "text": "StackingClassifier(classifiers, meta_classifier, use_probas=False, verbose=0)  A Stacking classifier for scikit-learn estimators for classification.  Parameters    classifiers  : array-like, shape = [n_regressors]  A list of classifiers.\nInvoking the  fit  method on the  StackingClassifer  will fit clones\nof these original classifiers that will\nbe stored in the class attribute self.clfs_ .    meta_classifier  : object  The meta-classifier to be fitted on the ensemble of\nclassifiers    use_probas  : bool (default: False)  If True, trains meta-classifier based on predicted probabilities\ninstead of class labels.    verbose  : int, optional (default=0)  Controls the verbosity of the building process.\n-  verbose=0  (default): Prints nothing\n-  verbose=1 : Prints the number & name of the regressor being fitted\n-  verbose=2 : Prints info about the parameters of the\nregressor being fitted\n-  verbose>2 : Changes  verbose  param of the underlying regressor to\nself.verbose - 2    Attributes    clfs_  : list, shape=[n_classifiers]  Fitted classifiers (clones of the original classifiers)    meta_clf_  : estimator  Fitted meta-classifier (clone of the original meta-estimator)",
            "title": "StackingClassifier"
        },
        {
            "location": "/api_subpackages/mlxtend.classifier/#methods_7",
            "text": "fit(X, y)  Fit ensemble classifers and the meta-classifier.  Parameters    X  : {array-like, sparse matrix}, shape = [n_samples, n_features]  Training vectors, where n_samples is the number of samples and\nn_features is the number of features.    y  : array-like, shape = [n_samples]  Target values.    Returns   self  : object    fit_transform(X, y=None,  fit_params)  Fit to data, then transform it.  Fits transformer to X and y with optional parameters fit_params\nand returns a transformed version of X.  Parameters    X  : numpy array of shape [n_samples, n_features]  Training set.    y  : numpy array of shape [n_samples]  Target values.    Returns    X_new  : numpy array of shape [n_samples, n_features_new]  Transformed array.     get_params(deep=True)  Return estimator parameter names for GridSearch support.   predict(X)  Predict target values for X.  Parameters    X  : {array-like, sparse matrix}, shape = [n_samples, n_features]  Training vectors, where n_samples is the number of samples and\nn_features is the number of features.    Returns    labels  : array-like, shape = [n_samples]  Predicted class labels.     predict_proba(X)  Predict class probabilities for X.  Parameters    X  : {array-like, sparse matrix}, shape = [n_samples, n_features]  Training vectors, where n_samples is the number of samples and\nn_features is the number of features.    Returns    proba  : array-like, shape = [n_samples, n_classes]  Probability for each class per sample.     score(X, y, sample_weight=None)  Returns the mean accuracy on the given test data and labels.  In multi-label classification, this is the subset accuracy\nwhich is a harsh metric since you require for each sample that\neach label set be correctly predicted.  Parameters    X  : array-like, shape = (n_samples, n_features)  Test samples.    y  : array-like, shape = (n_samples) or (n_samples, n_outputs)  True labels for X.    sample_weight  : array-like, shape = [n_samples], optional  Sample weights.    Returns    score  : float  Mean accuracy of self.predict(X) wrt. y.     set_params( params)  Set the parameters of this estimator.  The method works on simple estimators as well as on nested objects\n(such as pipelines). The former have parameters of the form\n``<component>__<parameter>`` so that it's possible to update each\ncomponent of a nested object.  Returns  self",
            "title": "Methods"
        },
        {
            "location": "/api_subpackages/mlxtend.tf_classifier/",
            "text": "mlxtend version: 0.4.2dev0 \n\n\nTfMultiLayerPerceptron\n\n\nTfMultiLayerPerceptron(eta=0.5, epochs=50, hidden_layers=[50, 10], n_classes=None, activations=['logistic', 'logistic'], optimizer='gradientdescent', momentum=0.0, l1=0.0, l2=0.0, dropout=1.0, decay=[0.0, 1.0], minibatches=1, random_seed=None, print_progress=0, dtype=None)\n\n\nMulti-layer perceptron classifier.\n\n\nParameters\n\n\n\n\n\n\neta\n : float (default: 0.5)\n\n\nLearning rate (between 0.0 and 1.0)\n\n\n\n\n\n\nepochs\n : int (default: 50)\n\n\nPasses over the training dataset.\nPrior to each epoch, the dataset is shuffled\nif \nminibatches > 1\n to prevent cycles in stochastic gradient descent.\n\n\n\n\n\n\nhidden_layers\n : list (default: [50, 10])\n\n\nNumber of units per hidden layer. By default 50 units in the\nfirst hidden layer, and 10 hidden units in the second hidden layer.\n\n\n\n\n\n\nn_classes\n : int (default: None)\n\n\nA positive integer to declare the number of class labels\nif not all class labels are present in a partial training set.\nGets the number of class labels automatically if None.\n\n\n\n\n\n\nactivations\n : list (default: ['logistic', 'logistic'])\n\n\nActivation functions for each layer.\nAvailable actiavtion functions:\n\"logistic\", \"relu\", \"tanh\", \"relu6\", \"elu\", \"softplus\", \"softsign\"\n\n\n\n\n\n\noptimizer\n : str (default: \"gradientdescent\")\n\n\nOptimizer to minimize the cost function:\n\"gradientdescent\", \"momentum\", \"adam\", \"ftrl\", \"adagrad\"\n\n\n\n\n\n\nmomentum\n : float (default: 0.0)\n\n\nMomentum constant for momentum learning; only applies if\noptimizer='momentum'\n\n\n\n\n\n\nl1\n : float (default: 0.0)\n\n\nL1 regularization strength; only applies if optimizer='ftrl'\n\n\n\n\n\n\nl2\n : float (default: 0.0)\n\n\nregularization strength; only applies if optimizer='ftrl'\n\n\n\n\n\n\ndropout\n : float (default: 1.0)\n\n\nA float between in the range (0.0, 1.0] to specify\nthe probability that each element is kept.\n\n\n\n\n\n\ndecay\n : list, shape=[decay_rate, decay_steps] (default: [0.0, 1])\n\n\nParameter to specify the exponential decay of the learning rate eta\nfor adaptive learning (eta * decay_rate ^ (epoch / decay_steps)).\n\n\n\n\n\n\nminibatches\n : int (default: 1)\n\n\nDivide the training data into \nk\n minibatches\nfor accelerated stochastic gradient descent learning.\nGradient Descent Learning if \nminibatches\n = 1\nStochastic Gradient Descent learning if \nminibatches\n = len(y)\nMinibatch learning if \nminibatches\n > 1\n\n\n\n\n\n\nrandom_seed\n : int (default: None)\n\n\nSet random state for shuffling and initializing the weights.\n\n\n\n\n\n\nprint_progress\n : int (default: 0)\n\n\nPrints progress in fitting to stderr.\n0: No output\n1: Epochs elapsed and cost\n2: 1 plus time elapsed\n3: 2 plus estimated time until completion\n\n\n\n\n\n\ndtype\n : Array-type (default: None)\n\n\nUses tensorflow.float32 if None.\n\n\n\n\n\n\nAttributes\n\n\n\n\n\n\nw_\n : 2d-array, shape=[n_features, n_classes]\n\n\nWeights after fitting.\n\n\n\n\n\n\nb_\n : 1D-array, shape=[n_classes]\n\n\nBias units after fitting.\n\n\n\n\n\n\ncost_\n : list\n\n\nList of floats, the average cross_entropy for each epoch.\n\n\n\n\n\n\nMethods\n\n\n\n\n\nfit(X, y, init_params=True)\n\n\nLearn model from training data.\n\n\nParameters\n\n\n\n\n\n\nX\n : {array-like, sparse matrix}, shape = [n_samples, n_features]\n\n\nTraining vectors, where n_samples is the number of samples and\nn_features is the number of features.\n\n\n\n\n\n\ny\n : array-like, shape = [n_samples]\n\n\nTarget values.\n\n\n\n\n\n\ninit_params\n : bool (default: True)\n\n\nRe-initializes model parameters prior to fitting.\nSet False to continue training with weights from\na previous model fitting.\n\n\n\n\n\n\nReturns\n\n\n\n\nself\n : object\n\n\n\n\n\n\n\npredict(X)\n\n\nPredict class labels of X.\n\n\nParameters\n\n\n\n\n\n\nX\n : {array-like, sparse matrix}, shape = [n_samples, n_features]\n\n\nTraining vectors, where n_samples is the number of samples and\nn_features is the number of features.\n\n\n\n\n\n\nReturns\n\n\n\n\n\n\nclass_labels\n : array-like, shape = [n_samples]\n\n\nPredicted class labels.\n\n\n\n\n\n\n\n\n\npredict_proba(X)\n\n\nPredict class probabilities of X from the net input.\n\n\nParameters\n\n\n\n\n\n\nX\n : {array-like, sparse matrix}, shape = [n_samples, n_features]\n\n\nTraining vectors, where n_samples is the number of samples and\nn_features is the number of features.\n\n\n\n\n\n\nReturns\n\n\n\n\nClass probabilties\n : array-like, shape= [n_samples, n_classes]\n\n\n\n\n\n\n\nscore(X, y)\n\n\nCompute the prediction accuracy\n\n\nParameters\n\n\n\n\n\n\nX\n : {array-like, sparse matrix}, shape = [n_samples, n_features]\n\n\nTraining vectors, where n_samples is the number of samples and\nn_features is the number of features.\n\n\n\n\n\n\ny\n : array-like, shape = [n_samples]\n\n\nTarget values (true class labels).\n\n\n\n\n\n\nReturns\n\n\n\n\n\n\nacc\n : float\n\n\nThe prediction accuracy as a float\nbetween 0.0 and 1.0 (perfect score).\n\n\n\n\n\n\nTfSoftmaxRegression\n\n\nTfSoftmaxRegression(eta=0.5, epochs=50, n_classes=None, minibatches=1, random_seed=None, print_progress=0, dtype=None)\n\n\nSoftmax regression classifier.\n\n\nParameters\n\n\n\n\n\n\neta\n : float (default: 0.5)\n\n\nLearning rate (between 0.0 and 1.0)\n\n\n\n\n\n\nepochs\n : int (default: 50)\n\n\nPasses over the training dataset.\nPrior to each epoch, the dataset is shuffled\nif \nminibatches > 1\n to prevent cycles in stochastic gradient descent.\n\n\n\n\n\n\nn_classes\n : int (default: None)\n\n\nA positive integer to declare the number of class labels\nif not all class labels are present in a partial training set.\nGets the number of class labels automatically if None.\n\n\n\n\n\n\nminibatches\n : int (default: 1)\n\n\nDivide the training data into \nk\n minibatches\nfor accelerated stochastic gradient descent learning.\nGradient Descent Learning if \nminibatches\n = 1\nStochastic Gradient Descent learning if \nminibatches\n = len(y)\nMinibatch learning if \nminibatches\n > 1\n\n\n\n\n\n\nrandom_seed\n : int (default: None)\n\n\nSet random state for shuffling and initializing the weights.\n\n\n\n\n\n\nprint_progress\n : int (default: 0)\n\n\nPrints progress in fitting to stderr.\n0: No output\n1: Epochs elapsed and cost\n2: 1 plus time elapsed\n3: 2 plus estimated time until completion\n\n\n\n\n\n\ndtype\n : Array-type (default: None)\n\n\nUses tensorflow.float32 if None.\n\n\n\n\n\n\nAttributes\n\n\n\n\n\n\nw_\n : 2d-array, shape=[n_features, n_classes]\n\n\nWeights after fitting.\n\n\n\n\n\n\nb_\n : 1D-array, shape=[n_classes]\n\n\nBias units after fitting.\n\n\n\n\n\n\ncost_\n : list\n\n\nList of floats, the average cross_entropy for each epoch.\n\n\n\n\n\n\nMethods\n\n\n\n\n\nfit(X, y, init_params=True)\n\n\nLearn model from training data.\n\n\nParameters\n\n\n\n\n\n\nX\n : {array-like, sparse matrix}, shape = [n_samples, n_features]\n\n\nTraining vectors, where n_samples is the number of samples and\nn_features is the number of features.\n\n\n\n\n\n\ny\n : array-like, shape = [n_samples]\n\n\nTarget values.\n\n\n\n\n\n\ninit_params\n : bool (default: True)\n\n\nRe-initializes model parameters prior to fitting.\nSet False to continue training with weights from\na previous model fitting.\n\n\n\n\n\n\nReturns\n\n\n\n\nself\n : object\n\n\n\n\n\n\n\npredict(X)\n\n\nPredict targets from X.\n\n\nParameters\n\n\n\n\n\n\nX\n : {array-like, sparse matrix}, shape = [n_samples, n_features]\n\n\nTraining vectors, where n_samples is the number of samples and\nn_features is the number of features.\n\n\n\n\n\n\nReturns\n\n\n\n\n\n\ntarget_values\n : array-like, shape = [n_samples]\n\n\nPredicted target values.\n\n\n\n\n\n\n\n\n\npredict_proba(X)\n\n\nPredict class probabilities of X from the net input.\n\n\nParameters\n\n\n\n\n\n\nX\n : {array-like, sparse matrix}, shape = [n_samples, n_features]\n\n\nTraining vectors, where n_samples is the number of samples and\nn_features is the number of features.\n\n\n\n\n\n\nReturns\n\n\n\n\nClass probabilties\n : array-like, shape= [n_samples, n_classes]\n\n\n\n\n\n\n\nscore(X, y)\n\n\nCompute the prediction accuracy\n\n\nParameters\n\n\n\n\n\n\nX\n : {array-like, sparse matrix}, shape = [n_samples, n_features]\n\n\nTraining vectors, where n_samples is the number of samples and\nn_features is the number of features.\n\n\n\n\n\n\ny\n : array-like, shape = [n_samples]\n\n\nTarget values (true class labels).\n\n\n\n\n\n\nReturns\n\n\n\n\n\n\nacc\n : float\n\n\nThe prediction accuracy as a float\nbetween 0.0 and 1.0 (perfect score).",
            "title": "Mlxtend.tf classifier"
        },
        {
            "location": "/api_subpackages/mlxtend.tf_classifier/#tfmultilayerperceptron",
            "text": "TfMultiLayerPerceptron(eta=0.5, epochs=50, hidden_layers=[50, 10], n_classes=None, activations=['logistic', 'logistic'], optimizer='gradientdescent', momentum=0.0, l1=0.0, l2=0.0, dropout=1.0, decay=[0.0, 1.0], minibatches=1, random_seed=None, print_progress=0, dtype=None)  Multi-layer perceptron classifier.  Parameters    eta  : float (default: 0.5)  Learning rate (between 0.0 and 1.0)    epochs  : int (default: 50)  Passes over the training dataset.\nPrior to each epoch, the dataset is shuffled\nif  minibatches > 1  to prevent cycles in stochastic gradient descent.    hidden_layers  : list (default: [50, 10])  Number of units per hidden layer. By default 50 units in the\nfirst hidden layer, and 10 hidden units in the second hidden layer.    n_classes  : int (default: None)  A positive integer to declare the number of class labels\nif not all class labels are present in a partial training set.\nGets the number of class labels automatically if None.    activations  : list (default: ['logistic', 'logistic'])  Activation functions for each layer.\nAvailable actiavtion functions:\n\"logistic\", \"relu\", \"tanh\", \"relu6\", \"elu\", \"softplus\", \"softsign\"    optimizer  : str (default: \"gradientdescent\")  Optimizer to minimize the cost function:\n\"gradientdescent\", \"momentum\", \"adam\", \"ftrl\", \"adagrad\"    momentum  : float (default: 0.0)  Momentum constant for momentum learning; only applies if\noptimizer='momentum'    l1  : float (default: 0.0)  L1 regularization strength; only applies if optimizer='ftrl'    l2  : float (default: 0.0)  regularization strength; only applies if optimizer='ftrl'    dropout  : float (default: 1.0)  A float between in the range (0.0, 1.0] to specify\nthe probability that each element is kept.    decay  : list, shape=[decay_rate, decay_steps] (default: [0.0, 1])  Parameter to specify the exponential decay of the learning rate eta\nfor adaptive learning (eta * decay_rate ^ (epoch / decay_steps)).    minibatches  : int (default: 1)  Divide the training data into  k  minibatches\nfor accelerated stochastic gradient descent learning.\nGradient Descent Learning if  minibatches  = 1\nStochastic Gradient Descent learning if  minibatches  = len(y)\nMinibatch learning if  minibatches  > 1    random_seed  : int (default: None)  Set random state for shuffling and initializing the weights.    print_progress  : int (default: 0)  Prints progress in fitting to stderr.\n0: No output\n1: Epochs elapsed and cost\n2: 1 plus time elapsed\n3: 2 plus estimated time until completion    dtype  : Array-type (default: None)  Uses tensorflow.float32 if None.    Attributes    w_  : 2d-array, shape=[n_features, n_classes]  Weights after fitting.    b_  : 1D-array, shape=[n_classes]  Bias units after fitting.    cost_  : list  List of floats, the average cross_entropy for each epoch.",
            "title": "TfMultiLayerPerceptron"
        },
        {
            "location": "/api_subpackages/mlxtend.tf_classifier/#methods",
            "text": "fit(X, y, init_params=True)  Learn model from training data.  Parameters    X  : {array-like, sparse matrix}, shape = [n_samples, n_features]  Training vectors, where n_samples is the number of samples and\nn_features is the number of features.    y  : array-like, shape = [n_samples]  Target values.    init_params  : bool (default: True)  Re-initializes model parameters prior to fitting.\nSet False to continue training with weights from\na previous model fitting.    Returns   self  : object    predict(X)  Predict class labels of X.  Parameters    X  : {array-like, sparse matrix}, shape = [n_samples, n_features]  Training vectors, where n_samples is the number of samples and\nn_features is the number of features.    Returns    class_labels  : array-like, shape = [n_samples]  Predicted class labels.     predict_proba(X)  Predict class probabilities of X from the net input.  Parameters    X  : {array-like, sparse matrix}, shape = [n_samples, n_features]  Training vectors, where n_samples is the number of samples and\nn_features is the number of features.    Returns   Class probabilties  : array-like, shape= [n_samples, n_classes]    score(X, y)  Compute the prediction accuracy  Parameters    X  : {array-like, sparse matrix}, shape = [n_samples, n_features]  Training vectors, where n_samples is the number of samples and\nn_features is the number of features.    y  : array-like, shape = [n_samples]  Target values (true class labels).    Returns    acc  : float  The prediction accuracy as a float\nbetween 0.0 and 1.0 (perfect score).",
            "title": "Methods"
        },
        {
            "location": "/api_subpackages/mlxtend.tf_classifier/#tfsoftmaxregression",
            "text": "TfSoftmaxRegression(eta=0.5, epochs=50, n_classes=None, minibatches=1, random_seed=None, print_progress=0, dtype=None)  Softmax regression classifier.  Parameters    eta  : float (default: 0.5)  Learning rate (between 0.0 and 1.0)    epochs  : int (default: 50)  Passes over the training dataset.\nPrior to each epoch, the dataset is shuffled\nif  minibatches > 1  to prevent cycles in stochastic gradient descent.    n_classes  : int (default: None)  A positive integer to declare the number of class labels\nif not all class labels are present in a partial training set.\nGets the number of class labels automatically if None.    minibatches  : int (default: 1)  Divide the training data into  k  minibatches\nfor accelerated stochastic gradient descent learning.\nGradient Descent Learning if  minibatches  = 1\nStochastic Gradient Descent learning if  minibatches  = len(y)\nMinibatch learning if  minibatches  > 1    random_seed  : int (default: None)  Set random state for shuffling and initializing the weights.    print_progress  : int (default: 0)  Prints progress in fitting to stderr.\n0: No output\n1: Epochs elapsed and cost\n2: 1 plus time elapsed\n3: 2 plus estimated time until completion    dtype  : Array-type (default: None)  Uses tensorflow.float32 if None.    Attributes    w_  : 2d-array, shape=[n_features, n_classes]  Weights after fitting.    b_  : 1D-array, shape=[n_classes]  Bias units after fitting.    cost_  : list  List of floats, the average cross_entropy for each epoch.",
            "title": "TfSoftmaxRegression"
        },
        {
            "location": "/api_subpackages/mlxtend.tf_classifier/#methods_1",
            "text": "fit(X, y, init_params=True)  Learn model from training data.  Parameters    X  : {array-like, sparse matrix}, shape = [n_samples, n_features]  Training vectors, where n_samples is the number of samples and\nn_features is the number of features.    y  : array-like, shape = [n_samples]  Target values.    init_params  : bool (default: True)  Re-initializes model parameters prior to fitting.\nSet False to continue training with weights from\na previous model fitting.    Returns   self  : object    predict(X)  Predict targets from X.  Parameters    X  : {array-like, sparse matrix}, shape = [n_samples, n_features]  Training vectors, where n_samples is the number of samples and\nn_features is the number of features.    Returns    target_values  : array-like, shape = [n_samples]  Predicted target values.     predict_proba(X)  Predict class probabilities of X from the net input.  Parameters    X  : {array-like, sparse matrix}, shape = [n_samples, n_features]  Training vectors, where n_samples is the number of samples and\nn_features is the number of features.    Returns   Class probabilties  : array-like, shape= [n_samples, n_classes]    score(X, y)  Compute the prediction accuracy  Parameters    X  : {array-like, sparse matrix}, shape = [n_samples, n_features]  Training vectors, where n_samples is the number of samples and\nn_features is the number of features.    y  : array-like, shape = [n_samples]  Target values (true class labels).    Returns    acc  : float  The prediction accuracy as a float\nbetween 0.0 and 1.0 (perfect score).",
            "title": "Methods"
        },
        {
            "location": "/api_subpackages/mlxtend.data/",
            "text": "mlxtend version: 0.4.2dev0 \n\n\nautompg_data\n\n\nautompg_data()\n\n\nAuto MPG dataset.\n\n\n\n\n\n\nSource\n : https://archive.ics.uci.edu/ml/datasets/Auto+MPG\n\n\n\n\n\n\nNumber of samples\n : 392\n\n\n\n\n\n\nContinuous target variable\n : mpg\n\n\nDataset Attributes:\n\n\n\n\n1) cylinders:  multi-valued discrete\n\n\n2) displacement: continuous\n\n\n3) horsepower: continuous\n\n\n4) weight: continuous\n\n\n5) acceleration: continuous\n\n\n6) model year: multi-valued discrete\n\n\n7) origin: multi-valued discrete\n\n\n8) car name: string (unique for each instance)\n\n\n\n\n\n\n\n\nReturns\n\n\n\n\n\n\nX, y\n : [n_samples, n_features], [n_targets]\n\n\nX is the feature matrix with 392 auto samples as rows\nand 8 feature columns (6 rows with NaNs removed).\ny is a 1-dimensional array of the target MPG values.\n\n\n\n\n\n\nboston_housing_data\n\n\nboston_housing_data()\n\n\nBoston Housing dataset.\n\n\n\n\n\n\nSource\n : https://archive.ics.uci.edu/ml/datasets/Housing\n\n\n\n\n\n\nNumber of samples\n : 506\n\n\n\n\n\n\nContinuous target variable\n : MEDV\n\n\nMEDV = Median value of owner-occupied homes in $1000's\n\n\nDataset Attributes:\n\n\n\n\n1) CRIM      per capita crime rate by town\n\n\n2) ZN        proportion of residential land zoned for lots over\n25,000 sq.ft.\n\n\n3) INDUS     proportion of non-retail business acres per town\n\n\n4) CHAS      Charles River dummy variable (= 1 if tract bounds\nriver; 0 otherwise)\n\n\n5) NOX       nitric oxides concentration (parts per 10 million)\n\n\n6) RM        average number of rooms per dwelling\n\n\n7) AGE       proportion of owner-occupied units built prior to 1940\n\n\n8) DIS       weighted distances to five Boston employment centres\n\n\n9) RAD       index of accessibility to radial highways\n\n\n10) TAX      full-value property-tax rate per $10,000\n\n\n11) PTRATIO  pupil-teacher ratio by town\n\n\n12) B        1000(Bk - 0.63)^2 where Bk is the proportion of b. by town\n\n\n13) LSTAT    % lower status of the population\n\n\n\n\n\n\n\n\nReturns\n\n\n\n\n\n\nX, y\n : [n_samples, n_features], [n_class_labels]\n\n\nX is the feature matrix with 506 housing samples as rows\nand 13 feature columns.\ny is a 1-dimensional array of the continuous target variable MEDV\n\n\n\n\n\n\niris_data\n\n\niris_data()\n\n\nIris flower dataset.\n\n\n\n\n\n\nSource\n : https://archive.ics.uci.edu/ml/datasets/Iris\n\n\n\n\n\n\nNumber of samples\n : 150\n\n\n\n\n\n\nClass labels\n : {0, 1, 2}, distribution: [50, 50, 50]\n\n\n0 = setosa, 1 = versicolor, 2 = virginica.\n\n\nDataset Attributes:\n\n\n\n\n1) sepal length [cm]\n\n\n2) sepal width [cm]\n\n\n3) petal length [cm]\n\n\n4) petal width [cm]\n\n\n\n\n\n\n\n\nReturns\n\n\n\n\n\n\nX, y\n : [n_samples, n_features], [n_class_labels]\n\n\nX is the feature matrix with 150 flower samples as rows,\nand 4 feature columns sepal length, sepal width,\npetal length, and petal width.\ny is a 1-dimensional array of the class labels {0, 1, 2}\n\n\n\n\n\n\nloadlocal_mnist\n\n\nloadlocal_mnist(images_path, labels_path)\n\n\nRead MNIST from ubyte files.\n\n\nParameters\n\n\n\n\n\n\nimages_path\n : str\n\n\npath to the test or train MNIST ubyte file\n\n\n\n\n\n\nlabels_path\n : str\n\n\npath to the test or train MNIST class labels file\n\n\n\n\n\n\nReturns\n\n\n\n\n\n\nimages\n : [n_samples, n_pixels] numpy.array\n\n\nPixel values of the images.\n\n\n\n\n\n\nlabels\n : [n_samples] numpy array\n\n\nTarget class labels\n\n\n\n\n\n\nmnist_data\n\n\nmnist_data()\n\n\n5000 samples from the MNIST handwritten digits dataset.\n\n\n\n\nData Source\n : http://yann.lecun.com/exdb/mnist/\n\n\n\n\nReturns\n\n\n\n\n\n\nX, y\n : [n_samples, n_features], [n_class_labels]\n\n\nX is the feature matrix with 5000 image samples as rows,\neach row consists of 28x28 pixels that were unrolled into\n784 pixel feature vectors.\ny contains the 10 unique class labels 0-9.\n\n\n\n\n\n\nthree_blobs_data\n\n\nthree_blobs_data()\n\n\nA random dataset of 3 2D blobs for clustering.\n\n\n\n\n\n\nNumber of samples\n : 150\n\n\n\n\n\n\nSuggested labels\n : {0, 1, 2}, distribution: [50, 50, 50]\n\n\n\n\n\n\nReturns\n\n\n\n\n\n\nX, y\n : [n_samples, n_features], [n_cluster_labels]\n\n\nX is the feature matrix with 159 samples as rows\nand 2 feature columns.\ny is a 1-dimensional array of the 3 suggested cluster labels 0, 1, 2\n\n\n\n\n\n\nwine_data\n\n\nwine_data()\n\n\nWine dataset.\n\n\n\n\n\n\nSource\n : https://archive.ics.uci.edu/ml/datasets/Wine\n\n\n\n\n\n\nNumber of samples\n : 178\n\n\n\n\n\n\nClass labels\n : {0, 1, 2}, distribution: [59, 71, 48]\n\n\nDataset Attributes:\n\n\n\n\n1) Alcohol\n\n\n2) Malic acid\n\n\n3) Ash\n\n\n4) Alcalinity of ash\n\n\n5) Magnesium\n\n\n6) Total phenols\n\n\n7) Flavanoids\n\n\n8) Nonflavanoid phenols\n\n\n9) Proanthocyanins\n\n\n10) Color intensity\n\n\n11) Hue\n\n\n12) OD280/OD315 of diluted wines\n\n\n13) Proline\n\n\n\n\n\n\n\n\nReturns\n\n\n\n\n\n\nX, y\n : [n_samples, n_features], [n_class_labels]\n\n\nX is the feature matrix with 178 wine samples as rows\nand 13 feature columns.\ny is a 1-dimensional array of the 3 class labels 0, 1, 2",
            "title": "Mlxtend.data"
        },
        {
            "location": "/api_subpackages/mlxtend.data/#autompg_data",
            "text": "autompg_data()  Auto MPG dataset.    Source  : https://archive.ics.uci.edu/ml/datasets/Auto+MPG    Number of samples  : 392    Continuous target variable  : mpg  Dataset Attributes:   1) cylinders:  multi-valued discrete  2) displacement: continuous  3) horsepower: continuous  4) weight: continuous  5) acceleration: continuous  6) model year: multi-valued discrete  7) origin: multi-valued discrete  8) car name: string (unique for each instance)     Returns    X, y  : [n_samples, n_features], [n_targets]  X is the feature matrix with 392 auto samples as rows\nand 8 feature columns (6 rows with NaNs removed).\ny is a 1-dimensional array of the target MPG values.",
            "title": "autompg_data"
        },
        {
            "location": "/api_subpackages/mlxtend.data/#boston_housing_data",
            "text": "boston_housing_data()  Boston Housing dataset.    Source  : https://archive.ics.uci.edu/ml/datasets/Housing    Number of samples  : 506    Continuous target variable  : MEDV  MEDV = Median value of owner-occupied homes in $1000's  Dataset Attributes:   1) CRIM      per capita crime rate by town  2) ZN        proportion of residential land zoned for lots over\n25,000 sq.ft.  3) INDUS     proportion of non-retail business acres per town  4) CHAS      Charles River dummy variable (= 1 if tract bounds\nriver; 0 otherwise)  5) NOX       nitric oxides concentration (parts per 10 million)  6) RM        average number of rooms per dwelling  7) AGE       proportion of owner-occupied units built prior to 1940  8) DIS       weighted distances to five Boston employment centres  9) RAD       index of accessibility to radial highways  10) TAX      full-value property-tax rate per $10,000  11) PTRATIO  pupil-teacher ratio by town  12) B        1000(Bk - 0.63)^2 where Bk is the proportion of b. by town  13) LSTAT    % lower status of the population     Returns    X, y  : [n_samples, n_features], [n_class_labels]  X is the feature matrix with 506 housing samples as rows\nand 13 feature columns.\ny is a 1-dimensional array of the continuous target variable MEDV",
            "title": "boston_housing_data"
        },
        {
            "location": "/api_subpackages/mlxtend.data/#iris_data",
            "text": "iris_data()  Iris flower dataset.    Source  : https://archive.ics.uci.edu/ml/datasets/Iris    Number of samples  : 150    Class labels  : {0, 1, 2}, distribution: [50, 50, 50]  0 = setosa, 1 = versicolor, 2 = virginica.  Dataset Attributes:   1) sepal length [cm]  2) sepal width [cm]  3) petal length [cm]  4) petal width [cm]     Returns    X, y  : [n_samples, n_features], [n_class_labels]  X is the feature matrix with 150 flower samples as rows,\nand 4 feature columns sepal length, sepal width,\npetal length, and petal width.\ny is a 1-dimensional array of the class labels {0, 1, 2}",
            "title": "iris_data"
        },
        {
            "location": "/api_subpackages/mlxtend.data/#loadlocal_mnist",
            "text": "loadlocal_mnist(images_path, labels_path)  Read MNIST from ubyte files.  Parameters    images_path  : str  path to the test or train MNIST ubyte file    labels_path  : str  path to the test or train MNIST class labels file    Returns    images  : [n_samples, n_pixels] numpy.array  Pixel values of the images.    labels  : [n_samples] numpy array  Target class labels",
            "title": "loadlocal_mnist"
        },
        {
            "location": "/api_subpackages/mlxtend.data/#mnist_data",
            "text": "mnist_data()  5000 samples from the MNIST handwritten digits dataset.   Data Source  : http://yann.lecun.com/exdb/mnist/   Returns    X, y  : [n_samples, n_features], [n_class_labels]  X is the feature matrix with 5000 image samples as rows,\neach row consists of 28x28 pixels that were unrolled into\n784 pixel feature vectors.\ny contains the 10 unique class labels 0-9.",
            "title": "mnist_data"
        },
        {
            "location": "/api_subpackages/mlxtend.data/#three_blobs_data",
            "text": "three_blobs_data()  A random dataset of 3 2D blobs for clustering.    Number of samples  : 150    Suggested labels  : {0, 1, 2}, distribution: [50, 50, 50]    Returns    X, y  : [n_samples, n_features], [n_cluster_labels]  X is the feature matrix with 159 samples as rows\nand 2 feature columns.\ny is a 1-dimensional array of the 3 suggested cluster labels 0, 1, 2",
            "title": "three_blobs_data"
        },
        {
            "location": "/api_subpackages/mlxtend.data/#wine_data",
            "text": "wine_data()  Wine dataset.    Source  : https://archive.ics.uci.edu/ml/datasets/Wine    Number of samples  : 178    Class labels  : {0, 1, 2}, distribution: [59, 71, 48]  Dataset Attributes:   1) Alcohol  2) Malic acid  3) Ash  4) Alcalinity of ash  5) Magnesium  6) Total phenols  7) Flavanoids  8) Nonflavanoid phenols  9) Proanthocyanins  10) Color intensity  11) Hue  12) OD280/OD315 of diluted wines  13) Proline     Returns    X, y  : [n_samples, n_features], [n_class_labels]  X is the feature matrix with 178 wine samples as rows\nand 13 feature columns.\ny is a 1-dimensional array of the 3 class labels 0, 1, 2",
            "title": "wine_data"
        },
        {
            "location": "/api_subpackages/mlxtend.evaluate/",
            "text": "mlxtend version: 0.4.2dev0 \n\n\nconfusion_matrix\n\n\nconfusion_matrix(y_target, y_predicted, binary=False, positive_label=1)\n\n\nCompute a confusion matrix/contingency table.\n\n\nParameters\n\n\n\n\n\n\ny_target\n : array-like, shape=[n_samples]\n\n\nTrue class labels.\n\n\n\n\n\n\ny_predicted\n : array-like, shape=[n_samples]\n\n\nPredicted class labels.\n\n\n\n\n\n\nbinary\n : bool (default: False)\n\n\nMaps a multi-class problem onto a\nbinary confusion matrix, where\nthe positive class is 1 and\nall other classes are 0.\n\n\n\n\n\n\npositive_label\n : int (default: 1)\n\n\nClass label of the positive class.\n\n\n\n\n\n\nReturns\n\n\n\n\nmat\n : array-like, shape=[n_classes, n_classes]\n\n\n\n\nplot_confusion_matrix\n\n\nplot_confusion_matrix(conf_mat, hide_spines=False, hide_ticks=False, figsize=(2.5, 2.5), cmap=None, alpha=0.3)\n\n\nPlot a confusion matrix via matplotlib.\n\n\nParameters\n\n\n\n\n\n\nconf_mat\n : array-like, shape = [n_classes, n_classes]\n\n\nConfusion matrix from evaluate.confusion matrix.\n\n\n\n\n\n\nhide_spines\n : bool (default: False)\n\n\nHides axis spines if True.\n\n\n\n\n\n\nhide_ticks\n : bool (default: False)\n\n\nHides axis ticks if True\n\n\n\n\n\n\nfigsize\n : tuple (default: (2.5, 2.5))\n\n\nHeight and width of the figure\n\n\n\n\n\n\ncmap\n : matplotlib colormap (default: \nNone\n)\n\n\nUses matplotlib.pyplot.cm.Blues if \nNone\n\n\n\n\n\n\nReturns\n\n\n\n\n\n\nfig, ax\n : matplotlib.pyplot subplot objects\n\n\nFigure and axis elements of the subplot.\n\n\n\n\n\n\nplot_decision_regions\n\n\nplot_decision_regions(X, y, clf, ax=None, X_highlight=None, res=0.02, legend=1, hide_spines=True, markers='s^oxv<>', colors='red,blue,limegreen,gray,cyan')\n\n\nPlot decision regions of a classifier.\n\n\nParameters\n\n\n\n\n\n\nX\n : array-like, shape = [n_samples, n_features]\n\n\nFeature Matrix.\n\n\n\n\n\n\ny\n : array-like, shape = [n_samples]\n\n\nTrue class labels.\n\n\n\n\n\n\nclf\n : Classifier object.\n\n\nMust have a .predict method.\n\n\n\n\n\n\nax\n : matplotlib.axes.Axes (default: None)\n\n\nAn existing matplotlib Axes. Creates\none if ax=None.\n\n\n\n\n\n\nX_highlight\n : array-like, shape = [n_samples, n_features] (default: None)\n\n\nAn array with data points that are used to highlight samples in \nX\n.\n\n\n\n\n\n\nres\n : float (default: 0.02)\n\n\nGrid width. Lower values increase the resolution but\nslow down the plotting.\n\n\n\n\n\n\nhide_spines\n : bool (default: True)\n\n\nHide axis spines if True.\n\n\n\n\n\n\nlegend\n : int (default: 1)\n\n\nInteger to specify the legend location.\nNo legend if legend is 0.\n\n\n\n\n\n\nmarkers\n : list\n\n\nScatterplot markers.\n\n\n\n\n\n\ncolors\n : str (default 'red,blue,limegreen,gray,cyan')\n\n\nComma separated list of colors.\n\n\n\n\n\n\nReturns\n\n\n\n\nax\n : matplotlib.axes.Axes object\n\n\n\n\nplot_learning_curves\n\n\nplot_learning_curves(X_train, y_train, X_test, y_test, clf, train_marker='o', test_marker='^', scoring='misclassification error', suppress_plot=False, print_model=True, style='fivethirtyeight', legend_loc='best')\n\n\nPlots learning curves of a classifier.\n\n\nParameters\n\n\n\n\n\n\nX_train\n : array-like, shape = [n_samples, n_features]\n\n\nFeature matrix of the training dataset.\n\n\n\n\n\n\ny_train\n : array-like, shape = [n_samples]\n\n\nTrue class labels of the training dataset.\n\n\n\n\n\n\nX_test\n : array-like, shape = [n_samples, n_features]\n\n\nFeature matrix of the test dataset.\n\n\n\n\n\n\ny_test\n : array-like, shape = [n_samples]\n\n\nTrue class labels of the test dataset.\n\n\n\n\n\n\nclf\n : Classifier object. Must have a .predict .fit method.\n\n\n\n\n\n\ntrain_marker\n : str (default: 'o')\n\n\nMarker for the training set line plot.\n\n\n\n\n\n\ntest_marker\n : str (default: '^')\n\n\nMarker for the test set line plot.\n\n\n\n\n\n\nscoring\n : str (default: 'misclassification error')\n\n\nIf not 'misclassification error', accepts the following metrics\n(from scikit-learn):\n\n\n\n\n\n\nsuppress_plot=False\n : bool (default: False)\n\n\nSuppress matplotlib plots if True. Recommended\nfor testing purposes.\n\n\n\n\n\n\nprint_model\n : bool (default: True)\n\n\nPrint model parameters in plot title if True.\n\n\n\n\n\n\nstyle\n : str (default: 'fivethirtyeight')\n\n\nMatplotlib style\n\n\n\n\n\n\nlegend_loc\n : str (default: 'best')\n\n\nWhere to place the plot legend:\n\n\n\n\n\n\nReturns\n\n\n\n\nerrors\n : (training_error, test_error): tuple of lists\n\n\n\n\nscoring\n\n\nscoring(y_target, y_predicted, metric='error', positive_label=1, unique_labels='auto')\n\n\nCompute a scoring metric for supervised learning.\n\n\nParameters\n\n\n\n\n\n\ny_target\n : array-like, shape=[n_values]\n\n\nTrue class labels or target values.\n\n\n\n\n\n\ny_predicted\n : array-like, shape=[n_values]\n\n\nPredicted class labels or target values.\n\n\n\n\n\n\nmetric\n : str (default: 'error')\n\n\nPerformance metric:\n'accuracy': (TP + TN)/(FP + FN + TP + TN) = 1-ERR\n\n\n'per-class accuracy': Average per-class accuracy\n\n\n'per-class error':  Average per-class error\n\n\n'error': (TP + TN)/(FP+ FN + TP + TN) = 1-ACC\n\n\n'false_positive_rate': FP/N = FP/(FP + TN)\n\n\n'true_positive_rate': TP/P = TP/(FN + TP)\n\n\n'true_negative_rate': TN/N = TN/(FP + TN)\n\n\n'precision': TP/(TP + FP)\n\n\n'recall': equal to 'true_positive_rate'\n\n\n'sensitivity': equal to 'true_positive_rate' or 'recall'\n\n\n'specificity': equal to 'true_negative_rate'\n\n\n'f1': 2 * (PRE * REC)/(PRE + REC)\n\n\n'matthews_corr_coef':  (TP\nTN - FP\nFN)\n/ (sqrt{(TP + FP)( TP + FN )( TN + FP )( TN + FN )})\n\n\nWhere:\n[TP: True positives, TN = True negatives,\n\n\nTN: True negatives, FN = False negatives]\n\n\n\n\n\n\npositive_label\n : int (default: 1)\n\n\nLabel of the positive class for binary classification\nmetrics.\n\n\n\n\n\n\nunique_labels\n : str or array-like (default: 'auto')\n\n\nIf 'auto', deduces the unique class labels from\ny_target\n\n\n\n\n\n\nReturns\n\n\n\n\nscore\n : float",
            "title": "Mlxtend.evaluate"
        },
        {
            "location": "/api_subpackages/mlxtend.evaluate/#confusion_matrix",
            "text": "confusion_matrix(y_target, y_predicted, binary=False, positive_label=1)  Compute a confusion matrix/contingency table.  Parameters    y_target  : array-like, shape=[n_samples]  True class labels.    y_predicted  : array-like, shape=[n_samples]  Predicted class labels.    binary  : bool (default: False)  Maps a multi-class problem onto a\nbinary confusion matrix, where\nthe positive class is 1 and\nall other classes are 0.    positive_label  : int (default: 1)  Class label of the positive class.    Returns   mat  : array-like, shape=[n_classes, n_classes]",
            "title": "confusion_matrix"
        },
        {
            "location": "/api_subpackages/mlxtend.evaluate/#plot_confusion_matrix",
            "text": "plot_confusion_matrix(conf_mat, hide_spines=False, hide_ticks=False, figsize=(2.5, 2.5), cmap=None, alpha=0.3)  Plot a confusion matrix via matplotlib.  Parameters    conf_mat  : array-like, shape = [n_classes, n_classes]  Confusion matrix from evaluate.confusion matrix.    hide_spines  : bool (default: False)  Hides axis spines if True.    hide_ticks  : bool (default: False)  Hides axis ticks if True    figsize  : tuple (default: (2.5, 2.5))  Height and width of the figure    cmap  : matplotlib colormap (default:  None )  Uses matplotlib.pyplot.cm.Blues if  None    Returns    fig, ax  : matplotlib.pyplot subplot objects  Figure and axis elements of the subplot.",
            "title": "plot_confusion_matrix"
        },
        {
            "location": "/api_subpackages/mlxtend.evaluate/#plot_decision_regions",
            "text": "plot_decision_regions(X, y, clf, ax=None, X_highlight=None, res=0.02, legend=1, hide_spines=True, markers='s^oxv<>', colors='red,blue,limegreen,gray,cyan')  Plot decision regions of a classifier.  Parameters    X  : array-like, shape = [n_samples, n_features]  Feature Matrix.    y  : array-like, shape = [n_samples]  True class labels.    clf  : Classifier object.  Must have a .predict method.    ax  : matplotlib.axes.Axes (default: None)  An existing matplotlib Axes. Creates\none if ax=None.    X_highlight  : array-like, shape = [n_samples, n_features] (default: None)  An array with data points that are used to highlight samples in  X .    res  : float (default: 0.02)  Grid width. Lower values increase the resolution but\nslow down the plotting.    hide_spines  : bool (default: True)  Hide axis spines if True.    legend  : int (default: 1)  Integer to specify the legend location.\nNo legend if legend is 0.    markers  : list  Scatterplot markers.    colors  : str (default 'red,blue,limegreen,gray,cyan')  Comma separated list of colors.    Returns   ax  : matplotlib.axes.Axes object",
            "title": "plot_decision_regions"
        },
        {
            "location": "/api_subpackages/mlxtend.evaluate/#plot_learning_curves",
            "text": "plot_learning_curves(X_train, y_train, X_test, y_test, clf, train_marker='o', test_marker='^', scoring='misclassification error', suppress_plot=False, print_model=True, style='fivethirtyeight', legend_loc='best')  Plots learning curves of a classifier.  Parameters    X_train  : array-like, shape = [n_samples, n_features]  Feature matrix of the training dataset.    y_train  : array-like, shape = [n_samples]  True class labels of the training dataset.    X_test  : array-like, shape = [n_samples, n_features]  Feature matrix of the test dataset.    y_test  : array-like, shape = [n_samples]  True class labels of the test dataset.    clf  : Classifier object. Must have a .predict .fit method.    train_marker  : str (default: 'o')  Marker for the training set line plot.    test_marker  : str (default: '^')  Marker for the test set line plot.    scoring  : str (default: 'misclassification error')  If not 'misclassification error', accepts the following metrics\n(from scikit-learn):    suppress_plot=False  : bool (default: False)  Suppress matplotlib plots if True. Recommended\nfor testing purposes.    print_model  : bool (default: True)  Print model parameters in plot title if True.    style  : str (default: 'fivethirtyeight')  Matplotlib style    legend_loc  : str (default: 'best')  Where to place the plot legend:    Returns   errors  : (training_error, test_error): tuple of lists",
            "title": "plot_learning_curves"
        },
        {
            "location": "/api_subpackages/mlxtend.evaluate/#scoring",
            "text": "scoring(y_target, y_predicted, metric='error', positive_label=1, unique_labels='auto')  Compute a scoring metric for supervised learning.  Parameters    y_target  : array-like, shape=[n_values]  True class labels or target values.    y_predicted  : array-like, shape=[n_values]  Predicted class labels or target values.    metric  : str (default: 'error')  Performance metric:\n'accuracy': (TP + TN)/(FP + FN + TP + TN) = 1-ERR  'per-class accuracy': Average per-class accuracy  'per-class error':  Average per-class error  'error': (TP + TN)/(FP+ FN + TP + TN) = 1-ACC  'false_positive_rate': FP/N = FP/(FP + TN)  'true_positive_rate': TP/P = TP/(FN + TP)  'true_negative_rate': TN/N = TN/(FP + TN)  'precision': TP/(TP + FP)  'recall': equal to 'true_positive_rate'  'sensitivity': equal to 'true_positive_rate' or 'recall'  'specificity': equal to 'true_negative_rate'  'f1': 2 * (PRE * REC)/(PRE + REC)  'matthews_corr_coef':  (TP TN - FP FN)\n/ (sqrt{(TP + FP)( TP + FN )( TN + FP )( TN + FN )})  Where:\n[TP: True positives, TN = True negatives,  TN: True negatives, FN = False negatives]    positive_label  : int (default: 1)  Label of the positive class for binary classification\nmetrics.    unique_labels  : str or array-like (default: 'auto')  If 'auto', deduces the unique class labels from\ny_target    Returns   score  : float",
            "title": "scoring"
        },
        {
            "location": "/api_subpackages/mlxtend.feature_selection/",
            "text": "mlxtend version: 0.4.2dev0 \n\n\nColumnSelector\n\n\nColumnSelector(cols=None)\n\n\nBase class for all estimators in scikit-learn\n\n\nNotes\n\n\nAll estimators should specify all the parameters that can be set\n    at the class level in their \n__init__\n as explicit keyword\narguments (no \n*args\n or \n**kwargs\n).\n\n\nMethods\n\n\n\n\n\nfit(X, y=None)\n\n\nMock method. Does nothing.\n\n\nParameters\n\n\n\n\n\n\nX\n : {array-like, sparse matrix}, shape = [n_samples, n_features]\n\n\nTraining vectors, where n_samples is the number of samples and\nn_features is the number of features.\n\n\n\n\n\n\ny\n : array-like, shape = [n_samples] (default: None)\n\n\n\n\n\n\nReturns\n\n\nself\n\n\n\n\n\nfit_transform(X, y=None)\n\n\nReturn a slice of the input array.\n\n\nParameters\n\n\n\n\n\n\nX\n : {array-like, sparse matrix}, shape = [n_samples, n_features]\n\n\nTraining vectors, where n_samples is the number of samples and\nn_features is the number of features.\n\n\n\n\n\n\ny\n : array-like, shape = [n_samples] (default: None)\n\n\n\n\n\n\nReturns\n\n\n\n\n\n\nX_slice\n : shape = [n_samples, k_features]\n\n\nSubset of the feature space where k_features <= n_features\n\n\n\n\n\n\n\n\n\nget_params(deep=True)\n\n\nGet parameters for this estimator.\n\n\nParameters\n\n\ndeep: boolean, optional\n    If True, will return the parameters for this estimator and\n    contained subobjects that are estimators.\n\n\nReturns\n\n\n\n\n\n\nparams\n : mapping of string to any\n\n\nParameter names mapped to their values.\n\n\n\n\n\n\n\n\n\nset_params(\nparams)\n\n\nSet the parameters of this estimator.\n\n\nThe method works on simple estimators as well as on nested objects\n(such as pipelines). The former have parameters of the form\n``<component>__<parameter>`` so that it's possible to update each\ncomponent of a nested object.\n\n\n\nReturns\n\n\nself\n\n\n\n\n\ntransform(X, y=None)\n\n\nNone\n\n\nplot_sequential_feature_selection\n\n\nplot_sequential_feature_selection(metric_dict, kind='std_dev', color='blue', bcolor='steelblue', marker='o', alpha=0.2, ylabel='Performance', confidence_interval=0.95)\n\n\nPlot sequential feature selection.\n\n\nParameters\n\n\n\n\n\n\nmetric_dict\n : mlxtend.SequentialFeatureSelector.get_metric_dict() object\n\n\n\n\n\n\nkind\n : str (default: \"std_dev\")\n\n\nThe kind of error bar or confidence interval in\n{'std_dev', 'std_err', 'ci', None}.\n\n\n\n\n\n\ncolor\n : str (default: \"blue\")\n\n\nColor of the lineplot (accepts any matplotlib color name)\n\n\n\n\n\n\nbcolor\n : str (default: \"steelblue\").\n\n\nColor of the error bars / confidence intervals\n(accepts any matplotlib color name).\n\n\n\n\n\n\nmarker\n : str (default: \"o\")\n\n\nMarker of the line plot\n(accepts any matplotlib marker name).\n\n\n\n\n\n\nalpha\n : float in [0, 1] (default: 0.2)\n\n\nTransparency of the error bars / confidence intervals.\n\n\n\n\n\n\nylabel\n : str (default: \"Performance\")\n\n\nY-axis label.\n\n\n\n\n\n\nconfidence_interval\n : float (default: 0.95)\n\n\nConfidence level if \nkind='ci'\n.\n\n\n\n\n\n\nReturns\n\n\n\n\nfig\n : matplotlib.pyplot.figure() object\n\n\n\n\nSequentialFeatureSelector\n\n\nSequentialFeatureSelector(estimator, k_features='best', forward=True, floating=False, print_progress=True, scoring='accuracy', cv=5, skip_if_stuck=True, n_jobs=1, pre_dispatch='2\nn_jobs', clone_estimator=True)*\n\n\nSequential Feature Selection for Classification and Regression.\n\n\nParameters\n\n\n\n\n\n\nestimator\n : scikit-learn classifier or regressor\n\n\n\n\n\n\nk_features\n : int or tuple (new in 0.4.2) (default: 1)\n\n\nNumber of features to select,\nwhere k_features < the full feature set.\nNew in 0.4.2: A tuple containing a min and max value can be provided,\nand the SFS will consider return any feature combination between\nmin and max that scored highest in cross-validtion. For example,\nthe tuple (1, 4) will return any combination from\n1 up to 4 features instead of a fixed number of features k.\n\n\n\n\n\n\nforward\n : bool (default: True)\n\n\nForward selection if True,\nbackward selection otherwise\n\n\n\n\n\n\nfloating\n : bool (default: False)\n\n\nAdds a conditional exclusion/inclusion if True.\n\n\n\n\n\n\nprint_progress\n : bool (default: True)\n\n\nPrints progress as the number of epochs\nto stderr.\n\n\n\n\n\n\nscoring\n : str, (default='accuracy')\n\n\nScoring metric in {accuracy, f1, precision, recall, roc_auc}\nfor classifiers,\n{'mean_absolute_error', 'mean_squared_error',\n'median_absolute_error', 'r2'} for regressors,\nor a callable object or function with\nsignature \nscorer(estimator, X, y)\n.\n\n\n\n\n\n\ncv\n : int (default: 5)\n\n\nScikit-learn cross-validation generator or \nint\n.\nIf estimator is a classifier (or y consists of integer class labels),\nstratified k-fold is performed, and regular k-fold cross-validation\notherwise.\nNo cross-validation if cv is None, False, or 0.\nskip_if_stuck: bool (default: True)\nSet to True to skip conditional\nexclusion/inclusion if floating=True and\nalgorithm gets stuck in cycles.\n\n\n\n\n\n\nn_jobs\n : int (default: 1)\n\n\nThe number of CPUs to use for cross validation. -1 means 'all CPUs'.\n\n\n\n\n\n\npre_dispatch\n : int, or string (default: '2*n_jobs')\n\n\nControls the number of jobs that get dispatched\nduring parallel execution in cross_val_score.\nReducing this number can be useful to avoid an explosion of\nmemory consumption when more jobs get dispatched than CPUs can process.\nThis parameter can be:\nNone, in which case all the jobs are immediately created and spawned.\nUse this for lightweight and fast-running jobs,\nto avoid delays due to on-demand spawning of the jobs\nAn int, giving the exact number of total jobs that are spawned\nA string, giving an expression as a function\nof n_jobs, as in \n2*n_jobs\n\n\n\n\n\n\nclone_estimator\n : bool (default: True)\n\n\nClones estimator if True; works with the original estimator instance\nif False. Set to False if the estimator doesn't\nimplement scikit-learn's set_params and get_params methods.\nIn addition, it is required to set cv=0, and n_jobs=1.\n\n\n\n\n\n\nAttributes\n\n\n\n\n\n\nk_feature_idx_\n : array-like, shape = [n_predictions]\n\n\nFeature Indices of the selected feature subsets.\n\n\n\n\n\n\nk_score_\n : float\n\n\nCross validation average score of the selected subset.\n\n\n\n\n\n\nsubsets_\n : dict\n\n\nA dictionary of selected feature subsets during the\nsequential selection, where the dictionary keys are\nthe lengths k of these feature subsets. The dictionary\nvalues are dictionaries themselves with the following\nkeys: 'feature_idx' (tuple of indices of the feature subset)\n'cv_scores' (list individual cross-validation scores)\n'avg_score' (average cross-validation score)\n\n\n\n\n\n\nMethods\n\n\n\n\n\nfit(X, y)\n\n\nPerform feature selection and learn model from training data.\n\n\nParameters\n\n\n\n\n\n\nX\n : {array-like, sparse matrix}, shape = [n_samples, n_features]\n\n\nTraining vectors, where n_samples is the number of samples and\nn_features is the number of features.\n\n\n\n\n\n\ny\n : array-like, shape = [n_samples]\n\n\nTarget values.\n\n\n\n\n\n\nReturns\n\n\n\n\nself\n : object\n\n\n\n\n\n\n\nfit_transform(X, y)\n\n\nFit to training data then reduce X to its most important features.\n\n\nParameters\n\n\n\n\n\n\nX\n : {array-like, sparse matrix}, shape = [n_samples, n_features]\n\n\nTraining vectors, where n_samples is the number of samples and\nn_features is the number of features.\n\n\n\n\n\n\nReturns\n\n\nReduced feature subset of X, shape={n_samples, k_features}\n\n\n\n\n\nget_metric_dict(confidence_interval=0.95)\n\n\nReturn metric dictionary\n\n\nParameters\n\n\n\n\n\n\nconfidence_interval\n : float (default: 0.95)\n\n\nA positive float between 0.0 and 1.0 to compute the confidence\ninterval bounds of the CV score averages.\n\n\n\n\n\n\nReturns\n\n\nDictionary with items where each dictionary value is a list\n    with the number of iterations (number of feature subsets) as\n    its length. The dictionary keys corresponding to these lists\n    are as follows:\n    'feature_idx': tuple of the indices of the feature subset\n    'cv_scores': list with individual CV scores\n    'avg_score': of CV average scores\n    'std_dev': standard deviation of the CV score average\n    'std_err': standard error of the CV score average\n    'ci_bound': confidence interval bound of the CV score average\n\n\n\n\n\nget_params(deep=True)\n\n\nGet parameters for this estimator.\n\n\nParameters\n\n\ndeep: boolean, optional\n    If True, will return the parameters for this estimator and\n    contained subobjects that are estimators.\n\n\nReturns\n\n\n\n\n\n\nparams\n : mapping of string to any\n\n\nParameter names mapped to their values.\n\n\n\n\n\n\n\n\n\nset_params(\nparams)\n\n\nSet the parameters of this estimator.\n\n\nThe method works on simple estimators as well as on nested objects\n(such as pipelines). The former have parameters of the form\n``<component>__<parameter>`` so that it's possible to update each\ncomponent of a nested object.\n\n\n\nReturns\n\n\nself\n\n\n\n\n\ntransform(X)\n\n\nReduce X to its most important features.\n\n\nParameters\n\n\n\n\n\n\nX\n : {array-like, sparse matrix}, shape = [n_samples, n_features]\n\n\nTraining vectors, where n_samples is the number of samples and\nn_features is the number of features.\n\n\n\n\n\n\nReturns\n\n\nReduced feature subset of X, shape={n_samples, k_features}",
            "title": "Mlxtend.feature selection"
        },
        {
            "location": "/api_subpackages/mlxtend.feature_selection/#columnselector",
            "text": "ColumnSelector(cols=None)  Base class for all estimators in scikit-learn  Notes  All estimators should specify all the parameters that can be set\n    at the class level in their  __init__  as explicit keyword\narguments (no  *args  or  **kwargs ).",
            "title": "ColumnSelector"
        },
        {
            "location": "/api_subpackages/mlxtend.feature_selection/#methods",
            "text": "fit(X, y=None)  Mock method. Does nothing.  Parameters    X  : {array-like, sparse matrix}, shape = [n_samples, n_features]  Training vectors, where n_samples is the number of samples and\nn_features is the number of features.    y  : array-like, shape = [n_samples] (default: None)    Returns  self   fit_transform(X, y=None)  Return a slice of the input array.  Parameters    X  : {array-like, sparse matrix}, shape = [n_samples, n_features]  Training vectors, where n_samples is the number of samples and\nn_features is the number of features.    y  : array-like, shape = [n_samples] (default: None)    Returns    X_slice  : shape = [n_samples, k_features]  Subset of the feature space where k_features <= n_features     get_params(deep=True)  Get parameters for this estimator.  Parameters  deep: boolean, optional\n    If True, will return the parameters for this estimator and\n    contained subobjects that are estimators.  Returns    params  : mapping of string to any  Parameter names mapped to their values.     set_params( params)  Set the parameters of this estimator.  The method works on simple estimators as well as on nested objects\n(such as pipelines). The former have parameters of the form\n``<component>__<parameter>`` so that it's possible to update each\ncomponent of a nested object.  Returns  self   transform(X, y=None)  None",
            "title": "Methods"
        },
        {
            "location": "/api_subpackages/mlxtend.feature_selection/#plot_sequential_feature_selection",
            "text": "plot_sequential_feature_selection(metric_dict, kind='std_dev', color='blue', bcolor='steelblue', marker='o', alpha=0.2, ylabel='Performance', confidence_interval=0.95)  Plot sequential feature selection.  Parameters    metric_dict  : mlxtend.SequentialFeatureSelector.get_metric_dict() object    kind  : str (default: \"std_dev\")  The kind of error bar or confidence interval in\n{'std_dev', 'std_err', 'ci', None}.    color  : str (default: \"blue\")  Color of the lineplot (accepts any matplotlib color name)    bcolor  : str (default: \"steelblue\").  Color of the error bars / confidence intervals\n(accepts any matplotlib color name).    marker  : str (default: \"o\")  Marker of the line plot\n(accepts any matplotlib marker name).    alpha  : float in [0, 1] (default: 0.2)  Transparency of the error bars / confidence intervals.    ylabel  : str (default: \"Performance\")  Y-axis label.    confidence_interval  : float (default: 0.95)  Confidence level if  kind='ci' .    Returns   fig  : matplotlib.pyplot.figure() object",
            "title": "plot_sequential_feature_selection"
        },
        {
            "location": "/api_subpackages/mlxtend.feature_selection/#sequentialfeatureselector",
            "text": "SequentialFeatureSelector(estimator, k_features='best', forward=True, floating=False, print_progress=True, scoring='accuracy', cv=5, skip_if_stuck=True, n_jobs=1, pre_dispatch='2 n_jobs', clone_estimator=True)*  Sequential Feature Selection for Classification and Regression.  Parameters    estimator  : scikit-learn classifier or regressor    k_features  : int or tuple (new in 0.4.2) (default: 1)  Number of features to select,\nwhere k_features < the full feature set.\nNew in 0.4.2: A tuple containing a min and max value can be provided,\nand the SFS will consider return any feature combination between\nmin and max that scored highest in cross-validtion. For example,\nthe tuple (1, 4) will return any combination from\n1 up to 4 features instead of a fixed number of features k.    forward  : bool (default: True)  Forward selection if True,\nbackward selection otherwise    floating  : bool (default: False)  Adds a conditional exclusion/inclusion if True.    print_progress  : bool (default: True)  Prints progress as the number of epochs\nto stderr.    scoring  : str, (default='accuracy')  Scoring metric in {accuracy, f1, precision, recall, roc_auc}\nfor classifiers,\n{'mean_absolute_error', 'mean_squared_error',\n'median_absolute_error', 'r2'} for regressors,\nor a callable object or function with\nsignature  scorer(estimator, X, y) .    cv  : int (default: 5)  Scikit-learn cross-validation generator or  int .\nIf estimator is a classifier (or y consists of integer class labels),\nstratified k-fold is performed, and regular k-fold cross-validation\notherwise.\nNo cross-validation if cv is None, False, or 0.\nskip_if_stuck: bool (default: True)\nSet to True to skip conditional\nexclusion/inclusion if floating=True and\nalgorithm gets stuck in cycles.    n_jobs  : int (default: 1)  The number of CPUs to use for cross validation. -1 means 'all CPUs'.    pre_dispatch  : int, or string (default: '2*n_jobs')  Controls the number of jobs that get dispatched\nduring parallel execution in cross_val_score.\nReducing this number can be useful to avoid an explosion of\nmemory consumption when more jobs get dispatched than CPUs can process.\nThis parameter can be:\nNone, in which case all the jobs are immediately created and spawned.\nUse this for lightweight and fast-running jobs,\nto avoid delays due to on-demand spawning of the jobs\nAn int, giving the exact number of total jobs that are spawned\nA string, giving an expression as a function\nof n_jobs, as in  2*n_jobs    clone_estimator  : bool (default: True)  Clones estimator if True; works with the original estimator instance\nif False. Set to False if the estimator doesn't\nimplement scikit-learn's set_params and get_params methods.\nIn addition, it is required to set cv=0, and n_jobs=1.    Attributes    k_feature_idx_  : array-like, shape = [n_predictions]  Feature Indices of the selected feature subsets.    k_score_  : float  Cross validation average score of the selected subset.    subsets_  : dict  A dictionary of selected feature subsets during the\nsequential selection, where the dictionary keys are\nthe lengths k of these feature subsets. The dictionary\nvalues are dictionaries themselves with the following\nkeys: 'feature_idx' (tuple of indices of the feature subset)\n'cv_scores' (list individual cross-validation scores)\n'avg_score' (average cross-validation score)",
            "title": "SequentialFeatureSelector"
        },
        {
            "location": "/api_subpackages/mlxtend.feature_selection/#methods_1",
            "text": "fit(X, y)  Perform feature selection and learn model from training data.  Parameters    X  : {array-like, sparse matrix}, shape = [n_samples, n_features]  Training vectors, where n_samples is the number of samples and\nn_features is the number of features.    y  : array-like, shape = [n_samples]  Target values.    Returns   self  : object    fit_transform(X, y)  Fit to training data then reduce X to its most important features.  Parameters    X  : {array-like, sparse matrix}, shape = [n_samples, n_features]  Training vectors, where n_samples is the number of samples and\nn_features is the number of features.    Returns  Reduced feature subset of X, shape={n_samples, k_features}   get_metric_dict(confidence_interval=0.95)  Return metric dictionary  Parameters    confidence_interval  : float (default: 0.95)  A positive float between 0.0 and 1.0 to compute the confidence\ninterval bounds of the CV score averages.    Returns  Dictionary with items where each dictionary value is a list\n    with the number of iterations (number of feature subsets) as\n    its length. The dictionary keys corresponding to these lists\n    are as follows:\n    'feature_idx': tuple of the indices of the feature subset\n    'cv_scores': list with individual CV scores\n    'avg_score': of CV average scores\n    'std_dev': standard deviation of the CV score average\n    'std_err': standard error of the CV score average\n    'ci_bound': confidence interval bound of the CV score average   get_params(deep=True)  Get parameters for this estimator.  Parameters  deep: boolean, optional\n    If True, will return the parameters for this estimator and\n    contained subobjects that are estimators.  Returns    params  : mapping of string to any  Parameter names mapped to their values.     set_params( params)  Set the parameters of this estimator.  The method works on simple estimators as well as on nested objects\n(such as pipelines). The former have parameters of the form\n``<component>__<parameter>`` so that it's possible to update each\ncomponent of a nested object.  Returns  self   transform(X)  Reduce X to its most important features.  Parameters    X  : {array-like, sparse matrix}, shape = [n_samples, n_features]  Training vectors, where n_samples is the number of samples and\nn_features is the number of features.    Returns  Reduced feature subset of X, shape={n_samples, k_features}",
            "title": "Methods"
        },
        {
            "location": "/api_subpackages/mlxtend.feature_extraction/",
            "text": "mlxtend version: 0.4.2dev0 \n\n\nLinearDiscriminantAnalysis\n\n\nLinearDiscriminantAnalysis(n_discriminants=None)\n\n\nLinear Discriminant Analysis Class\n\n\nParameters\n\n\n\n\n\n\nn_discriminants\n : int (default: None)\n\n\nThe number of discrimants for transformation.\nKeeps the original dimensions of the dataset if \nNone\n.\n\n\n\n\n\n\nAttributes\n\n\n\n\n\n\nw_\n : array-like, shape=[n_features, n_discriminants]\n\n\nProjection matrix\n\n\n\n\n\n\ne_vals_\n : array-like, shape=[n_features]\n\n\nEigenvalues in sorted order.\n\n\n\n\n\n\ne_vecs_\n : array-like, shape=[n_features]\n\n\nEigenvectors in sorted order.\n\n\n\n\n\n\nMethods\n\n\n\n\n\nfit(X, y, n_classes=None)\n\n\nFit the LDA model with X.\n\n\nParameters\n\n\n\n\n\n\nX\n : {array-like, sparse matrix}, shape = [n_samples, n_features]\n\n\nTraining vectors, where n_samples is the number of samples and\nn_features is the number of features.\n\n\n\n\n\n\ny\n : array-like, shape = [n_samples]\n\n\nTarget values.\n\n\n\n\n\n\nn_classes\n : int (default: None)\n\n\nA positive integer to declare the number of class labels\nif not all class labels are present in a partial training set.\nGets the number of class labels automatically if None.\n\n\n\n\n\n\nReturns\n\n\n\n\nself\n : object\n\n\n\n\n\n\n\ntransform(X)\n\n\nApply the linear transformation on X.\n\n\nParameters\n\n\n\n\n\n\nX\n : {array-like, sparse matrix}, shape = [n_samples, n_features]\n\n\nTraining vectors, where n_samples is the number of samples and\nn_features is the number of features.\n\n\n\n\n\n\nReturns\n\n\n\n\n\n\nX_projected\n : np.ndarray, shape = [n_samples, n_discriminants]\n\n\nProjected training vectors.\n\n\n\n\n\n\nPrincipalComponentAnalysis\n\n\nPrincipalComponentAnalysis(n_components=None, solver='eigen')\n\n\nPrincipal Component Analysis Class\n\n\nParameters\n\n\n\n\n\n\nn_components\n : int (default: None)\n\n\nThe number of principal components for transformation.\nKeeps the original dimensions of the dataset if \nNone\n.\n\n\n\n\n\n\nsolver\n : str (default: 'eigen')\n\n\nMethod for performing the matrix decomposition.\n\n\n\n\n\n\nAttributes\n\n\n\n\n\n\nw_\n : array-like, shape=[n_features, n_components]\n\n\nProjection matrix\n\n\n\n\n\n\ne_vals_\n : array-like, shape=[n_features]\n\n\nEigenvalues in sorted order.\n\n\n\n\n\n\ne_vecs_\n : array-like, shape=[n_features]\n\n\nEigenvectors in sorted order.\n\n\n\n\n\n\nMethods\n\n\n\n\n\nfit(X)\n\n\nLearn model from training data.\n\n\nParameters\n\n\n\n\n\n\nX\n : {array-like, sparse matrix}, shape = [n_samples, n_features]\n\n\nTraining vectors, where n_samples is the number of samples and\nn_features is the number of features.\n\n\n\n\n\n\nReturns\n\n\n\n\nself\n : object\n\n\n\n\n\n\n\ntransform(X)\n\n\nApply the linear transformation on X.\n\n\nParameters\n\n\n\n\n\n\nX\n : {array-like, sparse matrix}, shape = [n_samples, n_features]\n\n\nTraining vectors, where n_samples is the number of samples and\nn_features is the number of features.\n\n\n\n\n\n\nReturns\n\n\n\n\n\n\nX_projected\n : np.ndarray, shape = [n_samples, n_components]\n\n\nProjected training vectors.\n\n\n\n\n\n\nRBFKernelPCA\n\n\nRBFKernelPCA(gamma=15.0, n_components=None, copy_X=True)\n\n\nRBF Kernel Principal Component Analysis for dimensionality reduction.\n\n\nParameters\n\n\n\n\n\n\ngamma\n : float (default: 15.0)\n\n\nFree parameter (coefficient) of the RBF kernel.\n\n\n\n\n\n\nn_components\n : int (default: None)\n\n\nThe number of principal components for transformation.\nKeeps the original dimensions of the dataset if \nNone\n.\n\n\n\n\n\n\ncopy_X\n : bool (default: True)\n\n\nCopies training data, which is required to compute the projection\nof new data via the transform method. Uses a reference to X if False.\n\n\n\n\n\n\nAttributes\n\n\n\n\n\n\ne_vals_\n : array-like, shape=[n_features]\n\n\nEigenvalues in sorted order.\n\n\n\n\n\n\ne_vecs_\n : array-like, shape=[n_features]\n\n\nEigenvectors in sorted order.\n\n\n\n\n\n\nX_projected_\n : array-like, shape=[n_samples, n_components]\n\n\nTraining samples projected along the component axes.\n\n\n\n\n\n\nMethods\n\n\n\n\n\nfit(X)\n\n\nLearn model from training data.\n\n\nParameters\n\n\n\n\n\n\nX\n : {array-like, sparse matrix}, shape = [n_samples, n_features]\n\n\nTraining vectors, where n_samples is the number of samples and\nn_features is the number of features.\n\n\n\n\n\n\nReturns\n\n\n\n\nself\n : object\n\n\n\n\n\n\n\ntransform(X)\n\n\nApply the non-linear transformation on X.\n\n\nParameters\n\n\n\n\n\n\nX\n : {array-like, sparse matrix}, shape = [n_samples, n_features]\n\n\nTraining vectors, where n_samples is the number of samples and\nn_features is the number of features.\n\n\n\n\n\n\nReturns\n\n\n\n\n\n\nX_projected\n : np.ndarray, shape = [n_samples, n_components]\n\n\nProjected training vectors.\n\n\n\n\n\n\nRBFKernelPrincipalComponentAnalysis\n\n\nRBFKernelPrincipalComponentAnalysis(gamma=15.0, n_components=None, copy_X=True)\n\n\nRBF Kernel Principal Component Analysis for dimensionality reduction.\n\n\nParameters\n\n\n\n\n\n\ngamma\n : float (default: 15.0)\n\n\nFree parameter (coefficient) of the RBF kernel.\n\n\n\n\n\n\nn_components\n : int (default: None)\n\n\nThe number of principal components for transformation.\nKeeps the original dimensions of the dataset if \nNone\n.\n\n\n\n\n\n\ncopy_X\n : bool (default: True)\n\n\nCopies training data, which is required to compute the projection\nof new data via the transform method. Uses a reference to X if False.\n\n\n\n\n\n\nAttributes\n\n\n\n\n\n\ne_vals_\n : array-like, shape=[n_features]\n\n\nEigenvalues in sorted order.\n\n\n\n\n\n\ne_vecs_\n : array-like, shape=[n_features]\n\n\nEigenvectors in sorted order.\n\n\n\n\n\n\nX_projected_\n : array-like, shape=[n_samples, n_components]\n\n\nTraining samples projected along the component axes.\n\n\n\n\n\n\nMethods\n\n\n\n\n\nfit(X)\n\n\nFit the PCA model with X.\n\n\nParameters\n\n\n\n\n\n\nX\n : {array-like, sparse matrix}, shape = [n_samples, n_features]\n\n\nTraining vectors, where n_samples is the number of samples and\nn_features is the number of features.\n\n\n\n\n\n\nReturns\n\n\n\n\nself\n : object\n\n\n\n\n\n\n\ntransform(X)\n\n\nApply the non-linear transformation on X.\n\n\nParameters\n\n\n\n\n\n\nX\n : {array-like, sparse matrix}, shape = [n_samples, n_features]\n\n\nTraining vectors, where n_samples is the number of samples and\nn_features is the number of features.\n\n\n\n\n\n\nReturns\n\n\n\n\n\n\nX_projected\n : np.ndarray, shape = [n_samples, n_components]\n\n\nProjected training vectors.",
            "title": "Mlxtend.feature extraction"
        },
        {
            "location": "/api_subpackages/mlxtend.feature_extraction/#lineardiscriminantanalysis",
            "text": "LinearDiscriminantAnalysis(n_discriminants=None)  Linear Discriminant Analysis Class  Parameters    n_discriminants  : int (default: None)  The number of discrimants for transformation.\nKeeps the original dimensions of the dataset if  None .    Attributes    w_  : array-like, shape=[n_features, n_discriminants]  Projection matrix    e_vals_  : array-like, shape=[n_features]  Eigenvalues in sorted order.    e_vecs_  : array-like, shape=[n_features]  Eigenvectors in sorted order.",
            "title": "LinearDiscriminantAnalysis"
        },
        {
            "location": "/api_subpackages/mlxtend.feature_extraction/#methods",
            "text": "fit(X, y, n_classes=None)  Fit the LDA model with X.  Parameters    X  : {array-like, sparse matrix}, shape = [n_samples, n_features]  Training vectors, where n_samples is the number of samples and\nn_features is the number of features.    y  : array-like, shape = [n_samples]  Target values.    n_classes  : int (default: None)  A positive integer to declare the number of class labels\nif not all class labels are present in a partial training set.\nGets the number of class labels automatically if None.    Returns   self  : object    transform(X)  Apply the linear transformation on X.  Parameters    X  : {array-like, sparse matrix}, shape = [n_samples, n_features]  Training vectors, where n_samples is the number of samples and\nn_features is the number of features.    Returns    X_projected  : np.ndarray, shape = [n_samples, n_discriminants]  Projected training vectors.",
            "title": "Methods"
        },
        {
            "location": "/api_subpackages/mlxtend.feature_extraction/#principalcomponentanalysis",
            "text": "PrincipalComponentAnalysis(n_components=None, solver='eigen')  Principal Component Analysis Class  Parameters    n_components  : int (default: None)  The number of principal components for transformation.\nKeeps the original dimensions of the dataset if  None .    solver  : str (default: 'eigen')  Method for performing the matrix decomposition.    Attributes    w_  : array-like, shape=[n_features, n_components]  Projection matrix    e_vals_  : array-like, shape=[n_features]  Eigenvalues in sorted order.    e_vecs_  : array-like, shape=[n_features]  Eigenvectors in sorted order.",
            "title": "PrincipalComponentAnalysis"
        },
        {
            "location": "/api_subpackages/mlxtend.feature_extraction/#methods_1",
            "text": "fit(X)  Learn model from training data.  Parameters    X  : {array-like, sparse matrix}, shape = [n_samples, n_features]  Training vectors, where n_samples is the number of samples and\nn_features is the number of features.    Returns   self  : object    transform(X)  Apply the linear transformation on X.  Parameters    X  : {array-like, sparse matrix}, shape = [n_samples, n_features]  Training vectors, where n_samples is the number of samples and\nn_features is the number of features.    Returns    X_projected  : np.ndarray, shape = [n_samples, n_components]  Projected training vectors.",
            "title": "Methods"
        },
        {
            "location": "/api_subpackages/mlxtend.feature_extraction/#rbfkernelpca",
            "text": "RBFKernelPCA(gamma=15.0, n_components=None, copy_X=True)  RBF Kernel Principal Component Analysis for dimensionality reduction.  Parameters    gamma  : float (default: 15.0)  Free parameter (coefficient) of the RBF kernel.    n_components  : int (default: None)  The number of principal components for transformation.\nKeeps the original dimensions of the dataset if  None .    copy_X  : bool (default: True)  Copies training data, which is required to compute the projection\nof new data via the transform method. Uses a reference to X if False.    Attributes    e_vals_  : array-like, shape=[n_features]  Eigenvalues in sorted order.    e_vecs_  : array-like, shape=[n_features]  Eigenvectors in sorted order.    X_projected_  : array-like, shape=[n_samples, n_components]  Training samples projected along the component axes.",
            "title": "RBFKernelPCA"
        },
        {
            "location": "/api_subpackages/mlxtend.feature_extraction/#methods_2",
            "text": "fit(X)  Learn model from training data.  Parameters    X  : {array-like, sparse matrix}, shape = [n_samples, n_features]  Training vectors, where n_samples is the number of samples and\nn_features is the number of features.    Returns   self  : object    transform(X)  Apply the non-linear transformation on X.  Parameters    X  : {array-like, sparse matrix}, shape = [n_samples, n_features]  Training vectors, where n_samples is the number of samples and\nn_features is the number of features.    Returns    X_projected  : np.ndarray, shape = [n_samples, n_components]  Projected training vectors.",
            "title": "Methods"
        },
        {
            "location": "/api_subpackages/mlxtend.feature_extraction/#rbfkernelprincipalcomponentanalysis",
            "text": "RBFKernelPrincipalComponentAnalysis(gamma=15.0, n_components=None, copy_X=True)  RBF Kernel Principal Component Analysis for dimensionality reduction.  Parameters    gamma  : float (default: 15.0)  Free parameter (coefficient) of the RBF kernel.    n_components  : int (default: None)  The number of principal components for transformation.\nKeeps the original dimensions of the dataset if  None .    copy_X  : bool (default: True)  Copies training data, which is required to compute the projection\nof new data via the transform method. Uses a reference to X if False.    Attributes    e_vals_  : array-like, shape=[n_features]  Eigenvalues in sorted order.    e_vecs_  : array-like, shape=[n_features]  Eigenvectors in sorted order.    X_projected_  : array-like, shape=[n_samples, n_components]  Training samples projected along the component axes.",
            "title": "RBFKernelPrincipalComponentAnalysis"
        },
        {
            "location": "/api_subpackages/mlxtend.feature_extraction/#methods_3",
            "text": "fit(X)  Fit the PCA model with X.  Parameters    X  : {array-like, sparse matrix}, shape = [n_samples, n_features]  Training vectors, where n_samples is the number of samples and\nn_features is the number of features.    Returns   self  : object    transform(X)  Apply the non-linear transformation on X.  Parameters    X  : {array-like, sparse matrix}, shape = [n_samples, n_features]  Training vectors, where n_samples is the number of samples and\nn_features is the number of features.    Returns    X_projected  : np.ndarray, shape = [n_samples, n_components]  Projected training vectors.",
            "title": "Methods"
        },
        {
            "location": "/api_subpackages/mlxtend.cluster/",
            "text": "mlxtend version: 0.4.2dev0 \n\n\nKmeans\n\n\nKmeans(k, max_iter=10, convergence_tolerance=1e-05, random_seed=None, print_progress=0)\n\n\nK-means clustering class.\n\n\nAdded in 0.4.1dev\n\n\n\nParameters\n\n\n\n\n\n\nk\n : int\n\n\nNumber of clusters\n\n\n\n\n\n\nmax_iter\n : int (default: 10)\n\n\nNumber of iterations during cluster assignment.\nCluster re-assignment stops automatically when the algorithm\nconverged.\n\n\n\n\n\n\nconvergence_tolerance\n : float (default: 1e-05)\n\n\nCompares current centroids with centroids of the previous iteration\nusing the given tolerance (a small positive float)to determine\nif the algorithm converged early.\n\n\n\n\n\n\nrandom_seed\n : int (default: None)\n\n\nSet random state for the initial centroid assignment.\n\n\n\n\n\n\nprint_progress\n : int (default: 0)\n\n\nPrints progress in fitting to stderr.\n0: No output\n1: Iterations elapsed\n2: 1 plus time elapsed\n3: 2 plus estimated time until completion\n\n\n\n\n\n\nAttributes\n\n\n\n\n\n\ncentroids_\n : 2d-array, shape={k, n_features}\n\n\nFeature values of the k cluster centroids.\n\n\n\n\n\n\ncusters_\n : dictionary\n\n\nThe cluster assignments stored as a Python dictionary;\nthe dictionary keys denote the cluster indeces and the items are\nPython lists of the sample indices that were assigned to each\ncluster.\n\n\n\n\n\n\niterations_\n : int\n\n\nNumber of iterations until convergence.\n\n\n\n\n\n\nMethods\n\n\n\n\n\nfit(X, init_params=True)\n\n\nLearn model from training data.\n\n\nParameters\n\n\n\n\n\n\nX\n : {array-like, sparse matrix}, shape = [n_samples, n_features]\n\n\nTraining vectors, where n_samples is the number of samples and\nn_features is the number of features.\n\n\n\n\n\n\ninit_params\n : bool (default: True)\n\n\nRe-initializes model parameters prior to fitting.\nSet False to continue training with weights from\na previous model fitting.\n\n\n\n\n\n\nReturns\n\n\n\n\nself\n : object\n\n\n\n\n\n\n\npredict(X)\n\n\nPredict targets from X.\n\n\nParameters\n\n\n\n\n\n\nX\n : {array-like, sparse matrix}, shape = [n_samples, n_features]\n\n\nTraining vectors, where n_samples is the number of samples and\nn_features is the number of features.\n\n\n\n\n\n\nReturns\n\n\n\n\n\n\ntarget_values\n : array-like, shape = [n_samples]\n\n\nPredicted target values.",
            "title": "Mlxtend.cluster"
        },
        {
            "location": "/api_subpackages/mlxtend.cluster/#kmeans",
            "text": "Kmeans(k, max_iter=10, convergence_tolerance=1e-05, random_seed=None, print_progress=0)  K-means clustering class.  Added in 0.4.1dev  Parameters    k  : int  Number of clusters    max_iter  : int (default: 10)  Number of iterations during cluster assignment.\nCluster re-assignment stops automatically when the algorithm\nconverged.    convergence_tolerance  : float (default: 1e-05)  Compares current centroids with centroids of the previous iteration\nusing the given tolerance (a small positive float)to determine\nif the algorithm converged early.    random_seed  : int (default: None)  Set random state for the initial centroid assignment.    print_progress  : int (default: 0)  Prints progress in fitting to stderr.\n0: No output\n1: Iterations elapsed\n2: 1 plus time elapsed\n3: 2 plus estimated time until completion    Attributes    centroids_  : 2d-array, shape={k, n_features}  Feature values of the k cluster centroids.    custers_  : dictionary  The cluster assignments stored as a Python dictionary;\nthe dictionary keys denote the cluster indeces and the items are\nPython lists of the sample indices that were assigned to each\ncluster.    iterations_  : int  Number of iterations until convergence.",
            "title": "Kmeans"
        },
        {
            "location": "/api_subpackages/mlxtend.cluster/#methods",
            "text": "fit(X, init_params=True)  Learn model from training data.  Parameters    X  : {array-like, sparse matrix}, shape = [n_samples, n_features]  Training vectors, where n_samples is the number of samples and\nn_features is the number of features.    init_params  : bool (default: True)  Re-initializes model parameters prior to fitting.\nSet False to continue training with weights from\na previous model fitting.    Returns   self  : object    predict(X)  Predict targets from X.  Parameters    X  : {array-like, sparse matrix}, shape = [n_samples, n_features]  Training vectors, where n_samples is the number of samples and\nn_features is the number of features.    Returns    target_values  : array-like, shape = [n_samples]  Predicted target values.",
            "title": "Methods"
        },
        {
            "location": "/api_subpackages/mlxtend.tf_cluster/",
            "text": "mlxtend version: 0.4.2dev0 \n\n\nTfKmeans\n\n\nTfKmeans(k, max_iter=10, convergence_tolerance=1e-05, random_seed=None, print_progress=0, dtype=None)\n\n\nTensorFlow K-means clustering class.\n\n\nAdded in 0.4.1dev\n\n\n\nParameters\n\n\n\n\n\n\nk\n : int\n\n\nNumber of clusters\n\n\n\n\n\n\nmax_iter\n : int (default: 10)\n\n\nNumber of iterations during cluster assignment.\nCluster re-assignment stops automatically when the algorithm\nconverged.\n\n\n\n\n\n\nconvergence_tolerance\n : float (default: 1e-05)\n\n\nCompares current centroids with centroids of the previous iteration\nusing the given tolerance (a small positive float)to determine\nif the algorithm converged early.\n\n\n\n\n\n\nrandom_seed\n : int (default: None)\n\n\nSet random state for the initial centroid assignment.\n\n\n\n\n\n\nprint_progress\n : int (default: 0)\n\n\nPrints progress in fitting to stderr.\n0: No output\n1: Iterations elapsed\n2: 1 plus time elapsed\n3: 2 plus estimated time until completion\n\n\n\n\n\n\ndtype\n : Array-type (default: None)\n\n\nUses tensorflow.float32 if None.\n\n\n\n\n\n\nAttributes\n\n\n\n\n\n\ncentroids_\n : 2d-array, shape = {k, n_features}\n\n\nFeature values of the k cluster centroids.\n\n\n\n\n\n\ncusters_\n : dictionary\n\n\nThe cluster assignments stored as a Python dictionary;\nthe dictionary keys denote the cluster indeces and the items are\nPython lists of the sample indices that were assigned to each\ncluster.\n\n\n\n\n\n\niterations_\n : int\n\n\nNumber of iterations until convergence.\n\n\n\n\n\n\nMethods\n\n\n\n\n\nfit(X, init_params=True)\n\n\nLearn model from training data.\n\n\nParameters\n\n\n\n\n\n\nX\n : {array-like, sparse matrix}, shape = [n_samples, n_features]\n\n\nTraining vectors, where n_samples is the number of samples and\nn_features is the number of features.\n\n\n\n\n\n\ninit_params\n : bool (default: True)\n\n\nRe-initializes model parameters prior to fitting.\nSet False to continue training with weights from\na previous model fitting.\n\n\n\n\n\n\nReturns\n\n\n\n\nself\n : object\n\n\n\n\n\n\n\npredict(X)\n\n\nPredict targets from X.\n\n\nParameters\n\n\n\n\n\n\nX\n : {array-like, sparse matrix}, shape = [n_samples, n_features]\n\n\nTraining vectors, where n_samples is the number of samples and\nn_features is the number of features.\n\n\n\n\n\n\nReturns\n\n\n\n\n\n\ntarget_values\n : array-like, shape = [n_samples]\n\n\nPredicted target values.",
            "title": "Mlxtend.tf cluster"
        },
        {
            "location": "/api_subpackages/mlxtend.tf_cluster/#tfkmeans",
            "text": "TfKmeans(k, max_iter=10, convergence_tolerance=1e-05, random_seed=None, print_progress=0, dtype=None)  TensorFlow K-means clustering class.  Added in 0.4.1dev  Parameters    k  : int  Number of clusters    max_iter  : int (default: 10)  Number of iterations during cluster assignment.\nCluster re-assignment stops automatically when the algorithm\nconverged.    convergence_tolerance  : float (default: 1e-05)  Compares current centroids with centroids of the previous iteration\nusing the given tolerance (a small positive float)to determine\nif the algorithm converged early.    random_seed  : int (default: None)  Set random state for the initial centroid assignment.    print_progress  : int (default: 0)  Prints progress in fitting to stderr.\n0: No output\n1: Iterations elapsed\n2: 1 plus time elapsed\n3: 2 plus estimated time until completion    dtype  : Array-type (default: None)  Uses tensorflow.float32 if None.    Attributes    centroids_  : 2d-array, shape = {k, n_features}  Feature values of the k cluster centroids.    custers_  : dictionary  The cluster assignments stored as a Python dictionary;\nthe dictionary keys denote the cluster indeces and the items are\nPython lists of the sample indices that were assigned to each\ncluster.    iterations_  : int  Number of iterations until convergence.",
            "title": "TfKmeans"
        },
        {
            "location": "/api_subpackages/mlxtend.tf_cluster/#methods",
            "text": "fit(X, init_params=True)  Learn model from training data.  Parameters    X  : {array-like, sparse matrix}, shape = [n_samples, n_features]  Training vectors, where n_samples is the number of samples and\nn_features is the number of features.    init_params  : bool (default: True)  Re-initializes model parameters prior to fitting.\nSet False to continue training with weights from\na previous model fitting.    Returns   self  : object    predict(X)  Predict targets from X.  Parameters    X  : {array-like, sparse matrix}, shape = [n_samples, n_features]  Training vectors, where n_samples is the number of samples and\nn_features is the number of features.    Returns    target_values  : array-like, shape = [n_samples]  Predicted target values.",
            "title": "Methods"
        },
        {
            "location": "/api_subpackages/mlxtend.file_io/",
            "text": "mlxtend version: 0.4.2dev0 \n\n\nfind_filegroups\n\n\nfind_filegroups(paths, substring='', extensions=None, validity_check=True, ignore_invisible=True, rstrip='', ignore_substring=None)\n\n\nFind and collect files from different directories in a python dictionary.\n\n\nParameters\n\n\n\n\n\n\npaths\n : \nlist\n\n\nPaths of the directories to be searched. Dictionary keys are build from\nthe first directory.\n\n\n\n\n\n\nsubstring\n : \nstr\n (default: '')\n\n\nSubstring that all files have to contain to be considered.\n\n\n\n\n\n\nextensions\n : \nlist\n (default: None)\n\n\nNone\n or \nlist\n of allowed file extensions for each path.\nIf provided, the number of extensions must match the number of \npaths\n.\n\n\n\n\n\n\nvalidity_check\n : \nbool\n (default: None)\n\n\nIf \nTrue\n, checks if all dictionary values\nhave the same number of file paths. Prints\na warning and returns an empty dictionary if the validity check failed.\n\n\n\n\n\n\nignore_invisible\n : \nbool\n (default: True)\n\n\nIf \nTrue\n, ignores invisible files\n(i.e., files starting with a period).\n\n\n\n\n\n\nrstrip\n : \nstr\n (default: '')\n\n\nIf provided, strips characters from right side of the file\nbase names after splitting the extension.\nUseful to trim different filenames to a common stem.\nE.g,. \"abc_d.txt\" and \"abc_d_.csv\" would share\nthe stem \"abc_d\" if rstrip is set to \"_\".\n\n\n\n\n\n\nignore_substring\n : \nstr\n (default: None)\n\n\nIgnores files that contain the specified substring.\n\n\n\n\n\n\nReturns\n\n\n\n\n\n\ngroups\n : \ndict\n\n\nDictionary of files paths. Keys are the file names\nfound in the first directory listed\nin \npaths\n (without file extension).\n\n\n\n\n\n\nfind_files\n\n\nfind_files(substring, path, recursive=False, check_ext=None, ignore_invisible=True, ignore_substring=None)\n\n\nFind files in a directory based on substring matching.\n\n\nParameters\n\n\n\n\n\n\nsubstring\n : \nstr\n\n\nSubstring of the file to be matched.\n\n\n\n\n\n\npath\n : \nstr\n\n\nPath where to look.\nrecursive: \nbool\n\nIf true, searches subdirectories recursively.\ncheck_ext: \nstr\n\nIf string (e.g., '.txt'), only returns files that\nmatch the specified file extension.\n\n\n\n\n\n\nignore_invisible\n : \nbool\n\n\nIf \nTrue\n, ignores invisible files\n(i.e., files starting with a period).\n\n\n\n\n\n\nignore_substring\n : \nstr\n\n\nIgnores files that contain the specified substring.\n\n\n\n\n\n\nReturns\n\n\n\n\n\n\nresults\n : \nlist\n\n\nList of the matched files.",
            "title": "Mlxtend.file io"
        },
        {
            "location": "/api_subpackages/mlxtend.file_io/#find_filegroups",
            "text": "find_filegroups(paths, substring='', extensions=None, validity_check=True, ignore_invisible=True, rstrip='', ignore_substring=None)  Find and collect files from different directories in a python dictionary.  Parameters    paths  :  list  Paths of the directories to be searched. Dictionary keys are build from\nthe first directory.    substring  :  str  (default: '')  Substring that all files have to contain to be considered.    extensions  :  list  (default: None)  None  or  list  of allowed file extensions for each path.\nIf provided, the number of extensions must match the number of  paths .    validity_check  :  bool  (default: None)  If  True , checks if all dictionary values\nhave the same number of file paths. Prints\na warning and returns an empty dictionary if the validity check failed.    ignore_invisible  :  bool  (default: True)  If  True , ignores invisible files\n(i.e., files starting with a period).    rstrip  :  str  (default: '')  If provided, strips characters from right side of the file\nbase names after splitting the extension.\nUseful to trim different filenames to a common stem.\nE.g,. \"abc_d.txt\" and \"abc_d_.csv\" would share\nthe stem \"abc_d\" if rstrip is set to \"_\".    ignore_substring  :  str  (default: None)  Ignores files that contain the specified substring.    Returns    groups  :  dict  Dictionary of files paths. Keys are the file names\nfound in the first directory listed\nin  paths  (without file extension).",
            "title": "find_filegroups"
        },
        {
            "location": "/api_subpackages/mlxtend.file_io/#find_files",
            "text": "find_files(substring, path, recursive=False, check_ext=None, ignore_invisible=True, ignore_substring=None)  Find files in a directory based on substring matching.  Parameters    substring  :  str  Substring of the file to be matched.    path  :  str  Path where to look.\nrecursive:  bool \nIf true, searches subdirectories recursively.\ncheck_ext:  str \nIf string (e.g., '.txt'), only returns files that\nmatch the specified file extension.    ignore_invisible  :  bool  If  True , ignores invisible files\n(i.e., files starting with a period).    ignore_substring  :  str  Ignores files that contain the specified substring.    Returns    results  :  list  List of the matched files.",
            "title": "find_files"
        },
        {
            "location": "/api_subpackages/mlxtend.general_plotting/",
            "text": "mlxtend version: 0.4.2dev0 \n\n\ncategory_scatter\n\n\ncategory_scatter(x, y, label_col, data, markers='sxo^v', colors=('blue', 'green', 'red', 'purple', 'gray', 'cyan'), alpha=0.7, markersize=20.0, legend_loc='best')\n\n\nScatter plot to plot categories in different colors/markerstyles.\n\n\nParameters\n\n\n\n\n\n\nx\n : str or int\n\n\nDataFrame column name of the x-axis values or\ninteger for the numpy ndarray column index.\n\n\n\n\n\n\ny\n : str\n\n\nDataFrame column name of the y-axis values or\ninteger for the numpy ndarray column index\n\n\n\n\n\n\ndata\n : Pandas DataFrame object or NumPy ndarray.\n\n\n\n\n\n\nmarkers\n : str\n\n\nMarkers that are cycled through the label category.\n\n\n\n\n\n\ncolors\n : tuple\n\n\nColors that are cycled through the label category.\n\n\n\n\n\n\nalpha\n : float (default: 0.7)\n\n\nParameter to control the transparency.\n\n\n\n\n\n\nmarkersize\n : float (default` : 20.0)\n\n\nParameter to control the marker size.\n\n\n\n\n\n\nlegend_loc\n : str (default: 'best')\n\n\nLocation of the plot legend\n{best, upper left, upper right, lower left, lower right}\nNo legend if legend_loc=False\n\n\n\n\n\n\nReturns\n\n\n\n\nfig\n : matplotlig.pyplot figure object\n\n\n\n\nenrichment_plot\n\n\nenrichment_plot(df, colors='bgrkcy', markers=' ', linestyles='-', alpha=0.5, lw=2, where='post', grid=True, count_label='Count', xlim='auto', ylim='auto', invert_axes=False, legend_loc='best', ax=None)\n\n\nPlot stacked barplots\n\n\nParameters\n\n\n\n\n\n\ndf\n : pandas.DataFrame\n\n\nA pandas DataFrame where columns represent the different categories.\ncolors: str (default: 'bgrcky')\nThe colors of the bars.\n\n\n\n\n\n\nmarkers\n : str (default: ' ')\n\n\nMatplotlib markerstyles, e.g,\n'sov' for square,circle, and triangle markers.\n\n\n\n\n\n\nlinestyles\n : str (default: '-')\n\n\nMatplotlib linestyles, e.g.,\n'-,--' to cycle normal and dashed lines. Note\nthat the different linestyles need to be separated by commas.\n\n\n\n\n\n\nalpha\n : float (default: 0.5)\n\n\nTransparency level from 0.0 to 1.0.\n\n\n\n\n\n\nlw\n : int or float (default: 2)\n\n\nLinewidth parameter.\n\n\n\n\n\n\nwhere\n : {'post', 'pre', 'mid'} (default: 'post')\n\n\nStarting location of the steps.\n\n\n\n\n\n\ngrid\n : bool (default: \nTrue\n)\n\n\nPlots a grid if True.\n\n\n\n\n\n\ncount_label\n : str (default: 'Count')\n\n\nLabel for the \"Count\"-axis.\n\n\n\n\n\n\nxlim\n : 'auto' or array-like [min, max] (default: 'auto')\n\n\nMin and maximum position of the x-axis range.\n\n\n\n\n\n\nylim\n : 'auto' or array-like [min, max] (default: 'auto')\n\n\nMin and maximum position of the y-axis range.\n\n\n\n\n\n\ninvert_axes\n : bool (default: False)\n\n\nPlots count on the x-axis if True.\n\n\n\n\n\n\nlegend_loc\n : str (default: 'best')\n\n\nLocation of the plot legend\n{best, upper left, upper right, lower left, lower right}\nNo legend if legend_loc=False\n\n\n\n\n\n\nax\n : matplotlib axis, optional (default: None)\n\n\nUse this axis for plotting or make a new one otherwise\n\n\n\n\n\n\nReturns\n\n\n\n\nax\n : matplotlib axis\n\n\n\n\nremove_borders\n\n\nremove_borders(axes, left=False, bottom=False, right=True, top=True)\n\n\nRemove chart junk from matplotlib plots.\n\n\nParameters\n\n\n\n\n\n\naxes\n : iterable\n\n\nAn iterable containing plt.gca()\nor plt.subplot() objects, e.g. [plt.gca()].\n\n\n\n\n\n\nleft\n : bool (default: \nFalse\n)\n\n\nHide left axis spine if True.\n\n\n\n\n\n\nbottom\n : bool (default: \nFalse\n)\n\n\nHide bottom axis spine if True.\n\n\n\n\n\n\nright\n : bool (default: \nTrue\n)\n\n\nHide right axis spine if True.\n\n\n\n\n\n\ntop\n : bool (default: \nTrue\n)\n\n\nHide top axis spine if True.\n\n\n\n\n\n\nstacked_barplot\n\n\nstacked_barplot(df, bar_width='auto', colors='bgrcky', labels='index', rotation=90, legend_loc='best')\n\n\nFunction to plot stacked barplots\n\n\nParameters\n\n\n\n\n\n\ndf\n : pandas.DataFrame\n\n\nA pandas DataFrame where the index denotes the\nx-axis labels, and the columns contain the different\nmeasurements for each row.\nbar_width: 'auto' or float (default: 'auto')\nParameter to set the widths of the bars. if\n'auto', the width is automatically determined by\nthe number of columns in the dataset.\ncolors: str (default: 'bgrcky')\nThe colors of the bars.\nlabels: 'index' or iterable (default: 'index')\nIf 'index', the DataFrame index will be used as\nx-tick labels.\nrotation: int (default: 90)\nParameter to rotate the x-axis labels.\n\n\n\n\n\n\nlegend_loc\n : str (default: 'best')\n\n\nLocation of the plot legend\n{best, upper left, upper right, lower left, lower right}\nNo legend if legend_loc=False\n\n\n\n\n\n\nReturns\n\n\n\n\nfig\n : matplotlib.pyplot figure object",
            "title": "Mlxtend.general plotting"
        },
        {
            "location": "/api_subpackages/mlxtend.general_plotting/#category_scatter",
            "text": "category_scatter(x, y, label_col, data, markers='sxo^v', colors=('blue', 'green', 'red', 'purple', 'gray', 'cyan'), alpha=0.7, markersize=20.0, legend_loc='best')  Scatter plot to plot categories in different colors/markerstyles.  Parameters    x  : str or int  DataFrame column name of the x-axis values or\ninteger for the numpy ndarray column index.    y  : str  DataFrame column name of the y-axis values or\ninteger for the numpy ndarray column index    data  : Pandas DataFrame object or NumPy ndarray.    markers  : str  Markers that are cycled through the label category.    colors  : tuple  Colors that are cycled through the label category.    alpha  : float (default: 0.7)  Parameter to control the transparency.    markersize  : float (default` : 20.0)  Parameter to control the marker size.    legend_loc  : str (default: 'best')  Location of the plot legend\n{best, upper left, upper right, lower left, lower right}\nNo legend if legend_loc=False    Returns   fig  : matplotlig.pyplot figure object",
            "title": "category_scatter"
        },
        {
            "location": "/api_subpackages/mlxtend.general_plotting/#enrichment_plot",
            "text": "enrichment_plot(df, colors='bgrkcy', markers=' ', linestyles='-', alpha=0.5, lw=2, where='post', grid=True, count_label='Count', xlim='auto', ylim='auto', invert_axes=False, legend_loc='best', ax=None)  Plot stacked barplots  Parameters    df  : pandas.DataFrame  A pandas DataFrame where columns represent the different categories.\ncolors: str (default: 'bgrcky')\nThe colors of the bars.    markers  : str (default: ' ')  Matplotlib markerstyles, e.g,\n'sov' for square,circle, and triangle markers.    linestyles  : str (default: '-')  Matplotlib linestyles, e.g.,\n'-,--' to cycle normal and dashed lines. Note\nthat the different linestyles need to be separated by commas.    alpha  : float (default: 0.5)  Transparency level from 0.0 to 1.0.    lw  : int or float (default: 2)  Linewidth parameter.    where  : {'post', 'pre', 'mid'} (default: 'post')  Starting location of the steps.    grid  : bool (default:  True )  Plots a grid if True.    count_label  : str (default: 'Count')  Label for the \"Count\"-axis.    xlim  : 'auto' or array-like [min, max] (default: 'auto')  Min and maximum position of the x-axis range.    ylim  : 'auto' or array-like [min, max] (default: 'auto')  Min and maximum position of the y-axis range.    invert_axes  : bool (default: False)  Plots count on the x-axis if True.    legend_loc  : str (default: 'best')  Location of the plot legend\n{best, upper left, upper right, lower left, lower right}\nNo legend if legend_loc=False    ax  : matplotlib axis, optional (default: None)  Use this axis for plotting or make a new one otherwise    Returns   ax  : matplotlib axis",
            "title": "enrichment_plot"
        },
        {
            "location": "/api_subpackages/mlxtend.general_plotting/#remove_borders",
            "text": "remove_borders(axes, left=False, bottom=False, right=True, top=True)  Remove chart junk from matplotlib plots.  Parameters    axes  : iterable  An iterable containing plt.gca()\nor plt.subplot() objects, e.g. [plt.gca()].    left  : bool (default:  False )  Hide left axis spine if True.    bottom  : bool (default:  False )  Hide bottom axis spine if True.    right  : bool (default:  True )  Hide right axis spine if True.    top  : bool (default:  True )  Hide top axis spine if True.",
            "title": "remove_borders"
        },
        {
            "location": "/api_subpackages/mlxtend.general_plotting/#stacked_barplot",
            "text": "stacked_barplot(df, bar_width='auto', colors='bgrcky', labels='index', rotation=90, legend_loc='best')  Function to plot stacked barplots  Parameters    df  : pandas.DataFrame  A pandas DataFrame where the index denotes the\nx-axis labels, and the columns contain the different\nmeasurements for each row.\nbar_width: 'auto' or float (default: 'auto')\nParameter to set the widths of the bars. if\n'auto', the width is automatically determined by\nthe number of columns in the dataset.\ncolors: str (default: 'bgrcky')\nThe colors of the bars.\nlabels: 'index' or iterable (default: 'index')\nIf 'index', the DataFrame index will be used as\nx-tick labels.\nrotation: int (default: 90)\nParameter to rotate the x-axis labels.    legend_loc  : str (default: 'best')  Location of the plot legend\n{best, upper left, upper right, lower left, lower right}\nNo legend if legend_loc=False    Returns   fig  : matplotlib.pyplot figure object",
            "title": "stacked_barplot"
        },
        {
            "location": "/api_subpackages/mlxtend.preprocessing/",
            "text": "mlxtend version: 0.4.2dev0 \n\n\nCopyTransformer\n\n\nCopyTransformer()\n\n\nTransformer that returns a copy of the input array\n\n\nMethods\n\n\n\n\n\nfit(X, y=None)\n\n\nMock method. Does nothing.\n\n\nParameters\n\n\n\n\n\n\nX\n : {array-like, sparse matrix}, shape = [n_samples, n_features]\n\n\nTraining vectors, where n_samples is the number of samples and\nn_features is the number of features.\n\n\n\n\n\n\ny\n : array-like, shape = [n_samples] (default: None)\n\n\n\n\n\n\nReturns\n\n\nself\n\n\n\n\n\nfit_transform(X, y=None)\n\n\nReturn a copy of the input array.\n\n\nParameters\n\n\n\n\n\n\nX\n : {array-like, sparse matrix}, shape = [n_samples, n_features]\n\n\nTraining vectors, where n_samples is the number of samples and\nn_features is the number of features.\n\n\n\n\n\n\ny\n : array-like, shape = [n_samples] (default: None)\n\n\n\n\n\n\nReturns\n\n\n\n\nX_copy\n : copy of the input X array.\n\n\n\n\n\n\n\nget_params(deep=True)\n\n\nGet parameters for this estimator.\n\n\nParameters\n\n\ndeep: boolean, optional\n    If True, will return the parameters for this estimator and\n    contained subobjects that are estimators.\n\n\nReturns\n\n\n\n\n\n\nparams\n : mapping of string to any\n\n\nParameter names mapped to their values.\n\n\n\n\n\n\n\n\n\nset_params(\nparams)\n\n\nSet the parameters of this estimator.\n\n\nThe method works on simple estimators as well as on nested objects\n(such as pipelines). The former have parameters of the form\n``<component>__<parameter>`` so that it's possible to update each\ncomponent of a nested object.\n\n\n\nReturns\n\n\nself\n\n\n\n\n\ntransform(X, y=None)\n\n\nReturn a copy of the input array.\n\n\nParameters\n\n\n\n\n\n\nX\n : {array-like, sparse matrix}, shape = [n_samples, n_features]\n\n\nTraining vectors, where n_samples is the number of samples and\nn_features is the number of features.\n\n\n\n\n\n\ny\n : array-like, shape = [n_samples] (default: None)\n\n\n\n\n\n\nReturns\n\n\n\n\nX_copy\n : copy of the input X array.\n\n\n\n\nDenseTransformer\n\n\nDenseTransformer(return_copy=True)\n\n\nConvert a sparse array into a dense array.\n\n\nMethods\n\n\n\n\n\nfit(X, y=None)\n\n\nMock method. Does nothing.\n\n\nParameters\n\n\n\n\n\n\nX\n : {array-like, sparse matrix}, shape = [n_samples, n_features]\n\n\nTraining vectors, where n_samples is the number of samples and\nn_features is the number of features.\n\n\n\n\n\n\ny\n : array-like, shape = [n_samples] (default: None)\n\n\n\n\n\n\nReturns\n\n\nself\n\n\n\n\n\nfit_transform(X, y=None)\n\n\nReturn a dense version of the input array.\n\n\nParameters\n\n\n\n\n\n\nX\n : {array-like, sparse matrix}, shape = [n_samples, n_features]\n\n\nTraining vectors, where n_samples is the number of samples and\nn_features is the number of features.\n\n\n\n\n\n\ny\n : array-like, shape = [n_samples] (default: None)\n\n\n\n\n\n\nReturns\n\n\n\n\nX_dense\n : dense version of the input X array.\n\n\n\n\n\n\n\nget_params(deep=True)\n\n\nGet parameters for this estimator.\n\n\nParameters\n\n\ndeep: boolean, optional\n    If True, will return the parameters for this estimator and\n    contained subobjects that are estimators.\n\n\nReturns\n\n\n\n\n\n\nparams\n : mapping of string to any\n\n\nParameter names mapped to their values.\n\n\n\n\n\n\n\n\n\nset_params(\nparams)\n\n\nSet the parameters of this estimator.\n\n\nThe method works on simple estimators as well as on nested objects\n(such as pipelines). The former have parameters of the form\n``<component>__<parameter>`` so that it's possible to update each\ncomponent of a nested object.\n\n\n\nReturns\n\n\nself\n\n\n\n\n\ntransform(X, y=None)\n\n\nReturn a dense version of the input array.\n\n\nParameters\n\n\n\n\n\n\nX\n : {array-like, sparse matrix}, shape = [n_samples, n_features]\n\n\nTraining vectors, where n_samples is the number of samples and\nn_features is the number of features.\n\n\n\n\n\n\ny\n : array-like, shape = [n_samples] (default: None)\n\n\n\n\n\n\nReturns\n\n\n\n\nX_dense\n : dense version of the input X array.\n\n\n\n\nMeanCenterer\n\n\nMeanCenterer()\n\n\nColumn centering of vectors and matrices.\n\n\nAttributes\n\n\n\n\n\n\ncol_means\n : numpy.ndarray [n_columns]\n\n\nNumPy array storing the mean values for centering after fitting\nthe MeanCenterer object.\n\n\n\n\n\n\nMethods\n\n\n\n\n\nfit(X)\n\n\nGets the column means for mean centering.\n\n\nParameters\n\n\n\n\n\n\nX\n : {array-like, sparse matrix}, shape = [n_samples, n_features]\n\n\nArray of data vectors, where n_samples is the number of samples and\nn_features is the number of features.\n\n\n\n\n\n\nReturns\n\n\nself\n\n\n\n\n\nfit_transform(X)\n\n\nFits and transforms an arry.\n\n\nParameters\n\n\n\n\n\n\nX\n : {array-like, sparse matrix}, shape = [n_samples, n_features]\n\n\nArray of data vectors, where n_samples is the number of samples and\nn_features is the number of features.\n\n\n\n\n\n\nReturns\n\n\n\n\n\n\nX_tr\n : {array-like, sparse matrix}, shape = [n_samples, n_features]\n\n\nA copy of the input array with the columns centered.\n\n\n\n\n\n\n\n\n\ntransform(X)\n\n\nCenters a NumPy array.\n\n\nParameters\n\n\n\n\n\n\nX\n : {array-like, sparse matrix}, shape = [n_samples, n_features]\n\n\nArray of data vectors, where n_samples is the number of samples and\nn_features is the number of features.\n\n\n\n\n\n\nReturns\n\n\n\n\n\n\nX_tr\n : {array-like, sparse matrix}, shape = [n_samples, n_features]\n\n\nA copy of the input array with the columns centered.\n\n\n\n\n\n\nminmax_scaling\n\n\nminmax_scaling(array, columns, min_val=0, max_val=1)\n\n\nMin max scaling of pandas' DataFrames.\n\n\nParameters\n\n\n\n\n\n\narray\n : pandas DataFrame or NumPy ndarray, shape = [n_rows, n_columns].\n\n\n\n\n\n\ncolumns\n : array-like, shape = [n_columns]\n\n\nArray-like with column names, e.g., ['col1', 'col2', ...]\nor column indices [0, 2, 4, ...]\n\n\n\n\n\n\nmin_val\n : \nint\n or \nfloat\n, optional (default=\n0\n)\n\n\nminimum value after rescaling.\n\n\n\n\n\n\nmin_val\n : \nint\n or \nfloat\n, optional (default=\n1\n)\n\n\nmaximum value after rescaling.\n\n\n\n\n\n\nReturns\n\n\n\n\n\n\ndf_new\n : pandas DataFrame object.\n\n\nCopy of the array or DataFrame with rescaled columns.\n\n\n\n\n\n\none_hot\n\n\none_hot(y, num_labels='auto', dtype='float')\n\n\nOne-hot encoding of class labels\n\n\nParameters\n\n\n\n\n\n\ny\n : array-like, shape = [n_classlabels]\n\n\nPython list or numpy array consisting of class labels.\n\n\n\n\n\n\nnum_labels\n : int or 'auto'\n\n\nNumber of unique labels in the class label array. Infers the number\nof unique labels from the input array if set to 'auto'.\n\n\n\n\n\n\ndtype\n : str\n\n\nNumPy array type (float, float32, float64) of the output array.\n\n\n\n\n\n\nReturns\n\n\n\n\n\n\nary\n : numpy.ndarray, shape = [n_classlabels]\n\n\nOne-hot encoded array, where each sample is represented as\na row vector in the returned array.\n\n\n\n\n\n\nshuffle_arrays_unison\n\n\nshuffle_arrays_unison(arrays, random_seed=None)\n\n\nShuffle NumPy arrays in unison.\n\n\nParameters\n\n\n\n\n\n\narrays\n : array-like, shape = [n_arrays]\n\n\nA list of NumPy arrays.\n\n\n\n\n\n\nrandom_seed\n : int (default: None)\n\n\nSets the random state.\n\n\n\n\n\n\nReturns\n\n\n\n\nshuffled_arrays\n : A list of NumPy arrays after shuffling.\n\n\n\n\nExamples\n\n\n>>> import numpy as np\n>>> from mlxtend.preprocessing import shuffle_arrays_unison\n>>> X1 = np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]])\n>>> y1 = np.array([1, 2, 3])\n>>> X2, y2 = shuffle_arrays_unison(arrays=[X1, y1], random_seed=3)\n>>> assert(X2.all() == np.array([[4, 5, 6], [1, 2, 3], [7, 8, 9]]).all())\n>>> assert(y2.all() == np.array([2, 1, 3]).all())\n>>>\n\n\n\nstandardize\n\n\nstandardize(array, columns=None, ddof=0, return_params=False, params=None)\n\n\nStandardize columns in pandas DataFrames.\n\n\nParameters\n\n\n\n\n\n\narray\n : pandas DataFrame or NumPy ndarray, shape = [n_rows, n_columns].\n\n\n\n\n\n\ncolumns\n : array-like, shape = [n_columns] (default: None)\n\n\nArray-like with column names, e.g., ['col1', 'col2', ...]\nor column indices [0, 2, 4, ...]\nIf None, standardizes all columns.\n\n\n\n\n\n\nddof\n : int (default: 0)\n\n\nDelta Degrees of Freedom. The divisor used in calculations\nis N - ddof, where N represents the number of elements.\n\n\n\n\n\n\nreturn_params\n : dict (default: False)\n\n\nIf set to True, a dictionary is returned in addition to the\nstandardized array. The parameter dictionary contains the\ncolumn means ('avgs') and standard deviations ('stds') of\nthe individual columns.\n\n\n\n\n\n\nparams\n : dict (default: None)\n\n\nA dictionary with column means and standard deviations as\nreturned by the \nstandardize\n function if \nreturn_params\n\nwas set to True. If a \nparams\n dictionary is provided, the\n\nstandardize\n function will use these instead of computing\nthem from the current array.\n\n\n\n\n\n\nNotes\n\n\nIf all values in a given column are the same, these values are all\n    set to \n0.0\n. The standard deviation in the \nparameters\n dictionary\n    is consequently set to \n1.0\n to avoid dividing by zero.\n\n\nReturns\n\n\n\n\n\n\ndf_new\n : pandas DataFrame object.\n\n\nCopy of the array or DataFrame with standardized columns.",
            "title": "Mlxtend.preprocessing"
        },
        {
            "location": "/api_subpackages/mlxtend.preprocessing/#copytransformer",
            "text": "CopyTransformer()  Transformer that returns a copy of the input array",
            "title": "CopyTransformer"
        },
        {
            "location": "/api_subpackages/mlxtend.preprocessing/#methods",
            "text": "fit(X, y=None)  Mock method. Does nothing.  Parameters    X  : {array-like, sparse matrix}, shape = [n_samples, n_features]  Training vectors, where n_samples is the number of samples and\nn_features is the number of features.    y  : array-like, shape = [n_samples] (default: None)    Returns  self   fit_transform(X, y=None)  Return a copy of the input array.  Parameters    X  : {array-like, sparse matrix}, shape = [n_samples, n_features]  Training vectors, where n_samples is the number of samples and\nn_features is the number of features.    y  : array-like, shape = [n_samples] (default: None)    Returns   X_copy  : copy of the input X array.    get_params(deep=True)  Get parameters for this estimator.  Parameters  deep: boolean, optional\n    If True, will return the parameters for this estimator and\n    contained subobjects that are estimators.  Returns    params  : mapping of string to any  Parameter names mapped to their values.     set_params( params)  Set the parameters of this estimator.  The method works on simple estimators as well as on nested objects\n(such as pipelines). The former have parameters of the form\n``<component>__<parameter>`` so that it's possible to update each\ncomponent of a nested object.  Returns  self   transform(X, y=None)  Return a copy of the input array.  Parameters    X  : {array-like, sparse matrix}, shape = [n_samples, n_features]  Training vectors, where n_samples is the number of samples and\nn_features is the number of features.    y  : array-like, shape = [n_samples] (default: None)    Returns   X_copy  : copy of the input X array.",
            "title": "Methods"
        },
        {
            "location": "/api_subpackages/mlxtend.preprocessing/#densetransformer",
            "text": "DenseTransformer(return_copy=True)  Convert a sparse array into a dense array.",
            "title": "DenseTransformer"
        },
        {
            "location": "/api_subpackages/mlxtend.preprocessing/#methods_1",
            "text": "fit(X, y=None)  Mock method. Does nothing.  Parameters    X  : {array-like, sparse matrix}, shape = [n_samples, n_features]  Training vectors, where n_samples is the number of samples and\nn_features is the number of features.    y  : array-like, shape = [n_samples] (default: None)    Returns  self   fit_transform(X, y=None)  Return a dense version of the input array.  Parameters    X  : {array-like, sparse matrix}, shape = [n_samples, n_features]  Training vectors, where n_samples is the number of samples and\nn_features is the number of features.    y  : array-like, shape = [n_samples] (default: None)    Returns   X_dense  : dense version of the input X array.    get_params(deep=True)  Get parameters for this estimator.  Parameters  deep: boolean, optional\n    If True, will return the parameters for this estimator and\n    contained subobjects that are estimators.  Returns    params  : mapping of string to any  Parameter names mapped to their values.     set_params( params)  Set the parameters of this estimator.  The method works on simple estimators as well as on nested objects\n(such as pipelines). The former have parameters of the form\n``<component>__<parameter>`` so that it's possible to update each\ncomponent of a nested object.  Returns  self   transform(X, y=None)  Return a dense version of the input array.  Parameters    X  : {array-like, sparse matrix}, shape = [n_samples, n_features]  Training vectors, where n_samples is the number of samples and\nn_features is the number of features.    y  : array-like, shape = [n_samples] (default: None)    Returns   X_dense  : dense version of the input X array.",
            "title": "Methods"
        },
        {
            "location": "/api_subpackages/mlxtend.preprocessing/#meancenterer",
            "text": "MeanCenterer()  Column centering of vectors and matrices.  Attributes    col_means  : numpy.ndarray [n_columns]  NumPy array storing the mean values for centering after fitting\nthe MeanCenterer object.",
            "title": "MeanCenterer"
        },
        {
            "location": "/api_subpackages/mlxtend.preprocessing/#methods_2",
            "text": "fit(X)  Gets the column means for mean centering.  Parameters    X  : {array-like, sparse matrix}, shape = [n_samples, n_features]  Array of data vectors, where n_samples is the number of samples and\nn_features is the number of features.    Returns  self   fit_transform(X)  Fits and transforms an arry.  Parameters    X  : {array-like, sparse matrix}, shape = [n_samples, n_features]  Array of data vectors, where n_samples is the number of samples and\nn_features is the number of features.    Returns    X_tr  : {array-like, sparse matrix}, shape = [n_samples, n_features]  A copy of the input array with the columns centered.     transform(X)  Centers a NumPy array.  Parameters    X  : {array-like, sparse matrix}, shape = [n_samples, n_features]  Array of data vectors, where n_samples is the number of samples and\nn_features is the number of features.    Returns    X_tr  : {array-like, sparse matrix}, shape = [n_samples, n_features]  A copy of the input array with the columns centered.",
            "title": "Methods"
        },
        {
            "location": "/api_subpackages/mlxtend.preprocessing/#minmax_scaling",
            "text": "minmax_scaling(array, columns, min_val=0, max_val=1)  Min max scaling of pandas' DataFrames.  Parameters    array  : pandas DataFrame or NumPy ndarray, shape = [n_rows, n_columns].    columns  : array-like, shape = [n_columns]  Array-like with column names, e.g., ['col1', 'col2', ...]\nor column indices [0, 2, 4, ...]    min_val  :  int  or  float , optional (default= 0 )  minimum value after rescaling.    min_val  :  int  or  float , optional (default= 1 )  maximum value after rescaling.    Returns    df_new  : pandas DataFrame object.  Copy of the array or DataFrame with rescaled columns.",
            "title": "minmax_scaling"
        },
        {
            "location": "/api_subpackages/mlxtend.preprocessing/#one_hot",
            "text": "one_hot(y, num_labels='auto', dtype='float')  One-hot encoding of class labels  Parameters    y  : array-like, shape = [n_classlabels]  Python list or numpy array consisting of class labels.    num_labels  : int or 'auto'  Number of unique labels in the class label array. Infers the number\nof unique labels from the input array if set to 'auto'.    dtype  : str  NumPy array type (float, float32, float64) of the output array.    Returns    ary  : numpy.ndarray, shape = [n_classlabels]  One-hot encoded array, where each sample is represented as\na row vector in the returned array.",
            "title": "one_hot"
        },
        {
            "location": "/api_subpackages/mlxtend.preprocessing/#shuffle_arrays_unison",
            "text": "shuffle_arrays_unison(arrays, random_seed=None)  Shuffle NumPy arrays in unison.  Parameters    arrays  : array-like, shape = [n_arrays]  A list of NumPy arrays.    random_seed  : int (default: None)  Sets the random state.    Returns   shuffled_arrays  : A list of NumPy arrays after shuffling.   Examples  >>> import numpy as np\n>>> from mlxtend.preprocessing import shuffle_arrays_unison\n>>> X1 = np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]])\n>>> y1 = np.array([1, 2, 3])\n>>> X2, y2 = shuffle_arrays_unison(arrays=[X1, y1], random_seed=3)\n>>> assert(X2.all() == np.array([[4, 5, 6], [1, 2, 3], [7, 8, 9]]).all())\n>>> assert(y2.all() == np.array([2, 1, 3]).all())\n>>>",
            "title": "shuffle_arrays_unison"
        },
        {
            "location": "/api_subpackages/mlxtend.preprocessing/#standardize",
            "text": "standardize(array, columns=None, ddof=0, return_params=False, params=None)  Standardize columns in pandas DataFrames.  Parameters    array  : pandas DataFrame or NumPy ndarray, shape = [n_rows, n_columns].    columns  : array-like, shape = [n_columns] (default: None)  Array-like with column names, e.g., ['col1', 'col2', ...]\nor column indices [0, 2, 4, ...]\nIf None, standardizes all columns.    ddof  : int (default: 0)  Delta Degrees of Freedom. The divisor used in calculations\nis N - ddof, where N represents the number of elements.    return_params  : dict (default: False)  If set to True, a dictionary is returned in addition to the\nstandardized array. The parameter dictionary contains the\ncolumn means ('avgs') and standard deviations ('stds') of\nthe individual columns.    params  : dict (default: None)  A dictionary with column means and standard deviations as\nreturned by the  standardize  function if  return_params \nwas set to True. If a  params  dictionary is provided, the standardize  function will use these instead of computing\nthem from the current array.    Notes  If all values in a given column are the same, these values are all\n    set to  0.0 . The standard deviation in the  parameters  dictionary\n    is consequently set to  1.0  to avoid dividing by zero.  Returns    df_new  : pandas DataFrame object.  Copy of the array or DataFrame with standardized columns.",
            "title": "standardize"
        },
        {
            "location": "/api_subpackages/mlxtend.regressor/",
            "text": "mlxtend version: 0.4.2dev0 \n\n\nLinearRegression\n\n\nLinearRegression(eta=0.01, epochs=50, minibatches=None, random_seed=None, print_progress=0)\n\n\nOrdinary least squares linear regression.\n\n\nParameters\n\n\n\n\n\n\neta\n : float (default: 0.01)\n\n\nsolver rate (between 0.0 and 1.0)\n\n\n\n\n\n\nepochs\n : int (default: 50)\n\n\nPasses over the training dataset.\nPrior to each epoch, the dataset is shuffled\nif \nminibatches > 1\n to prevent cycles in stochastic gradient descent.\n\n\n\n\n\n\nminibatches\n : int (default: None)\n\n\nThe number of minibatches for gradient-based optimization.\nIf None: Normal Equations (closed-form solution)\nIf 1: Gradient Descent learning\nIf len(y): Stochastic Gradient Descent learning\nIf 1 < minibatches < len(y): Minibatch learning\n\n\n\n\n\n\nrandom_seed\n : int (default: None)\n\n\nSet random state for shuffling and initializing the weights.\n\n\n\n\n\n\nprint_progress\n : int (default: 0)\n\n\nPrints progress in fitting to stderr if not solver='normal equation'\n0: No output\n1: Epochs elapsed and cost\n2: 1 plus time elapsed\n3: 2 plus estimated time until completion\n\n\n\n\n\n\nAttributes\n\n\n\n\n\n\nw_\n : 2d-array, shape={n_features, 1}\n\n\nModel weights after fitting.\n\n\n\n\n\n\nb_\n : 1d-array, shape={1,}\n\n\nBias unit after fitting.\n\n\n\n\n\n\ncost_\n : list\n\n\nSum of squared errors after each epoch;\nignored if solver='normal equation'\n\n\n\n\n\n\nMethods\n\n\n\n\n\nfit(X, y, init_params=True)\n\n\nLearn model from training data.\n\n\nParameters\n\n\n\n\n\n\nX\n : {array-like, sparse matrix}, shape = [n_samples, n_features]\n\n\nTraining vectors, where n_samples is the number of samples and\nn_features is the number of features.\n\n\n\n\n\n\ny\n : array-like, shape = [n_samples]\n\n\nTarget values.\n\n\n\n\n\n\ninit_params\n : bool (default: True)\n\n\nRe-initializes model parameters prior to fitting.\nSet False to continue training with weights from\na previous model fitting.\n\n\n\n\n\n\nReturns\n\n\n\n\nself\n : object\n\n\n\n\n\n\n\npredict(X)\n\n\nPredict targets from X.\n\n\nParameters\n\n\n\n\n\n\nX\n : {array-like, sparse matrix}, shape = [n_samples, n_features]\n\n\nTraining vectors, where n_samples is the number of samples and\nn_features is the number of features.\n\n\n\n\n\n\nReturns\n\n\n\n\n\n\ntarget_values\n : array-like, shape = [n_samples]\n\n\nPredicted target values.\n\n\n\n\n\n\nStackingRegressor\n\n\nStackingRegressor(regressors, meta_regressor, verbose=0)\n\n\nA Stacking regressor for scikit-learn estimators for regression.\n\n\nParameters\n\n\n\n\n\n\nregressors\n : array-like, shape = [n_regressors]\n\n\nA list of regressors.\nInvoking the \nfit\n method on the \nStackingRegressor\n will fit clones\nof those original regressors that will\nbe stored in the class attribute\n\nself.regr_\n.\n\n\n\n\n\n\nmeta_regressor\n : object\n\n\nThe meta-regressor to be fitted on the ensemble of\nregressors\n\n\n\n\n\n\nverbose\n : int, optional (default=0)\n\n\nControls the verbosity of the building process.\n- \nverbose=0\n (default): Prints nothing\n- \nverbose=1\n: Prints the number & name of the regressor being fitted\n- \nverbose=2\n: Prints info about the parameters of the\nregressor being fitted\n- \nverbose>2\n: Changes \nverbose\n param of the underlying regressor to\nself.verbose - 2\n\n\n\n\n\n\nAttributes\n\n\n\n\n\n\nregr_\n : list, shape=[n_regressors]\n\n\nFitted regressors (clones of the original regressors)\n\n\n\n\n\n\nmeta_regr_\n : estimator\n\n\nFitted meta-regressor (clone of the original meta-estimator)\n\n\n\n\n\n\ncoef_\n : array-like, shape = [n_features]\n\n\nModel coefficients of the fitted meta-estimator\n\n\n\n\n\n\nintercept_\n : float\n\n\nIntercept of the fitted meta-estimator\n\n\n\n\n\n\nMethods\n\n\n\n\n\nfit(X, y)\n\n\nLearn weight coefficients from training data for each regressor.\n\n\nParameters\n\n\n\n\n\n\nX\n : {array-like, sparse matrix}, shape = [n_samples, n_features]\n\n\nTraining vectors, where n_samples is the number of samples and\nn_features is the number of features.\n\n\n\n\n\n\ny\n : array-like, shape = [n_samples]\n\n\nTarget values.\n\n\n\n\n\n\nReturns\n\n\n\n\nself\n : object\n\n\n\n\n\n\n\nfit_transform(X, y=None, \nfit_params)\n\n\nFit to data, then transform it.\n\n\nFits transformer to X and y with optional parameters fit_params\nand returns a transformed version of X.\n\n\n\nParameters\n\n\n\n\n\n\nX\n : numpy array of shape [n_samples, n_features]\n\n\nTraining set.\n\n\n\n\n\n\ny\n : numpy array of shape [n_samples]\n\n\nTarget values.\n\n\n\n\n\n\nReturns\n\n\n\n\n\n\nX_new\n : numpy array of shape [n_samples, n_features_new]\n\n\nTransformed array.\n\n\n\n\n\n\n\n\n\nget_params(deep=True)\n\n\nReturn estimator parameter names for GridSearch support.\n\n\n\n\n\npredict(X)\n\n\nPredict target values for X.\n\n\nParameters\n\n\n\n\n\n\nX\n : {array-like, sparse matrix}, shape = [n_samples, n_features]\n\n\nTraining vectors, where n_samples is the number of samples and\nn_features is the number of features.\n\n\n\n\n\n\nReturns\n\n\n\n\n\n\ny_target\n : array-like, shape = [n_samples]\n\n\nPredicted target values.\n\n\n\n\n\n\n\n\n\nscore(X, y, sample_weight=None)\n\n\nReturns the coefficient of determination R^2 of the prediction.\n\n\nThe coefficient R^2 is defined as (1 - u/v), where u is the regression\n\n\n\nsum of squares ((y_true - y_pred) \n 2).sum() and v is the residual\nsum of squares ((y_true - y_true.mean()) \n 2).sum().\n\n\nBest possible score is 1.0 and it can be negative (because the\n\n\nmodel can be arbitrarily worse). A constant model that always\n    predicts the expected value of y, disregarding the input features,\n    would get a R^2 score of 0.0.\n\n\nParameters\n\n\n\n\n\n\nX\n : array-like, shape = (n_samples, n_features)\n\n\nTest samples.\n\n\n\n\n\n\ny\n : array-like, shape = (n_samples) or (n_samples, n_outputs)\n\n\nTrue values for X.\n\n\n\n\n\n\nsample_weight\n : array-like, shape = [n_samples], optional\n\n\nSample weights.\n\n\n\n\n\n\nReturns\n\n\n\n\n\n\nscore\n : float\n\n\nR^2 of self.predict(X) wrt. y.\n\n\n\n\n\n\n\n\n\nset_params(\nparams)\n\n\nSet the parameters of this estimator.\n\n\nThe method works on simple estimators as well as on nested objects\n(such as pipelines). The former have parameters of the form\n``<component>__<parameter>`` so that it's possible to update each\ncomponent of a nested object.\n\n\n\nReturns\n\n\nself\n\n\nProperties\n\n\n\n\n\ncoef_\n\n\nNone\n\n\n\n\n\nintercept_\n\n\nNone",
            "title": "Mlxtend.regressor"
        },
        {
            "location": "/api_subpackages/mlxtend.regressor/#linearregression",
            "text": "LinearRegression(eta=0.01, epochs=50, minibatches=None, random_seed=None, print_progress=0)  Ordinary least squares linear regression.  Parameters    eta  : float (default: 0.01)  solver rate (between 0.0 and 1.0)    epochs  : int (default: 50)  Passes over the training dataset.\nPrior to each epoch, the dataset is shuffled\nif  minibatches > 1  to prevent cycles in stochastic gradient descent.    minibatches  : int (default: None)  The number of minibatches for gradient-based optimization.\nIf None: Normal Equations (closed-form solution)\nIf 1: Gradient Descent learning\nIf len(y): Stochastic Gradient Descent learning\nIf 1 < minibatches < len(y): Minibatch learning    random_seed  : int (default: None)  Set random state for shuffling and initializing the weights.    print_progress  : int (default: 0)  Prints progress in fitting to stderr if not solver='normal equation'\n0: No output\n1: Epochs elapsed and cost\n2: 1 plus time elapsed\n3: 2 plus estimated time until completion    Attributes    w_  : 2d-array, shape={n_features, 1}  Model weights after fitting.    b_  : 1d-array, shape={1,}  Bias unit after fitting.    cost_  : list  Sum of squared errors after each epoch;\nignored if solver='normal equation'",
            "title": "LinearRegression"
        },
        {
            "location": "/api_subpackages/mlxtend.regressor/#methods",
            "text": "fit(X, y, init_params=True)  Learn model from training data.  Parameters    X  : {array-like, sparse matrix}, shape = [n_samples, n_features]  Training vectors, where n_samples is the number of samples and\nn_features is the number of features.    y  : array-like, shape = [n_samples]  Target values.    init_params  : bool (default: True)  Re-initializes model parameters prior to fitting.\nSet False to continue training with weights from\na previous model fitting.    Returns   self  : object    predict(X)  Predict targets from X.  Parameters    X  : {array-like, sparse matrix}, shape = [n_samples, n_features]  Training vectors, where n_samples is the number of samples and\nn_features is the number of features.    Returns    target_values  : array-like, shape = [n_samples]  Predicted target values.",
            "title": "Methods"
        },
        {
            "location": "/api_subpackages/mlxtend.regressor/#stackingregressor",
            "text": "StackingRegressor(regressors, meta_regressor, verbose=0)  A Stacking regressor for scikit-learn estimators for regression.  Parameters    regressors  : array-like, shape = [n_regressors]  A list of regressors.\nInvoking the  fit  method on the  StackingRegressor  will fit clones\nof those original regressors that will\nbe stored in the class attribute self.regr_ .    meta_regressor  : object  The meta-regressor to be fitted on the ensemble of\nregressors    verbose  : int, optional (default=0)  Controls the verbosity of the building process.\n-  verbose=0  (default): Prints nothing\n-  verbose=1 : Prints the number & name of the regressor being fitted\n-  verbose=2 : Prints info about the parameters of the\nregressor being fitted\n-  verbose>2 : Changes  verbose  param of the underlying regressor to\nself.verbose - 2    Attributes    regr_  : list, shape=[n_regressors]  Fitted regressors (clones of the original regressors)    meta_regr_  : estimator  Fitted meta-regressor (clone of the original meta-estimator)    coef_  : array-like, shape = [n_features]  Model coefficients of the fitted meta-estimator    intercept_  : float  Intercept of the fitted meta-estimator",
            "title": "StackingRegressor"
        },
        {
            "location": "/api_subpackages/mlxtend.regressor/#methods_1",
            "text": "fit(X, y)  Learn weight coefficients from training data for each regressor.  Parameters    X  : {array-like, sparse matrix}, shape = [n_samples, n_features]  Training vectors, where n_samples is the number of samples and\nn_features is the number of features.    y  : array-like, shape = [n_samples]  Target values.    Returns   self  : object    fit_transform(X, y=None,  fit_params)  Fit to data, then transform it.  Fits transformer to X and y with optional parameters fit_params\nand returns a transformed version of X.  Parameters    X  : numpy array of shape [n_samples, n_features]  Training set.    y  : numpy array of shape [n_samples]  Target values.    Returns    X_new  : numpy array of shape [n_samples, n_features_new]  Transformed array.     get_params(deep=True)  Return estimator parameter names for GridSearch support.   predict(X)  Predict target values for X.  Parameters    X  : {array-like, sparse matrix}, shape = [n_samples, n_features]  Training vectors, where n_samples is the number of samples and\nn_features is the number of features.    Returns    y_target  : array-like, shape = [n_samples]  Predicted target values.     score(X, y, sample_weight=None)  Returns the coefficient of determination R^2 of the prediction.  The coefficient R^2 is defined as (1 - u/v), where u is the regression  sum of squares ((y_true - y_pred)   2).sum() and v is the residual\nsum of squares ((y_true - y_true.mean())   2).sum().  Best possible score is 1.0 and it can be negative (because the  model can be arbitrarily worse). A constant model that always\n    predicts the expected value of y, disregarding the input features,\n    would get a R^2 score of 0.0.  Parameters    X  : array-like, shape = (n_samples, n_features)  Test samples.    y  : array-like, shape = (n_samples) or (n_samples, n_outputs)  True values for X.    sample_weight  : array-like, shape = [n_samples], optional  Sample weights.    Returns    score  : float  R^2 of self.predict(X) wrt. y.     set_params( params)  Set the parameters of this estimator.  The method works on simple estimators as well as on nested objects\n(such as pipelines). The former have parameters of the form\n``<component>__<parameter>`` so that it's possible to update each\ncomponent of a nested object.  Returns  self",
            "title": "Methods"
        },
        {
            "location": "/api_subpackages/mlxtend.regressor/#properties",
            "text": "coef_  None   intercept_  None",
            "title": "Properties"
        },
        {
            "location": "/api_subpackages/mlxtend.tf_regressor/",
            "text": "mlxtend version: 0.4.2dev0 \n\n\nTfLinearRegression\n\n\nTfLinearRegression(eta=0.1, epochs=50, print_progress=0, random_seed=None, dtype=None)\n\n\nEstimator for Linear Regression in TensorFlow using Gradient Descent.\n\n\nAdded in version 0.4.1\n\n\n\nMethods\n\n\n\n\n\nfit(X, y, init_params=True)\n\n\nLearn model from training data.\n\n\nParameters\n\n\n\n\n\n\nX\n : {array-like, sparse matrix}, shape = [n_samples, n_features]\n\n\nTraining vectors, where n_samples is the number of samples and\nn_features is the number of features.\n\n\n\n\n\n\ny\n : array-like, shape = [n_samples]\n\n\nTarget values.\n\n\n\n\n\n\ninit_params\n : bool (default: True)\n\n\nRe-initializes model parameters prior to fitting.\nSet False to continue training with weights from\na previous model fitting.\n\n\n\n\n\n\nReturns\n\n\n\n\nself\n : object\n\n\n\n\n\n\n\npredict(X)\n\n\nPredict class labels of X.\n\n\nParameters\n\n\n\n\n\n\nX\n : {array-like, sparse matrix}, shape = [n_samples, n_features]\n\n\nTraining vectors, where n_samples is the number of samples and\nn_features is the number of features.\n\n\n\n\n\n\nReturns\n\n\n\n\n\n\nclass_labels\n : array-like, shape = [n_samples]\n\n\nPredicted class labels.",
            "title": "Mlxtend.tf regressor"
        },
        {
            "location": "/api_subpackages/mlxtend.tf_regressor/#tflinearregression",
            "text": "TfLinearRegression(eta=0.1, epochs=50, print_progress=0, random_seed=None, dtype=None)  Estimator for Linear Regression in TensorFlow using Gradient Descent.  Added in version 0.4.1",
            "title": "TfLinearRegression"
        },
        {
            "location": "/api_subpackages/mlxtend.tf_regressor/#methods",
            "text": "fit(X, y, init_params=True)  Learn model from training data.  Parameters    X  : {array-like, sparse matrix}, shape = [n_samples, n_features]  Training vectors, where n_samples is the number of samples and\nn_features is the number of features.    y  : array-like, shape = [n_samples]  Target values.    init_params  : bool (default: True)  Re-initializes model parameters prior to fitting.\nSet False to continue training with weights from\na previous model fitting.    Returns   self  : object    predict(X)  Predict class labels of X.  Parameters    X  : {array-like, sparse matrix}, shape = [n_samples, n_features]  Training vectors, where n_samples is the number of samples and\nn_features is the number of features.    Returns    class_labels  : array-like, shape = [n_samples]  Predicted class labels.",
            "title": "Methods"
        },
        {
            "location": "/api_subpackages/mlxtend.regression_utils/",
            "text": "mlxtend version: 0.4.2dev0 \n\n\nplot_linear_regression\n\n\nplot_linear_regression(X, y, model=LinearRegression(copy_X=True, fit_intercept=True, n_jobs=1, normalize=False), corr_func='pearsonr', scattercolor='blue', fit_style='k--', legend=True, xlim='auto')\n\n\nPlot a linear regression line fit.\n\n\nParameters\n\n\n\n\n\n\nX\n : numpy array, shape = [n_samples,]\n\n\nSamples.\n\n\n\n\n\n\ny\n : numpy array, shape (n_samples,)\n\n\nTarget values\nmodel: object (default: sklearn.linear_model.LinearRegression)\nEstimator object for regression. Must implement\na .fit() and .predict() method.\ncorr_func: str or function (default: 'pearsonr')\nUses \npearsonr\n from scipy.stats if corr_func='pearsonr'.\nto compute the regression slope. If not 'pearsonr', the \ncorr_func\n,\nthe \ncorr_func\n parameter expects a function of the form\nfunc(\n, \n) as inputs, which is expected to return\na tuple \n(<correlation_coefficient>, <some_unused_value>)\n.\nscattercolor: string (default: blue)\nColor of scatter plot points.\nfit_style: string (default: k--)\nStyle for the line fit.\nlegend: bool (default: True)\nPlots legend with corr_coeff coef.,\nfit coef., and intercept values.\nxlim: array-like (x_min, x_max) or 'auto' (default: 'auto')\nX-axis limits for the linear line fit.\n\n\n\n\n\n\nReturns\n\n\n\n\n\n\nregression_fit\n : tuple\n\n\nintercept, slope, corr_coeff (float, float, float)",
            "title": "Mlxtend.regression utils"
        },
        {
            "location": "/api_subpackages/mlxtend.regression_utils/#plot_linear_regression",
            "text": "plot_linear_regression(X, y, model=LinearRegression(copy_X=True, fit_intercept=True, n_jobs=1, normalize=False), corr_func='pearsonr', scattercolor='blue', fit_style='k--', legend=True, xlim='auto')  Plot a linear regression line fit.  Parameters    X  : numpy array, shape = [n_samples,]  Samples.    y  : numpy array, shape (n_samples,)  Target values\nmodel: object (default: sklearn.linear_model.LinearRegression)\nEstimator object for regression. Must implement\na .fit() and .predict() method.\ncorr_func: str or function (default: 'pearsonr')\nUses  pearsonr  from scipy.stats if corr_func='pearsonr'.\nto compute the regression slope. If not 'pearsonr', the  corr_func ,\nthe  corr_func  parameter expects a function of the form\nfunc( ,  ) as inputs, which is expected to return\na tuple  (<correlation_coefficient>, <some_unused_value>) .\nscattercolor: string (default: blue)\nColor of scatter plot points.\nfit_style: string (default: k--)\nStyle for the line fit.\nlegend: bool (default: True)\nPlots legend with corr_coeff coef.,\nfit coef., and intercept values.\nxlim: array-like (x_min, x_max) or 'auto' (default: 'auto')\nX-axis limits for the linear line fit.    Returns    regression_fit  : tuple  intercept, slope, corr_coeff (float, float, float)",
            "title": "plot_linear_regression"
        },
        {
            "location": "/api_subpackages/mlxtend.text/",
            "text": "mlxtend version: 0.4.2dev0 \n\n\ngeneralize_names\n\n\ngeneralize_names(name, output_sep=' ', firstname_output_letters=1)\n\n\nGeneralize a person's first and last name.\n\n\nReturns a person's name in the format\n    \n<last_name><separator><firstname letter(s)> (all lowercase)\n\n\nParameters\n\n\n\n\n\n\nname\n : \nstr\n\n\nName of the player\n\n\n\n\n\n\noutput_sep\n : \nstr\n (default: ' ')\n\n\nString for separating last name and first name in the output.\n\n\n\n\n\n\nfirstname_output_letters\n : \nint\n\n\nNumber of letters in the abbreviated first name.\n\n\n\n\n\n\nReturns\n\n\n\n\n\n\ngen_name\n : \nstr\n\n\nThe generalized name.\n\n\n\n\n\n\ngeneralize_names_duplcheck\n\n\ngeneralize_names_duplcheck(df, col_name)\n\n\nGeneralizes names and removes duplicates.\n\n\nApplies mlxtend.text.generalize_names to a DataFrame\n    with 1 first name letter by default\n    and uses more first name letters if duplicates are detected.\n\n\nParameters\n\n\n\n\n\n\ndf\n : \npandas.DataFrame\n\n\nDataFrame that contains a column where\ngeneralize_names should be applied.\n\n\n\n\n\n\ncol_name\n : \nstr\n\n\nName of the DataFrame column where \ngeneralize_names\n\nfunction should be applied to.\n\n\n\n\n\n\nReturns\n\n\n\n\n\n\ndf_new\n : \nstr\n\n\nNew DataFrame object where generalize_names function has\nbeen applied without duplicates.\n\n\n\n\n\n\ntokenizer_emoticons\n\n\ntokenizer_emoticons(text)\n\n\nReturn emoticons from text\n\n\nExample:\n>>> tokenizer_emoticons('</a>This :) is :( a test :-)!')\n[':)', ':(', ':-)']\n\n\n\ntokenizer_words_and_emoticons\n\n\ntokenizer_words_and_emoticons(text)\n\n\nConvert text to lowercase words and emoticons.\n\n\nExample:\n>>> tokenizer_words_and_emoticons('</a>This :) is :( a test :-)!')\n['this', 'is', 'a', 'test', ':)', ':(', ':-)']",
            "title": "Mlxtend.text"
        },
        {
            "location": "/api_subpackages/mlxtend.text/#generalize_names",
            "text": "generalize_names(name, output_sep=' ', firstname_output_letters=1)  Generalize a person's first and last name.  Returns a person's name in the format\n     <last_name><separator><firstname letter(s)> (all lowercase)  Parameters    name  :  str  Name of the player    output_sep  :  str  (default: ' ')  String for separating last name and first name in the output.    firstname_output_letters  :  int  Number of letters in the abbreviated first name.    Returns    gen_name  :  str  The generalized name.",
            "title": "generalize_names"
        },
        {
            "location": "/api_subpackages/mlxtend.text/#generalize_names_duplcheck",
            "text": "generalize_names_duplcheck(df, col_name)  Generalizes names and removes duplicates.  Applies mlxtend.text.generalize_names to a DataFrame\n    with 1 first name letter by default\n    and uses more first name letters if duplicates are detected.  Parameters    df  :  pandas.DataFrame  DataFrame that contains a column where\ngeneralize_names should be applied.    col_name  :  str  Name of the DataFrame column where  generalize_names \nfunction should be applied to.    Returns    df_new  :  str  New DataFrame object where generalize_names function has\nbeen applied without duplicates.",
            "title": "generalize_names_duplcheck"
        },
        {
            "location": "/api_subpackages/mlxtend.text/#tokenizer_emoticons",
            "text": "tokenizer_emoticons(text)  Return emoticons from text  Example:\n>>> tokenizer_emoticons('</a>This :) is :( a test :-)!')\n[':)', ':(', ':-)']",
            "title": "tokenizer_emoticons"
        },
        {
            "location": "/api_subpackages/mlxtend.text/#tokenizer_words_and_emoticons",
            "text": "tokenizer_words_and_emoticons(text)  Convert text to lowercase words and emoticons.  Example:\n>>> tokenizer_words_and_emoticons('</a>This :) is :( a test :-)!')\n['this', 'is', 'a', 'test', ':)', ':(', ':-)']",
            "title": "tokenizer_words_and_emoticons"
        },
        {
            "location": "/api_subpackages/mlxtend.utils/",
            "text": "mlxtend version: 0.4.2dev0 \n\n\nassert_raises\n\n\nassert_raises(exception_type, message, func, \nargs, *\nkwargs)\n\n\nCheck that an exception is raised with a specific message\n\n\nParameters\n\n\n\n\n\n\nexception_type\n : exception\n\n\nThe exception that should be raised\n\n\n\n\n\n\nmessage\n : str (default: None)\n\n\nThe error message that should be raised. Ignored if False or None.\n\n\n\n\n\n\nfunc\n : callable\n\n\nThe function that raises the exception\n\n\n\n\n\n\n*args\n : positional arguments to \nfunc\n.\n\n\n\n\n\n\n**kwargs\n : keyword arguments to \nfunc\n\n\n\n\n\n\ncheck_Xy\n\n\ncheck_Xy(X, y, y_int=True)\n\n\nNone\n\n\nCounter\n\n\nCounter(stderr=False, start_newline=True)\n\n\nClass to display the progress of for-loop iterators.\n\n\nParameters\n\n\n\n\n\n\nstderr\n : bool (default: True)\n\n\nPrints output to sys.stderr if True; uses sys.stdout otherwise.\n\n\n\n\n\n\nstart_newline\n : bool (default: True)\n\n\nPrepends a new line to the counter, which prevents overwriting counters\nif multiple counters are printed in succession.\n\n\n\n\n\n\nAttributes\n\n\n\n\n\n\ncurr_iter\n : int\n\n\nThe current iteration.\n\n\n\n\n\n\nstart_time\n : int\n\n\nThe system's time in seconds when the Counter was initialized.\n\n\n\n\n\n\nMethods\n\n\n\n\n\nupdate()\n\n\nPrint current iteration and time elapsed.",
            "title": "Mlxtend.utils"
        },
        {
            "location": "/api_subpackages/mlxtend.utils/#assert_raises",
            "text": "assert_raises(exception_type, message, func,  args, * kwargs)  Check that an exception is raised with a specific message  Parameters    exception_type  : exception  The exception that should be raised    message  : str (default: None)  The error message that should be raised. Ignored if False or None.    func  : callable  The function that raises the exception    *args  : positional arguments to  func .    **kwargs  : keyword arguments to  func",
            "title": "assert_raises"
        },
        {
            "location": "/api_subpackages/mlxtend.utils/#check_xy",
            "text": "check_Xy(X, y, y_int=True)  None",
            "title": "check_Xy"
        },
        {
            "location": "/api_subpackages/mlxtend.utils/#counter",
            "text": "Counter(stderr=False, start_newline=True)  Class to display the progress of for-loop iterators.  Parameters    stderr  : bool (default: True)  Prints output to sys.stderr if True; uses sys.stdout otherwise.    start_newline  : bool (default: True)  Prepends a new line to the counter, which prevents overwriting counters\nif multiple counters are printed in succession.    Attributes    curr_iter  : int  The current iteration.    start_time  : int  The system's time in seconds when the Counter was initialized.",
            "title": "Counter"
        },
        {
            "location": "/api_subpackages/mlxtend.utils/#methods",
            "text": "update()  Print current iteration and time elapsed.",
            "title": "Methods"
        },
        {
            "location": "/installation/",
            "text": "Installing mlxtend\n\n\n\n\nPyPI\n\n\nTo install mlxtend, just execute  \n\n\npip install mlxtend  \n\n\n\n\nAlternatively, you download the package manually from the Python Package Index \nhttps://pypi.python.org/pypi/mlxtend\n, unzip it, navigate into the package, and use the command:\n\n\npython setup.py install\n\n\n\n\nUpgrading via \npip\n\n\nTo upgrade an existing version of mlxtend from PyPI, execute\n\n\npip install mlxtend --upgrade --no-deps\n\n\n\n\nPlease note that the dependencies (NumPy and SciPy) will also be upgraded if you omit the \n--no-deps\n flag; use the \n--no-deps\n (\"no dependencies\") flag if you don't want this.\n\n\nDev Version\n\n\nThe mlxtend version on PyPI may always one step behind; you can install the latest development version from the GitHub repository by executing\n\n\npip install git+git://github.com/rasbt/mlxtend.git\n\n\n\n\nOr, you can fork the GitHub repository from https://github.com/rasbt/mlxtend and install mlxtend from your local drive via\n\n\npython setup.py install\n\n\n\n\nAnaconda/Conda\n\n\nConda\n packages are now available for Mac, Windows, and Linux. You can install mlxtend using conda by executing\n\n\nconda install -c rasbt mlxtend",
            "title": "Installation"
        },
        {
            "location": "/installation/#installing-mlxtend",
            "text": "",
            "title": "Installing mlxtend"
        },
        {
            "location": "/installation/#pypi",
            "text": "To install mlxtend, just execute    pip install mlxtend    Alternatively, you download the package manually from the Python Package Index  https://pypi.python.org/pypi/mlxtend , unzip it, navigate into the package, and use the command:  python setup.py install",
            "title": "PyPI"
        },
        {
            "location": "/installation/#upgrading-via-pip",
            "text": "To upgrade an existing version of mlxtend from PyPI, execute  pip install mlxtend --upgrade --no-deps  Please note that the dependencies (NumPy and SciPy) will also be upgraded if you omit the  --no-deps  flag; use the  --no-deps  (\"no dependencies\") flag if you don't want this.",
            "title": "Upgrading via pip"
        },
        {
            "location": "/installation/#dev-version",
            "text": "The mlxtend version on PyPI may always one step behind; you can install the latest development version from the GitHub repository by executing  pip install git+git://github.com/rasbt/mlxtend.git  Or, you can fork the GitHub repository from https://github.com/rasbt/mlxtend and install mlxtend from your local drive via  python setup.py install",
            "title": "Dev Version"
        },
        {
            "location": "/installation/#anacondaconda",
            "text": "Conda  packages are now available for Mac, Windows, and Linux. You can install mlxtend using conda by executing  conda install -c rasbt mlxtend",
            "title": "Anaconda/Conda"
        },
        {
            "location": "/changelog/",
            "text": "Release Notes\n\n\n\n\nVersion 0.4.2 (2016-08-24)\n\n\nDownloads\n\n\n\n\nSource code (zip)\n\n\nSource code (tar.gz)\n\n\nPDF documentation\n\n\n\n\nNew Features\n\n\n\n\nAdded \npreprocessing.CopyTransformer\n, a mock class that returns copies of\nimput arrays via \ntransform\n and \nfit_transform\n\n\n\n\nChanges\n\n\n\n\nAdded AppVeyor to CI to ensure MS Windows compatibility\n\n\nDataset are now saved as compressed .txt or .csv files rather than being imported as Python objects\n\n\nfeature_selection.SequentialFeatureSelector\n now supports the selection of \nk_features\n using a tuple to specify a \"min-max\" \nk_features\n range\n\n\nAdded \"SVD solver\" option to the \nPrincipalComponentAnalysis\n\n\nRaise a \nAttributeError\n with \"not fitted\" message in \nSequentialFeatureSelector\n if \ntransform\n or \nget_metric_dict\n are called prior to \nfit\n\n\nUse small, positive bias units in \nTfMultiLayerPerceptron\n's hidden layer(s) if the activations are ReLUs in order to avoid dead neurons\n\n\nAdded an optional \nclone_estimator\n parameter to the \nSequentialFeatureSelector\n that defaults to \nTrue\n, avoiding the modification of the original estimator objects\n\n\nMore rigorous type and shape checks in the \nevaluate.plot_decision_regions\n function\n\n\nDenseTransformer\n now doesn't raise and error if the input array is \nnot\n sparse\n\n\nAPI clean-up using scikit-learn's \nBaseEstimator\n as parent class for \nfeature_selection.ColumnSelector\n\n\n\n\nBug Fixes\n\n\n\n\nFixed a problem when a tuple-range was provided as argument to the \nSequentialFeatureSelector\n's \nk_features\n parameter and the scoring metric was more negative than -1 (e.g., as in scikit-learn's MSE scoring function) via \nwahutch\n\n\nFixed an \nAttributeError\n issue when \nverbose\n > 1 in \nStackingClassifier\n\n\nFixed a bug in \nclassifier.SoftmaxRegression\n where the mean values of the offsets were used to update the bias units rather than their sum\n\n\nFixed rare bug in MLP \n_layer_mapping\n functions that caused a swap between the random number generation seed when initializing weights and biases\n\n\n\n\nVersion 0.4.1 (2016-05-01)\n\n\nDownloads\n\n\n\n\nSource code (zip)\n\n\nSource code (tar.gz)\n\n\nPDF documentation\n\n\n\n\nNew Features\n\n\n\n\nNew TensorFlow estimator for Linear Regression (\ntf_regressor.TfLinearRegression\n)\n\n\nNew k-means clustering estimator (\ncluster.Kmeans\n)\n\n\nNew TensorFlow k-means clustering estimator (\ntf_cluster.Kmeans\n)\n\n\n\n\nChanges\n\n\n\n\nDue to refactoring of the estimator classes, the \ninit_weights\n parameter of the \nfit\n methods was globally renamed to \ninit_params\n\n\nOverall performance improvements of estimators due to code clean-up and refactoring\n\n\nAdded several additional checks for correct array types and more meaningful exception messages\n\n\nAdded optional \ndropout\n to the \ntf_classifier.TfMultiLayerPerceptron\n classifier for regularization\n\n\nAdded an optional \ndecay\n parameter to the \ntf_classifier.TfMultiLayerPerceptron\n classifier for adaptive learning via an exponential decay of the learning rate eta\n\n\nReplaced old \nNeuralNetMLP\n by more streamlined \nMultiLayerPerceptron\n (\nclassifier.MultiLayerPerceptron\n); now also with softmax in the output layer and categorical cross-entropy loss.\n\n\nUnified \ninit_params\n parameter for fit functions to continue training where the algorithm left off (if supported)\n\n\n\n\nVersion 0.4.0 (2016-04-09)\n\n\nNew Features\n\n\n\n\nNew \nTfSoftmaxRegression\n classifier using Tensorflow (\ntf_classifier.TfSoftmaxRegression\n)\n\n\nNew \nSoftmaxRegression\n classifier (\nclassifier.SoftmaxRegression\n)\n\n\nNew \nTfMultiLayerPerceptron\n classifier using Tensorflow (\ntf_classifier.TfMultiLayerPerceptron\n)\n\n\nNew \nStackingRegressor\n (\nregressor.StackingRegressor\n)\n\n\nNew \nStackingClassifier\n (\nclassifier.StackingClassifier\n)\n\n\nNew function for one-hot encoding of class labels (\npreprocessing.one_hot\n)\n\n\nAdded \nGridSearch\n support to the \nSequentialFeatureSelector\n (\nfeature_selection/.SequentialFeatureSelector\n)\n\n\nevaluate.plot_decision_regions\n improvements:\n\n\nFunction now handles class y-class labels correctly if array is of type \nfloat\n\n\nCorrect handling of input arguments \nmarkers\n and \ncolors\n\n\nAccept an existing \nAxes\n via the \nax\n argument\n\n\n\n\n\n\nNew \nprint_progress\n parameter for all generalized models and multi-layer neural networks for printing time elapsed, ETA, and the current cost of the current epoch\n\n\nMinibatch learning for \nclassifier.LogisticRegression\n, \nclassifier.Adaline\n, and \nregressor.LinearRegression\n plus streamlined API\n\n\nNew Principal Component Analysis class via \nmlxtend.feature_extraction.PrincipalComponentAnalysis\n\n\nNew RBF Kernel Principal Component Analysis class via \nmlxtend.feature_extraction.RBFKernelPCA\n\n\nNew Linear Discriminant Analysis class via \nmlxtend.feature_extraction.LinearDiscriminantAnalysis\n\n\n\n\nChanges\n\n\n\n\nThe \ncolumn\n parameter in \nmlxtend.preprocessing.standardize\n now defaults to \nNone\n to standardize all columns more conveniently\n\n\n\n\nVersion 0.3.0 (2016-01-31)\n\n\nDownloads\n\n\n\n\nSource code (zip)\n\n\nSource code (tar.gz)\n\n\n\n\nNew Features\n\n\n\n\nAdded a progress bar tracker to \nclassifier.NeuralNetMLP\n\n\nAdded a function to score predicted vs. target class labels \nevaluate.scoring\n\n\nAdded confusion matrix functions to create (\nevaluate.confusion_matrix\n) and plot (\nevaluate.plot_confusion_matrix\n) confusion matrices\n\n\nNew style parameter and improved axis scaling in \nmlxtend.evaluate.plot_learning_curves\n\n\nAdded \nloadlocal_mnist\n to \nmlxtend.data\n for streaming MNIST from a local byte files into numpy arrays\n\n\nNew \nNeuralNetMLP\n parameters: \nrandom_weights\n, \nshuffle_init\n, \nshuffle_epoch\n\n\nNew \nSFS\n features such as the generation of pandas \nDataFrame\n results tables and plotting functions (with confidence intervals, standard deviation, and standard error bars)\n\n\nAdded support for regression estimators in \nSFS\n\n\nAdded Boston \nhousing dataset\n\n\nNew \nshuffle\n parameter for \nclassifier.NeuralNetMLP\n\n\n\n\nChanges\n\n\n\n\nThe \nmlxtend.preprocessing.standardize\n function now optionally returns the parameters, which are estimated from the array, for re-use. A further improvement makes the \nstandardize\n function smarter in order to avoid zero-division errors\n\n\nCosmetic improvements to the \nevaluate.plot_decision_regions\n function such as hiding plot axes\n\n\nRenaming of \nclassifier.EnsembleClassfier\n to \nclassifier.EnsembleVoteClassifier\n\n\nImproved random weight initialization in \nPerceptron\n, \nAdaline\n, \nLinearRegression\n, and \nLogisticRegression\n\n\nChanged \nlearning\n parameter of \nmlxtend.classifier.Adaline\n to \nsolver\n and added \"normal equation\" as closed-form solution solver\n\n\nHide y-axis labels in \nmlxtend.evaluate.plot_decision_regions\n in 1 dimensional evaluations\n\n\nSequential Feature Selection algorithms were unified into a single \nSequentialFeatureSelector\n class with parameters to enable floating selection and toggle between forward and backward selection.\n\n\nStratified sampling of MNIST (now 500x random samples from each of the 10 digit categories)\n\n\nRenaming \nmlxtend.plotting\n to \nmlxtend.general_plotting\n in order to distinguish general plotting function from specialized utility function such as \nevaluate.plot_decision_regions\n\n\n\n\nVersion 0.2.9 (2015-07-14)\n\n\nDownloads\n\n\n\n\nSource code (zip)\n\n\nSource code (tar.gz)\n\n\n\n\nNew Features\n\n\n\n\nSequential Feature Selection algorithms: SFS, SFFS, SBS, and SFBS\n\n\n\n\nChanges\n\n\n\n\nChanged \nregularization\n & \nlambda\n parameters in \nLogisticRegression\n to single parameter \nl2_lambda\n\n\n\n\nVersion 0.2.8 (2015-06-27)\n\n\n\n\nAPI changes:\n\n\nmlxtend.sklearn.EnsembleClassifier\n -> \nmlxtend.classifier.EnsembleClassifier\n\n\nmlxtend.sklearn.ColumnSelector\n -> \nmlxtend.feature_selection.ColumnSelector\n\n\nmlxtend.sklearn.DenseTransformer\n -> \nmlxtend.preprocessing.DenseTransformer\n\n\nmlxtend.pandas.standardizing\n ->  \nmlxtend.preprocessing.standardizing\n\n\nmlxtend.pandas.minmax_scaling\n ->  \nmlxtend.preprocessing.minmax_scaling\n\n\nmlxtend.matplotlib\n -> \nmlxtend.plotting\n\n\n\n\n\n\nAdded momentum learning parameter (alpha coefficient) to \nmlxtend.classifier.NeuralNetMLP\n.\n\n\nAdded adaptive learning rate (decrease constant) to \nmlxtend.classifier.NeuralNetMLP\n.\n\n\nmlxtend.pandas.minmax_scaling\n became \nmlxtend.preprocessing.minmax_scaling\n  and also supports NumPy arrays now\n\n\nmlxtend.pandas.standardizing\n became \nmlxtend.preprocessing.standardizing\n and now supports both NumPy arrays and pandas DataFrames; also, now \nddof\n parameters to set the degrees of freedom when calculating the standard deviation\n\n\n\n\nVersion 0.2.7 (2015-06-20)\n\n\n\n\nAdded multilayer perceptron (feedforward artificial neural network) classifier as \nmlxtend.classifier.NeuralNetMLP\n.\n\n\nAdded 5000 labeled trainingsamples from the MNIST handwritten digits dataset to \nmlxtend.data\n\n\n\n\nVersion 0.2.6 (2015-05-08)\n\n\n\n\nAdded ordinary least square regression using different solvers (gradient and stochastic gradient descent, and the closed form solution (normal equation)\n\n\nAdded option for random weight initialization to logistic regression classifier and updated l2 regularization\n\n\nAdded \nwine\n dataset to \nmlxtend.data\n\n\nAdded \ninvert_axes\n parameter \nmlxtend.matplotlib.enrichtment_plot\n to optionally plot the \"Count\" on the x-axis\n\n\nNew \nverbose\n parameter for \nmlxtend.sklearn.EnsembleClassifier\n by \nAlejandro C. Bahnsen\n\n\nAdded \nmlxtend.pandas.standardizing\n to standardize columns in a Pandas DataFrame\n\n\nAdded parameters \nlinestyles\n and \nmarkers\n to \nmlxtend.matplotlib.enrichment_plot\n\n\nmlxtend.regression.lin_regplot\n automatically adds np.newaxis and works w. python lists\n\n\nAdded tokenizers: \nmlxtend.text.extract_emoticons\n and \nmlxtend.text.extract_words_and_emoticons\n\n\n\n\nVersion 0.2.5 (2015-04-17)\n\n\n\n\nAdded Sequential Backward Selection (mlxtend.sklearn.SBS)\n\n\nAdded \nX_highlight\n parameter to \nmlxtend.evaluate.plot_decision_regions\n for highlighting test data points.\n\n\nAdded mlxtend.regression.lin_regplot to plot the fitted line from linear regression.\n\n\nAdded mlxtend.matplotlib.stacked_barplot to conveniently produce stacked barplots using pandas \nDataFrame\ns.\n\n\nAdded mlxtend.matplotlib.enrichment_plot\n\n\n\n\nVersion 0.2.4 (2015-03-15)\n\n\n\n\nAdded \nscoring\n to \nmlxtend.evaluate.learning_curves\n (by user pfsq)\n\n\nFixed setup.py bug caused by the missing README.html file\n\n\nmatplotlib.category_scatter for pandas DataFrames and Numpy arrays\n\n\n\n\nVersion 0.2.3 (2015-03-11)\n\n\n\n\nAdded Logistic regression\n\n\nGradient descent and stochastic gradient descent perceptron was changed\n  to Adaline (Adaptive Linear Neuron)\n\n\nPerceptron and Adaline for {0, 1} classes\n\n\nAdded \nmlxtend.preprocessing.shuffle_arrays_unison\n function to\n  shuffle one or more NumPy arrays.\n\n\nAdded shuffle and random seed parameter to stochastic gradient descent classifier.\n\n\nAdded \nrstrip\n parameter to \nmlxtend.file_io.find_filegroups\n to allow trimming of base names.\n\n\nAdded \nignore_substring\n parameter to \nmlxtend.file_io.find_filegroups\n and \nfind_files\n.\n\n\nReplaced .rstrip in \nmlxtend.file_io.find_filegroups\n with more robust regex.\n\n\nGridsearch support for \nmlxtend.sklearn.EnsembleClassifier\n\n\n\n\nVersion 0.2.2 (2015-03-01)\n\n\n\n\nImproved robustness of EnsembleClassifier.\n\n\nExtended plot_decision_regions() functionality for plotting 1D decision boundaries.\n\n\nFunction matplotlib.plot_decision_regions was reorganized  to evaluate.plot_decision_regions .\n\n\nevaluate.plot_learning_curves() function added.\n\n\nAdded Rosenblatt, gradient descent, and stochastic gradient descent perceptrons.\n\n\n\n\nVersion 0.2.1 (2015-01-20)\n\n\n\n\nAdded mlxtend.pandas.minmax_scaling - a function to rescale pandas DataFrame columns.\n\n\nSlight update to the EnsembleClassifier interface (additional \nvoting\n parameter)\n\n\nFixed EnsembleClassifier to return correct class labels if class labels are not\n  integers from 0 to n.\n\n\nAdded new matplotlib function to plot decision regions of classifiers.\n\n\n\n\nVersion 0.2.0 (2015-01-13)\n\n\n\n\nImproved mlxtend.text.generalize_duplcheck to remove duplicates and prevent endless looping issue.\n\n\nAdded \nrecursive\n search parameter to mlxtend.file_io.find_files.\n\n\nAdded \ncheck_ext\n parameter mlxtend.file_io.find_files to search based on file extensions.\n\n\nDefault parameter to ignore invisible files for mlxtend.file_io.find.\n\n\nAdded \ntransform\n and \nfit_transform\n to the \nEnsembleClassifier\n.\n\n\nAdded mlxtend.file_io.find_filegroups function.\n\n\n\n\nVersion 0.1.9 (2015-01-10)\n\n\n\n\nImplemented scikit-learn EnsembleClassifier (majority voting rule) class.\n\n\n\n\nVersion 0.1.8 (2015-01-07)\n\n\n\n\nImprovements to mlxtend.text.generalize_names to handle certain Dutch last name prefixes (van, van der, de, etc.).\n\n\nAdded mlxtend.text.generalize_name_duplcheck function to apply mlxtend.text.generalize_names function to a pandas DataFrame without creating duplicates.\n\n\n\n\nVersion 0.1.7 (2015-01-07)\n\n\n\n\nAdded text utilities with name generalization function.\n\n\nAdded  and file_io utilities.\n\n\n\n\nVersion 0.1.6 (2015-01-04)\n\n\n\n\nAdded combinations and permutations estimators.\n\n\n\n\nVersion 0.1.5 (2014-12-11)\n\n\n\n\nAdded \nDenseTransformer\n for pipelines and grid search.\n\n\n\n\nVersion 0.1.4 (2014-08-20)\n\n\n\n\nmean_centering\n function is now a Class that creates \nMeanCenterer\n objects\n  that can be used to fit data via the \nfit\n method, and center data at the column\n  means via the \ntransform\n and \nfit_transform\n method.\n\n\n\n\nVersion 0.1.3 (2014-08-19)\n\n\n\n\nAdded \npreprocessing\n module and \nmean_centering\n function.\n\n\n\n\nVersion 0.1.2 (2014-08-19)\n\n\n\n\nAdded \nmatplotlib\n utilities and \nremove_borders\n function.\n\n\n\n\nVersion 0.1.1 (2014-08-13)\n\n\n\n\nSimplified code for ColumnSelector.",
            "title": "Release Notes"
        },
        {
            "location": "/changelog/#release-notes",
            "text": "",
            "title": "Release Notes"
        },
        {
            "location": "/changelog/#version-042-2016-08-24",
            "text": "",
            "title": "Version 0.4.2 (2016-08-24)"
        },
        {
            "location": "/changelog/#downloads",
            "text": "Source code (zip)  Source code (tar.gz)  PDF documentation",
            "title": "Downloads"
        },
        {
            "location": "/changelog/#new-features",
            "text": "Added  preprocessing.CopyTransformer , a mock class that returns copies of\nimput arrays via  transform  and  fit_transform",
            "title": "New Features"
        },
        {
            "location": "/changelog/#changes",
            "text": "Added AppVeyor to CI to ensure MS Windows compatibility  Dataset are now saved as compressed .txt or .csv files rather than being imported as Python objects  feature_selection.SequentialFeatureSelector  now supports the selection of  k_features  using a tuple to specify a \"min-max\"  k_features  range  Added \"SVD solver\" option to the  PrincipalComponentAnalysis  Raise a  AttributeError  with \"not fitted\" message in  SequentialFeatureSelector  if  transform  or  get_metric_dict  are called prior to  fit  Use small, positive bias units in  TfMultiLayerPerceptron 's hidden layer(s) if the activations are ReLUs in order to avoid dead neurons  Added an optional  clone_estimator  parameter to the  SequentialFeatureSelector  that defaults to  True , avoiding the modification of the original estimator objects  More rigorous type and shape checks in the  evaluate.plot_decision_regions  function  DenseTransformer  now doesn't raise and error if the input array is  not  sparse  API clean-up using scikit-learn's  BaseEstimator  as parent class for  feature_selection.ColumnSelector",
            "title": "Changes"
        },
        {
            "location": "/changelog/#bug-fixes",
            "text": "Fixed a problem when a tuple-range was provided as argument to the  SequentialFeatureSelector 's  k_features  parameter and the scoring metric was more negative than -1 (e.g., as in scikit-learn's MSE scoring function) via  wahutch  Fixed an  AttributeError  issue when  verbose  > 1 in  StackingClassifier  Fixed a bug in  classifier.SoftmaxRegression  where the mean values of the offsets were used to update the bias units rather than their sum  Fixed rare bug in MLP  _layer_mapping  functions that caused a swap between the random number generation seed when initializing weights and biases",
            "title": "Bug Fixes"
        },
        {
            "location": "/changelog/#version-041-2016-05-01",
            "text": "",
            "title": "Version 0.4.1 (2016-05-01)"
        },
        {
            "location": "/changelog/#downloads_1",
            "text": "Source code (zip)  Source code (tar.gz)  PDF documentation",
            "title": "Downloads"
        },
        {
            "location": "/changelog/#new-features_1",
            "text": "New TensorFlow estimator for Linear Regression ( tf_regressor.TfLinearRegression )  New k-means clustering estimator ( cluster.Kmeans )  New TensorFlow k-means clustering estimator ( tf_cluster.Kmeans )",
            "title": "New Features"
        },
        {
            "location": "/changelog/#changes_1",
            "text": "Due to refactoring of the estimator classes, the  init_weights  parameter of the  fit  methods was globally renamed to  init_params  Overall performance improvements of estimators due to code clean-up and refactoring  Added several additional checks for correct array types and more meaningful exception messages  Added optional  dropout  to the  tf_classifier.TfMultiLayerPerceptron  classifier for regularization  Added an optional  decay  parameter to the  tf_classifier.TfMultiLayerPerceptron  classifier for adaptive learning via an exponential decay of the learning rate eta  Replaced old  NeuralNetMLP  by more streamlined  MultiLayerPerceptron  ( classifier.MultiLayerPerceptron ); now also with softmax in the output layer and categorical cross-entropy loss.  Unified  init_params  parameter for fit functions to continue training where the algorithm left off (if supported)",
            "title": "Changes"
        },
        {
            "location": "/changelog/#version-040-2016-04-09",
            "text": "",
            "title": "Version 0.4.0 (2016-04-09)"
        },
        {
            "location": "/changelog/#new-features_2",
            "text": "New  TfSoftmaxRegression  classifier using Tensorflow ( tf_classifier.TfSoftmaxRegression )  New  SoftmaxRegression  classifier ( classifier.SoftmaxRegression )  New  TfMultiLayerPerceptron  classifier using Tensorflow ( tf_classifier.TfMultiLayerPerceptron )  New  StackingRegressor  ( regressor.StackingRegressor )  New  StackingClassifier  ( classifier.StackingClassifier )  New function for one-hot encoding of class labels ( preprocessing.one_hot )  Added  GridSearch  support to the  SequentialFeatureSelector  ( feature_selection/.SequentialFeatureSelector )  evaluate.plot_decision_regions  improvements:  Function now handles class y-class labels correctly if array is of type  float  Correct handling of input arguments  markers  and  colors  Accept an existing  Axes  via the  ax  argument    New  print_progress  parameter for all generalized models and multi-layer neural networks for printing time elapsed, ETA, and the current cost of the current epoch  Minibatch learning for  classifier.LogisticRegression ,  classifier.Adaline , and  regressor.LinearRegression  plus streamlined API  New Principal Component Analysis class via  mlxtend.feature_extraction.PrincipalComponentAnalysis  New RBF Kernel Principal Component Analysis class via  mlxtend.feature_extraction.RBFKernelPCA  New Linear Discriminant Analysis class via  mlxtend.feature_extraction.LinearDiscriminantAnalysis",
            "title": "New Features"
        },
        {
            "location": "/changelog/#changes_2",
            "text": "The  column  parameter in  mlxtend.preprocessing.standardize  now defaults to  None  to standardize all columns more conveniently",
            "title": "Changes"
        },
        {
            "location": "/changelog/#version-030-2016-01-31",
            "text": "",
            "title": "Version 0.3.0 (2016-01-31)"
        },
        {
            "location": "/changelog/#downloads_2",
            "text": "Source code (zip)  Source code (tar.gz)",
            "title": "Downloads"
        },
        {
            "location": "/changelog/#new-features_3",
            "text": "Added a progress bar tracker to  classifier.NeuralNetMLP  Added a function to score predicted vs. target class labels  evaluate.scoring  Added confusion matrix functions to create ( evaluate.confusion_matrix ) and plot ( evaluate.plot_confusion_matrix ) confusion matrices  New style parameter and improved axis scaling in  mlxtend.evaluate.plot_learning_curves  Added  loadlocal_mnist  to  mlxtend.data  for streaming MNIST from a local byte files into numpy arrays  New  NeuralNetMLP  parameters:  random_weights ,  shuffle_init ,  shuffle_epoch  New  SFS  features such as the generation of pandas  DataFrame  results tables and plotting functions (with confidence intervals, standard deviation, and standard error bars)  Added support for regression estimators in  SFS  Added Boston  housing dataset  New  shuffle  parameter for  classifier.NeuralNetMLP",
            "title": "New Features"
        },
        {
            "location": "/changelog/#changes_3",
            "text": "The  mlxtend.preprocessing.standardize  function now optionally returns the parameters, which are estimated from the array, for re-use. A further improvement makes the  standardize  function smarter in order to avoid zero-division errors  Cosmetic improvements to the  evaluate.plot_decision_regions  function such as hiding plot axes  Renaming of  classifier.EnsembleClassfier  to  classifier.EnsembleVoteClassifier  Improved random weight initialization in  Perceptron ,  Adaline ,  LinearRegression , and  LogisticRegression  Changed  learning  parameter of  mlxtend.classifier.Adaline  to  solver  and added \"normal equation\" as closed-form solution solver  Hide y-axis labels in  mlxtend.evaluate.plot_decision_regions  in 1 dimensional evaluations  Sequential Feature Selection algorithms were unified into a single  SequentialFeatureSelector  class with parameters to enable floating selection and toggle between forward and backward selection.  Stratified sampling of MNIST (now 500x random samples from each of the 10 digit categories)  Renaming  mlxtend.plotting  to  mlxtend.general_plotting  in order to distinguish general plotting function from specialized utility function such as  evaluate.plot_decision_regions",
            "title": "Changes"
        },
        {
            "location": "/changelog/#version-029-2015-07-14",
            "text": "",
            "title": "Version 0.2.9 (2015-07-14)"
        },
        {
            "location": "/changelog/#downloads_3",
            "text": "Source code (zip)  Source code (tar.gz)",
            "title": "Downloads"
        },
        {
            "location": "/changelog/#new-features_4",
            "text": "Sequential Feature Selection algorithms: SFS, SFFS, SBS, and SFBS",
            "title": "New Features"
        },
        {
            "location": "/changelog/#changes_4",
            "text": "Changed  regularization  &  lambda  parameters in  LogisticRegression  to single parameter  l2_lambda",
            "title": "Changes"
        },
        {
            "location": "/changelog/#version-028-2015-06-27",
            "text": "API changes:  mlxtend.sklearn.EnsembleClassifier  ->  mlxtend.classifier.EnsembleClassifier  mlxtend.sklearn.ColumnSelector  ->  mlxtend.feature_selection.ColumnSelector  mlxtend.sklearn.DenseTransformer  ->  mlxtend.preprocessing.DenseTransformer  mlxtend.pandas.standardizing  ->   mlxtend.preprocessing.standardizing  mlxtend.pandas.minmax_scaling  ->   mlxtend.preprocessing.minmax_scaling  mlxtend.matplotlib  ->  mlxtend.plotting    Added momentum learning parameter (alpha coefficient) to  mlxtend.classifier.NeuralNetMLP .  Added adaptive learning rate (decrease constant) to  mlxtend.classifier.NeuralNetMLP .  mlxtend.pandas.minmax_scaling  became  mlxtend.preprocessing.minmax_scaling   and also supports NumPy arrays now  mlxtend.pandas.standardizing  became  mlxtend.preprocessing.standardizing  and now supports both NumPy arrays and pandas DataFrames; also, now  ddof  parameters to set the degrees of freedom when calculating the standard deviation",
            "title": "Version 0.2.8 (2015-06-27)"
        },
        {
            "location": "/changelog/#version-027-2015-06-20",
            "text": "Added multilayer perceptron (feedforward artificial neural network) classifier as  mlxtend.classifier.NeuralNetMLP .  Added 5000 labeled trainingsamples from the MNIST handwritten digits dataset to  mlxtend.data",
            "title": "Version 0.2.7 (2015-06-20)"
        },
        {
            "location": "/changelog/#version-026-2015-05-08",
            "text": "Added ordinary least square regression using different solvers (gradient and stochastic gradient descent, and the closed form solution (normal equation)  Added option for random weight initialization to logistic regression classifier and updated l2 regularization  Added  wine  dataset to  mlxtend.data  Added  invert_axes  parameter  mlxtend.matplotlib.enrichtment_plot  to optionally plot the \"Count\" on the x-axis  New  verbose  parameter for  mlxtend.sklearn.EnsembleClassifier  by  Alejandro C. Bahnsen  Added  mlxtend.pandas.standardizing  to standardize columns in a Pandas DataFrame  Added parameters  linestyles  and  markers  to  mlxtend.matplotlib.enrichment_plot  mlxtend.regression.lin_regplot  automatically adds np.newaxis and works w. python lists  Added tokenizers:  mlxtend.text.extract_emoticons  and  mlxtend.text.extract_words_and_emoticons",
            "title": "Version 0.2.6 (2015-05-08)"
        },
        {
            "location": "/changelog/#version-025-2015-04-17",
            "text": "Added Sequential Backward Selection (mlxtend.sklearn.SBS)  Added  X_highlight  parameter to  mlxtend.evaluate.plot_decision_regions  for highlighting test data points.  Added mlxtend.regression.lin_regplot to plot the fitted line from linear regression.  Added mlxtend.matplotlib.stacked_barplot to conveniently produce stacked barplots using pandas  DataFrame s.  Added mlxtend.matplotlib.enrichment_plot",
            "title": "Version 0.2.5 (2015-04-17)"
        },
        {
            "location": "/changelog/#version-024-2015-03-15",
            "text": "Added  scoring  to  mlxtend.evaluate.learning_curves  (by user pfsq)  Fixed setup.py bug caused by the missing README.html file  matplotlib.category_scatter for pandas DataFrames and Numpy arrays",
            "title": "Version 0.2.4 (2015-03-15)"
        },
        {
            "location": "/changelog/#version-023-2015-03-11",
            "text": "Added Logistic regression  Gradient descent and stochastic gradient descent perceptron was changed\n  to Adaline (Adaptive Linear Neuron)  Perceptron and Adaline for {0, 1} classes  Added  mlxtend.preprocessing.shuffle_arrays_unison  function to\n  shuffle one or more NumPy arrays.  Added shuffle and random seed parameter to stochastic gradient descent classifier.  Added  rstrip  parameter to  mlxtend.file_io.find_filegroups  to allow trimming of base names.  Added  ignore_substring  parameter to  mlxtend.file_io.find_filegroups  and  find_files .  Replaced .rstrip in  mlxtend.file_io.find_filegroups  with more robust regex.  Gridsearch support for  mlxtend.sklearn.EnsembleClassifier",
            "title": "Version 0.2.3 (2015-03-11)"
        },
        {
            "location": "/changelog/#version-022-2015-03-01",
            "text": "Improved robustness of EnsembleClassifier.  Extended plot_decision_regions() functionality for plotting 1D decision boundaries.  Function matplotlib.plot_decision_regions was reorganized  to evaluate.plot_decision_regions .  evaluate.plot_learning_curves() function added.  Added Rosenblatt, gradient descent, and stochastic gradient descent perceptrons.",
            "title": "Version 0.2.2 (2015-03-01)"
        },
        {
            "location": "/changelog/#version-021-2015-01-20",
            "text": "Added mlxtend.pandas.minmax_scaling - a function to rescale pandas DataFrame columns.  Slight update to the EnsembleClassifier interface (additional  voting  parameter)  Fixed EnsembleClassifier to return correct class labels if class labels are not\n  integers from 0 to n.  Added new matplotlib function to plot decision regions of classifiers.",
            "title": "Version 0.2.1 (2015-01-20)"
        },
        {
            "location": "/changelog/#version-020-2015-01-13",
            "text": "Improved mlxtend.text.generalize_duplcheck to remove duplicates and prevent endless looping issue.  Added  recursive  search parameter to mlxtend.file_io.find_files.  Added  check_ext  parameter mlxtend.file_io.find_files to search based on file extensions.  Default parameter to ignore invisible files for mlxtend.file_io.find.  Added  transform  and  fit_transform  to the  EnsembleClassifier .  Added mlxtend.file_io.find_filegroups function.",
            "title": "Version 0.2.0 (2015-01-13)"
        },
        {
            "location": "/changelog/#version-019-2015-01-10",
            "text": "Implemented scikit-learn EnsembleClassifier (majority voting rule) class.",
            "title": "Version 0.1.9 (2015-01-10)"
        },
        {
            "location": "/changelog/#version-018-2015-01-07",
            "text": "Improvements to mlxtend.text.generalize_names to handle certain Dutch last name prefixes (van, van der, de, etc.).  Added mlxtend.text.generalize_name_duplcheck function to apply mlxtend.text.generalize_names function to a pandas DataFrame without creating duplicates.",
            "title": "Version 0.1.8 (2015-01-07)"
        },
        {
            "location": "/changelog/#version-017-2015-01-07",
            "text": "Added text utilities with name generalization function.  Added  and file_io utilities.",
            "title": "Version 0.1.7 (2015-01-07)"
        },
        {
            "location": "/changelog/#version-016-2015-01-04",
            "text": "Added combinations and permutations estimators.",
            "title": "Version 0.1.6 (2015-01-04)"
        },
        {
            "location": "/changelog/#version-015-2014-12-11",
            "text": "Added  DenseTransformer  for pipelines and grid search.",
            "title": "Version 0.1.5 (2014-12-11)"
        },
        {
            "location": "/changelog/#version-014-2014-08-20",
            "text": "mean_centering  function is now a Class that creates  MeanCenterer  objects\n  that can be used to fit data via the  fit  method, and center data at the column\n  means via the  transform  and  fit_transform  method.",
            "title": "Version 0.1.4 (2014-08-20)"
        },
        {
            "location": "/changelog/#version-013-2014-08-19",
            "text": "Added  preprocessing  module and  mean_centering  function.",
            "title": "Version 0.1.3 (2014-08-19)"
        },
        {
            "location": "/changelog/#version-012-2014-08-19",
            "text": "Added  matplotlib  utilities and  remove_borders  function.",
            "title": "Version 0.1.2 (2014-08-19)"
        },
        {
            "location": "/changelog/#version-011-2014-08-13",
            "text": "Simplified code for ColumnSelector.",
            "title": "Version 0.1.1 (2014-08-13)"
        },
        {
            "location": "/contributing/",
            "text": "How to Contribute\n\n\n\n\nI would be very happy about any kind of contributions that help to improve and extend the functionality of mlxtend.\n\n\nQuick Contributor Checklist\n\n\nThis is a quick checklist about the different steps of a typical contribution to mlxtend (and\nother open source projects). Consider copying this list to a local text file (or the issue tracker)\nand checking off items as you go.\n\n\n\n\n[ ]  Open a new \"issue\" on GitHub to discuss the new feature / bug fix  \n\n\n[ ]  Fork the mlxtend repository from GitHub (if not already done earlier)\n\n\n[ ]  Create and check out a new topic branch   \n\n\n[ ]  Implement a new feature or apply the bug-fix  \n\n\n[ ]  Add appropriate unit test functions  \n\n\n[ ]  Run \nnosetests ./mlxtend -sv\n and make sure that all unit tests pass  \n\n\n[ ]  Check/improve the test coverage by running \nnosetests ./mlxtend --with-coverage\n\n\n[ ]  Check for style issues by running \nflake8 ./mlxtend\n (you may want to run \nnosetests\n again after you made modifications to the code)\n\n\n[ ]  Add a note about the modification/contribution to the \n./docs/sources/changelog.md\n file  \n\n\n[ ]  Modify documentation in the appropriate location under \nmlxtend/docs/sources/\n  \n\n\n[ ]  Push the topic branch to the server and create a pull request\n\n\n[ ]  Check the Travis-CI build passed at \nhttps://travis-ci.org/rasbt/mlxtend\n\n\n[ ]  Check/improve the unit test coverage at \nhttps://coveralls.io/github/rasbt/mlxtend\n\n\n[ ]  Check/improve the code health at \nhttps://landscape.io/github/rasbt/mlxtend\n\n\n[ ]  Squash (many small) commits to a larger commit\n\n\n\n\n\n\n\nTips for Contributors\n\n\nGetting Started - Creating a New Issue and Forking the Repository\n\n\n\n\nIf you don't have a \nGitHub\n account, yet, please create one to contribute to this project.\n\n\nPlease submit a ticket for your issue to discuss the fix or new feature before too much time and effort is spent for the implementation.\n\n\n\n\n\n\n\n\nFork the \nmlxtend\n repository from the GitHub web interface.\n\n\n\n\n\n\n\n\nClone the \nmlxtend\n repository to your local machine by executing\n \ngit clone https://github.com/<your_username>/mlxtend.git\n\n\n\n\nSyncing an Existing Fork\n\n\nIf you already forked mlxtend earlier, you can bring you \"Fork\" up to date\nwith the master branch as follows:\n\n\n1. Configuring a remote that points to the upstream repository on GitHub\n\n\nList the current configured remote repository of your fork by executing\n\n\n$ git remote -v\n\n\n\n\nIf you see something like\n\n\norigin  https://github.com/<your username>/mlxtend.git (fetch)\norigin  https://github.com/<your username>/mlxtend.git (push)\n\n\n\n\nyou need to specify a new remote \nupstream\n repository via\n\n\n$ git remote add upstream https://github.com/rasbt/mlxtend.git\n\n\n\n\nNow, verify the new upstream repository you've specified for your fork by executing\n\n\n$ git remote -v\n\n\n\n\nYou should see following output if everything is configured correctly:\n\n\norigin  https://github.com/<your username>/mlxtend.git (fetch)\norigin  https://github.com/<your username>/mlxtend.git (push)\nupstream    https://github.com/rasbt/mlxtend.git (fetch)\nupstream    https://github.com/rasbt/mlxtend.git (push)\n\n\n\n\n2. Syncing your Fork\n\n\nFirst, fetch the updates of the original project's master branch by executing:\n\n\n$ git fetch upstream\n\n\n\n\nYou should see the following output\n\n\nremote: Counting objects: xx, done.\nremote: Compressing objects: 100% (xx/xx), done.\nremote: Total xx (delta xx), reused xx (delta x)\nUnpacking objects: 100% (xx/xx), done.\nFrom https://github.com/rasbt/mlxtend\n * [new branch]      master     -> upstream/master\n\n\n\n\nThis means that the commits to the \nrasbt/mlxtend\n master branch are now\nstored in the local branch \nupstream/master\n.\n\n\nIf you are not already on your local project's master branch, execute\n\n\n$ git checkout master\n\n\n\n\nFinally, merge the changes in upstream/master to your local master branch by\nexecuting\n\n\n$ git merge upstream/master\n\n\n\n\nwhich will give you an output that looks similar to\n\n\nUpdating xxx...xxx\nFast-forward\nSOME FILE1                    |    12 +++++++\nSOME FILE2                    |    10 +++++++\n2 files changed, 22 insertions(+),\n\n\n\n\n*The Main Workflow - Making Changes in a New Topic Branch\n\n\nListed below are the 9 typical steps of a contribution.\n\n\n1. Discussing the Feature or Modification\n\n\nBefore you start coding, please discuss the new feature, bugfix, or other modification to the project\non the project's \nissue tracker\n. Before you open a \"new issue,\" please\ndo a quick search to see if a similar issue has been submitted already.\n\n\n2. Creating a new feature branch\n\n\nPlease avoid working directly on the master branch but create a new feature branch:\n\n\n$ git branch <new_feature>\n\n\n\n\nSwitch to the new feature branch by executing\n\n\n$ git checkout <new_feature>\n\n\n\n\n3. Developing the new feature / bug fix\n\n\nNow it's time to modify existing code or to contribute new code to the project.\n\n\n4. Testing your code\n\n\nAdd the respective unit tests and check if they pass:\n\n\n$ nosetests -sv\n\n\n\n\nUse the \n--with-coverage\n flag to ensure that all code is being covered in the unit tests:\n\n\n$ nosetests --with-coverage\n\n\n\n\n5. Documenting changes\n\n\nPlease add an entry to the \nmlxtend/docs/sources/changelog.md\n file.\nIf it is a new feature, it would also be nice if you could update the documentation in appropriate location in \nmlxtend/sources\n.\n\n\n6. Committing changes\n\n\nWhen you are ready to commit the changes, please provide a meaningful \ncommit\n message:\n\n\n$ git add <modifies_files> # or `git add .`\n$ git commit -m '<meaningful commit message>'\n\n\n\n\n7. Optional: squashing commits\n\n\nIf you made multiple smaller commits, it would be nice if you could group them into a larger, summarizing commit. First, list your recent commit via\n\n\n$ git log\n\n\n\n\nwhich will list the commits from newest to oldest in the following format by default:\n\n\ncommit 046e3af8a9127df8eac879454f029937c8a31c41\nAuthor: rasbt <mail@sebastianraschka.com>\nDate:   Tue Nov 24 03:46:37 2015 -0500\n\n    fixed setup.py\n\ncommit c3c00f6ba0e8f48bbe1c9081b8ae3817e57ecc5c\nAuthor: rasbt <mail@sebastianraschka.com>\nDate:   Tue Nov 24 03:04:39 2015 -0500\n\n        documented feature x\n\ncommit d87934fe8726c46f0b166d6290a3bf38915d6e75\nAuthor: rasbt <mail@sebastianraschka.com>\nDate:   Tue Nov 24 02:44:45 2015 -0500\n\n        added support for feature x\n\n\n\n\nAssuming that it would make sense to group these 3 commits into one, we can execute\n\n\n$ git rebase -i HEAD~3\n\n\n\n\nwhich will bring our default git editor with the following contents:\n\n\npick d87934f added support for feature x\npick c3c00f6 documented feature x\npick 046e3af fixed setup.py\n\n\n\n\nSince \nc3c00f6\n and \n046e3af\n are related to the original commit of \nfeature x\n, let's keep the \nd87934f\n and squash the 2 following commits into this initial one by changes the lines to\n\n\npick d87934f added support for feature x\nsquash c3c00f6 documented feature x\nsquash 046e3af fixed setup.py\n\n\n\n\nNow, save the changes in your editor. Now, quitting the editor will apply the \nrebase\n changes, and the editor will open a second time, prompting you to enter a new commit message. In this case, we could enter \nsupport for feature x\n to summarize the contributions.\n\n\n8. Uploading changes\n\n\nPush your changes to a topic branch to the git server by executing:\n\n\n$ git push origin <feature_branch>\n\n\n\n\n9. Submitting a \npull request\n\n\nGo to your GitHub repository online, select the new feature branch, and submit a new pull request:\n\n\n\n\n\n\n\nNotes for Developers\n\n\nBuilding the documentation\n\n\nThe documentation is built via \nMkDocs\n; to ensure that the documentation is rendered correctly, you can view the documentation locally by executing \nmkdocs serve\n from the \nmlxtend/docs\n directory.\n\n\nFor example,\n\n\n~/github/mlxtend/docs$ mkdocs serve\n\n\n\n\n1. Building the API documentation\n\n\nTo build the API documentation, navigate to \nmlxtend/docs\n and execute the \nmake_api.py\n file from this directory via\n\n\n~/github/mlxtend/docs$ python make_api.py\n\n\n\n\nThis should place the API documentation into the correct directories into the two directories:\n\n\n\n\nmlxtend/docs/sources/api_modules\n\n\nmlxtend/docs/sources/api_subpackes\n\n\n\n\n2. Editing the User Guide\n\n\nThe documents containing code examples for the \"User Guide\" are generated from IPython Notebook files. In order to convert a IPython notebook file to markdown after editing, please follow the following steps:\n\n\n\n\nModify or edit the existing notebook.\n\n\nExecute all cells in the current notebook and make sure that no errors occur.\n\n\nConvert the notebook to markdown using the \nipynb2markdown.py\n converter\n\n\n\n\n~/github/mlxtend/docs$ python ipynb2markdown.py --ipynb_path ./sources/user_guide/subpackage/notebookname.ipynb\n\n\n\n\nNote\n  \n\n\nIf you are adding a new document, please also include it in the pages section in the \nmlxtend/docs/mkdocs.yml\n file.\n\n\n3. Building static HTML files of the documentation\n\n\nFirst, please check the documenation via localhost (http://127.0.0.1:8000/):\n\n\n~/github/mlxtend/docs$ mkdocs serve\n\n\n\n\nNext, build the static HTML files of the mlxtend documentation via\n\n\n~/github/mlxtend/docs$ mkdocs build --clean\n\n\n\n\nTo deploy the documentation, execute\n\n\n~/github/mlxtend/docs$ mkdocs gh-deploy --clean\n\n\n\n\n4. Generate a PDF of the documentation\n\n\nTo generate a PDF version of the documentation, simply \ncd\n into the \nmlxtend/docs\n directory and execute:\n\n\npython md2pdf.py\n\n\n\n\nUploading a new version to PyPI\n\n\n1. Creating a new testing environment\n\n\nAssuming we are using \nconda\n, create a new python environment via\n\n\n$ conda create -n 'mlxtend-testing' python=3 numpy scipy pandas\n\n\n\n\nNext, activate the environment by executing\n\n\n$ source activate mlxtend-testing\n\n\n\n\n2. Installing the package from local files\n\n\nTest the installation by executing\n\n\n$ python setup.py install --record files.txt\n\n\n\n\nthe \n--record files.txt\n flag will create a \nfiles.txt\n file listing the locations where these files will be installed.\n\n\nTry to import the package to see if it works, for example, by executing\n\n\n$ python -c 'import mlxtend; print(mlxtend.__file__)'\n\n\n\n\nIf everything seems to be fine, remove the installation via\n\n\n$ cat files.txt | xargs rm -rf ; rm files.txt\n\n\n\n\nNext, test if \npip\n is able to install the packages. First, navigate to a different directory, and from there, install the package:\n\n\n$ pip install mlxtend/\n\n\n\n\nand uninstall it again\n\n\n$ pip uninstall mlxtend\n\n\n\n\n3. Deploying the package\n\n\nConsider deploying the package to the PyPI test server first. The setup instructions can be found \nhere\n.\n\n\n$ python setup.py sdist bdist_wheel upload -r https://testpypi.python.org/pypi\n\n\n\n\nTest if it can be installed from there by executing\n\n\n$ pip install -i https://testpypi.python.org/pypi mlxtend\n\n\n\n\nand uninstall it\n\n\n$ pip uninstall mlxtend\n\n\n\n\nAfter this dry-run succeeded, repeat this process using the \"real\" PyPI:\n\n\n$ python setup.py sdist bdist_wheel upload\n\n\n\n\n4. Removing the virtual environment\n\n\nFinally, to cleanup our local drive, remove the virtual testing environment via\n\n\n$ conda remove --name 'mlxtend-testing' --all\n\n\n\n\nCreating a Conda build\n\n\nGeneral instruction on how to create a Conda package can be found at \nhttp://conda.pydata.org/docs/build_tutorials/pkgs.html\n.\n\n\n1.\n Create a conda skeleton in your home directory:\n\n\nconda skeleton pypi mlxtend --version 0.4.2\n\n\n\n\nExecuting the command above will create a new directory called \nmlxtend\n. Please inspect the meta.yaml file and modify section if appropriate.\n\n\n2.\n Create builds for Python 2.7, 3.4, and 3.5 by executing the following commands:\n\n\nconda build --python 2.7 mlxtend\nconda build --python 3.5 mlxtend\nconda build --python 3.4 mlxtend\n\n\n\n\n3.\n Create a new conda environment and install the package locally to check if it works correctly:\n\n\nconda create -n testmlxtend python=3\nsource activate testmlxtend\nconda install --use-local mlxtend\nconda uninstall mlxtend\nsource deactivate\nconda remove --name 'testmlxtend' --all\n\n\n\n\n4.\n Convert the packages for other platforms (modify the paths in the following commands if necessary):\n\n\nconda convert --platform all /Users/Sebastian/miniconda3/conda-bld/osx-64/mlxtend-0.4.2-py27_0.tar.bz2 -o mlxtend_py27/\n\nconda convert --platform all /Users/Sebastian/miniconda3/conda-bld/osx-64/mlxtend-0.4.2-py34_0.tar.bz2 -o mlxtend_py34/\n\nconda convert --platform all /Users/Sebastian/miniconda3/conda-bld/osx-64/mlxtend-0.4.2-py35_0.tar.bz2 -o mlxtend_py35/\n\n\n\n\n5.\n Finally, upload the packages to Anaconda.org (modify the paths in the following commands if necessary):\n\n\nanaconda upload /Users/Sebastian/mlxtend_py35/win-64/mlxtend-0.4.2-py35_0.tar.bz2\nanaconda upload /Users/Sebastian/mlxtend_py35/win-32/mlxtend-0.4.2-py35_0.tar.bz2\nanaconda upload /Users/Sebastian/mlxtend_py35/osx-64/mlxtend-0.4.2-py35_0.tar.bz2\nanaconda upload /Users/Sebastian/mlxtend_py35/linux-64/mlxtend-0.4.2-py35_0.tar.bz2\nanaconda upload /Users/Sebastian/mlxtend_py35/linux-32/mlxtend-0.4.2-py35_0.tar.bz2\nanaconda upload /Users/Sebastian/mlxtend_py34/win-64/mlxtend-0.4.2-py34_0.tar.bz2\nanaconda upload /Users/Sebastian/mlxtend_py34/win-32/mlxtend-0.4.2-py34_0.tar.bz2\nanaconda upload /Users/Sebastian/mlxtend_py34/osx-64/mlxtend-0.4.2-py34_0.tar.bz2\nanaconda upload /Users/Sebastian/mlxtend_py34/linux-64/mlxtend-0.4.2-py34_0.tar.bz2\nanaconda upload /Users/Sebastian/mlxtend_py34/linux-32/mlxtend-0.4.2-py34_0.tar.bz2\nanaconda upload /Users/Sebastian/mlxtend_py27/win-64/mlxtend-0.4.2-py27_0.tar.bz2\nanaconda upload /Users/Sebastian/mlxtend_py27/win-32/mlxtend-0.4.2-py27_0.tar.bz2\nanaconda upload /Users/Sebastian/mlxtend_py27/osx-64/mlxtend-0.4.2-py27_0.tar.bz2\nanaconda upload /Users/Sebastian/mlxtend_py27/linux-64/mlxtend-0.4.2-py27_0.tar.bz2\nanaconda upload /Users/Sebastian/mlxtend_py27/linux-32/mlxtend-0.4.2-py27_0.tar.bz2\n\n\n\n\n6.\n Install the deployed packages to check if the upload was successful:\n\n\nconda install -c rasbt mlxtend\nconda uninstall mlxtend",
            "title": "How To Contribute"
        },
        {
            "location": "/contributing/#how-to-contribute",
            "text": "I would be very happy about any kind of contributions that help to improve and extend the functionality of mlxtend.",
            "title": "How to Contribute"
        },
        {
            "location": "/contributing/#quick-contributor-checklist",
            "text": "This is a quick checklist about the different steps of a typical contribution to mlxtend (and\nother open source projects). Consider copying this list to a local text file (or the issue tracker)\nand checking off items as you go.   [ ]  Open a new \"issue\" on GitHub to discuss the new feature / bug fix    [ ]  Fork the mlxtend repository from GitHub (if not already done earlier)  [ ]  Create and check out a new topic branch     [ ]  Implement a new feature or apply the bug-fix    [ ]  Add appropriate unit test functions    [ ]  Run  nosetests ./mlxtend -sv  and make sure that all unit tests pass    [ ]  Check/improve the test coverage by running  nosetests ./mlxtend --with-coverage  [ ]  Check for style issues by running  flake8 ./mlxtend  (you may want to run  nosetests  again after you made modifications to the code)  [ ]  Add a note about the modification/contribution to the  ./docs/sources/changelog.md  file    [ ]  Modify documentation in the appropriate location under  mlxtend/docs/sources/     [ ]  Push the topic branch to the server and create a pull request  [ ]  Check the Travis-CI build passed at  https://travis-ci.org/rasbt/mlxtend  [ ]  Check/improve the unit test coverage at  https://coveralls.io/github/rasbt/mlxtend  [ ]  Check/improve the code health at  https://landscape.io/github/rasbt/mlxtend  [ ]  Squash (many small) commits to a larger commit",
            "title": "Quick Contributor Checklist"
        },
        {
            "location": "/contributing/#tips-for-contributors",
            "text": "",
            "title": "Tips for Contributors"
        },
        {
            "location": "/contributing/#getting-started-creating-a-new-issue-and-forking-the-repository",
            "text": "If you don't have a  GitHub  account, yet, please create one to contribute to this project.  Please submit a ticket for your issue to discuss the fix or new feature before too much time and effort is spent for the implementation.     Fork the  mlxtend  repository from the GitHub web interface.     Clone the  mlxtend  repository to your local machine by executing\n  git clone https://github.com/<your_username>/mlxtend.git",
            "title": "Getting Started - Creating a New Issue and Forking the Repository"
        },
        {
            "location": "/contributing/#syncing-an-existing-fork",
            "text": "If you already forked mlxtend earlier, you can bring you \"Fork\" up to date\nwith the master branch as follows:",
            "title": "Syncing an Existing Fork"
        },
        {
            "location": "/contributing/#1-configuring-a-remote-that-points-to-the-upstream-repository-on-github",
            "text": "List the current configured remote repository of your fork by executing  $ git remote -v  If you see something like  origin  https://github.com/<your username>/mlxtend.git (fetch)\norigin  https://github.com/<your username>/mlxtend.git (push)  you need to specify a new remote  upstream  repository via  $ git remote add upstream https://github.com/rasbt/mlxtend.git  Now, verify the new upstream repository you've specified for your fork by executing  $ git remote -v  You should see following output if everything is configured correctly:  origin  https://github.com/<your username>/mlxtend.git (fetch)\norigin  https://github.com/<your username>/mlxtend.git (push)\nupstream    https://github.com/rasbt/mlxtend.git (fetch)\nupstream    https://github.com/rasbt/mlxtend.git (push)",
            "title": "1. Configuring a remote that points to the upstream repository on GitHub"
        },
        {
            "location": "/contributing/#2-syncing-your-fork",
            "text": "First, fetch the updates of the original project's master branch by executing:  $ git fetch upstream  You should see the following output  remote: Counting objects: xx, done.\nremote: Compressing objects: 100% (xx/xx), done.\nremote: Total xx (delta xx), reused xx (delta x)\nUnpacking objects: 100% (xx/xx), done.\nFrom https://github.com/rasbt/mlxtend\n * [new branch]      master     -> upstream/master  This means that the commits to the  rasbt/mlxtend  master branch are now\nstored in the local branch  upstream/master .  If you are not already on your local project's master branch, execute  $ git checkout master  Finally, merge the changes in upstream/master to your local master branch by\nexecuting  $ git merge upstream/master  which will give you an output that looks similar to  Updating xxx...xxx\nFast-forward\nSOME FILE1                    |    12 +++++++\nSOME FILE2                    |    10 +++++++\n2 files changed, 22 insertions(+),",
            "title": "2. Syncing your Fork"
        },
        {
            "location": "/contributing/#the-main-workflow-making-changes-in-a-new-topic-branch",
            "text": "Listed below are the 9 typical steps of a contribution.",
            "title": "*The Main Workflow - Making Changes in a New Topic Branch"
        },
        {
            "location": "/contributing/#1-discussing-the-feature-or-modification",
            "text": "Before you start coding, please discuss the new feature, bugfix, or other modification to the project\non the project's  issue tracker . Before you open a \"new issue,\" please\ndo a quick search to see if a similar issue has been submitted already.",
            "title": "1. Discussing the Feature or Modification"
        },
        {
            "location": "/contributing/#2-creating-a-new-feature-branch",
            "text": "Please avoid working directly on the master branch but create a new feature branch:  $ git branch <new_feature>  Switch to the new feature branch by executing  $ git checkout <new_feature>",
            "title": "2. Creating a new feature branch"
        },
        {
            "location": "/contributing/#3-developing-the-new-feature-bug-fix",
            "text": "Now it's time to modify existing code or to contribute new code to the project.",
            "title": "3. Developing the new feature / bug fix"
        },
        {
            "location": "/contributing/#4-testing-your-code",
            "text": "Add the respective unit tests and check if they pass:  $ nosetests -sv  Use the  --with-coverage  flag to ensure that all code is being covered in the unit tests:  $ nosetests --with-coverage",
            "title": "4. Testing your code"
        },
        {
            "location": "/contributing/#5-documenting-changes",
            "text": "Please add an entry to the  mlxtend/docs/sources/changelog.md  file.\nIf it is a new feature, it would also be nice if you could update the documentation in appropriate location in  mlxtend/sources .",
            "title": "5. Documenting changes"
        },
        {
            "location": "/contributing/#6-committing-changes",
            "text": "When you are ready to commit the changes, please provide a meaningful  commit  message:  $ git add <modifies_files> # or `git add .`\n$ git commit -m '<meaningful commit message>'",
            "title": "6. Committing changes"
        },
        {
            "location": "/contributing/#7-optional-squashing-commits",
            "text": "If you made multiple smaller commits, it would be nice if you could group them into a larger, summarizing commit. First, list your recent commit via  $ git log  which will list the commits from newest to oldest in the following format by default:  commit 046e3af8a9127df8eac879454f029937c8a31c41\nAuthor: rasbt <mail@sebastianraschka.com>\nDate:   Tue Nov 24 03:46:37 2015 -0500\n\n    fixed setup.py\n\ncommit c3c00f6ba0e8f48bbe1c9081b8ae3817e57ecc5c\nAuthor: rasbt <mail@sebastianraschka.com>\nDate:   Tue Nov 24 03:04:39 2015 -0500\n\n        documented feature x\n\ncommit d87934fe8726c46f0b166d6290a3bf38915d6e75\nAuthor: rasbt <mail@sebastianraschka.com>\nDate:   Tue Nov 24 02:44:45 2015 -0500\n\n        added support for feature x  Assuming that it would make sense to group these 3 commits into one, we can execute  $ git rebase -i HEAD~3  which will bring our default git editor with the following contents:  pick d87934f added support for feature x\npick c3c00f6 documented feature x\npick 046e3af fixed setup.py  Since  c3c00f6  and  046e3af  are related to the original commit of  feature x , let's keep the  d87934f  and squash the 2 following commits into this initial one by changes the lines to  pick d87934f added support for feature x\nsquash c3c00f6 documented feature x\nsquash 046e3af fixed setup.py  Now, save the changes in your editor. Now, quitting the editor will apply the  rebase  changes, and the editor will open a second time, prompting you to enter a new commit message. In this case, we could enter  support for feature x  to summarize the contributions.",
            "title": "7. Optional: squashing commits"
        },
        {
            "location": "/contributing/#8-uploading-changes",
            "text": "Push your changes to a topic branch to the git server by executing:  $ git push origin <feature_branch>",
            "title": "8. Uploading changes"
        },
        {
            "location": "/contributing/#9-submitting-a-pull-request",
            "text": "Go to your GitHub repository online, select the new feature branch, and submit a new pull request:",
            "title": "9. Submitting a pull request"
        },
        {
            "location": "/contributing/#notes-for-developers",
            "text": "",
            "title": "Notes for Developers"
        },
        {
            "location": "/contributing/#building-the-documentation",
            "text": "The documentation is built via  MkDocs ; to ensure that the documentation is rendered correctly, you can view the documentation locally by executing  mkdocs serve  from the  mlxtend/docs  directory.  For example,  ~/github/mlxtend/docs$ mkdocs serve",
            "title": "Building the documentation"
        },
        {
            "location": "/contributing/#1-building-the-api-documentation",
            "text": "To build the API documentation, navigate to  mlxtend/docs  and execute the  make_api.py  file from this directory via  ~/github/mlxtend/docs$ python make_api.py  This should place the API documentation into the correct directories into the two directories:   mlxtend/docs/sources/api_modules  mlxtend/docs/sources/api_subpackes",
            "title": "1. Building the API documentation"
        },
        {
            "location": "/contributing/#2-editing-the-user-guide",
            "text": "The documents containing code examples for the \"User Guide\" are generated from IPython Notebook files. In order to convert a IPython notebook file to markdown after editing, please follow the following steps:   Modify or edit the existing notebook.  Execute all cells in the current notebook and make sure that no errors occur.  Convert the notebook to markdown using the  ipynb2markdown.py  converter   ~/github/mlxtend/docs$ python ipynb2markdown.py --ipynb_path ./sources/user_guide/subpackage/notebookname.ipynb  Note     If you are adding a new document, please also include it in the pages section in the  mlxtend/docs/mkdocs.yml  file.",
            "title": "2. Editing the User Guide"
        },
        {
            "location": "/contributing/#3-building-static-html-files-of-the-documentation",
            "text": "First, please check the documenation via localhost (http://127.0.0.1:8000/):  ~/github/mlxtend/docs$ mkdocs serve  Next, build the static HTML files of the mlxtend documentation via  ~/github/mlxtend/docs$ mkdocs build --clean  To deploy the documentation, execute  ~/github/mlxtend/docs$ mkdocs gh-deploy --clean",
            "title": "3. Building static HTML files of the documentation"
        },
        {
            "location": "/contributing/#4-generate-a-pdf-of-the-documentation",
            "text": "To generate a PDF version of the documentation, simply  cd  into the  mlxtend/docs  directory and execute:  python md2pdf.py",
            "title": "4. Generate a PDF of the documentation"
        },
        {
            "location": "/contributing/#uploading-a-new-version-to-pypi",
            "text": "",
            "title": "Uploading a new version to PyPI"
        },
        {
            "location": "/contributing/#1-creating-a-new-testing-environment",
            "text": "Assuming we are using  conda , create a new python environment via  $ conda create -n 'mlxtend-testing' python=3 numpy scipy pandas  Next, activate the environment by executing  $ source activate mlxtend-testing",
            "title": "1. Creating a new testing environment"
        },
        {
            "location": "/contributing/#2-installing-the-package-from-local-files",
            "text": "Test the installation by executing  $ python setup.py install --record files.txt  the  --record files.txt  flag will create a  files.txt  file listing the locations where these files will be installed.  Try to import the package to see if it works, for example, by executing  $ python -c 'import mlxtend; print(mlxtend.__file__)'  If everything seems to be fine, remove the installation via  $ cat files.txt | xargs rm -rf ; rm files.txt  Next, test if  pip  is able to install the packages. First, navigate to a different directory, and from there, install the package:  $ pip install mlxtend/  and uninstall it again  $ pip uninstall mlxtend",
            "title": "2. Installing the package from local files"
        },
        {
            "location": "/contributing/#3-deploying-the-package",
            "text": "Consider deploying the package to the PyPI test server first. The setup instructions can be found  here .  $ python setup.py sdist bdist_wheel upload -r https://testpypi.python.org/pypi  Test if it can be installed from there by executing  $ pip install -i https://testpypi.python.org/pypi mlxtend  and uninstall it  $ pip uninstall mlxtend  After this dry-run succeeded, repeat this process using the \"real\" PyPI:  $ python setup.py sdist bdist_wheel upload",
            "title": "3. Deploying the package"
        },
        {
            "location": "/contributing/#4-removing-the-virtual-environment",
            "text": "Finally, to cleanup our local drive, remove the virtual testing environment via  $ conda remove --name 'mlxtend-testing' --all",
            "title": "4. Removing the virtual environment"
        },
        {
            "location": "/contributing/#creating-a-conda-build",
            "text": "General instruction on how to create a Conda package can be found at  http://conda.pydata.org/docs/build_tutorials/pkgs.html .  1.  Create a conda skeleton in your home directory:  conda skeleton pypi mlxtend --version 0.4.2  Executing the command above will create a new directory called  mlxtend . Please inspect the meta.yaml file and modify section if appropriate.  2.  Create builds for Python 2.7, 3.4, and 3.5 by executing the following commands:  conda build --python 2.7 mlxtend\nconda build --python 3.5 mlxtend\nconda build --python 3.4 mlxtend  3.  Create a new conda environment and install the package locally to check if it works correctly:  conda create -n testmlxtend python=3\nsource activate testmlxtend\nconda install --use-local mlxtend\nconda uninstall mlxtend\nsource deactivate\nconda remove --name 'testmlxtend' --all  4.  Convert the packages for other platforms (modify the paths in the following commands if necessary):  conda convert --platform all /Users/Sebastian/miniconda3/conda-bld/osx-64/mlxtend-0.4.2-py27_0.tar.bz2 -o mlxtend_py27/\n\nconda convert --platform all /Users/Sebastian/miniconda3/conda-bld/osx-64/mlxtend-0.4.2-py34_0.tar.bz2 -o mlxtend_py34/\n\nconda convert --platform all /Users/Sebastian/miniconda3/conda-bld/osx-64/mlxtend-0.4.2-py35_0.tar.bz2 -o mlxtend_py35/  5.  Finally, upload the packages to Anaconda.org (modify the paths in the following commands if necessary):  anaconda upload /Users/Sebastian/mlxtend_py35/win-64/mlxtend-0.4.2-py35_0.tar.bz2\nanaconda upload /Users/Sebastian/mlxtend_py35/win-32/mlxtend-0.4.2-py35_0.tar.bz2\nanaconda upload /Users/Sebastian/mlxtend_py35/osx-64/mlxtend-0.4.2-py35_0.tar.bz2\nanaconda upload /Users/Sebastian/mlxtend_py35/linux-64/mlxtend-0.4.2-py35_0.tar.bz2\nanaconda upload /Users/Sebastian/mlxtend_py35/linux-32/mlxtend-0.4.2-py35_0.tar.bz2\nanaconda upload /Users/Sebastian/mlxtend_py34/win-64/mlxtend-0.4.2-py34_0.tar.bz2\nanaconda upload /Users/Sebastian/mlxtend_py34/win-32/mlxtend-0.4.2-py34_0.tar.bz2\nanaconda upload /Users/Sebastian/mlxtend_py34/osx-64/mlxtend-0.4.2-py34_0.tar.bz2\nanaconda upload /Users/Sebastian/mlxtend_py34/linux-64/mlxtend-0.4.2-py34_0.tar.bz2\nanaconda upload /Users/Sebastian/mlxtend_py34/linux-32/mlxtend-0.4.2-py34_0.tar.bz2\nanaconda upload /Users/Sebastian/mlxtend_py27/win-64/mlxtend-0.4.2-py27_0.tar.bz2\nanaconda upload /Users/Sebastian/mlxtend_py27/win-32/mlxtend-0.4.2-py27_0.tar.bz2\nanaconda upload /Users/Sebastian/mlxtend_py27/osx-64/mlxtend-0.4.2-py27_0.tar.bz2\nanaconda upload /Users/Sebastian/mlxtend_py27/linux-64/mlxtend-0.4.2-py27_0.tar.bz2\nanaconda upload /Users/Sebastian/mlxtend_py27/linux-32/mlxtend-0.4.2-py27_0.tar.bz2  6.  Install the deployed packages to check if the upload was successful:  conda install -c rasbt mlxtend\nconda uninstall mlxtend",
            "title": "Creating a Conda build"
        },
        {
            "location": "/contributors/",
            "text": "Contributors\n\n\nFor the current list of contributors to mlxtend, please see the GitHub contributor page at \nhttps://github.com/rasbt/mlxtend/graphs/contributors\n.",
            "title": "Contributors"
        },
        {
            "location": "/contributors/#contributors",
            "text": "For the current list of contributors to mlxtend, please see the GitHub contributor page at  https://github.com/rasbt/mlxtend/graphs/contributors .",
            "title": "Contributors"
        },
        {
            "location": "/license/",
            "text": "This project is released under a permissive new BSD open source license and commercially usable. There is no warranty; not even for merchantability or fitness for a particular purpose.\n\n\n\n\n\n\nIn addition, you may use, copy, modify, and redistribute all artistic creative works (figures and images) included in this distribution under the directory \naccording to the terms and conditions of the Creative Commons Attribution 4.0 International License. (Computer-generated graphics such as the plots produced by matplotlib fall under the BSD license mentioned above).\n\n\n\n\n\n\nnew BSD License\n\n\n\n\nNew BSD License\n\n\nCopyright (c) 2014-2016, Sebastian Raschka. All rights reserved.\n\n\nRedistribution and use in source and binary forms, with or without\nmodification, are permitted provided that the following conditions are met:\n\n\n\n\n\n\nRedistributions of source code must retain the above copyright notice, this\n  list of conditions and the following disclaimer.\n\n\n\n\n\n\nRedistributions in binary form must reproduce the above copyright notice,\n  this list of conditions and the following disclaimer in the documentation\n  and/or other materials provided with the distribution.\n\n\n\n\n\n\nNeither the name of mlxtend nor the names of its\n  contributors may be used to endorse or promote products derived from\n  this software without specific prior written permission.\n\n\n\n\n\n\nTHIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS \"AS IS\"\nAND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE\nIMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE\nDISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE LIABLE\nFOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL\nDAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR\nSERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER\nCAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY,\nOR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE\nOF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n\n\nCreative Commons Attribution 4.0 International License\n\n\nmlxtend documentation figures are licensed under a\nCreative Commons Attribution 4.0 International License.\n\n\nhttp://creativecommons.org/licenses/by-sa/4.0/\n.\n\n\nYou are free to:\n\n\nShare \u2014 copy and redistribute the material in any medium or format\nAdapt \u2014 remix, transform, and build upon the material\nfor any purpose, even commercially.\nThe licensor cannot revoke these freedoms as long as you follow the license terms.\n\n\nUnder the following terms:\n\n\nAttribution \u2014 You must give appropriate credit, provide a link to the license, and indicate if changes were made. You may do so in any reasonable manner, but not in any way that suggests the licensor endorses you or your use.\n\n\nNo additional restrictions \u2014 You may not apply legal terms or technological measures that legally restrict others from doing anything the license permits.",
            "title": "License"
        },
        {
            "location": "/license/#new-bsd-license",
            "text": "New BSD License  Copyright (c) 2014-2016, Sebastian Raschka. All rights reserved.  Redistribution and use in source and binary forms, with or without\nmodification, are permitted provided that the following conditions are met:    Redistributions of source code must retain the above copyright notice, this\n  list of conditions and the following disclaimer.    Redistributions in binary form must reproduce the above copyright notice,\n  this list of conditions and the following disclaimer in the documentation\n  and/or other materials provided with the distribution.    Neither the name of mlxtend nor the names of its\n  contributors may be used to endorse or promote products derived from\n  this software without specific prior written permission.    THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS \"AS IS\"\nAND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE\nIMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE\nDISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE LIABLE\nFOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL\nDAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR\nSERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER\nCAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY,\nOR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE\nOF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.",
            "title": "new BSD License"
        },
        {
            "location": "/license/#creative-commons-attribution-40-international-license",
            "text": "mlxtend documentation figures are licensed under a\nCreative Commons Attribution 4.0 International License.  http://creativecommons.org/licenses/by-sa/4.0/ .",
            "title": "Creative Commons Attribution 4.0 International License"
        },
        {
            "location": "/license/#you-are-free-to",
            "text": "Share \u2014 copy and redistribute the material in any medium or format\nAdapt \u2014 remix, transform, and build upon the material\nfor any purpose, even commercially.\nThe licensor cannot revoke these freedoms as long as you follow the license terms.",
            "title": "You are free to:"
        },
        {
            "location": "/license/#under-the-following-terms",
            "text": "Attribution \u2014 You must give appropriate credit, provide a link to the license, and indicate if changes were made. You may do so in any reasonable manner, but not in any way that suggests the licensor endorses you or your use.  No additional restrictions \u2014 You may not apply legal terms or technological measures that legally restrict others from doing anything the license permits.",
            "title": "Under the following terms:"
        },
        {
            "location": "/cite/",
            "text": "Citing mlxtend\n\n\nIf you use mlxtend as part of your workflow in a scientific publication, please consider citing the mlxtend repository with the following DOI:\n\n\n\n\n@misc{raschkas_2016_49235,\n  author       = {Raschka, Sebastian},\n  title        = {Mlxtend},\n  month        = apr,\n  year         = 2016,\n  doi          = {10.5281/zenodo.49235},\n  url          = {http://dx.doi.org/10.5281/zenodo.49235}\n}",
            "title": "Citing Mlxtend"
        },
        {
            "location": "/cite/#citing-mlxtend",
            "text": "If you use mlxtend as part of your workflow in a scientific publication, please consider citing the mlxtend repository with the following DOI:   @misc{raschkas_2016_49235,\n  author       = {Raschka, Sebastian},\n  title        = {Mlxtend},\n  month        = apr,\n  year         = 2016,\n  doi          = {10.5281/zenodo.49235},\n  url          = {http://dx.doi.org/10.5281/zenodo.49235}\n}",
            "title": "Citing mlxtend"
        }
    ]
}
{
    "docs": [
        {
            "location": "/",
            "text": "A library consisting of useful tools and extensions for the day-to-day data science tasks.\n\n\n\n\n\n\n\n\nSebastian Raschka 2014-2015\n\n\nCurrent PyPI version: \n\nCurrent GitHub version: 0.2.9\n\n\n\n\n\n\n\nLinks\n\n\n\n\nDocumentation: \nhttp://rasbt.github.io/mlxtend/\n\n\nSource code repository: \nhttps://github.com/rasbt/mlxtend\n\n\nPyPI: \nhttps://pypi.python.org/pypi/mlxtend\n\n\nQuestions? Check out the \nGoogle Groups mailing list\n\n\n\n\n\n\n\n\n\nRecent changes\n\n\n\n\nSequential Feature Selection algorithms: \nSFS\n, \nSFFS\n, and \nSFBS\n\n\nNeural Network / Multilayer Perceptron classifier\n\n\nOrdinary least square regression\n using different solvers (gradient and stochastic gradient descent, and the closed form solution)\n\n\n\n\n\n\n\n\nQuick Install\n\n\n\n\nlatest version (from GitHub): \npip install git+git://github.com/rasbt/mlxtend.git#egg=mlxtend\n\n\nlatest PyPI version: \npip install mlxtend\n\n\n\n\n\n\n\n\nExamples\n\n\nfrom mlxtend.evaluate import plot_decision_regions\nimport matplotlib.pyplot as plt\nfrom sklearn import datasets\nfrom sklearn.svm import SVC\n\n# Loading some example data\niris = datasets.load_iris()\nX = iris.data[:, [0,2]]\ny = iris.target\n\n# Training a classifier\nsvm = SVC(C=0.5, kernel='linear')\nsvm.fit(X,y)\n\n# Plotting decision regions\nplot_decision_regions(X, y, clf=svm, res=0.02, legend=2)\n\n# Adding axes annotations\nplt.xlabel('sepal length [cm]')\nplt.ylabel('petal length [cm]')\nplt.title('SVM on Iris')\nplt.show()",
            "title": "Home"
        },
        {
            "location": "/#links",
            "text": "Documentation:  http://rasbt.github.io/mlxtend/  Source code repository:  https://github.com/rasbt/mlxtend  PyPI:  https://pypi.python.org/pypi/mlxtend  Questions? Check out the  Google Groups mailing list",
            "title": "Links"
        },
        {
            "location": "/#recent-changes",
            "text": "Sequential Feature Selection algorithms:  SFS ,  SFFS , and  SFBS  Neural Network / Multilayer Perceptron classifier  Ordinary least square regression  using different solvers (gradient and stochastic gradient descent, and the closed form solution)",
            "title": "Recent changes"
        },
        {
            "location": "/#quick-install",
            "text": "latest version (from GitHub):  pip install git+git://github.com/rasbt/mlxtend.git#egg=mlxtend  latest PyPI version:  pip install mlxtend",
            "title": "Quick Install"
        },
        {
            "location": "/#examples",
            "text": "from mlxtend.evaluate import plot_decision_regions\nimport matplotlib.pyplot as plt\nfrom sklearn import datasets\nfrom sklearn.svm import SVC\n\n# Loading some example data\niris = datasets.load_iris()\nX = iris.data[:, [0,2]]\ny = iris.target\n\n# Training a classifier\nsvm = SVC(C=0.5, kernel='linear')\nsvm.fit(X,y)\n\n# Plotting decision regions\nplot_decision_regions(X, y, clf=svm, res=0.02, legend=2)\n\n# Adding axes annotations\nplt.xlabel('sepal length [cm]')\nplt.ylabel('petal length [cm]')\nplt.title('SVM on Iris')\nplt.show()",
            "title": "Examples"
        },
        {
            "location": "/docs/classifier/neuralnet_mlp/",
            "text": "mlxtend\nSebastian Raschka, last updated: 06/24/2015\n\n\nNeural Network - Multilayer Perceptron\n\n\n\n\nfrom mlxtend.classifier import NeuralNetMLP\n\n\n\n\nImplementation of a feedforward artificial neural network (multilayer perceptron, MLP).\nAlthough the code is fully working and can be used for common classification tasks, this implementation is not geared towards efficiency but clarity -- the original code was written for demonstration purposes (a more detailed blog article and step by step walkthrough is going to follow).\n\n\n\n[Note: \nx\n0\n and \na\n0\n are the bias units ( \nx\n0\n=1, \na\n0\n=1); the activation is calculated as   \nsigmoid(z) = g(z) = 1 / (1+exp(-z))\n,   where the net input \nz\n of the first layer is defined as  \nz\n(2)\n = \nw\n(1)\na\n(1)\nT\n, and the net input of the second layer is defined as  \nz\n(3)\n = \nw\n(2)\na\n(2)\n, respectively; \nw\n(k)\n are the weight matrices of the corresponding layers; \na\n(1)\n is equal to the input features plus bias unit, \na\n(1)\n = [1,  \nx\n ]]\n\n\n\n\nA fact sheet of the current implementation:\n\n\n\n\nBinary and multi-class classification\n\n\n1 input layer, 1 hidden layer, 1 output layer\n\n\nlogistic (sigmoid) activation functions (see the activation function \ncheatsheet\n)\n\n\nlearning via batch gradient descent or mini-batch gradient descent using the \nbackpropagation\n algorithm\n\n\noptional L1 and/or L2 regularization (penalty)\n\n\nMomentum learning: \u0394\nw\nt\n= -\u03b7  \u2207\nw\nE(\nw\n) + \u03b1 \u0394\nw\nt-1\n (where \u03b1 and \u03b7  are the momentum constant and  the learning rate, respectively)\n\n\nAdaptive learning rate: \u03b7 / (1 + \nt\n \u00d7 \nd\n), where \nd\n is the decrease constant. \n\n\n\n\nFor more details, please see the \nsource code\n.\n\n\n\n\n\n\n\n\nExample 1 - Classify Iris\n\n\n[An IPython notebook to execute those examples can be found \nhere\n]\n\n\nLoad 2 features from Iris (petal length and petal width) for visualization purposes.\n\n\nfrom mlxtend.data import iris_data\nX, y = iris_data()\nX = X[:, 2:]\n\n\n\nTrain neural network for 3 output flower classes ('Setosa', 'Versicolor', 'Virginica'), regular gradient decent (minibatches=1), 30 hidden units, and no regularization.\n\n\n>>> from mlxtend.classifier import NeuralNetMLP\n>>> import numpy as np\n>>> nn1 = NeuralNetMLP(n_output=3, \n...     n_features=X.shape[1], \n...     n_hidden=30, \n...     l2=0.0, \n...     l1=0.0, \n...     epochs=5000, \n...     eta=0.001, \n...     alpha=0.1,\n...     minibatches=1, \n...     shuffle=True,\n...     random_state=1)\n>>> nn1.fit(X, y)\n>>> y_pred = nn1.predict(X)\n>>> acc = np.sum(y == y_pred, axis=0) / X.shape[0]\n>>> print('Accuracy: %.2f%%' % (acc * 100))\nAccuracy: 96.00%\n\n\n\nNow, check if the gradient descent converged after 5000 epochs, and choose smaller learning rate (\neta\n) otherwise.\n\n\n>>> import matplotlib.pyplot as plt\n>>> plt.plot(range(len(nn1.cost_)), nn1.cost_)\n>>> plt.ylim([0, 300])\n>>> plt.ylabel('Cost')\n>>> plt.xlabel('Epochs')\n>>> plt.show()\n\n\n\n\n\nNote:\n In practice, it is recommended to standardize the features for faster and smoother convergence.\n\n\n>>> X_std = np.copy(X)\n>>> for i in range(2):\n...     X_std[:,i] = (X[:,i] - X[:,i].mean()) / X[:,i].std()\n\n>>> nn2 = NeuralNetMLP(n_output=3, \n...     n_features=X_std.shape[1], \n...     n_hidden=30, \n...     l2=0.0, \n...     l1=0.0, \n...     epochs=1000, \n...     eta=0.05,\n...     alpha=0.1,\n...     minibatches=1, \n...     shuffle=True,\n...     random_state=1)\n\n\n\n\n\n>>> nn2.fit(X_std, y)\n>>> y_pred = nn2.predict(X_std)\n>>> acc = np.sum(y == y_pred, axis=0) / X_std.shape[0]\n>>> print('Accuracy: %.2f%%' % (acc * 100))\nAccuracy: 96.00%\n\n\n\n\n\nVisualize the decision regions:\n\n\n>>> from mlxtend.evaluate import plot_decision_regions\n>>> plot_decision_regions(X, y, clf=nn1)\n>>> plt.xlabel('petal length [cm]')\n>>> plt.ylabel('petal width [cm]')\n>>> plt.show()\n\n\n\n\n\n\n\n\n\n\n\nExample 2 - Classify handwritten digits from MNIST\n\n\n[An IPython notebook to execute those examples can be found \nhere\n]\n\n\nLoad a 5000-sample subset of the \nMNIST dataset\n (please see \nthis tutorial\n if you want to download and read in the complete MNIST dataset).\n\n\n>>> from mlxtend.data import mnist_data\n>>> X, y = mnist_data()\n\n\n\nVisualize a sample from the MNIST dataset to check if it was loaded correctly.\n\n\n>>> def plot_digit(X, y, idx):\n...     img = X[idx].reshape(28,28)\n...     plt.imshow(img, cmap='Greys',  interpolation='nearest')\n...     plt.title('true label: %d' % y[idx])\n...     plt.show()\n>>> plot_digit(X, y, 4)\n\n\n\n\n\nInitialize the neural network to recognize the 10 different digits (0-10) using 300 epochs and mini-batch learning.\n\n\n>>> nn = NeuralNetMLP(n_output=10, n_features=X.shape[1], \n...         n_hidden=100, \n...         l2=0.0, \n...         l1=0.0, \n...         epochs=300, \n...         eta=0.0005, \n...         minibatches=50, \n...         random_state=1)\n\n\n\nLearn the features while printing the progress to get an idea about how long it may take.\n\n\n>>> nn.fit(X, y, print_progress=True)\nEpoch: 300/300\n>>> y_pred = nn.predict(X)\n>>> print('Accuracy: %.2f%%' % (acc * 100))\nAccuracy: 95.48%\n\n\n\nCheck for convergence.\n\n\n>>> plt.plot(range(len(nn.cost_)), nn.cost_)\n>>> plt.ylim([0, 500])\n>>> plt.ylabel('Cost')\n>>> plt.xlabel('Mini-batches * Epochs')\n>>> plt.show()\n\n\n\n\n\n>>> plt.plot(range(len(nn.cost_)//50), nn.cost_[::50], color='red')\n>>> plt.ylim([0, 500])\n>>> plt.ylabel('Cost')\n>>> plt.xlabel('Epochs')\n>>> plt.show()\n\n\n\n\n\n\n\n\n\n\n\nDefault Parameters\n\n\nclass NeuralNetMLP(object):\n    \"\"\" Feedforward neural network / Multi-layer perceptron classifier.\n\n    Parameters\n    ------------\n    n_output : int\n      Number of output units, should be equal to the\n      number of unique class labels.\n\n    n_features : int\n      Number of features (dimensions) in the target dataset.\n      Should be equal to the number of columns in the X array.\n\n    n_hidden : int (default: 30)\n      Number of hidden units.\n\n    l1 : float (default: 0.0)\n      Lambda value for L1-regularization.\n      No regularization if l1=0.0 (default)\n\n    l2 : float (default: 0.0)\n      Lambda value for L2-regularization.\n      No regularization if l2=0.0 (default)\n\n    epochs : int (default: 500)\n      Number of passes over the training set.\n\n    eta : float (default: 0.01)\n      Learning rate.\n\n    alpha : float (default: 0.0)\n      Momentum constant. Factor multiplied with the\n      gradient of the previous epoch t-1 to improve\n      learning speed\n      w(t) := w(t) - (grad(t) + alpha*grad(t-1))\n\n    decrease_const : float (default: 0.0)\n      Decrease constant. Shrinks the learning rate\n      after each epoch via eta / (1 + epoch*decrease_const)\n\n    shuffle : bool (default: False)\n      Shuffles training data every epoch if True to prevent circles.\n\n    minibatches : int (default: 1)\n      Divides training data into k minibatches for efficiency.\n      Normal gradient descent learning if k=1 (default).\n\n    random_state : int (default: None)\n      Set random state for shuffling and initializing the weights.\n\n    Attributes\n    -----------\n    cost_ : list\n      Sum of squared errors after each epoch.\n\n    \"\"\"\n\n\n\n\n\n\nMethods\n\n\ndef fit(self, X, y, print_progress=False):\n    \"\"\" Learn weights from training data.\n\n    Parameters\n    -----------\n    X : array, shape = [n_samples, n_features]\n      Input layer with original features.\n\n    y : array, shape = [n_samples]\n      Target class labels.\n\n    print_progress : bool (default: False)\n      Prints progress as the number of epochs\n      to stderr.\n\n    Returns:\n    ----------\n    self\n\n    \"\"\"\n\n\n\ndef predict(self, X):\n    \"\"\"Predict class labels\n\n    Parameters\n    -----------\n    X : array, shape = [n_samples, n_features]\n      Input layer with original features.\n\n    Returns:\n    ----------\n    y_pred : array, shape = [n_samples]\n      Predicted class labels.\n\n    \"\"\"",
            "title": "Neuralnet mlp"
        },
        {
            "location": "/docs/classifier/neuralnet_mlp/#neural-network-multilayer-perceptron",
            "text": "from mlxtend.classifier import NeuralNetMLP   Implementation of a feedforward artificial neural network (multilayer perceptron, MLP).\nAlthough the code is fully working and can be used for common classification tasks, this implementation is not geared towards efficiency but clarity -- the original code was written for demonstration purposes (a more detailed blog article and step by step walkthrough is going to follow).  \n[Note:  x 0  and  a 0  are the bias units (  x 0 =1,  a 0 =1); the activation is calculated as    sigmoid(z) = g(z) = 1 / (1+exp(-z)) ,   where the net input  z  of the first layer is defined as   z (2)  =  w (1) a (1) T , and the net input of the second layer is defined as   z (3)  =  w (2) a (2) , respectively;  w (k)  are the weight matrices of the corresponding layers;  a (1)  is equal to the input features plus bias unit,  a (1)  = [1,   x  ]]   A fact sheet of the current implementation:   Binary and multi-class classification  1 input layer, 1 hidden layer, 1 output layer  logistic (sigmoid) activation functions (see the activation function  cheatsheet )  learning via batch gradient descent or mini-batch gradient descent using the  backpropagation  algorithm  optional L1 and/or L2 regularization (penalty)  Momentum learning: \u0394 w t = -\u03b7  \u2207 w E( w ) + \u03b1 \u0394 w t-1  (where \u03b1 and \u03b7  are the momentum constant and  the learning rate, respectively)  Adaptive learning rate: \u03b7 / (1 +  t  \u00d7  d ), where  d  is the decrease constant.    For more details, please see the  source code .",
            "title": "Neural Network - Multilayer Perceptron"
        },
        {
            "location": "/docs/classifier/neuralnet_mlp/#example-1-classify-iris",
            "text": "[An IPython notebook to execute those examples can be found  here ]  Load 2 features from Iris (petal length and petal width) for visualization purposes.  from mlxtend.data import iris_data\nX, y = iris_data()\nX = X[:, 2:]  Train neural network for 3 output flower classes ('Setosa', 'Versicolor', 'Virginica'), regular gradient decent (minibatches=1), 30 hidden units, and no regularization.  >>> from mlxtend.classifier import NeuralNetMLP\n>>> import numpy as np\n>>> nn1 = NeuralNetMLP(n_output=3, \n...     n_features=X.shape[1], \n...     n_hidden=30, \n...     l2=0.0, \n...     l1=0.0, \n...     epochs=5000, \n...     eta=0.001, \n...     alpha=0.1,\n...     minibatches=1, \n...     shuffle=True,\n...     random_state=1)\n>>> nn1.fit(X, y)\n>>> y_pred = nn1.predict(X)\n>>> acc = np.sum(y == y_pred, axis=0) / X.shape[0]\n>>> print('Accuracy: %.2f%%' % (acc * 100))\nAccuracy: 96.00%  Now, check if the gradient descent converged after 5000 epochs, and choose smaller learning rate ( eta ) otherwise.  >>> import matplotlib.pyplot as plt\n>>> plt.plot(range(len(nn1.cost_)), nn1.cost_)\n>>> plt.ylim([0, 300])\n>>> plt.ylabel('Cost')\n>>> plt.xlabel('Epochs')\n>>> plt.show()   Note:  In practice, it is recommended to standardize the features for faster and smoother convergence.  >>> X_std = np.copy(X)\n>>> for i in range(2):\n...     X_std[:,i] = (X[:,i] - X[:,i].mean()) / X[:,i].std()\n\n>>> nn2 = NeuralNetMLP(n_output=3, \n...     n_features=X_std.shape[1], \n...     n_hidden=30, \n...     l2=0.0, \n...     l1=0.0, \n...     epochs=1000, \n...     eta=0.05,\n...     alpha=0.1,\n...     minibatches=1, \n...     shuffle=True,\n...     random_state=1)   >>> nn2.fit(X_std, y)\n>>> y_pred = nn2.predict(X_std)\n>>> acc = np.sum(y == y_pred, axis=0) / X_std.shape[0]\n>>> print('Accuracy: %.2f%%' % (acc * 100))\nAccuracy: 96.00%   Visualize the decision regions:  >>> from mlxtend.evaluate import plot_decision_regions\n>>> plot_decision_regions(X, y, clf=nn1)\n>>> plt.xlabel('petal length [cm]')\n>>> plt.ylabel('petal width [cm]')\n>>> plt.show()",
            "title": "Example 1 - Classify Iris"
        },
        {
            "location": "/docs/classifier/neuralnet_mlp/#example-2-classify-handwritten-digits-from-mnist",
            "text": "[An IPython notebook to execute those examples can be found  here ]  Load a 5000-sample subset of the  MNIST dataset  (please see  this tutorial  if you want to download and read in the complete MNIST dataset).  >>> from mlxtend.data import mnist_data\n>>> X, y = mnist_data()  Visualize a sample from the MNIST dataset to check if it was loaded correctly.  >>> def plot_digit(X, y, idx):\n...     img = X[idx].reshape(28,28)\n...     plt.imshow(img, cmap='Greys',  interpolation='nearest')\n...     plt.title('true label: %d' % y[idx])\n...     plt.show()\n>>> plot_digit(X, y, 4)   Initialize the neural network to recognize the 10 different digits (0-10) using 300 epochs and mini-batch learning.  >>> nn = NeuralNetMLP(n_output=10, n_features=X.shape[1], \n...         n_hidden=100, \n...         l2=0.0, \n...         l1=0.0, \n...         epochs=300, \n...         eta=0.0005, \n...         minibatches=50, \n...         random_state=1)  Learn the features while printing the progress to get an idea about how long it may take.  >>> nn.fit(X, y, print_progress=True)\nEpoch: 300/300\n>>> y_pred = nn.predict(X)\n>>> print('Accuracy: %.2f%%' % (acc * 100))\nAccuracy: 95.48%  Check for convergence.  >>> plt.plot(range(len(nn.cost_)), nn.cost_)\n>>> plt.ylim([0, 500])\n>>> plt.ylabel('Cost')\n>>> plt.xlabel('Mini-batches * Epochs')\n>>> plt.show()   >>> plt.plot(range(len(nn.cost_)//50), nn.cost_[::50], color='red')\n>>> plt.ylim([0, 500])\n>>> plt.ylabel('Cost')\n>>> plt.xlabel('Epochs')\n>>> plt.show()",
            "title": "Example 2 - Classify handwritten digits from MNIST"
        },
        {
            "location": "/docs/classifier/neuralnet_mlp/#default-parameters",
            "text": "class NeuralNetMLP(object):\n    \"\"\" Feedforward neural network / Multi-layer perceptron classifier.\n\n    Parameters\n    ------------\n    n_output : int\n      Number of output units, should be equal to the\n      number of unique class labels.\n\n    n_features : int\n      Number of features (dimensions) in the target dataset.\n      Should be equal to the number of columns in the X array.\n\n    n_hidden : int (default: 30)\n      Number of hidden units.\n\n    l1 : float (default: 0.0)\n      Lambda value for L1-regularization.\n      No regularization if l1=0.0 (default)\n\n    l2 : float (default: 0.0)\n      Lambda value for L2-regularization.\n      No regularization if l2=0.0 (default)\n\n    epochs : int (default: 500)\n      Number of passes over the training set.\n\n    eta : float (default: 0.01)\n      Learning rate.\n\n    alpha : float (default: 0.0)\n      Momentum constant. Factor multiplied with the\n      gradient of the previous epoch t-1 to improve\n      learning speed\n      w(t) := w(t) - (grad(t) + alpha*grad(t-1))\n\n    decrease_const : float (default: 0.0)\n      Decrease constant. Shrinks the learning rate\n      after each epoch via eta / (1 + epoch*decrease_const)\n\n    shuffle : bool (default: False)\n      Shuffles training data every epoch if True to prevent circles.\n\n    minibatches : int (default: 1)\n      Divides training data into k minibatches for efficiency.\n      Normal gradient descent learning if k=1 (default).\n\n    random_state : int (default: None)\n      Set random state for shuffling and initializing the weights.\n\n    Attributes\n    -----------\n    cost_ : list\n      Sum of squared errors after each epoch.\n\n    \"\"\"",
            "title": "Default Parameters"
        },
        {
            "location": "/docs/classifier/neuralnet_mlp/#methods",
            "text": "def fit(self, X, y, print_progress=False):\n    \"\"\" Learn weights from training data.\n\n    Parameters\n    -----------\n    X : array, shape = [n_samples, n_features]\n      Input layer with original features.\n\n    y : array, shape = [n_samples]\n      Target class labels.\n\n    print_progress : bool (default: False)\n      Prints progress as the number of epochs\n      to stderr.\n\n    Returns:\n    ----------\n    self\n\n    \"\"\"  def predict(self, X):\n    \"\"\"Predict class labels\n\n    Parameters\n    -----------\n    X : array, shape = [n_samples, n_features]\n      Input layer with original features.\n\n    Returns:\n    ----------\n    y_pred : array, shape = [n_samples]\n      Predicted class labels.\n\n    \"\"\"",
            "title": "Methods"
        },
        {
            "location": "/docs/classifier/adaline/",
            "text": "mlxtend\nSebastian Raschka, last updated: 05/14/2015\n\n\nAdaline\n\n\n\n\nfrom mlxtend.classifier import Adaline\n\n\n\n\nImplementation of Adaline (Adaptive Linear Neuron; a single-layer artificial neural network) using the Widrow-Hoff delta rule. [2].\n\n\n[2] B. Widrow, M. E. Hoff, et al. Adaptive switching circuits. 1960.\n\n\n\n\nFor more usage examples please see the \nIPython Notebook\n.\n\n\nA detailed explanation about the Adaline learning algorithm can be found here \nArtificial Neurons and Single-Layer Neural Networks\n- How Machine Learning Algorithms Work Part 1\n.\n\n\n\n\n\nExample 1 - Stochastic Gradient Descent\n\n\nfrom mlxtend.data import iris_data\nfrom mlxtend.evaluate import plot_decision_regions\nfrom mlxtend.classifier import Adaline\nimport matplotlib.pyplot as plt\n\n# Loading Data\n\nX, y = iris_data()\nX = X[:, [0, 3]] # sepal length and petal width\nX = X[0:100] # class 0 and class 1\ny = y[0:100] # class 0 and class 1\n\n# standardize\nX[:,0] = (X[:,0] - X[:,0].mean()) / X[:,0].std()\nX[:,1] = (X[:,1] - X[:,1].mean()) / X[:,1].std()\n\n\nada = Adaline(epochs=30, eta=0.01, learning='sgd', random_seed=1)\nada.fit(X, y)\nplot_decision_regions(X, y, clf=ada)\nplt.title('Adaline - Stochastic Gradient Descent')\nplt.show()\n\nplt.plot(range(len(ada.cost_)), ada.cost_, marker='o')\nplt.xlabel('Iterations')\nplt.ylabel('Cost')\nplt.show()\n\n\n\n\n\n\n\n\n\n\nDefault Parameters\n\n\nclass Adaline(object):\n    \"\"\" ADAptive LInear NEuron classifier.\n\n    Parameters\n    ------------\n    eta : float\n      Learning rate (between 0.0 and 1.0)\n\n    epochs : int\n      Passes over the training dataset.\n\n    learning : str (default: sgd)\n      Gradient decent (gd) or stochastic gradient descent (sgd)\n\n    shuffle : bool (default: False)\n        Shuffles training data every epoch if True to prevent circles.\n\n    random_seed : int (default: None)\n        Set random state for shuffling and initializing the weights.\n\n    zero_init_weight : bool (default: False)\n        If True, weights are initialized to zero instead of small random\n        numbers in the interval [0,1]\n\n    Attributes\n    -----------\n    w_ : 1d-array\n      Weights after fitting.\n\n    cost_ : list\n      Sum of squared errors after each epoch.\n\n    \"\"\"\n\n\n\n\n\n\nMethods\n\n\n    def fit(self, X, y, init_weights=True):\n        \"\"\" Fit training data.\n\n        Parameters\n        ----------\n        X : {array-like, sparse matrix}, shape = [n_samples, n_features]\n            Training vectors, where n_samples is the number of samples and\n            n_features is the number of features.\n\n        y : array-like, shape = [n_samples]\n            Target values.\n\n        init_weights : bool (default: True)\n            (Re)initializes weights to small random floats if True.\n\n        Returns\n        -------\n        self : object\n        \"\"\"\n\n\n\n    def predict(self, X):\n        \"\"\"\n        Predict class labels for X.\n\n        Parameters\n        ----------\n        X : {array-like, sparse matrix}, shape = [n_samples, n_features]\n            Training vectors, where n_samples is the number of samples and\n            n_features is the number of features.\n\n        Returns\n        ----------\n        class : int\n          Predicted class label.\n\n        \"\"\"",
            "title": "Adaline"
        },
        {
            "location": "/docs/classifier/adaline/#adaline",
            "text": "from mlxtend.classifier import Adaline   Implementation of Adaline (Adaptive Linear Neuron; a single-layer artificial neural network) using the Widrow-Hoff delta rule. [2].  [2] B. Widrow, M. E. Hoff, et al. Adaptive switching circuits. 1960.   For more usage examples please see the  IPython Notebook .  A detailed explanation about the Adaline learning algorithm can be found here  Artificial Neurons and Single-Layer Neural Networks\n- How Machine Learning Algorithms Work Part 1 .",
            "title": "Adaline"
        },
        {
            "location": "/docs/classifier/adaline/#example-1-stochastic-gradient-descent",
            "text": "from mlxtend.data import iris_data\nfrom mlxtend.evaluate import plot_decision_regions\nfrom mlxtend.classifier import Adaline\nimport matplotlib.pyplot as plt\n\n# Loading Data\n\nX, y = iris_data()\nX = X[:, [0, 3]] # sepal length and petal width\nX = X[0:100] # class 0 and class 1\ny = y[0:100] # class 0 and class 1\n\n# standardize\nX[:,0] = (X[:,0] - X[:,0].mean()) / X[:,0].std()\nX[:,1] = (X[:,1] - X[:,1].mean()) / X[:,1].std()\n\n\nada = Adaline(epochs=30, eta=0.01, learning='sgd', random_seed=1)\nada.fit(X, y)\nplot_decision_regions(X, y, clf=ada)\nplt.title('Adaline - Stochastic Gradient Descent')\nplt.show()\n\nplt.plot(range(len(ada.cost_)), ada.cost_, marker='o')\nplt.xlabel('Iterations')\nplt.ylabel('Cost')\nplt.show()",
            "title": "Example 1 - Stochastic Gradient Descent"
        },
        {
            "location": "/docs/classifier/adaline/#default-parameters",
            "text": "class Adaline(object):\n    \"\"\" ADAptive LInear NEuron classifier.\n\n    Parameters\n    ------------\n    eta : float\n      Learning rate (between 0.0 and 1.0)\n\n    epochs : int\n      Passes over the training dataset.\n\n    learning : str (default: sgd)\n      Gradient decent (gd) or stochastic gradient descent (sgd)\n\n    shuffle : bool (default: False)\n        Shuffles training data every epoch if True to prevent circles.\n\n    random_seed : int (default: None)\n        Set random state for shuffling and initializing the weights.\n\n    zero_init_weight : bool (default: False)\n        If True, weights are initialized to zero instead of small random\n        numbers in the interval [0,1]\n\n    Attributes\n    -----------\n    w_ : 1d-array\n      Weights after fitting.\n\n    cost_ : list\n      Sum of squared errors after each epoch.\n\n    \"\"\"",
            "title": "Default Parameters"
        },
        {
            "location": "/docs/classifier/adaline/#methods",
            "text": "def fit(self, X, y, init_weights=True):\n        \"\"\" Fit training data.\n\n        Parameters\n        ----------\n        X : {array-like, sparse matrix}, shape = [n_samples, n_features]\n            Training vectors, where n_samples is the number of samples and\n            n_features is the number of features.\n\n        y : array-like, shape = [n_samples]\n            Target values.\n\n        init_weights : bool (default: True)\n            (Re)initializes weights to small random floats if True.\n\n        Returns\n        -------\n        self : object\n        \"\"\"      def predict(self, X):\n        \"\"\"\n        Predict class labels for X.\n\n        Parameters\n        ----------\n        X : {array-like, sparse matrix}, shape = [n_samples, n_features]\n            Training vectors, where n_samples is the number of samples and\n            n_features is the number of features.\n\n        Returns\n        ----------\n        class : int\n          Predicted class label.\n\n        \"\"\"",
            "title": "Methods"
        },
        {
            "location": "/docs/classifier/perceptron/",
            "text": "mlxtend\nSebastian Raschka, last updated: 05/14/2015\n\n\n\n\n\nPerceptron\n\n\n\n\nfrom mlxtend.classifier import Perceptron\n\n\n\n\nImplementation of a Perceptron (single-layer artificial neural network) using the Rosenblatt Perceptron Rule [1].\n\n\n[1] F. Rosenblatt. The perceptron, a perceiving and recognizing automaton Project Para. Cornell Aeronautical Laboratory, 1957.\n\n\n\n\nFor more usage examples please see the \nIPython Notebook\n.\n\n\nA detailed explanation about the perceptron learning algorithm can be found here \nArtificial Neurons and Single-Layer Neural Networks\n- How Machine Learning Algorithms Work Part 1\n.\n\n\n\n\n\nExample\n\n\nfrom mlxtend.data import iris_data\nfrom mlxtend.evaluate import plot_decision_regions\nfrom mlxtend.classifier import Perceptron\nimport matplotlib.pyplot as plt\n\n# Loading Data\n\nX, y = iris_data()\nX = X[:, [0, 3]] # sepal length and petal width\nX = X[0:100] # class 0 and class 1\ny = y[0:100] # class 0 and class 1\n\n# standardize\nX[:,0] = (X[:,0] - X[:,0].mean()) / X[:,0].std()\nX[:,1] = (X[:,1] - X[:,1].mean()) / X[:,1].std()\n\n\n# Rosenblatt Perceptron\n\nppn = Perceptron(epochs=15, eta=0.01, random_seed=1)\nppn.fit(X, y)\n\nplot_decision_regions(X, y, clf=ppn)\nplt.title('Perceptron - Rosenblatt Perceptron Rule')\nplt.show()\n\nprint(ppn.w_)\n\nplt.plot(range(len(ppn.cost_)), ppn.cost_)\nplt.xlabel('Iterations')\nplt.ylabel('Missclassifications')\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\nDefault Parameters\n\n\nclass Perceptron(object):\n    \"\"\"Perceptron classifier.\n\n    Parameters\n    ------------\n    eta : float\n      Learning rate (between 0.0 and 1.0)\n\n    epochs : int\n      Passes over the training dataset.\n\n    random_state : int\n      Random state for initializing random weights.\n\n    zero_init_weight : bool (default: False)\n        If True, weights are initialized to zero instead of small random\n        numbers in the interval [0,1]\n\n    Attributes\n    -----------\n    w_ : 1d-array\n      Weights after fitting.\n\n    cost_ : list\n      Number of misclassifications in every epoch.\n\n    \"\"\"\n\n\n\n\n\n\nMethods\n\n\n    def fit(self, X, y, init_weights=True):\n        \"\"\" Fit training data.\n\n        Parameters\n        ----------\n        X : {array-like, sparse matrix}, shape = [n_samples, n_features]\n            Training vectors, where n_samples is the number of samples and\n            n_features is the number of features.\n\n        y : array-like, shape = [n_samples]\n            Target values.\n\n        init_weights : bool (default: True)\n            (Re)initializes weights to small random floats if True.\n\n        shuffle : bool (default: False)\n            Shuffles training data every epoch if True to prevent circles.\n\n        random_seed : int (default: None)\n            Set random state for shuffling and initializing the weights.\n\n        Returns\n        -------\n        self : object\n\n        \"\"\"\n\n\n\n    def predict(self, X):\n        \"\"\" Predict class labels for X.\n\n        Parameters\n        ----------\n        X : {array-like, sparse matrix}, shape = [n_samples, n_features]\n            Training vectors, where n_samples is the number of samples and\n            n_features is the number of features.\n        Returns\n        ----------\n        class : int\n          Predicted class label.\n\n        \"\"\"",
            "title": "Perceptron"
        },
        {
            "location": "/docs/classifier/perceptron/#perceptron",
            "text": "from mlxtend.classifier import Perceptron   Implementation of a Perceptron (single-layer artificial neural network) using the Rosenblatt Perceptron Rule [1].  [1] F. Rosenblatt. The perceptron, a perceiving and recognizing automaton Project Para. Cornell Aeronautical Laboratory, 1957.   For more usage examples please see the  IPython Notebook .  A detailed explanation about the perceptron learning algorithm can be found here  Artificial Neurons and Single-Layer Neural Networks\n- How Machine Learning Algorithms Work Part 1 .",
            "title": "Perceptron"
        },
        {
            "location": "/docs/classifier/perceptron/#example",
            "text": "from mlxtend.data import iris_data\nfrom mlxtend.evaluate import plot_decision_regions\nfrom mlxtend.classifier import Perceptron\nimport matplotlib.pyplot as plt\n\n# Loading Data\n\nX, y = iris_data()\nX = X[:, [0, 3]] # sepal length and petal width\nX = X[0:100] # class 0 and class 1\ny = y[0:100] # class 0 and class 1\n\n# standardize\nX[:,0] = (X[:,0] - X[:,0].mean()) / X[:,0].std()\nX[:,1] = (X[:,1] - X[:,1].mean()) / X[:,1].std()\n\n\n# Rosenblatt Perceptron\n\nppn = Perceptron(epochs=15, eta=0.01, random_seed=1)\nppn.fit(X, y)\n\nplot_decision_regions(X, y, clf=ppn)\nplt.title('Perceptron - Rosenblatt Perceptron Rule')\nplt.show()\n\nprint(ppn.w_)\n\nplt.plot(range(len(ppn.cost_)), ppn.cost_)\nplt.xlabel('Iterations')\nplt.ylabel('Missclassifications')\nplt.show()",
            "title": "Example"
        },
        {
            "location": "/docs/classifier/perceptron/#default-parameters",
            "text": "class Perceptron(object):\n    \"\"\"Perceptron classifier.\n\n    Parameters\n    ------------\n    eta : float\n      Learning rate (between 0.0 and 1.0)\n\n    epochs : int\n      Passes over the training dataset.\n\n    random_state : int\n      Random state for initializing random weights.\n\n    zero_init_weight : bool (default: False)\n        If True, weights are initialized to zero instead of small random\n        numbers in the interval [0,1]\n\n    Attributes\n    -----------\n    w_ : 1d-array\n      Weights after fitting.\n\n    cost_ : list\n      Number of misclassifications in every epoch.\n\n    \"\"\"",
            "title": "Default Parameters"
        },
        {
            "location": "/docs/classifier/perceptron/#methods",
            "text": "def fit(self, X, y, init_weights=True):\n        \"\"\" Fit training data.\n\n        Parameters\n        ----------\n        X : {array-like, sparse matrix}, shape = [n_samples, n_features]\n            Training vectors, where n_samples is the number of samples and\n            n_features is the number of features.\n\n        y : array-like, shape = [n_samples]\n            Target values.\n\n        init_weights : bool (default: True)\n            (Re)initializes weights to small random floats if True.\n\n        shuffle : bool (default: False)\n            Shuffles training data every epoch if True to prevent circles.\n\n        random_seed : int (default: None)\n            Set random state for shuffling and initializing the weights.\n\n        Returns\n        -------\n        self : object\n\n        \"\"\"      def predict(self, X):\n        \"\"\" Predict class labels for X.\n\n        Parameters\n        ----------\n        X : {array-like, sparse matrix}, shape = [n_samples, n_features]\n            Training vectors, where n_samples is the number of samples and\n            n_features is the number of features.\n        Returns\n        ----------\n        class : int\n          Predicted class label.\n\n        \"\"\"",
            "title": "Methods"
        },
        {
            "location": "/docs/classifier/logistic_regression/",
            "text": "mlxtend\n\nSebastian Raschka, last updated: 07/14/2015\n\n\n\n\n\nLogistic Regression\n\n\n\n\nfrom mlxtend.classifier import LogisticRegression\n\n\n\n\nImplementation of Logistic Regression  with different learning rules: Gradient descent and stochastic gradient descent.\n\n\n\n\nFor more usage examples please see the \nIPython Notebook\n.\n\n\nA more detailed article about the algorithms is in preparation.\n\n\n\n\n\nExample 1 - Stochastic Gradient Descent\n\n\nfrom mlxtend.data import iris_data\nfrom mlxtend.evaluate import plot_decision_regions\nfrom mlxtend.classifier import LogisticRegression\nimport matplotlib.pyplot as plt\n\n# Loading Data\n\nX, y = iris_data()\nX = X[:, [0, 3]] # sepal length and petal width\nX = X[0:100] # class 0 and class 1\ny = y[0:100] # class 0 and class 1\n\n# standardize\nX[:,0] = (X[:,0] - X[:,0].mean()) / X[:,0].std()\nX[:,1] = (X[:,1] - X[:,1].mean()) / X[:,1].std()\n\n\n\nlr = LogisticRegression(eta=0.01, epochs=100, learning='sgd')\nlr.fit(X, y)\n\nplot_decision_regions(X, y, clf=lr)\nplt.title('Logistic Regression - Stochastic Gradient Descent')\nplt.show()\n\nprint(lr.w_)\n\nplt.plot(range(len(lr.cost_)), lr.cost_)\nplt.xlabel('Iterations')\nplt.ylabel('Missclassifications')\nplt.show()\n\n\n\n\n\n\n\n\n\n\nDefault Parameters\n\n\nclass LogisticRegression(object):\n    \"\"\"Logistic regression classifier.\n\n    Parameters\n    ------------\n    eta : float\n      Learning rate (between 0.0 and 1.0)\n\n    epochs : int\n      Passes over the training dataset.\n\n    learning : str (default: sgd)\n      Learning rule, sgd (stochastic gradient descent)\n      or gd (gradient descent).\n\n    regularization : {None, 'l2'} (default: None)\n      Type of regularization. No regularization if\n      `regularization=None`.\n\n    l2_lambda : float\n      Regularization parameter for L2 regularization.\n      No regularization if ls_lambda_=0.0.\n\n    shuffle : bool (default: False)\n        Shuffles training data every epoch if True to prevent circles.\n\n    random_seed : int (default: None)\n        Set random state for shuffling and initializing the weights.\n\n    zero_init_weight : bool (default: False)\n        If True, weights are initialized to zero instead of small random\n        numbers in the interval [0,1]\n\n    \"\"\"\n\n\n\n\n\n\nMethods\n\n\n    def fit(self, X, y, init_weights=True):\n        \"\"\" Fit training data.\n\n        Parameters\n        ----------\n        X : {array-like, sparse matrix}, shape = [n_samples, n_features]\n            Training vectors, where n_samples is the number of samples and\n            n_features is the number of features.\n\n        y : array-like, shape = [n_samples]\n            Target values.\n\n        init_weights : bool (default: True)\n            (Re)initializes weights to small random floats if True.\n\n        Returns\n        -------\n        self : object\n\n        \"\"\"\n\n\n\n    def predict(self, X):\n        \"\"\"\n        Predict class labels for X.\n\n        Parameters\n        ----------\n        X : {array-like, sparse matrix}, shape = [n_samples, n_features]\n            Training vectors, where n_samples is the number of samples and\n            n_features is the number of features.\n\n        Returns\n        ----------\n        class : int\n          Predicted class label.\n\n        \"\"\"\n\n\n\n    def activation(self, X):\n        \"\"\"\n        Predict class probabilities for X.\n\n        Parameters\n        ----------\n        X : {array-like, sparse matrix}, shape = [n_samples, n_features]\n            Training vectors, where n_samples is the number of samples and\n            n_features is the number of features.\n\n        Returns\n        ----------\n          Class 1 probability : float\n\n        \"\"\"",
            "title": "Logistic regression"
        },
        {
            "location": "/docs/classifier/logistic_regression/#logistic-regression",
            "text": "from mlxtend.classifier import LogisticRegression   Implementation of Logistic Regression  with different learning rules: Gradient descent and stochastic gradient descent.   For more usage examples please see the  IPython Notebook .  A more detailed article about the algorithms is in preparation.",
            "title": "Logistic Regression"
        },
        {
            "location": "/docs/classifier/logistic_regression/#example-1-stochastic-gradient-descent",
            "text": "from mlxtend.data import iris_data\nfrom mlxtend.evaluate import plot_decision_regions\nfrom mlxtend.classifier import LogisticRegression\nimport matplotlib.pyplot as plt\n\n# Loading Data\n\nX, y = iris_data()\nX = X[:, [0, 3]] # sepal length and petal width\nX = X[0:100] # class 0 and class 1\ny = y[0:100] # class 0 and class 1\n\n# standardize\nX[:,0] = (X[:,0] - X[:,0].mean()) / X[:,0].std()\nX[:,1] = (X[:,1] - X[:,1].mean()) / X[:,1].std()\n\n\n\nlr = LogisticRegression(eta=0.01, epochs=100, learning='sgd')\nlr.fit(X, y)\n\nplot_decision_regions(X, y, clf=lr)\nplt.title('Logistic Regression - Stochastic Gradient Descent')\nplt.show()\n\nprint(lr.w_)\n\nplt.plot(range(len(lr.cost_)), lr.cost_)\nplt.xlabel('Iterations')\nplt.ylabel('Missclassifications')\nplt.show()",
            "title": "Example 1 - Stochastic Gradient Descent"
        },
        {
            "location": "/docs/classifier/logistic_regression/#default-parameters",
            "text": "class LogisticRegression(object):\n    \"\"\"Logistic regression classifier.\n\n    Parameters\n    ------------\n    eta : float\n      Learning rate (between 0.0 and 1.0)\n\n    epochs : int\n      Passes over the training dataset.\n\n    learning : str (default: sgd)\n      Learning rule, sgd (stochastic gradient descent)\n      or gd (gradient descent).\n\n    regularization : {None, 'l2'} (default: None)\n      Type of regularization. No regularization if\n      `regularization=None`.\n\n    l2_lambda : float\n      Regularization parameter for L2 regularization.\n      No regularization if ls_lambda_=0.0.\n\n    shuffle : bool (default: False)\n        Shuffles training data every epoch if True to prevent circles.\n\n    random_seed : int (default: None)\n        Set random state for shuffling and initializing the weights.\n\n    zero_init_weight : bool (default: False)\n        If True, weights are initialized to zero instead of small random\n        numbers in the interval [0,1]\n\n    \"\"\"",
            "title": "Default Parameters"
        },
        {
            "location": "/docs/classifier/logistic_regression/#methods",
            "text": "def fit(self, X, y, init_weights=True):\n        \"\"\" Fit training data.\n\n        Parameters\n        ----------\n        X : {array-like, sparse matrix}, shape = [n_samples, n_features]\n            Training vectors, where n_samples is the number of samples and\n            n_features is the number of features.\n\n        y : array-like, shape = [n_samples]\n            Target values.\n\n        init_weights : bool (default: True)\n            (Re)initializes weights to small random floats if True.\n\n        Returns\n        -------\n        self : object\n\n        \"\"\"      def predict(self, X):\n        \"\"\"\n        Predict class labels for X.\n\n        Parameters\n        ----------\n        X : {array-like, sparse matrix}, shape = [n_samples, n_features]\n            Training vectors, where n_samples is the number of samples and\n            n_features is the number of features.\n\n        Returns\n        ----------\n        class : int\n          Predicted class label.\n\n        \"\"\"      def activation(self, X):\n        \"\"\"\n        Predict class probabilities for X.\n\n        Parameters\n        ----------\n        X : {array-like, sparse matrix}, shape = [n_samples, n_features]\n            Training vectors, where n_samples is the number of samples and\n            n_features is the number of features.\n\n        Returns\n        ----------\n          Class 1 probability : float\n\n        \"\"\"",
            "title": "Methods"
        },
        {
            "location": "/docs/classifier/scikit-learn_ensemble_classifier/",
            "text": "mlxtend\n\nSebastian Raschka, last updated: 05/14/2015\n\n\n\n\n\nMajority Rule Ensemble Classifier\n\n\n\n\nfrom mlxtend.classifier import EnsembleClassifier\n\n\n\n\nAnd ensemble classifier (for scikit-learn estimators) that predicts class labels based on a majority voting rule (hard voting) or average predicted probabilities (soft voting).\n\n\nDecision regions plotted for 4 different classifiers:   \n\n\n\n\nPlease see the \nIPython Notebook\n for a detailed explanation and examples.\n\n\nThe \nEnsembleClassifier\n will likely be included in the scikit-learn library as \nVotingClassifier\n at some point, and during this implementation process, the \nEnsembleClassifier\n has been slightly improved based on valuable feedback from the scikit-learn community.\n\n\n\n\n\nCross-validation Example\n\n\nInput:\n\n\nfrom mlxtend.classifier import EnsembleClassifier\nfrom sklearn import cross_validation\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.naive_bayes import GaussianNB \nfrom sklearn.ensemble import RandomForestClassifier\nimport numpy as np\nfrom sklearn import datasets\n\niris = datasets.load_iris()\nX, y = iris.data[:, 1:3], iris.target\n\nnp.random.seed(123)\n\n################################\n# Initialize classifiers\n################################\n\nclf1 = LogisticRegression()\nclf2 = RandomForestClassifier()\nclf3 = GaussianNB()\n\n################################\n# Initialize EnsembleClassifier\n################################\n\n# hard voting    \neclf = EnsembleClassifier(clfs=[clf1, clf2, clf3], voting='hard')\n\n# soft voting (uniform weights)\n# eclf = EnsembleClassifier(clfs=[clf1, clf2, clf3], voting='soft')\n\n# soft voting with different weights\n# eclf = EnsembleClassifier(clfs=[clf1, clf2, clf3], voting='soft', weights=[1,2,10])\n\n\n\n################################\n# 5-fold Cross-Validation\n################################\n\nfor clf, label in zip([clf1, clf2, clf3, eclf], ['Logistic Regression', 'Random Forest', 'naive Bayes', 'Ensemble']):\n\n    scores = cross_validation.cross_val_score(clf, X, y, cv=5, scoring='accuracy')\n    print(\"Accuracy: %0.2f (+/- %0.2f) [%s]\" % (scores.mean(), scores.std(), label))\n\n\n\nOutput:\n\n\nAccuracy: 0.90 (+/- 0.05) [Logistic Regression]\nAccuracy: 0.92 (+/- 0.05) [Random Forest]\nAccuracy: 0.91 (+/- 0.04) [naive Bayes]\nAccuracy: 0.95 (+/- 0.05) [Ensemble]\n\n\n\n\n\n\n\nGridSearch Example\n\n\nThe \nEnsembleClassifier\n van also be used in combination with scikit-learns gridsearch module:\n\n\nfrom sklearn.grid_search import GridSearchCV\n\nclf1 = LogisticRegression(random_state=1)\nclf2 = RandomForestClassifier(random_state=1)\nclf3 = GaussianNB()\neclf = EnsembleClassifier(clfs=[clf1, clf2, clf3], voting='soft')\n\nparams = {'logisticregression__C': [1.0, 100.0],\n      'randomforestclassifier__n_estimators': [20, 200],}\n\ngrid = GridSearchCV(estimator=eclf, param_grid=params, cv=5)\ngrid.fit(iris.data, iris.target)\n\nfor params, mean_score, scores in grid.grid_scores_:\n    print(\"%0.3f (+/-%0.03f) for %r\"\n        % (mean_score, scores.std() / 2, params))\n\n\n\nOutput:\n\n\n0.953 (+/-0.013) for {'randomforestclassifier__n_estimators': 20, 'logisticregression__C': 1.0}\n0.960 (+/-0.012) for {'randomforestclassifier__n_estimators': 200, 'logisticregression__C': 1.0}\n0.960 (+/-0.012) for {'randomforestclassifier__n_estimators': 20, 'logisticregression__C': 100.0}\n0.953 (+/-0.017) for {'randomforestclassifier__n_estimators': 200, 'logisticregression__C': 100.0}\n\n\n\nNote\n:\n\n\nIf the \nEnsembleClassifier\n is initialized with multiple similar estimator objects, the estimator names are modified with consecutive integer indices, for example:\n\n\nclf1 = LogisticRegression(random_state=1)\nclf2 = RandomForestClassifier(random_state=1)\neclf = EnsembleClassifier(clfs=[clf1, clf1, clf2], voting='soft')\n\nparams = {'logisticregression-1__C': [1.0, 100.0],\n          'logisticregression-2__C': [1.0, 100.0],\n          'randomforestclassifier__n_estimators': [20, 200],}\n\ngrid = GridSearchCV(estimator=eclf, param_grid=params, cv=5)\ngrid.fit(iris.data, iris.target)\n\n\n\n\n\n\nDefault Parameters\n\n\nclass EnsembleClassifier(BaseEstimator, ClassifierMixin, TransformerMixin):\n    \"\"\" Soft Voting/Majority Rule classifier for unfitted clfs.\n\n\nParameters\n----------\nclfs : array-like, shape = [n_classifiers]\n  A list of classifiers.\n  Invoking the `fit` method on the `VotingClassifier` will fit clones\n  of those original classifiers that will be stored in the class attribute\n  `self.clfs_`.\n\nvoting : str, {'hard', 'soft'} (default='hard')\n  If 'hard', uses predicted class labels for majority rule voting.\n  Else if 'soft', predicts the class label based on the argmax of\n  the sums of the predicted probalities, which is recommended for\n  an ensemble of well-calibrated classifiers.\n\nweights : array-like, shape = [n_classifiers], optional (default=`None`)\n  Sequence of weights (`float` or `int`) to weight the occurances of\n  predicted class labels (`hard` voting) or class probabilities\n  before averaging (`soft` voting). Uses uniform weights if `None`.\n\nverbose : int, optional (default=0)\n  Controls the verbosity of the building process.\n    `verbose=0` (default): Prints nothing\n    `verbose=1`: Prints the number & name of the clf being fitted\n    `verbose=2`: Prints info about the parameters of the clf being fitted\n    `verbose>2`: Changes `verbose` param of the underlying clf to self.verbose - 2\n\nAttributes\n----------\nclasses_ : array-like, shape = [n_predictions]\n\nclf : array-like, shape = [n_predictions]\n  The unmodified input classifiers\n\nclf_ : array-like, shape = [n_predictions]\n  Fitted clones of the input classifiers\n\nExamples\n--------\n>>> import numpy as np\n>>> from sklearn.linear_model import LogisticRegression\n>>> from sklearn.naive_bayes import GaussianNB\n>>> from sklearn.ensemble import RandomForestClassifier\n>>> clf1 = LogisticRegression(random_state=1)\n>>> clf2 = RandomForestClassifier(random_state=1)\n>>> clf3 = GaussianNB()\n>>> X = np.array([[-1, -1], [-2, -1], [-3, -2], [1, 1], [2, 1], [3, 2]])\n>>> y = np.array([1, 1, 1, 2, 2, 2])\n>>> eclf1 = VotingClassifier(clfs=[clf1, clf2, clf3], voting='hard')\n>>> eclf1 = eclf1.fit(X, y)\n>>> print(eclf1.predict(X))\n[1 1 1 2 2 2]\n>>> eclf2 = VotingClassifier(clfs=[clf1, clf2, clf3], voting='soft')\n>>> eclf2 = eclf2.fit(X, y)\n>>> print(eclf2.predict(X))\n[1 1 1 2 2 2]\n>>> eclf3 = VotingClassifier(clfs=[clf1, clf2, clf3],\n...                          voting='soft', weights=[2,1,1])\n>>> eclf3 = eclf3.fit(X, y)\n>>> print(eclf3.predict(X))\n[1 1 1 2 2 2]\n>>>\n\"\"\"</pre>\n\n\n\n\n\n\nMethods\n\n\n    def fit(self, X, y):\n        \"\"\" Fit the clfs.\n\n        Parameters\n        ----------\n        X : {array-like, sparse matrix}, shape = [n_samples, n_features]\n            Training vectors, where n_samples is the number of samples and\n            n_features is the number of features.\n\n        y : array-like, shape = [n_samples]\n            Target values.\n\n        Returns\n        -------\n        self : object\n        \"\"\"\n\n\n\n    def predict(self, X):\n        \"\"\" Predict class labels for X.\n\n        Parameters\n        ----------\n        X : {array-like, sparse matrix}, shape = [n_samples, n_features]\n            Training vectors, where n_samples is the number of samples and\n            n_features is the number of features.\n\n        Returns\n        ----------\n        maj : array-like, shape = [n_samples]\n            Predicted class labels.\n        \"\"\"\n\n\n\npredict_proba(self, X):\n        \"\"\" Predict class probabilities for X.\n\n        Parameters\n        ----------\n        X : {array-like, sparse matrix}, shape = [n_samples, n_features]\n            Training vectors, where n_samples is the number of samples and\n            n_features is the number of features.\n\n        Returns\n        ----------\n        avg : array-like, shape = [n_samples, n_classes]\n            Weighted average probability for each class per sample.\n        \"\"\"\n\n\n\n    def transform(self, X):\n        \"\"\" Return class labels or probabilities for X for each estimator.\n\n        Parameters\n        ----------\n        X : {array-like, sparse matrix}, shape = [n_samples, n_features]\n            Training vectors, where n_samples is the number of samples and\n            n_features is the number of features.\n\n        Returns\n        -------\n        If `voting='soft'`:\n          array-like = [n_classifiers, n_samples, n_classes]\n            Class probabilties calculated by each classifier.\n        If `voting='hard'`:\n          array-like = [n_classifiers, n_samples]\n            Class labels predicted by each classifier.\n        \"\"\"",
            "title": "Scikit learn ensemble classifier"
        },
        {
            "location": "/docs/classifier/scikit-learn_ensemble_classifier/#majority-rule-ensemble-classifier",
            "text": "from mlxtend.classifier import EnsembleClassifier   And ensemble classifier (for scikit-learn estimators) that predicts class labels based on a majority voting rule (hard voting) or average predicted probabilities (soft voting).  Decision regions plotted for 4 different classifiers:      Please see the  IPython Notebook  for a detailed explanation and examples.  The  EnsembleClassifier  will likely be included in the scikit-learn library as  VotingClassifier  at some point, and during this implementation process, the  EnsembleClassifier  has been slightly improved based on valuable feedback from the scikit-learn community.",
            "title": "Majority Rule Ensemble Classifier"
        },
        {
            "location": "/docs/classifier/scikit-learn_ensemble_classifier/#cross-validation-example",
            "text": "Input:  from mlxtend.classifier import EnsembleClassifier\nfrom sklearn import cross_validation\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.naive_bayes import GaussianNB \nfrom sklearn.ensemble import RandomForestClassifier\nimport numpy as np\nfrom sklearn import datasets\n\niris = datasets.load_iris()\nX, y = iris.data[:, 1:3], iris.target\n\nnp.random.seed(123)\n\n################################\n# Initialize classifiers\n################################\n\nclf1 = LogisticRegression()\nclf2 = RandomForestClassifier()\nclf3 = GaussianNB()\n\n################################\n# Initialize EnsembleClassifier\n################################\n\n# hard voting    \neclf = EnsembleClassifier(clfs=[clf1, clf2, clf3], voting='hard')\n\n# soft voting (uniform weights)\n# eclf = EnsembleClassifier(clfs=[clf1, clf2, clf3], voting='soft')\n\n# soft voting with different weights\n# eclf = EnsembleClassifier(clfs=[clf1, clf2, clf3], voting='soft', weights=[1,2,10])\n\n\n\n################################\n# 5-fold Cross-Validation\n################################\n\nfor clf, label in zip([clf1, clf2, clf3, eclf], ['Logistic Regression', 'Random Forest', 'naive Bayes', 'Ensemble']):\n\n    scores = cross_validation.cross_val_score(clf, X, y, cv=5, scoring='accuracy')\n    print(\"Accuracy: %0.2f (+/- %0.2f) [%s]\" % (scores.mean(), scores.std(), label))  Output:  Accuracy: 0.90 (+/- 0.05) [Logistic Regression]\nAccuracy: 0.92 (+/- 0.05) [Random Forest]\nAccuracy: 0.91 (+/- 0.04) [naive Bayes]\nAccuracy: 0.95 (+/- 0.05) [Ensemble]",
            "title": "Cross-validation Example"
        },
        {
            "location": "/docs/classifier/scikit-learn_ensemble_classifier/#gridsearch-example",
            "text": "The  EnsembleClassifier  van also be used in combination with scikit-learns gridsearch module:  from sklearn.grid_search import GridSearchCV\n\nclf1 = LogisticRegression(random_state=1)\nclf2 = RandomForestClassifier(random_state=1)\nclf3 = GaussianNB()\neclf = EnsembleClassifier(clfs=[clf1, clf2, clf3], voting='soft')\n\nparams = {'logisticregression__C': [1.0, 100.0],\n      'randomforestclassifier__n_estimators': [20, 200],}\n\ngrid = GridSearchCV(estimator=eclf, param_grid=params, cv=5)\ngrid.fit(iris.data, iris.target)\n\nfor params, mean_score, scores in grid.grid_scores_:\n    print(\"%0.3f (+/-%0.03f) for %r\"\n        % (mean_score, scores.std() / 2, params))  Output:  0.953 (+/-0.013) for {'randomforestclassifier__n_estimators': 20, 'logisticregression__C': 1.0}\n0.960 (+/-0.012) for {'randomforestclassifier__n_estimators': 200, 'logisticregression__C': 1.0}\n0.960 (+/-0.012) for {'randomforestclassifier__n_estimators': 20, 'logisticregression__C': 100.0}\n0.953 (+/-0.017) for {'randomforestclassifier__n_estimators': 200, 'logisticregression__C': 100.0}  Note :  If the  EnsembleClassifier  is initialized with multiple similar estimator objects, the estimator names are modified with consecutive integer indices, for example:  clf1 = LogisticRegression(random_state=1)\nclf2 = RandomForestClassifier(random_state=1)\neclf = EnsembleClassifier(clfs=[clf1, clf1, clf2], voting='soft')\n\nparams = {'logisticregression-1__C': [1.0, 100.0],\n          'logisticregression-2__C': [1.0, 100.0],\n          'randomforestclassifier__n_estimators': [20, 200],}\n\ngrid = GridSearchCV(estimator=eclf, param_grid=params, cv=5)\ngrid.fit(iris.data, iris.target)",
            "title": "GridSearch Example"
        },
        {
            "location": "/docs/classifier/scikit-learn_ensemble_classifier/#default-parameters",
            "text": "class EnsembleClassifier(BaseEstimator, ClassifierMixin, TransformerMixin):\n    \"\"\" Soft Voting/Majority Rule classifier for unfitted clfs.  Parameters\n----------\nclfs : array-like, shape = [n_classifiers]\n  A list of classifiers.\n  Invoking the `fit` method on the `VotingClassifier` will fit clones\n  of those original classifiers that will be stored in the class attribute\n  `self.clfs_`.\n\nvoting : str, {'hard', 'soft'} (default='hard')\n  If 'hard', uses predicted class labels for majority rule voting.\n  Else if 'soft', predicts the class label based on the argmax of\n  the sums of the predicted probalities, which is recommended for\n  an ensemble of well-calibrated classifiers.\n\nweights : array-like, shape = [n_classifiers], optional (default=`None`)\n  Sequence of weights (`float` or `int`) to weight the occurances of\n  predicted class labels (`hard` voting) or class probabilities\n  before averaging (`soft` voting). Uses uniform weights if `None`.\n\nverbose : int, optional (default=0)\n  Controls the verbosity of the building process.\n    `verbose=0` (default): Prints nothing\n    `verbose=1`: Prints the number & name of the clf being fitted\n    `verbose=2`: Prints info about the parameters of the clf being fitted\n    `verbose>2`: Changes `verbose` param of the underlying clf to self.verbose - 2\n\nAttributes\n----------\nclasses_ : array-like, shape = [n_predictions]\n\nclf : array-like, shape = [n_predictions]\n  The unmodified input classifiers\n\nclf_ : array-like, shape = [n_predictions]\n  Fitted clones of the input classifiers\n\nExamples\n--------\n>>> import numpy as np\n>>> from sklearn.linear_model import LogisticRegression\n>>> from sklearn.naive_bayes import GaussianNB\n>>> from sklearn.ensemble import RandomForestClassifier\n>>> clf1 = LogisticRegression(random_state=1)\n>>> clf2 = RandomForestClassifier(random_state=1)\n>>> clf3 = GaussianNB()\n>>> X = np.array([[-1, -1], [-2, -1], [-3, -2], [1, 1], [2, 1], [3, 2]])\n>>> y = np.array([1, 1, 1, 2, 2, 2])\n>>> eclf1 = VotingClassifier(clfs=[clf1, clf2, clf3], voting='hard')\n>>> eclf1 = eclf1.fit(X, y)\n>>> print(eclf1.predict(X))\n[1 1 1 2 2 2]\n>>> eclf2 = VotingClassifier(clfs=[clf1, clf2, clf3], voting='soft')\n>>> eclf2 = eclf2.fit(X, y)\n>>> print(eclf2.predict(X))\n[1 1 1 2 2 2]\n>>> eclf3 = VotingClassifier(clfs=[clf1, clf2, clf3],\n...                          voting='soft', weights=[2,1,1])\n>>> eclf3 = eclf3.fit(X, y)\n>>> print(eclf3.predict(X))\n[1 1 1 2 2 2]\n>>>\n\"\"\"</pre>",
            "title": "Default Parameters"
        },
        {
            "location": "/docs/classifier/scikit-learn_ensemble_classifier/#methods",
            "text": "def fit(self, X, y):\n        \"\"\" Fit the clfs.\n\n        Parameters\n        ----------\n        X : {array-like, sparse matrix}, shape = [n_samples, n_features]\n            Training vectors, where n_samples is the number of samples and\n            n_features is the number of features.\n\n        y : array-like, shape = [n_samples]\n            Target values.\n\n        Returns\n        -------\n        self : object\n        \"\"\"      def predict(self, X):\n        \"\"\" Predict class labels for X.\n\n        Parameters\n        ----------\n        X : {array-like, sparse matrix}, shape = [n_samples, n_features]\n            Training vectors, where n_samples is the number of samples and\n            n_features is the number of features.\n\n        Returns\n        ----------\n        maj : array-like, shape = [n_samples]\n            Predicted class labels.\n        \"\"\"  predict_proba(self, X):\n        \"\"\" Predict class probabilities for X.\n\n        Parameters\n        ----------\n        X : {array-like, sparse matrix}, shape = [n_samples, n_features]\n            Training vectors, where n_samples is the number of samples and\n            n_features is the number of features.\n\n        Returns\n        ----------\n        avg : array-like, shape = [n_samples, n_classes]\n            Weighted average probability for each class per sample.\n        \"\"\"      def transform(self, X):\n        \"\"\" Return class labels or probabilities for X for each estimator.\n\n        Parameters\n        ----------\n        X : {array-like, sparse matrix}, shape = [n_samples, n_features]\n            Training vectors, where n_samples is the number of samples and\n            n_features is the number of features.\n\n        Returns\n        -------\n        If `voting='soft'`:\n          array-like = [n_classifiers, n_samples, n_classes]\n            Class probabilties calculated by each classifier.\n        If `voting='hard'`:\n          array-like = [n_classifiers, n_samples]\n            Class labels predicted by each classifier.\n        \"\"\"",
            "title": "Methods"
        },
        {
            "location": "/docs/feature_selection/sequential_backward_selection/",
            "text": "mlxtend\n\nSebastian Raschka, last updated: 10/11/2015\n\n\n\n\n\nSequential Backward Selection\n\n\n\n\nfrom mlxtend.feature_selection import SBS\n\n\n\n\nSequential Backward Selection (SBS) is  a classic feature selection algorithm -- a greedy search algorithm -- that has been developed as a suboptimal solution to the computationally often not feasible exhaustive search. In a nutshell, SBS removes one feature at the time based on the classifier performance until a feature subset of the desired size \nk\n is reached.\n\n\nRelated topics:\n\n\n\n\nSequential Forward Selection\n\n\nSequential Floating Forward Selection\n\n\nSequential Floating Backward Selection\n\n\n\n\nThe SBS Algorithm\n\n\n\n\nInput:\n the set of all features, $Y = {y_1, y_2, ..., y_d}$  \n\n\n\n\nThe SBS algorithm takes the whole feature set as input.\n\n\n\n\nOutput:\n $X_k = {x_j \\; | \\;j = 1, 2, ..., k; \\; x_j \\in Y}$, where $k = (0, 1, 2, ..., d)$\n\n\n\n\nSBS returns a subset of features; the number of selected features $k$, where $k < d$, has to be specified \na priori\n.\n\n\n\n\nInitialization:\n $X_0 = Y$, $k = d$\n\n\n\n\nWe initialize the algorithm with the given feature set so that the $k = d$.\n\n\n\n\nStep 1 (Exclusion):\n  \n\n\n$x^- = \\text{ arg max } J(x_k - x), \\text{  where } x \\in X_k$\n\n$X_k-1 = X_k - x^-$\n\n$k = k - 1$\n\n\nGo to Step 1\n  \n\n\n\n\nIn this step, we remove a feature, $x^-$ from our feature subset $X_k$.\n\n\n$x^-$ is the feature that maximizes our criterion function upon re,oval, that is, the feature that is associated with the best classifier performance if it is removed from $X_k$.\n\n\nWe repeat this procedure until the termination criterion is satisfied.\n\n\n\n\nTermination:\n $k = p$\n\n\n\n\nWe add features from the feature subset $X_k$ until the feature subset of size $k$ contains the number of desired features $p$ that we specified \na priori\n.\n\n\n\n\n\n\nExample\n\n\nInput:\n\n\nfrom mlxtend.feature_selection import SBS\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.datasets import load_iris\n\niris = load_iris()\nX = iris.data\ny = iris.target\n\nknn = KNeighborsClassifier(n_neighbors=4)\n\nsbs = SBS(knn, k_features=2, scoring='accuracy', cv=5)\nsbs.fit(X, y)\n\nprint('Indices of selected features:', sbs.indices_)\nprint('CV score of selected subset:', sbs.k_score_)\nprint('New feature subset:')\nsbs.transform(X)[0:5]\n\n\n\n\nOutput:\n\n\nIndices of selected features: (0, 3)\nCV score of selected subset: 0.96\nNew feature subset:\narray([[ 5.1,  0.2],\n   [ 4.9,  0.2],\n   [ 4.7,  0.2],\n   [ 4.6,  0.2],\n   [ 5. ,  0.2]])\n ```\n\n<br>\n<br>\n\nAs demonstrated below, the SBS algorithm can be a useful alternative to dimensionality reduction techniques to reduce overfitting and when the original features need to be preserved:\n\n\n```python\nimport matplotlib.pyplot as plt\nfrom mlxtend.data import wine_data\nfrom sklearn.preprocessing import StandardScaler\n\nscr = StandardScaler()\nX_std = scr.fit_transform(X)\n\nknn = KNeighborsClassifier(n_neighbors=4)\n\n# selecting features\nsbs = SBS(knn, k_features=1, scoring='accuracy', cv=5)\nsbs.fit(X_std, y)\n\n# plotting performance of feature subsets\nk_feat = [len(k) for k in sbs.subsets_]\n\nplt.plot(k_feat, sbs.scores_, marker='o')\nplt.ylabel('Accuracy')\nplt.xlabel('Number of features')\nplt.show()\n\n\n\n\n\n\nGridsearch Example 1\n\n\nSelecting the number of features in a pipeline.\n\n\nimport pandas as pd\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.grid_search import GridSearchCV\nfrom mlxtend.sklearn import SBS\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.datasets import load_iris\n\n##########################\n### Loading data\n##########################\n\niris = load_iris()\nX = iris.data\ny = iris.target\n\n##########################\n### Setting up pipeline\n##########################\nknn = KNeighborsClassifier(n_neighbors=4)\n\nsbs = SBS(estimator=knn, k_features=2, scoring='accuracy', cv=5)\n\npipeline = Pipeline([\n            ('scr', StandardScaler()),\n            ('sel', sbs),\n            ('clf', knn)])\n\nparameters = {'sel__k_features': [1,2,3,4]}\n\ngrid_search = GridSearchCV(pipeline, parameters, n_jobs=1, verbose=1)\n\n##########################\n### Running GridSearch\n##########################\ngrid_search.fit(X, y)\n\nprint(\"Best score: %0.3f\" % grid_search.best_score_)\nprint(\"Best parameters set:\")\nbest_parameters = grid_search.best_estimator_.get_params()\nfor param_name in sorted(parameters.keys()):\n    print(\"\\t%s: %r\" % (param_name, best_parameters[param_name]))\n\n\n\n\nOutput:\n\n\n[Parallel(n_jobs=1)]: Done   1 jobs       | elapsed:    0.1s\n[Parallel(n_jobs=1)]: Done  12 out of  12 | elapsed:    0.7s finished\nFitting 3 folds for each of 4 candidates, totalling 12 fits\nBest score: 0.960\nBest parameters set:\n    sel__k_features: 1\n\n\n\n\nGridsearch Example 2\n\n\nimport pandas as pd\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.grid_search import GridSearchCV\nfrom mlxtend.sklearn import SBS\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.datasets import load_iris\n\n##########################\n### Loading data\n##########################\n\niris = load_iris()\nX = iris.data\ny = iris.target\n\n##########################\n### Setting up pipeline\n##########################\nknn = KNeighborsClassifier(n_neighbors=4)\n\nsbs = SBS(estimator=knn, k_features=2, scoring='accuracy', cv=5)\n\npipeline = Pipeline([\n            ('scr', StandardScaler()),\n            ('sel', sbs),\n            ('clf', knn)])\n\nparameters = {'sel__k_features': [1, 2, 3, 4],\n              'sel__estimator__n_neighbors': [4, 5, 6],\n              'clf__n_neighbors': [4, 5, 6]}\n\ngrid_search = GridSearchCV(pipeline, parameters, n_jobs=1, verbose=1)\n\n##########################\n### Running GridSearch\n##########################\ngrid_search.fit(X, y)\n\nprint(\"Best score: %0.3f\" % grid_search.best_score_)\nprint(\"Best parameters set:\")\nbest_parameters = grid_search.best_estimator_.get_params()\nfor param_name in sorted(parameters.keys()):\n    print(\"\\t%s: %r\" % (param_name, best_parameters[param_name]))\n\n\n\n\nOutput:\n\n\n[Parallel(n_jobs=1)]: Done   1 jobs       | elapsed:    0.1s\n[Parallel(n_jobs=1)]: Done  50 jobs       | elapsed:    2.9s\nFitting 3 folds for each of 36 candidates, totalling 108 fits\nBest score: 0.973\nBest parameters set:\n    clf__n_neighbors: 5\n    sel__estimator__n_neighbors: 5\n    sel__k_features: 2\n\n\n\n\nThe final feature subset can then be obtained as follows:\n\n\nprint('Best feature subset:')\ngrid_search.best_estimator_.steps[1][1].indices_\n\n\n\n\nOutput:\n\n\nBest feature subset:\n(2, 3)\n\n\n\n\nDefault Parameters\n\n\nclass SBS(BaseEstimator, MetaEstimatorMixin):\n    \"\"\" Sequential Backward Selection for feature selection.\n\n    Parameters\n    ----------\n    estimator : scikit-learn estimator object\n\n    k_features : int\n      Number of features to select where k_features.\n\n    print_progress : bool (default: True)\n       Prints progress as the number of epochs\n       to stderr.\n\n    scoring : str, (default='accuracy')\n      Scoring metric for the cross validation scorer.\n\n    cv : int (default: 5)\n      Number of folds in StratifiedKFold.\n\n    n_jobs : int (default: 1)\n      The number of CPUs to use for cross validation. -1 means 'all CPUs'.\n\n    Attributes\n    ----------\n    indices_ : array-like, shape = [n_predictions]\n      Indices of the selected subsets.\n\n    k_score_ : float\n      Cross validation mean scores of the selected subset\n\n    subsets_ : list of tuples\n      Indices of the sequentially selected subsets.\n\n    scores_ : list\n      Cross validation mean scores of the sequentially selected subsets.\n\n    Examples\n    --------\n    >>> from sklearn.neighbors import KNeighborsClassifier\n    >>> from sklearn.datasets import load_iris\n    >>> iris = load_iris()\n    >>> X = iris.data\n    >>> y = iris.target\n    >>> knn = KNeighborsClassifier(n_neighbors=4)\n    >>> sbs = SBS(knn, k_features=2, scoring='accuracy', cv=5)\n    >>> sbs = sbs.fit(X, y)\n    >>> sbs.indices_\n    (0, 3)\n    >>> sbs.k_score_\n    0.96\n    >>> sbs.transform(X)\n    array([[ 5.1,  0.2],\n       [ 4.9,  0.2],\n       [ 4.7,  0.2],\n       [ 4.6,  0.2],\n       [ 5. ,  0.2]])\n\n    \"\"\"",
            "title": "Sequential backward selection"
        },
        {
            "location": "/docs/feature_selection/sequential_backward_selection/#sequential-backward-selection",
            "text": "from mlxtend.feature_selection import SBS   Sequential Backward Selection (SBS) is  a classic feature selection algorithm -- a greedy search algorithm -- that has been developed as a suboptimal solution to the computationally often not feasible exhaustive search. In a nutshell, SBS removes one feature at the time based on the classifier performance until a feature subset of the desired size  k  is reached.  Related topics:   Sequential Forward Selection  Sequential Floating Forward Selection  Sequential Floating Backward Selection",
            "title": "Sequential Backward Selection"
        },
        {
            "location": "/docs/feature_selection/sequential_backward_selection/#the-sbs-algorithm",
            "text": "Input:  the set of all features, $Y = {y_1, y_2, ..., y_d}$     The SBS algorithm takes the whole feature set as input.   Output:  $X_k = {x_j \\; | \\;j = 1, 2, ..., k; \\; x_j \\in Y}$, where $k = (0, 1, 2, ..., d)$   SBS returns a subset of features; the number of selected features $k$, where $k < d$, has to be specified  a priori .   Initialization:  $X_0 = Y$, $k = d$   We initialize the algorithm with the given feature set so that the $k = d$.   Step 1 (Exclusion):     $x^- = \\text{ arg max } J(x_k - x), \\text{  where } x \\in X_k$ \n$X_k-1 = X_k - x^-$ \n$k = k - 1$  Go to Step 1      In this step, we remove a feature, $x^-$ from our feature subset $X_k$.  $x^-$ is the feature that maximizes our criterion function upon re,oval, that is, the feature that is associated with the best classifier performance if it is removed from $X_k$.  We repeat this procedure until the termination criterion is satisfied.   Termination:  $k = p$   We add features from the feature subset $X_k$ until the feature subset of size $k$ contains the number of desired features $p$ that we specified  a priori .",
            "title": "The SBS Algorithm"
        },
        {
            "location": "/docs/feature_selection/sequential_backward_selection/#example",
            "text": "Input:  from mlxtend.feature_selection import SBS\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.datasets import load_iris\n\niris = load_iris()\nX = iris.data\ny = iris.target\n\nknn = KNeighborsClassifier(n_neighbors=4)\n\nsbs = SBS(knn, k_features=2, scoring='accuracy', cv=5)\nsbs.fit(X, y)\n\nprint('Indices of selected features:', sbs.indices_)\nprint('CV score of selected subset:', sbs.k_score_)\nprint('New feature subset:')\nsbs.transform(X)[0:5]  Output:  Indices of selected features: (0, 3)\nCV score of selected subset: 0.96\nNew feature subset:\narray([[ 5.1,  0.2],\n   [ 4.9,  0.2],\n   [ 4.7,  0.2],\n   [ 4.6,  0.2],\n   [ 5. ,  0.2]])\n ```\n\n<br>\n<br>\n\nAs demonstrated below, the SBS algorithm can be a useful alternative to dimensionality reduction techniques to reduce overfitting and when the original features need to be preserved:\n\n\n```python\nimport matplotlib.pyplot as plt\nfrom mlxtend.data import wine_data\nfrom sklearn.preprocessing import StandardScaler\n\nscr = StandardScaler()\nX_std = scr.fit_transform(X)\n\nknn = KNeighborsClassifier(n_neighbors=4)\n\n# selecting features\nsbs = SBS(knn, k_features=1, scoring='accuracy', cv=5)\nsbs.fit(X_std, y)\n\n# plotting performance of feature subsets\nk_feat = [len(k) for k in sbs.subsets_]\n\nplt.plot(k_feat, sbs.scores_, marker='o')\nplt.ylabel('Accuracy')\nplt.xlabel('Number of features')\nplt.show()",
            "title": "Example"
        },
        {
            "location": "/docs/feature_selection/sequential_backward_selection/#gridsearch-example-1",
            "text": "Selecting the number of features in a pipeline.  import pandas as pd\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.grid_search import GridSearchCV\nfrom mlxtend.sklearn import SBS\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.datasets import load_iris\n\n##########################\n### Loading data\n##########################\n\niris = load_iris()\nX = iris.data\ny = iris.target\n\n##########################\n### Setting up pipeline\n##########################\nknn = KNeighborsClassifier(n_neighbors=4)\n\nsbs = SBS(estimator=knn, k_features=2, scoring='accuracy', cv=5)\n\npipeline = Pipeline([\n            ('scr', StandardScaler()),\n            ('sel', sbs),\n            ('clf', knn)])\n\nparameters = {'sel__k_features': [1,2,3,4]}\n\ngrid_search = GridSearchCV(pipeline, parameters, n_jobs=1, verbose=1)\n\n##########################\n### Running GridSearch\n##########################\ngrid_search.fit(X, y)\n\nprint(\"Best score: %0.3f\" % grid_search.best_score_)\nprint(\"Best parameters set:\")\nbest_parameters = grid_search.best_estimator_.get_params()\nfor param_name in sorted(parameters.keys()):\n    print(\"\\t%s: %r\" % (param_name, best_parameters[param_name]))  Output:  [Parallel(n_jobs=1)]: Done   1 jobs       | elapsed:    0.1s\n[Parallel(n_jobs=1)]: Done  12 out of  12 | elapsed:    0.7s finished\nFitting 3 folds for each of 4 candidates, totalling 12 fits\nBest score: 0.960\nBest parameters set:\n    sel__k_features: 1",
            "title": "Gridsearch Example 1"
        },
        {
            "location": "/docs/feature_selection/sequential_backward_selection/#gridsearch-example-2",
            "text": "import pandas as pd\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.grid_search import GridSearchCV\nfrom mlxtend.sklearn import SBS\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.datasets import load_iris\n\n##########################\n### Loading data\n##########################\n\niris = load_iris()\nX = iris.data\ny = iris.target\n\n##########################\n### Setting up pipeline\n##########################\nknn = KNeighborsClassifier(n_neighbors=4)\n\nsbs = SBS(estimator=knn, k_features=2, scoring='accuracy', cv=5)\n\npipeline = Pipeline([\n            ('scr', StandardScaler()),\n            ('sel', sbs),\n            ('clf', knn)])\n\nparameters = {'sel__k_features': [1, 2, 3, 4],\n              'sel__estimator__n_neighbors': [4, 5, 6],\n              'clf__n_neighbors': [4, 5, 6]}\n\ngrid_search = GridSearchCV(pipeline, parameters, n_jobs=1, verbose=1)\n\n##########################\n### Running GridSearch\n##########################\ngrid_search.fit(X, y)\n\nprint(\"Best score: %0.3f\" % grid_search.best_score_)\nprint(\"Best parameters set:\")\nbest_parameters = grid_search.best_estimator_.get_params()\nfor param_name in sorted(parameters.keys()):\n    print(\"\\t%s: %r\" % (param_name, best_parameters[param_name]))  Output:  [Parallel(n_jobs=1)]: Done   1 jobs       | elapsed:    0.1s\n[Parallel(n_jobs=1)]: Done  50 jobs       | elapsed:    2.9s\nFitting 3 folds for each of 36 candidates, totalling 108 fits\nBest score: 0.973\nBest parameters set:\n    clf__n_neighbors: 5\n    sel__estimator__n_neighbors: 5\n    sel__k_features: 2  The final feature subset can then be obtained as follows:  print('Best feature subset:')\ngrid_search.best_estimator_.steps[1][1].indices_  Output:  Best feature subset:\n(2, 3)",
            "title": "Gridsearch Example 2"
        },
        {
            "location": "/docs/feature_selection/sequential_backward_selection/#default-parameters",
            "text": "class SBS(BaseEstimator, MetaEstimatorMixin):\n    \"\"\" Sequential Backward Selection for feature selection.\n\n    Parameters\n    ----------\n    estimator : scikit-learn estimator object\n\n    k_features : int\n      Number of features to select where k_features.\n\n    print_progress : bool (default: True)\n       Prints progress as the number of epochs\n       to stderr.\n\n    scoring : str, (default='accuracy')\n      Scoring metric for the cross validation scorer.\n\n    cv : int (default: 5)\n      Number of folds in StratifiedKFold.\n\n    n_jobs : int (default: 1)\n      The number of CPUs to use for cross validation. -1 means 'all CPUs'.\n\n    Attributes\n    ----------\n    indices_ : array-like, shape = [n_predictions]\n      Indices of the selected subsets.\n\n    k_score_ : float\n      Cross validation mean scores of the selected subset\n\n    subsets_ : list of tuples\n      Indices of the sequentially selected subsets.\n\n    scores_ : list\n      Cross validation mean scores of the sequentially selected subsets.\n\n    Examples\n    --------\n    >>> from sklearn.neighbors import KNeighborsClassifier\n    >>> from sklearn.datasets import load_iris\n    >>> iris = load_iris()\n    >>> X = iris.data\n    >>> y = iris.target\n    >>> knn = KNeighborsClassifier(n_neighbors=4)\n    >>> sbs = SBS(knn, k_features=2, scoring='accuracy', cv=5)\n    >>> sbs = sbs.fit(X, y)\n    >>> sbs.indices_\n    (0, 3)\n    >>> sbs.k_score_\n    0.96\n    >>> sbs.transform(X)\n    array([[ 5.1,  0.2],\n       [ 4.9,  0.2],\n       [ 4.7,  0.2],\n       [ 4.6,  0.2],\n       [ 5. ,  0.2]])\n\n    \"\"\"",
            "title": "Default Parameters"
        },
        {
            "location": "/docs/feature_selection/sequential_forward_selection/",
            "text": "mlxtend\n\nSebastian Raschka, last updated: 10/11/2015\n\n\n\n\n\nSequential Forward Selection\n\n\n\n\nfrom mlxtend.feature_selection import SFS\n\n\n\n\nSequential Forward Selection (SFS) is a classic feature selection algorithm -- a greedy search algorithm -- that has been developed as a suboptimal solution to the computationally often not feasible exhaustive search. In a nutshell, SFS adds one feature from the original feature set at the time, based on the classifier performance, until a feature subset of the desired size \nk\n is reached.\n\n\nRelated topics:\n\n\n\n\nSequential Backward Selection\n\n\nSequential Floating Forward Selection\n\n\nSequential Floating Backward Selection\n\n\n\n\n\n\nThe SFFS Algorithm\n\n\nInput:\n $Y = {y_1, y_2, ..., y_d}$  \n\n\n\n\nThe \nSFS\n algorithm takes the whole $d$-dimensional feature set as input.\n\n\n\n\nOutput:\n $X_k = {x_j \\; | \\;j = 1, 2, ..., k; \\; x_j \\in Y}$, where $k = (0, 1, 2, ..., d)$\n\n\n\n\nSFS returns a subset of features; the number of selected features $k$, where $k < d$, has to be specified \na priori\n.\n\n\n\n\nInitialization:\n $X_0 = \\emptyset$, $k = 0$\n\n\n\n\nWe initialize the algorithm with an empty set $\\emptyset$ (\"null set\") so that $k = 0$ (where $k$ is the size of the subset).\n\n\n\n\nStep 1 (Inclusion):\n  \n\n\n$x^+ = \\text{ arg max } J(x_k + x), \\text{ where }  x \\in Y - X_k$\n\n  $X_k+1 = X_k + x^+$\n\n  $k = k + 1$  \n\n\nGo to Step 2\n\n\n\n\nin this step, we add an additional feature, $x^+$, to our feature subset $X_k$.\n\n\n$x^+$ is the feature that maximizes our criterion function, that is, the feature that is associated with the best classifier performance if it is added to $X_k$.\n\n\n\n\nStep 2 (Conditional Exclusion):\n  \n\n\n$x^- = \\text{ arg max } J(x_k - x), \\text{ where } x \\in X_k$\n\n$if \\; J(x_k - x) > J(x_k - x)$:  \n\n\u00a0\u00a0\u00a0\u00a0 $X_k-1 = X_k - x^-$\n\n\u00a0\u00a0\u00a0\u00a0 $k = k - 1$  \n\n\nGo to Step 1\n  \n\n\n\n\nIn Step 2, we only remove a feature if the resulting subset would gain an increase in performance. We go back to Step 1.  \n\n\nSteps 1 and 2 are reapeated until the \nTermination\n criterion is reached.\n\n\n\n\nTermination:\n $k = p$\n\n\n\n\nWe add features from the feature subset $X_k$ until the feature subset of size $k$ contains the number of desired features $p$ that we specified \na priori\n.\n\n\n\n\n\n\nExample\n\n\nInput:\n\n\nfrom mlxtend.feature_selection import SFS\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.datasets import load_iris\n\niris = load_iris()\nX = iris.data\ny = iris.target\n\nknn = KNeighborsClassifier(n_neighbors=4)\n\nsfs = SFS(knn, k_features=2, scoring='accuracy', cv=5)\nsfs.fit(X, y)\n\nprint('Indices of selected features:', sfs.indices_)\nprint('CV score of selected subset:', sfs.k_score_)\nprint('New feature subset:')\nsfs.transform(X)[0:5]\n\n\n\n\nOutput:\n\n\nIndices of selected features: (2, 3)\nCV score of selected subset: 0.966666666667\nNew feature subset:\narray([[ 1.4,  0.2],\n       [ 1.4,  0.2],\n       [ 1.3,  0.2],\n       [ 1.5,  0.2],\n       [ 1.4,  0.2]])\n\n\n\n\nAs demonstrated below, the SFS algorithm can be a useful alternative to dimensionality reduction techniques to reduce overfitting and when the original features need to be preserved:\n\n\nimport matplotlib.pyplot as plt\nfrom mlxtend.data import wine_data\nfrom sklearn.preprocessing import StandardScaler\n\nscr = StandardScaler()\nX_std = scr.fit_transform(X)\n\nknn = KNeighborsClassifier(n_neighbors=4)\n\n# selecting features\nsfs = SFS(knn, k_features=1, scoring='accuracy', cv=5)\nsfs.fit(X_std, y)\n\n# plotting performance of feature subsets\nk_feat = [len(k) for k in sfs.subsets_]\n\nplt.plot(k_feat, sfs.scores_, marker='o')\nplt.ylabel('Accuracy')\nplt.xlabel('Number of features')\nplt.show()\n\n\n\n\n\n\nGridsearch Example 1\n\n\nSelecting the number of features in a pipeline.\n\n\nimport pandas as pd\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.grid_search import GridSearchCV\nfrom mlxtend.sklearn import SFS\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.datasets import load_iris\n\n##########################\n### Loading data\n##########################\n\niris = load_iris()\nX = iris.data\ny = iris.target\n\n##########################\n### Setting up pipeline\n##########################\nknn = KNeighborsClassifier(n_neighbors=4)\n\nsfs = SFS(estimator=knn, k_features=2, scoring='accuracy', cv=5)\n\npipeline = Pipeline([\n            ('scr', StandardScaler()),\n            ('sel', sfs),\n            ('clf', knn)])\n\nparameters = {'sel__k_features': [1,2,3,4]}\n\ngrid_search = GridSearchCV(pipeline, parameters, n_jobs=1, verbose=1)\n\n##########################\n### Running GridSearch\n##########################\ngrid_search.fit(X, y)\n\nprint(\"Best score: %0.3f\" % grid_search.best_score_)\nprint(\"Best parameters set:\")\nbest_parameters = grid_search.best_estimator_.get_params()\nfor param_name in sorted(parameters.keys()):\n    print(\"\\t%s: %r\" % (param_name, best_parameters[param_name]))\n\n\n\n\nOutput:\n\n\n[Parallel(n_jobs=1)]: Done   1 jobs       | elapsed:    0.1s\n[Parallel(n_jobs=1)]: Done  12 out of  12 | elapsed:    0.8s finished\nFitting 3 folds for each of 4 candidates, totalling 12 fits\nBest score: 0.960\nBest parameters set:\n    sel__k_features: 1\n\n\n\n\nGridsearch Example 2\n\n\nimport pandas as pd\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.grid_search import GridSearchCV\nfrom mlxtend.sklearn import SFS\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.datasets import load_iris\n\n##########################\n### Loading data\n##########################\n\niris = load_iris()\nX = iris.data\ny = iris.target\n\n##########################\n### Setting up pipeline\n##########################\nknn = KNeighborsClassifier(n_neighbors=4)\n\nsfs = SFS(estimator=knn, k_features=2, scoring='accuracy', cv=5)\n\npipeline = Pipeline([\n            ('scr', StandardScaler()),\n            ('sel', sfs),\n            ('clf', knn)])\n\nparameters = {'sel__k_features': [1, 2, 3, 4],\n              'sel__estimator__n_neighbors': [4, 5, 6],\n              'clf__n_neighbors': [4, 5, 6]}\n\ngrid_search = GridSearchCV(pipeline, parameters, n_jobs=1, verbose=1)\n\n##########################\n### Running GridSearch\n##########################\ngrid_search.fit(X, y)\n\nprint(\"Best score: %0.3f\" % grid_search.best_score_)\nprint(\"Best parameters set:\")\nbest_parameters = grid_search.best_estimator_.get_params()\nfor param_name in sorted(parameters.keys()):\n    print(\"\\t%s: %r\" % (param_name, best_parameters[param_name]))\n\n\n\n\nOutput:\n\n\n[Parallel(n_jobs=1)]: Done   1 jobs       | elapsed:    0.1s\n[Parallel(n_jobs=1)]: Done  50 jobs       | elapsed:    4.0s\nFitting 3 folds for each of 36 candidates, totalling 108 fits\nBest score: 0.973\nBest parameters set:\n    clf__n_neighbors: 5\n    sel__estimator__n_neighbors: 5\n    sel__k_features: 2\n\n\n\n\nThe final feature subset can then be obtained as follows:\n\n\nprint('Best feature subset:')\ngrid_search.best_estimator_.steps[1][1].indices_\n\n\n\n\nOutput:\n\n\nBest feature subset:\n(2, 3)\n\n\n\n\nDefault Parameters\n\n\nclass SFS(BaseEstimator, MetaEstimatorMixin):\n    \"\"\" Sequential Forward Selection for feature selection.\n\n    Parameters\n    ----------\n    estimator : scikit-learn estimator object\n\n    print_progress : bool (default: True)\n       Prints progress as the number of epochs\n       to stderr.\n\n    k_features : int\n      Number of features to select where k_features.\n\n    scoring : str, (default='accuracy')\n      Scoring metric for the cross validation scorer.\n\n    cv : int (default: 5)\n      Number of folds in StratifiedKFold.\n\n    n_jobs : int (default: 1)\n      The number of CPUs to use for cross validation. -1 means 'all CPUs'.\n\n    Attributes\n    ----------\n    indices_ : array-like, shape = [n_predictions]\n      Indices of the selected subsets.\n\n    k_score_ : float\n      Cross validation mean scores of the selected subset\n\n    subsets_ : list of tuples\n      Indices of the sequentially selected subsets.\n\n    scores_ : list\n      Cross validation mean scores of the sequentially selected subsets.\n\n    Examples\n    --------\n    >>> from sklearn.neighbors import KNeighborsClassifier\n    >>> from sklearn.datasets import load_iris\n    >>> iris = load_iris()\n    >>> X = iris.data\n    >>> y = iris.target\n    >>> knn = KNeighborsClassifier(n_neighbors=4)\n    >>> sfs = SFS(knn, k_features=2, scoring='accuracy', cv=5)\n    >>> sfs = sfs.fit(X, y)\n    >>> sfs.indices_\n    (0, 3)\n    >>> sfs.k_score_\n    0.96\n    >>> sfs.transform(X)\n    array([[ 5.1,  0.2],\n       [ 4.9,  0.2],\n       [ 4.7,  0.2],\n       [ 4.6,  0.2],\n       [ 5. ,  0.2]])\n\n    \"\"\"",
            "title": "Sequential forward selection"
        },
        {
            "location": "/docs/feature_selection/sequential_forward_selection/#sequential-forward-selection",
            "text": "from mlxtend.feature_selection import SFS   Sequential Forward Selection (SFS) is a classic feature selection algorithm -- a greedy search algorithm -- that has been developed as a suboptimal solution to the computationally often not feasible exhaustive search. In a nutshell, SFS adds one feature from the original feature set at the time, based on the classifier performance, until a feature subset of the desired size  k  is reached.  Related topics:   Sequential Backward Selection  Sequential Floating Forward Selection  Sequential Floating Backward Selection",
            "title": "Sequential Forward Selection"
        },
        {
            "location": "/docs/feature_selection/sequential_forward_selection/#the-sffs-algorithm",
            "text": "Input:  $Y = {y_1, y_2, ..., y_d}$     The  SFS  algorithm takes the whole $d$-dimensional feature set as input.   Output:  $X_k = {x_j \\; | \\;j = 1, 2, ..., k; \\; x_j \\in Y}$, where $k = (0, 1, 2, ..., d)$   SFS returns a subset of features; the number of selected features $k$, where $k < d$, has to be specified  a priori .   Initialization:  $X_0 = \\emptyset$, $k = 0$   We initialize the algorithm with an empty set $\\emptyset$ (\"null set\") so that $k = 0$ (where $k$ is the size of the subset).   Step 1 (Inclusion):     $x^+ = \\text{ arg max } J(x_k + x), \\text{ where }  x \\in Y - X_k$ \n  $X_k+1 = X_k + x^+$ \n  $k = k + 1$    Go to Step 2   in this step, we add an additional feature, $x^+$, to our feature subset $X_k$.  $x^+$ is the feature that maximizes our criterion function, that is, the feature that is associated with the best classifier performance if it is added to $X_k$.   Step 2 (Conditional Exclusion):     $x^- = \\text{ arg max } J(x_k - x), \\text{ where } x \\in X_k$ \n$if \\; J(x_k - x) > J(x_k - x)$:   \n\u00a0\u00a0\u00a0\u00a0 $X_k-1 = X_k - x^-$ \n\u00a0\u00a0\u00a0\u00a0 $k = k - 1$    Go to Step 1      In Step 2, we only remove a feature if the resulting subset would gain an increase in performance. We go back to Step 1.    Steps 1 and 2 are reapeated until the  Termination  criterion is reached.   Termination:  $k = p$   We add features from the feature subset $X_k$ until the feature subset of size $k$ contains the number of desired features $p$ that we specified  a priori .",
            "title": "The SFFS Algorithm"
        },
        {
            "location": "/docs/feature_selection/sequential_forward_selection/#example",
            "text": "Input:  from mlxtend.feature_selection import SFS\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.datasets import load_iris\n\niris = load_iris()\nX = iris.data\ny = iris.target\n\nknn = KNeighborsClassifier(n_neighbors=4)\n\nsfs = SFS(knn, k_features=2, scoring='accuracy', cv=5)\nsfs.fit(X, y)\n\nprint('Indices of selected features:', sfs.indices_)\nprint('CV score of selected subset:', sfs.k_score_)\nprint('New feature subset:')\nsfs.transform(X)[0:5]  Output:  Indices of selected features: (2, 3)\nCV score of selected subset: 0.966666666667\nNew feature subset:\narray([[ 1.4,  0.2],\n       [ 1.4,  0.2],\n       [ 1.3,  0.2],\n       [ 1.5,  0.2],\n       [ 1.4,  0.2]])  As demonstrated below, the SFS algorithm can be a useful alternative to dimensionality reduction techniques to reduce overfitting and when the original features need to be preserved:  import matplotlib.pyplot as plt\nfrom mlxtend.data import wine_data\nfrom sklearn.preprocessing import StandardScaler\n\nscr = StandardScaler()\nX_std = scr.fit_transform(X)\n\nknn = KNeighborsClassifier(n_neighbors=4)\n\n# selecting features\nsfs = SFS(knn, k_features=1, scoring='accuracy', cv=5)\nsfs.fit(X_std, y)\n\n# plotting performance of feature subsets\nk_feat = [len(k) for k in sfs.subsets_]\n\nplt.plot(k_feat, sfs.scores_, marker='o')\nplt.ylabel('Accuracy')\nplt.xlabel('Number of features')\nplt.show()",
            "title": "Example"
        },
        {
            "location": "/docs/feature_selection/sequential_forward_selection/#gridsearch-example-1",
            "text": "Selecting the number of features in a pipeline.  import pandas as pd\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.grid_search import GridSearchCV\nfrom mlxtend.sklearn import SFS\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.datasets import load_iris\n\n##########################\n### Loading data\n##########################\n\niris = load_iris()\nX = iris.data\ny = iris.target\n\n##########################\n### Setting up pipeline\n##########################\nknn = KNeighborsClassifier(n_neighbors=4)\n\nsfs = SFS(estimator=knn, k_features=2, scoring='accuracy', cv=5)\n\npipeline = Pipeline([\n            ('scr', StandardScaler()),\n            ('sel', sfs),\n            ('clf', knn)])\n\nparameters = {'sel__k_features': [1,2,3,4]}\n\ngrid_search = GridSearchCV(pipeline, parameters, n_jobs=1, verbose=1)\n\n##########################\n### Running GridSearch\n##########################\ngrid_search.fit(X, y)\n\nprint(\"Best score: %0.3f\" % grid_search.best_score_)\nprint(\"Best parameters set:\")\nbest_parameters = grid_search.best_estimator_.get_params()\nfor param_name in sorted(parameters.keys()):\n    print(\"\\t%s: %r\" % (param_name, best_parameters[param_name]))  Output:  [Parallel(n_jobs=1)]: Done   1 jobs       | elapsed:    0.1s\n[Parallel(n_jobs=1)]: Done  12 out of  12 | elapsed:    0.8s finished\nFitting 3 folds for each of 4 candidates, totalling 12 fits\nBest score: 0.960\nBest parameters set:\n    sel__k_features: 1",
            "title": "Gridsearch Example 1"
        },
        {
            "location": "/docs/feature_selection/sequential_forward_selection/#gridsearch-example-2",
            "text": "import pandas as pd\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.grid_search import GridSearchCV\nfrom mlxtend.sklearn import SFS\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.datasets import load_iris\n\n##########################\n### Loading data\n##########################\n\niris = load_iris()\nX = iris.data\ny = iris.target\n\n##########################\n### Setting up pipeline\n##########################\nknn = KNeighborsClassifier(n_neighbors=4)\n\nsfs = SFS(estimator=knn, k_features=2, scoring='accuracy', cv=5)\n\npipeline = Pipeline([\n            ('scr', StandardScaler()),\n            ('sel', sfs),\n            ('clf', knn)])\n\nparameters = {'sel__k_features': [1, 2, 3, 4],\n              'sel__estimator__n_neighbors': [4, 5, 6],\n              'clf__n_neighbors': [4, 5, 6]}\n\ngrid_search = GridSearchCV(pipeline, parameters, n_jobs=1, verbose=1)\n\n##########################\n### Running GridSearch\n##########################\ngrid_search.fit(X, y)\n\nprint(\"Best score: %0.3f\" % grid_search.best_score_)\nprint(\"Best parameters set:\")\nbest_parameters = grid_search.best_estimator_.get_params()\nfor param_name in sorted(parameters.keys()):\n    print(\"\\t%s: %r\" % (param_name, best_parameters[param_name]))  Output:  [Parallel(n_jobs=1)]: Done   1 jobs       | elapsed:    0.1s\n[Parallel(n_jobs=1)]: Done  50 jobs       | elapsed:    4.0s\nFitting 3 folds for each of 36 candidates, totalling 108 fits\nBest score: 0.973\nBest parameters set:\n    clf__n_neighbors: 5\n    sel__estimator__n_neighbors: 5\n    sel__k_features: 2  The final feature subset can then be obtained as follows:  print('Best feature subset:')\ngrid_search.best_estimator_.steps[1][1].indices_  Output:  Best feature subset:\n(2, 3)",
            "title": "Gridsearch Example 2"
        },
        {
            "location": "/docs/feature_selection/sequential_forward_selection/#default-parameters",
            "text": "class SFS(BaseEstimator, MetaEstimatorMixin):\n    \"\"\" Sequential Forward Selection for feature selection.\n\n    Parameters\n    ----------\n    estimator : scikit-learn estimator object\n\n    print_progress : bool (default: True)\n       Prints progress as the number of epochs\n       to stderr.\n\n    k_features : int\n      Number of features to select where k_features.\n\n    scoring : str, (default='accuracy')\n      Scoring metric for the cross validation scorer.\n\n    cv : int (default: 5)\n      Number of folds in StratifiedKFold.\n\n    n_jobs : int (default: 1)\n      The number of CPUs to use for cross validation. -1 means 'all CPUs'.\n\n    Attributes\n    ----------\n    indices_ : array-like, shape = [n_predictions]\n      Indices of the selected subsets.\n\n    k_score_ : float\n      Cross validation mean scores of the selected subset\n\n    subsets_ : list of tuples\n      Indices of the sequentially selected subsets.\n\n    scores_ : list\n      Cross validation mean scores of the sequentially selected subsets.\n\n    Examples\n    --------\n    >>> from sklearn.neighbors import KNeighborsClassifier\n    >>> from sklearn.datasets import load_iris\n    >>> iris = load_iris()\n    >>> X = iris.data\n    >>> y = iris.target\n    >>> knn = KNeighborsClassifier(n_neighbors=4)\n    >>> sfs = SFS(knn, k_features=2, scoring='accuracy', cv=5)\n    >>> sfs = sfs.fit(X, y)\n    >>> sfs.indices_\n    (0, 3)\n    >>> sfs.k_score_\n    0.96\n    >>> sfs.transform(X)\n    array([[ 5.1,  0.2],\n       [ 4.9,  0.2],\n       [ 4.7,  0.2],\n       [ 4.6,  0.2],\n       [ 5. ,  0.2]])\n\n    \"\"\"",
            "title": "Default Parameters"
        },
        {
            "location": "/docs/feature_selection/sequential_floating_backward_selection/",
            "text": "mlxtend\n\nSebastian Raschka, last updated: 10/11/2015\n\n\n\n\n\nSequential Floating Backward Selection\n\n\n\n\nfrom mlxtend.feature_selection import SFBS\n\n\n\n\nThe Sequential Floating Backward Selection (SFFS) algorithm can be considered as an extension of the simpler \nSBS\n algorithm. In contrast to SBS, the SFBS algorithm has an additional inclusion step to add features once they were excluded, so that a larger number of feature subset combinations can be sampled. It is important to emphasize that the addition of once excluded features is conditional. The \nconditional inclusion\n in SFBS only occurs if the resulting feature subset is assessed as \"better\" by the criterion function after removal of a particular feature. Furthermore, I added an optional check to skip the conditional inclusion step if the algorithm gets stuck in cycles.  \n\n\nRelated topics:\n\n\n\n\nSequential Forward Selection\n\n\nSequential Floating Forward Selection\n\n\nSequential Backward Selection\n\n\n\n\nThe SFBS Algorithm\n\n\n\n\nInput:\n the set of all features, $Y = {y_1, y_2, ..., y_d}$  \n\n\n\n\nThe SFBS algorithm takes the whole feature set as input.\n\n\n\n\nOutput:\n $X_k = {x_j \\; | \\;j = 1, 2, ..., k; \\; x_j \\in Y}$, where $k = (0, 1, 2, ..., d)$\n\n\n\n\nSFBS returns a subset of features; the number of selected features $k$, where $k < d$, has to be specified \na priori\n.\n\n\n\n\nInitialization:\n $X_0 = Y$, $k = d$\n\n\n\n\nWe initialize the algorithm with the given feature set so that the $k = d$.\n\n\n\n\nStep 1 (Exclusion):\n  \n\n\n$x^- = \\text{ arg max } J(x_k - x), \\text{  where } x \\in X_k$\n\n$X_k-1 = X_k - x^-$\n\n$k = k - 1$\n\n\nGo to Step 2\n  \n\n\n\n\nIn this step, we remove a feature, $x^-$ from our feature subset $X_k$.\n\n\n$x^-$ is the feature that maximizes our criterion function upon re,oval, that is, the feature that is associated with the best classifier performance if it is removed from $X_k$.\n\n\n\n\nStep 2 (Conditional Inclusion):\n\n\n\n$x^+ = \\text{ arg max } J(x_k + x), \\text{ where } x \\in Y - X_k$\n\n\nif J(x_k + x) > J(x_k + x)\n:  \n\n\u00a0\u00a0\u00a0\u00a0 $X_k+1 = X_k + x^+$\n\n\u00a0\u00a0\u00a0\u00a0 $k = k + 1$\n\n\nGo to Step 1\n  \n\n\n\n\nIn Step 2, we search for features that improve the classifier performance if they are added back to the feature subset. If such features exist, we add the feature $x^+$ for which the perfomance improvement is max.\n\n\nSteps 1 and 2 are reapeated until the \nTermination\n criterion is reached.\n\n\n\n\nTermination:\n $k = p$\n\n\n\n\nWe add features from the feature subset $X_k$ until the feature subset of size $k$ contains the number of desired features $p$ that we specified \na priori\n.\n\n\n\n\n\n\nExample\n\n\nInput:\n\n\nfrom mlxtend.feature_selection import SFBS\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.datasets import load_iris\n\niris = load_iris()\nX = iris.data\ny = iris.target\n\nknn = KNeighborsClassifier(n_neighbors=4)\n\nsfbs = SFBS(knn, k_features=2, scoring='accuracy', cv=5)\nsfbs.fit(X, y)\n\nprint('Indices of selected features:', sfbs.indices_)\nprint('CV score of selected subset:', sfbs.k_score_)\nprint('New feature subset:')\nsfbs.transform(X)[0:5]\n\n\n\n\nOutput:\n\n\nIndices of selected features: (0, 3)\nCV score of selected subset: 0.96\nNew feature subset:\narray([[ 1.4,  0.2],\n       [ 1.4,  0.2],\n       [ 1.3,  0.2],\n       [ 1.5,  0.2],\n       [ 1.4,  0.2]])\n\n\n\n\n\n\n\n\nAs demonstrated below, the SFBS algorithm can be a useful alternative to dimensionality reduction techniques to reduce overfitting and when the original features need to be preserved:\n\n\nimport matplotlib.pyplot as plt\nfrom mlxtend.data import wine_data\nfrom sklearn.preprocessing import StandardScaler\n\nscr = StandardScaler()\nX_std = scr.fit_transform(X)\n\nknn = KNeighborsClassifier(n_neighbors=4)\n\n# selecting features\nsfbs = SFBS(knn, k_features=1, scoring='accuracy', cv=5)\nsfbs.fit(X_std, y)\n\n# plotting performance of feature subsets\nk_feat = [len(k) for k in sfbs.subsets_]\n\nplt.plot(k_feat, sfbs.scores_, marker='o')\nplt.ylabel('Accuracy')\nplt.xlabel('Number of features')\nplt.show()\n\n\n\n\n\n\nGridsearch Example 1\n\n\nSelecting the number of features in a pipeline.\n\n\nimport pandas as pd\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.grid_search import GridSearchCV\nfrom mlxtend.sklearn import SFBS\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.datasets import load_iris\n\n##########################\n### Loading data\n##########################\n\niris = load_iris()\nX = iris.data\ny = iris.target\n\n##########################\n### Setting up pipeline\n##########################\nknn = KNeighborsClassifier(n_neighbors=4)\n\nsfbs = SFBS(estimator=knn, k_features=2, scoring='accuracy', cv=5)\n\npipeline = Pipeline([\n            ('scr', StandardScaler()),\n            ('sel', sfbs),\n            ('clf', knn)])\n\nparameters = {'sel__k_features': [1,2,3,4]}\n\ngrid_search = GridSearchCV(pipeline, parameters, n_jobs=1, verbose=1)\n\n##########################\n### Running GridSearch\n##########################\ngrid_search.fit(X, y)\n\nprint(\"Best score: %0.3f\" % grid_search.best_score_)\nprint(\"Best parameters set:\")\nbest_parameters = grid_search.best_estimator_.get_params()\nfor param_name in sorted(parameters.keys()):\n    print(\"\\t%s: %r\" % (param_name, best_parameters[param_name]))\n\n\n\n\nOutput:\n\n\n[Parallel(n_jobs=1)]: Done   1 jobs       | elapsed:    0.1s\n[Parallel(n_jobs=1)]: Done  12 out of  12 | elapsed:    1.3s finished\nFitting 3 folds for each of 4 candidates, totalling 12 fits\nBest score: 0.960\nBest parameters set:\n    sel__k_features: 1\n\n\n\n\nGridsearch Example 2\n\n\nimport pandas as pd\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.grid_search import GridSearchCV\nfrom mlxtend.sklearn import SFBS\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.datasets import load_iris\n\n##########################\n### Loading data\n##########################\n\niris = load_iris()\nX = iris.data\ny = iris.target\n\n##########################\n### Setting up pipeline\n##########################\nknn = KNeighborsClassifier(n_neighbors=4)\n\nsfbs = SFBS(estimator=knn, k_features=2, scoring='accuracy', cv=5)\n\npipeline = Pipeline([\n            ('scr', StandardScaler()),\n            ('sel', sfbs),\n            ('clf', knn)])\n\nparameters = {'sel__k_features': [1, 2, 3, 4],\n              'sel__estimator__n_neighbors': [4, 5, 6],\n              'clf__n_neighbors': [4, 5, 6]}\n\ngrid_search = GridSearchCV(pipeline, parameters, n_jobs=1, verbose=1)\n\n##########################\n### Running GridSearch\n##########################\ngrid_search.fit(X, y)\n\nprint(\"Best score: %0.3f\" % grid_search.best_score_)\nprint(\"Best parameters set:\")\nbest_parameters = grid_search.best_estimator_.get_params()\nfor param_name in sorted(parameters.keys()):\n    print(\"\\t%s: %r\" % (param_name, best_parameters[param_name]))\n\n\n\n\nOutput:\n\n\n[Parallel(n_jobs=1)]: Done   1 jobs       | elapsed:    0.2s\n[Parallel(n_jobs=1)]: Done  50 jobs       | elapsed:    5.0s\nFitting 3 folds for each of 36 candidates, totalling 108 fits\nBest score: 0.973\nBest parameters set:\n    clf__n_neighbors: 5\n    sel__estimator__n_neighbors: 5\n    sel__k_features: 2\n\n\n\n\nThe final feature subset can then be obtained as follows:\n\n\nprint('Best feature subset:')\ngrid_search.best_estimator_.steps[1][1].indices_\n\n\n\n\nOutput:\n\n\nBest feature subset:\n(2, 3)\n\n\n\n\nDefault Parameters\n\n\nclass SFBS(BaseEstimator, MetaEstimatorMixin):\n    \"\"\" Sequential Floating Backward Selection for feature selection.\n\n    Parameters\n    ----------\n    estimator : scikit-learn estimator object\n\n    k_features : int\n      Number of features to select where k_features.\n\n    print_progress : bool (default: True)\n       Prints progress as the number of epochs\n       to stderr.\n\n    scoring : str, (default='accuracy')\n      Scoring metric for the cross validation scorer.\n\n    cv : int (default: 5)\n      Number of folds in StratifiedKFold.\n\n    max_iter: int (default: -1)\n      Terminate early if number of `max_iter` is reached.\n\n    n_jobs : int (default: 1)\n      The number of CPUs to use for cross validation. -1 means 'all CPUs'.\n\n    Attributes\n    ----------\n    indices_ : array-like, shape = [n_predictions]\n      Indices of the selected subsets.\n\n    k_score_ : float\n      Cross validation mean score of the selected subset\n\n    subsets_ : list of lists\n      Indices of the sequentially selected subsets.\n\n    scores_ : list\n      Cross validation mean scores of the sequentially selected subsets.\n\n    Examples\n    --------\n    >>> from sklearn.neighbors import KNeighborsClassifier\n    >>> from sklearn.datasets import load_iris\n    >>> iris = load_iris()\n    >>> X = iris.data\n    >>> y = iris.target\n    >>> knn = KNeighborsClassifier(n_neighbors=4)\n    >>> sfbs = SFBS(knn, k_features=2, scoring='accuracy', cv=5)\n    >>> sfbs = sfbs.fit(X, y)\n    >>> sfbs.indices_\n    (2, 3)\n    >>> sfbs.transform(X[:5])\n    array([[ 1.4,  0.2],\n           [ 1.4,  0.2],\n           [ 1.3,  0.2],\n           [ 1.5,  0.2],\n           [ 1.4,  0.2]])\n\n    >>> print('best score: %.2f' % sfbs.k_score_)\n    best score: 0.97\n\n    \"\"\"",
            "title": "Sequential floating backward selection"
        },
        {
            "location": "/docs/feature_selection/sequential_floating_backward_selection/#sequential-floating-backward-selection",
            "text": "from mlxtend.feature_selection import SFBS   The Sequential Floating Backward Selection (SFFS) algorithm can be considered as an extension of the simpler  SBS  algorithm. In contrast to SBS, the SFBS algorithm has an additional inclusion step to add features once they were excluded, so that a larger number of feature subset combinations can be sampled. It is important to emphasize that the addition of once excluded features is conditional. The  conditional inclusion  in SFBS only occurs if the resulting feature subset is assessed as \"better\" by the criterion function after removal of a particular feature. Furthermore, I added an optional check to skip the conditional inclusion step if the algorithm gets stuck in cycles.    Related topics:   Sequential Forward Selection  Sequential Floating Forward Selection  Sequential Backward Selection",
            "title": "Sequential Floating Backward Selection"
        },
        {
            "location": "/docs/feature_selection/sequential_floating_backward_selection/#the-sfbs-algorithm",
            "text": "Input:  the set of all features, $Y = {y_1, y_2, ..., y_d}$     The SFBS algorithm takes the whole feature set as input.   Output:  $X_k = {x_j \\; | \\;j = 1, 2, ..., k; \\; x_j \\in Y}$, where $k = (0, 1, 2, ..., d)$   SFBS returns a subset of features; the number of selected features $k$, where $k < d$, has to be specified  a priori .   Initialization:  $X_0 = Y$, $k = d$   We initialize the algorithm with the given feature set so that the $k = d$.   Step 1 (Exclusion):     $x^- = \\text{ arg max } J(x_k - x), \\text{  where } x \\in X_k$ \n$X_k-1 = X_k - x^-$ \n$k = k - 1$  Go to Step 2      In this step, we remove a feature, $x^-$ from our feature subset $X_k$.  $x^-$ is the feature that maximizes our criterion function upon re,oval, that is, the feature that is associated with the best classifier performance if it is removed from $X_k$.   Step 2 (Conditional Inclusion):  \n$x^+ = \\text{ arg max } J(x_k + x), \\text{ where } x \\in Y - X_k$  if J(x_k + x) > J(x_k + x) :   \n\u00a0\u00a0\u00a0\u00a0 $X_k+1 = X_k + x^+$ \n\u00a0\u00a0\u00a0\u00a0 $k = k + 1$  Go to Step 1      In Step 2, we search for features that improve the classifier performance if they are added back to the feature subset. If such features exist, we add the feature $x^+$ for which the perfomance improvement is max.  Steps 1 and 2 are reapeated until the  Termination  criterion is reached.   Termination:  $k = p$   We add features from the feature subset $X_k$ until the feature subset of size $k$ contains the number of desired features $p$ that we specified  a priori .",
            "title": "The SFBS Algorithm"
        },
        {
            "location": "/docs/feature_selection/sequential_floating_backward_selection/#example",
            "text": "Input:  from mlxtend.feature_selection import SFBS\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.datasets import load_iris\n\niris = load_iris()\nX = iris.data\ny = iris.target\n\nknn = KNeighborsClassifier(n_neighbors=4)\n\nsfbs = SFBS(knn, k_features=2, scoring='accuracy', cv=5)\nsfbs.fit(X, y)\n\nprint('Indices of selected features:', sfbs.indices_)\nprint('CV score of selected subset:', sfbs.k_score_)\nprint('New feature subset:')\nsfbs.transform(X)[0:5]  Output:  Indices of selected features: (0, 3)\nCV score of selected subset: 0.96\nNew feature subset:\narray([[ 1.4,  0.2],\n       [ 1.4,  0.2],\n       [ 1.3,  0.2],\n       [ 1.5,  0.2],\n       [ 1.4,  0.2]])    As demonstrated below, the SFBS algorithm can be a useful alternative to dimensionality reduction techniques to reduce overfitting and when the original features need to be preserved:  import matplotlib.pyplot as plt\nfrom mlxtend.data import wine_data\nfrom sklearn.preprocessing import StandardScaler\n\nscr = StandardScaler()\nX_std = scr.fit_transform(X)\n\nknn = KNeighborsClassifier(n_neighbors=4)\n\n# selecting features\nsfbs = SFBS(knn, k_features=1, scoring='accuracy', cv=5)\nsfbs.fit(X_std, y)\n\n# plotting performance of feature subsets\nk_feat = [len(k) for k in sfbs.subsets_]\n\nplt.plot(k_feat, sfbs.scores_, marker='o')\nplt.ylabel('Accuracy')\nplt.xlabel('Number of features')\nplt.show()",
            "title": "Example"
        },
        {
            "location": "/docs/feature_selection/sequential_floating_backward_selection/#gridsearch-example-1",
            "text": "Selecting the number of features in a pipeline.  import pandas as pd\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.grid_search import GridSearchCV\nfrom mlxtend.sklearn import SFBS\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.datasets import load_iris\n\n##########################\n### Loading data\n##########################\n\niris = load_iris()\nX = iris.data\ny = iris.target\n\n##########################\n### Setting up pipeline\n##########################\nknn = KNeighborsClassifier(n_neighbors=4)\n\nsfbs = SFBS(estimator=knn, k_features=2, scoring='accuracy', cv=5)\n\npipeline = Pipeline([\n            ('scr', StandardScaler()),\n            ('sel', sfbs),\n            ('clf', knn)])\n\nparameters = {'sel__k_features': [1,2,3,4]}\n\ngrid_search = GridSearchCV(pipeline, parameters, n_jobs=1, verbose=1)\n\n##########################\n### Running GridSearch\n##########################\ngrid_search.fit(X, y)\n\nprint(\"Best score: %0.3f\" % grid_search.best_score_)\nprint(\"Best parameters set:\")\nbest_parameters = grid_search.best_estimator_.get_params()\nfor param_name in sorted(parameters.keys()):\n    print(\"\\t%s: %r\" % (param_name, best_parameters[param_name]))  Output:  [Parallel(n_jobs=1)]: Done   1 jobs       | elapsed:    0.1s\n[Parallel(n_jobs=1)]: Done  12 out of  12 | elapsed:    1.3s finished\nFitting 3 folds for each of 4 candidates, totalling 12 fits\nBest score: 0.960\nBest parameters set:\n    sel__k_features: 1",
            "title": "Gridsearch Example 1"
        },
        {
            "location": "/docs/feature_selection/sequential_floating_backward_selection/#gridsearch-example-2",
            "text": "import pandas as pd\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.grid_search import GridSearchCV\nfrom mlxtend.sklearn import SFBS\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.datasets import load_iris\n\n##########################\n### Loading data\n##########################\n\niris = load_iris()\nX = iris.data\ny = iris.target\n\n##########################\n### Setting up pipeline\n##########################\nknn = KNeighborsClassifier(n_neighbors=4)\n\nsfbs = SFBS(estimator=knn, k_features=2, scoring='accuracy', cv=5)\n\npipeline = Pipeline([\n            ('scr', StandardScaler()),\n            ('sel', sfbs),\n            ('clf', knn)])\n\nparameters = {'sel__k_features': [1, 2, 3, 4],\n              'sel__estimator__n_neighbors': [4, 5, 6],\n              'clf__n_neighbors': [4, 5, 6]}\n\ngrid_search = GridSearchCV(pipeline, parameters, n_jobs=1, verbose=1)\n\n##########################\n### Running GridSearch\n##########################\ngrid_search.fit(X, y)\n\nprint(\"Best score: %0.3f\" % grid_search.best_score_)\nprint(\"Best parameters set:\")\nbest_parameters = grid_search.best_estimator_.get_params()\nfor param_name in sorted(parameters.keys()):\n    print(\"\\t%s: %r\" % (param_name, best_parameters[param_name]))  Output:  [Parallel(n_jobs=1)]: Done   1 jobs       | elapsed:    0.2s\n[Parallel(n_jobs=1)]: Done  50 jobs       | elapsed:    5.0s\nFitting 3 folds for each of 36 candidates, totalling 108 fits\nBest score: 0.973\nBest parameters set:\n    clf__n_neighbors: 5\n    sel__estimator__n_neighbors: 5\n    sel__k_features: 2  The final feature subset can then be obtained as follows:  print('Best feature subset:')\ngrid_search.best_estimator_.steps[1][1].indices_  Output:  Best feature subset:\n(2, 3)",
            "title": "Gridsearch Example 2"
        },
        {
            "location": "/docs/feature_selection/sequential_floating_backward_selection/#default-parameters",
            "text": "class SFBS(BaseEstimator, MetaEstimatorMixin):\n    \"\"\" Sequential Floating Backward Selection for feature selection.\n\n    Parameters\n    ----------\n    estimator : scikit-learn estimator object\n\n    k_features : int\n      Number of features to select where k_features.\n\n    print_progress : bool (default: True)\n       Prints progress as the number of epochs\n       to stderr.\n\n    scoring : str, (default='accuracy')\n      Scoring metric for the cross validation scorer.\n\n    cv : int (default: 5)\n      Number of folds in StratifiedKFold.\n\n    max_iter: int (default: -1)\n      Terminate early if number of `max_iter` is reached.\n\n    n_jobs : int (default: 1)\n      The number of CPUs to use for cross validation. -1 means 'all CPUs'.\n\n    Attributes\n    ----------\n    indices_ : array-like, shape = [n_predictions]\n      Indices of the selected subsets.\n\n    k_score_ : float\n      Cross validation mean score of the selected subset\n\n    subsets_ : list of lists\n      Indices of the sequentially selected subsets.\n\n    scores_ : list\n      Cross validation mean scores of the sequentially selected subsets.\n\n    Examples\n    --------\n    >>> from sklearn.neighbors import KNeighborsClassifier\n    >>> from sklearn.datasets import load_iris\n    >>> iris = load_iris()\n    >>> X = iris.data\n    >>> y = iris.target\n    >>> knn = KNeighborsClassifier(n_neighbors=4)\n    >>> sfbs = SFBS(knn, k_features=2, scoring='accuracy', cv=5)\n    >>> sfbs = sfbs.fit(X, y)\n    >>> sfbs.indices_\n    (2, 3)\n    >>> sfbs.transform(X[:5])\n    array([[ 1.4,  0.2],\n           [ 1.4,  0.2],\n           [ 1.3,  0.2],\n           [ 1.5,  0.2],\n           [ 1.4,  0.2]])\n\n    >>> print('best score: %.2f' % sfbs.k_score_)\n    best score: 0.97\n\n    \"\"\"",
            "title": "Default Parameters"
        },
        {
            "location": "/docs/feature_selection/sequential_floating_forward_selection/",
            "text": "mlxtend\n\nSebastian Raschka, last updated: 10/11/2015\n\n\n\n\n\nSequential Floating Forward Selection\n\n\n\n\nfrom mlxtend.feature_selection import SFFS\n\n\n\n\nThe Sequential Floating Forward Selection (SFFS) algorithm can be considered as an extension of the simpler \nSFS\n algorithm. In contrast to SFS, the SFFS algorithm has an additional exclusion step to remove features once they were included, so that a larger number of feature subset combinations can be sampled. It is important to emphasize that the removal of included features is conditional. The \nconditional exclusion\n in SFFS only occurs if the resulting feature subset is assessed as \"better\" by the criterion function after removal of a particular feature. Furthermore, I added an optional check to skip the conditional exclusion step if the algorithm gets stuck in cycles.  \n\n\nRelated topics:\n\n\n\n\nSequential Forward Selection\n\n\nSequential Backward Selection\n\n\nSequential Floating Backward Selection\n\n\n\n\nThe SFFS Algorithm\n\n\n\n\nInput:\n the set of all features, $Y = {y_1, y_2, ..., y_d}$  \n\n\n\n\nThe \nSFFS\n algorithm takes the whole feature set as input, if our feature space consists of, e.g. 10, if our feature space consists of 10 dimensions (\nd = 10\n).\n\n\n\n\n\nOutput:\n a subset of features, $X_k = {x_j \\; | \\;j = 1, 2, ..., k; \\; x_j \\in Y}$, where $k = (0, 1, 2, ..., d)$\n\n\n\n\nThe returned output of the algorithm is a subset of the feature space of a specified size. E.g., a subset of 5 features from a 10-dimensional feature space (\nk = 5, d = 10\n).\n\n\n\n\n\nInitialization:\n $X_0 = Y$, $k = d$\n\n\n\n\nWe initialize the algorithm with an empty set (\"null set\") so that the \nk = 0\n (where \nk\n is the size of the subset)\n\n\n\n\n\nStep 1 (Inclusion):\n\n\n\n\u00a0\u00a0\u00a0\u00a0 $x^+ = \\text{ arg max } J(x_k + x), \\text{ where }  x \\in Y - X_k$\n\n\u00a0\u00a0\u00a0\u00a0 $X_k+1 = X_k + x^+$\n\n\u00a0\u00a0\u00a0\u00a0 $k = k + 1$  \n\n\u00a0\u00a0\u00a0\u00a0\nGo to Step 2\n\n\n \n\n\nStep 2 (Conditional Exclusion):\n\n\n\n\u00a0\u00a0\u00a0\u00a0 $x^- = \\text{ arg max } J(x_k - x), \\text{ where } x \\in X_k$\n\n\u00a0\u00a0\u00a0\u00a0$if \\; J(x_k - x) > J(x_k - x)$:  \n\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 $X_k-1 = X_k - x^- $\n\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 $k = k - 1$  \n\n\u00a0\u00a0\u00a0\u00a0\nGo to Step 1\n  \n\n\n\n\nIn step 1, we include the feature from the \nfeature space\n that leads to the best performance increase for our \nfeature subset\n (assessed by the \ncriterion function\n). Then, we go over to step 2\n\n\nIn step 2, we only remove a feature if the resulting subset would gain an increase in performance. We go back to step 1.  \n\n\nSteps 1 and 2 are reapeated until the \nTermination\n criterion is reached.\n\n\n\n\n\nTermination:\n stop when \nk\n equals the number of desired features\n\n\n\n\nExample\n\n\nInput:\n\n\nfrom mlxtend.feature_selection import SFFS\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.datasets import load_iris\n\niris = load_iris()\nX = iris.data\ny = iris.target\n\nknn = KNeighborsClassifier(n_neighbors=4)\n\nsffs = SFFS(knn, k_features=2, scoring='accuracy', cv=5)\nsffs.fit(X, y)\n\nprint('Indices of selected features:', sffs.indices_)\nprint('CV score of selected subset:', sffs.k_score_)\nprint('New feature subset:')\nsffs.transform(X)[0:5]\n\n\n\n\nOutput:\n\n\nIndices of selected features: (2, 3)\nCV score of selected subset: 0.966666666667\nNew feature subset:\nOut[7]:\narray([[ 1.4,  0.2],\n       [ 1.4,  0.2],\n       [ 1.3,  0.2],\n       [ 1.5,  0.2],\n       [ 1.4,  0.2]])\n ```\n\n<br>\n<br>\n\nAs demonstrated below, the SFFS algorithm can be a useful alternative to dimensionality reduction techniques to reduce overfitting and when the original features need to be preserved:\n\n```python\nimport matplotlib.pyplot as plt\nfrom mlxtend.data import wine_data\nfrom sklearn.preprocessing import StandardScaler\n\nscr = StandardScaler()\nX_std = scr.fit_transform(X)\n\nknn = KNeighborsClassifier(n_neighbors=4)\n\n# selecting features\nsffs = SFFS(knn, k_features=1, scoring='accuracy', cv=5)\nsffs.fit(X_std, y)\n\n# plotting performance of feature subsets\nk_feat = [len(k) for k in sffs.subsets_]\n\nplt.plot(k_feat, sffs.scores_, marker='o')\nplt.ylabel('Accuracy')\nplt.xlabel('Number of features')\nplt.show()\n\n\n\n\n\n\nGridsearch Example 1\n\n\nSelecting the number of features in a pipeline.\n\n\nimport pandas as pd\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.grid_search import GridSearchCV\nfrom mlxtend.sklearn import SFFS\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.datasets import load_iris\n\n##########################\n### Loading data\n##########################\n\niris = load_iris()\nX = iris.data\ny = iris.target\n\n##########################\n### Setting up pipeline\n##########################\nknn = KNeighborsClassifier(n_neighbors=4)\n\nsffs = SFFS(estimator=knn, k_features=2, scoring='accuracy', cv=5)\n\npipeline = Pipeline([\n            ('scr', StandardScaler()),\n            ('sel', sffs),\n            ('clf', knn)])\n\nparameters = {'sel__k_features': [1,2,3,4]}\n\ngrid_search = GridSearchCV(pipeline, parameters, n_jobs=1, verbose=1)\n\n##########################\n### Running GridSearch\n##########################\ngrid_search.fit(X, y)\n\nprint(\"Best score: %0.3f\" % grid_search.best_score_)\nprint(\"Best parameters set:\")\nbest_parameters = grid_search.best_estimator_.get_params()\nfor param_name in sorted(parameters.keys()):\n    print(\"\\t%s: %r\" % (param_name, best_parameters[param_name]))\n\n\n\n\nOutput:\n\n\n[Parallel(n_jobs=1)]: Done   1 jobs       | elapsed:    0.1s\n[Parallel(n_jobs=1)]: Done  12 out of  12 | elapsed:    1.8s finished\nFitting 3 folds for each of 4 candidates, totalling 12 fits\nBest score: 0.960\nBest parameters set:\n    sel__k_features: 1\n\n\n\n\nGridsearch Example 2\n\n\nimport pandas as pd\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.grid_search import GridSearchCV\nfrom mlxtend.sklearn import SFFS\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.datasets import load_iris\n\n##########################\n### Loading data\n##########################\n\niris = load_iris()\nX = iris.data\ny = iris.target\n\n##########################\n### Setting up pipeline\n##########################\nknn = KNeighborsClassifier(n_neighbors=4)\n\nsffs = SFFS(estimator=knn, k_features=2, scoring='accuracy', cv=5)\n\npipeline = Pipeline([\n            ('scr', StandardScaler()),\n            ('sel', sffs),\n            ('clf', knn)])\n\nparameters = {'sel__k_features': [1, 2, 3, 4],\n              'sel__estimator__n_neighbors': [4, 5, 6],\n              'clf__n_neighbors': [4, 5, 6]}\n\ngrid_search = GridSearchCV(pipeline, parameters, n_jobs=1, verbose=1)\n\n##########################\n### Running GridSearch\n##########################\ngrid_search.fit(X, y)\n\nprint(\"Best score: %0.3f\" % grid_search.best_score_)\nprint(\"Best parameters set:\")\nbest_parameters = grid_search.best_estimator_.get_params()\nfor param_name in sorted(parameters.keys()):\n    print(\"\\t%s: %r\" % (param_name, best_parameters[param_name]))\n\n\n\n\nOutput:\n\n\n[Parallel(n_jobs=1)]: Done   1 jobs       | elapsed:    0.1s\n[Parallel(n_jobs=1)]: Done  50 jobs       | elapsed:    6.7s\nFitting 3 folds for each of 36 candidates, totalling 108 fits\nBest score: 0.973\nBest parameters set:\n    clf__n_neighbors: 5\n    sel__estimator__n_neighbors: 5\n    sel__k_features: 2\n\n\n\n\nThe final feature subset can then be obtained as follows:\n\n\nprint('Best feature subset:')\ngrid_search.best_estimator_.steps[1][1].indices_\n\n\n\n\nOutput:\n\n\nBest feature subset:\n(2, 3)\n\n\n\n\nDefault Parameters\n\n\nclass SFFS(BaseEstimator, MetaEstimatorMixin):\n    \"\"\" Sequential Floating Backward Selection for feature selection.\n\n    Parameters\n    ----------\n    estimator : scikit-learn estimator object\n\n    print_progress : bool (default: True)\n       Prints progress as the number of epochs\n       to stderr.\n\n    k_features : int\n      Number of features to select where k_features.\n\n    scoring : str, (default='accuracy')\n      Scoring metric for the cross validation scorer.\n\n    cv : int (default: 5)\n      Number of folds in StratifiedKFold.\n\n    max_iter: int (default: -1)\n      Terminate early if number of `max_iter` is reached.\n\n    n_jobs : int (default: 1)\n      The number of CPUs to use for cross validation. -1 means 'all CPUs'.\n\n    Attributes\n    ----------\n    indices_ : array-like, shape = [n_predictions]\n      Indices of the selected subsets.\n\n    k_score_ : float\n      Cross validation mean score of the selected subset\n\n    subsets_ : list of lists\n      Indices of the sequentially selected subsets.\n\n    scores_ : list\n      Cross validation mean scores of the sequentially selected subsets.\n\n    Examples\n    --------\n    >>> from sklearn.neighbors import KNeighborsClassifier\n    >>> from sklearn.datasets import load_iris\n    >>> iris = load_iris()\n    >>> X = iris.data\n    >>> y = iris.target\n    >>> knn = KNeighborsClassifier(n_neighbors=4)\n    >>> sffs = SFFS(knn, k_features=2, scoring='accuracy', cv=5)\n    >>> sffs = sffs.fit(X, y)\n    >>> sffs.indices_\n    (2, 3)\n    >>> sffs.transform(X[:5])\n    array([[ 1.4,  0.2],\n           [ 1.4,  0.2],\n           [ 1.3,  0.2],\n           [ 1.5,  0.2],\n           [ 1.4,  0.2]])\n\n    >>> print('best score: %.2f' % sffs.k_score_)\n    best score: 0.97\n\n    \"\"\"",
            "title": "Sequential floating forward selection"
        },
        {
            "location": "/docs/feature_selection/sequential_floating_forward_selection/#sequential-floating-forward-selection",
            "text": "from mlxtend.feature_selection import SFFS   The Sequential Floating Forward Selection (SFFS) algorithm can be considered as an extension of the simpler  SFS  algorithm. In contrast to SFS, the SFFS algorithm has an additional exclusion step to remove features once they were included, so that a larger number of feature subset combinations can be sampled. It is important to emphasize that the removal of included features is conditional. The  conditional exclusion  in SFFS only occurs if the resulting feature subset is assessed as \"better\" by the criterion function after removal of a particular feature. Furthermore, I added an optional check to skip the conditional exclusion step if the algorithm gets stuck in cycles.    Related topics:   Sequential Forward Selection  Sequential Backward Selection  Sequential Floating Backward Selection",
            "title": "Sequential Floating Forward Selection"
        },
        {
            "location": "/docs/feature_selection/sequential_floating_forward_selection/#the-sffs-algorithm",
            "text": "Input:  the set of all features, $Y = {y_1, y_2, ..., y_d}$     The  SFFS  algorithm takes the whole feature set as input, if our feature space consists of, e.g. 10, if our feature space consists of 10 dimensions ( d = 10 ).   Output:  a subset of features, $X_k = {x_j \\; | \\;j = 1, 2, ..., k; \\; x_j \\in Y}$, where $k = (0, 1, 2, ..., d)$   The returned output of the algorithm is a subset of the feature space of a specified size. E.g., a subset of 5 features from a 10-dimensional feature space ( k = 5, d = 10 ).   Initialization:  $X_0 = Y$, $k = d$   We initialize the algorithm with an empty set (\"null set\") so that the  k = 0  (where  k  is the size of the subset)   Step 1 (Inclusion):  \n\u00a0\u00a0\u00a0\u00a0 $x^+ = \\text{ arg max } J(x_k + x), \\text{ where }  x \\in Y - X_k$ \n\u00a0\u00a0\u00a0\u00a0 $X_k+1 = X_k + x^+$ \n\u00a0\u00a0\u00a0\u00a0 $k = k + 1$   \n\u00a0\u00a0\u00a0\u00a0 Go to Step 2     Step 2 (Conditional Exclusion):  \n\u00a0\u00a0\u00a0\u00a0 $x^- = \\text{ arg max } J(x_k - x), \\text{ where } x \\in X_k$ \n\u00a0\u00a0\u00a0\u00a0$if \\; J(x_k - x) > J(x_k - x)$:   \n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 $X_k-1 = X_k - x^- $ \n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 $k = k - 1$   \n\u00a0\u00a0\u00a0\u00a0 Go to Step 1      In step 1, we include the feature from the  feature space  that leads to the best performance increase for our  feature subset  (assessed by the  criterion function ). Then, we go over to step 2  In step 2, we only remove a feature if the resulting subset would gain an increase in performance. We go back to step 1.    Steps 1 and 2 are reapeated until the  Termination  criterion is reached.   Termination:  stop when  k  equals the number of desired features",
            "title": "The SFFS Algorithm"
        },
        {
            "location": "/docs/feature_selection/sequential_floating_forward_selection/#example",
            "text": "Input:  from mlxtend.feature_selection import SFFS\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.datasets import load_iris\n\niris = load_iris()\nX = iris.data\ny = iris.target\n\nknn = KNeighborsClassifier(n_neighbors=4)\n\nsffs = SFFS(knn, k_features=2, scoring='accuracy', cv=5)\nsffs.fit(X, y)\n\nprint('Indices of selected features:', sffs.indices_)\nprint('CV score of selected subset:', sffs.k_score_)\nprint('New feature subset:')\nsffs.transform(X)[0:5]  Output:  Indices of selected features: (2, 3)\nCV score of selected subset: 0.966666666667\nNew feature subset:\nOut[7]:\narray([[ 1.4,  0.2],\n       [ 1.4,  0.2],\n       [ 1.3,  0.2],\n       [ 1.5,  0.2],\n       [ 1.4,  0.2]])\n ```\n\n<br>\n<br>\n\nAs demonstrated below, the SFFS algorithm can be a useful alternative to dimensionality reduction techniques to reduce overfitting and when the original features need to be preserved:\n\n```python\nimport matplotlib.pyplot as plt\nfrom mlxtend.data import wine_data\nfrom sklearn.preprocessing import StandardScaler\n\nscr = StandardScaler()\nX_std = scr.fit_transform(X)\n\nknn = KNeighborsClassifier(n_neighbors=4)\n\n# selecting features\nsffs = SFFS(knn, k_features=1, scoring='accuracy', cv=5)\nsffs.fit(X_std, y)\n\n# plotting performance of feature subsets\nk_feat = [len(k) for k in sffs.subsets_]\n\nplt.plot(k_feat, sffs.scores_, marker='o')\nplt.ylabel('Accuracy')\nplt.xlabel('Number of features')\nplt.show()",
            "title": "Example"
        },
        {
            "location": "/docs/feature_selection/sequential_floating_forward_selection/#gridsearch-example-1",
            "text": "Selecting the number of features in a pipeline.  import pandas as pd\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.grid_search import GridSearchCV\nfrom mlxtend.sklearn import SFFS\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.datasets import load_iris\n\n##########################\n### Loading data\n##########################\n\niris = load_iris()\nX = iris.data\ny = iris.target\n\n##########################\n### Setting up pipeline\n##########################\nknn = KNeighborsClassifier(n_neighbors=4)\n\nsffs = SFFS(estimator=knn, k_features=2, scoring='accuracy', cv=5)\n\npipeline = Pipeline([\n            ('scr', StandardScaler()),\n            ('sel', sffs),\n            ('clf', knn)])\n\nparameters = {'sel__k_features': [1,2,3,4]}\n\ngrid_search = GridSearchCV(pipeline, parameters, n_jobs=1, verbose=1)\n\n##########################\n### Running GridSearch\n##########################\ngrid_search.fit(X, y)\n\nprint(\"Best score: %0.3f\" % grid_search.best_score_)\nprint(\"Best parameters set:\")\nbest_parameters = grid_search.best_estimator_.get_params()\nfor param_name in sorted(parameters.keys()):\n    print(\"\\t%s: %r\" % (param_name, best_parameters[param_name]))  Output:  [Parallel(n_jobs=1)]: Done   1 jobs       | elapsed:    0.1s\n[Parallel(n_jobs=1)]: Done  12 out of  12 | elapsed:    1.8s finished\nFitting 3 folds for each of 4 candidates, totalling 12 fits\nBest score: 0.960\nBest parameters set:\n    sel__k_features: 1",
            "title": "Gridsearch Example 1"
        },
        {
            "location": "/docs/feature_selection/sequential_floating_forward_selection/#gridsearch-example-2",
            "text": "import pandas as pd\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.grid_search import GridSearchCV\nfrom mlxtend.sklearn import SFFS\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.datasets import load_iris\n\n##########################\n### Loading data\n##########################\n\niris = load_iris()\nX = iris.data\ny = iris.target\n\n##########################\n### Setting up pipeline\n##########################\nknn = KNeighborsClassifier(n_neighbors=4)\n\nsffs = SFFS(estimator=knn, k_features=2, scoring='accuracy', cv=5)\n\npipeline = Pipeline([\n            ('scr', StandardScaler()),\n            ('sel', sffs),\n            ('clf', knn)])\n\nparameters = {'sel__k_features': [1, 2, 3, 4],\n              'sel__estimator__n_neighbors': [4, 5, 6],\n              'clf__n_neighbors': [4, 5, 6]}\n\ngrid_search = GridSearchCV(pipeline, parameters, n_jobs=1, verbose=1)\n\n##########################\n### Running GridSearch\n##########################\ngrid_search.fit(X, y)\n\nprint(\"Best score: %0.3f\" % grid_search.best_score_)\nprint(\"Best parameters set:\")\nbest_parameters = grid_search.best_estimator_.get_params()\nfor param_name in sorted(parameters.keys()):\n    print(\"\\t%s: %r\" % (param_name, best_parameters[param_name]))  Output:  [Parallel(n_jobs=1)]: Done   1 jobs       | elapsed:    0.1s\n[Parallel(n_jobs=1)]: Done  50 jobs       | elapsed:    6.7s\nFitting 3 folds for each of 36 candidates, totalling 108 fits\nBest score: 0.973\nBest parameters set:\n    clf__n_neighbors: 5\n    sel__estimator__n_neighbors: 5\n    sel__k_features: 2  The final feature subset can then be obtained as follows:  print('Best feature subset:')\ngrid_search.best_estimator_.steps[1][1].indices_  Output:  Best feature subset:\n(2, 3)",
            "title": "Gridsearch Example 2"
        },
        {
            "location": "/docs/feature_selection/sequential_floating_forward_selection/#default-parameters",
            "text": "class SFFS(BaseEstimator, MetaEstimatorMixin):\n    \"\"\" Sequential Floating Backward Selection for feature selection.\n\n    Parameters\n    ----------\n    estimator : scikit-learn estimator object\n\n    print_progress : bool (default: True)\n       Prints progress as the number of epochs\n       to stderr.\n\n    k_features : int\n      Number of features to select where k_features.\n\n    scoring : str, (default='accuracy')\n      Scoring metric for the cross validation scorer.\n\n    cv : int (default: 5)\n      Number of folds in StratifiedKFold.\n\n    max_iter: int (default: -1)\n      Terminate early if number of `max_iter` is reached.\n\n    n_jobs : int (default: 1)\n      The number of CPUs to use for cross validation. -1 means 'all CPUs'.\n\n    Attributes\n    ----------\n    indices_ : array-like, shape = [n_predictions]\n      Indices of the selected subsets.\n\n    k_score_ : float\n      Cross validation mean score of the selected subset\n\n    subsets_ : list of lists\n      Indices of the sequentially selected subsets.\n\n    scores_ : list\n      Cross validation mean scores of the sequentially selected subsets.\n\n    Examples\n    --------\n    >>> from sklearn.neighbors import KNeighborsClassifier\n    >>> from sklearn.datasets import load_iris\n    >>> iris = load_iris()\n    >>> X = iris.data\n    >>> y = iris.target\n    >>> knn = KNeighborsClassifier(n_neighbors=4)\n    >>> sffs = SFFS(knn, k_features=2, scoring='accuracy', cv=5)\n    >>> sffs = sffs.fit(X, y)\n    >>> sffs.indices_\n    (2, 3)\n    >>> sffs.transform(X[:5])\n    array([[ 1.4,  0.2],\n           [ 1.4,  0.2],\n           [ 1.3,  0.2],\n           [ 1.5,  0.2],\n           [ 1.4,  0.2]])\n\n    >>> print('best score: %.2f' % sffs.k_score_)\n    best score: 0.97\n\n    \"\"\"",
            "title": "Default Parameters"
        },
        {
            "location": "/docs/feature_selection/column_selector/",
            "text": "mlxtend\n\nSebastian Raschka, last updated: 05/14/2015\n\n\n\n\n\nColumnSelector for Custom Feature Selection\n\n\n\n\nfrom mlxtend.feature_selection import ColumnSelector\n\n\n\n\nA feature selector for scikit-learn's Pipeline class that returns specified columns from a NumPy array; extremely useful in combination with scikit-learn's \nPipeline\n in cross-validation.\n\n\n\n\nAn example usage\n of the \nColumnSelector\n used in a pipeline for cross-validation on the Iris dataset.\n\n\n\n\n\n\n\nExample in \nPipeline\n:\n\n\nfrom mlxtend.feature_selection import ColumnSelector\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.preprocessing import StandardScaler\n\nclf_2col = Pipeline(steps=[\n    ('scaler', StandardScaler()),\n    ('reduce_dim', ColumnSelector(cols=(1,3))),    # extracts column 2 and 4\n    ('classifier', GaussianNB())   \n    ])\n\n\n\nColumnSelector\n has a \ntransform\n method that is used to select and return columns (features) from a NumPy array so that it can be used in the \nPipeline\n like other \ntransformation\n classes. \n\n\n### original data\n\nprint('First 3 rows before:\\n', X_train[:3,:])\nFirst 3 rows before:\n[[ 4.5  2.3  1.3  0.3]\n[ 6.7  3.3  5.7  2.1]\n[ 5.7  3.   4.2  1.2]]\n\n### after selection\n\ncols = ColumnExtractor(cols=(1,3)).transform(X_train)\nprint('First 3 rows:\\n', cols[:3,:])\n\nFirst 3 rows:\n[[ 2.3  0.3]\n[ 3.3  2.1]\n[ 3.   1.2]]\n\n\n\n\n\n\nDefault Parameters",
            "title": "Column selector"
        },
        {
            "location": "/docs/feature_selection/column_selector/#columnselector-for-custom-feature-selection",
            "text": "from mlxtend.feature_selection import ColumnSelector   A feature selector for scikit-learn's Pipeline class that returns specified columns from a NumPy array; extremely useful in combination with scikit-learn's  Pipeline  in cross-validation.   An example usage  of the  ColumnSelector  used in a pipeline for cross-validation on the Iris dataset.    Example in  Pipeline :  from mlxtend.feature_selection import ColumnSelector\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.preprocessing import StandardScaler\n\nclf_2col = Pipeline(steps=[\n    ('scaler', StandardScaler()),\n    ('reduce_dim', ColumnSelector(cols=(1,3))),    # extracts column 2 and 4\n    ('classifier', GaussianNB())   \n    ])  ColumnSelector  has a  transform  method that is used to select and return columns (features) from a NumPy array so that it can be used in the  Pipeline  like other  transformation  classes.   ### original data\n\nprint('First 3 rows before:\\n', X_train[:3,:])\nFirst 3 rows before:\n[[ 4.5  2.3  1.3  0.3]\n[ 6.7  3.3  5.7  2.1]\n[ 5.7  3.   4.2  1.2]]\n\n### after selection\n\ncols = ColumnExtractor(cols=(1,3)).transform(X_train)\nprint('First 3 rows:\\n', cols[:3,:])\n\nFirst 3 rows:\n[[ 2.3  0.3]\n[ 3.3  2.1]\n[ 3.   1.2]]",
            "title": "ColumnSelector for Custom Feature Selection"
        },
        {
            "location": "/docs/feature_selection/column_selector/#default-parameters",
            "text": "",
            "title": "Default Parameters"
        },
        {
            "location": "/docs/evaluate/plot_decision_regions/",
            "text": "mlxtend\n\nSebastian Raschka, last updated: 05/14/2015\n\n\n\n\n\nPlotting Decision Regions\n\n\n\n\nfrom mlxtend.evaluate import plot_decision_regions\n\n\n\n\n\n\n\n2D example\n\n\nfrom mlxtend.evaluate import plot_decision_regions\nimport matplotlib.pyplot as plt\nfrom sklearn import datasets\nfrom sklearn.svm import SVC\n\n# Loading some example data\niris = datasets.load_iris()\nX = iris.data[:, [0,2]]\ny = iris.target\n\n# Training a classifier\nsvm = SVC(C=0.5, kernel='linear')\nsvm.fit(X,y)\n\n# Plotting decision regions\nplot_decision_regions(X, y, clf=svm, res=0.02, legend=2)\n\n# Adding axes annotations\nplt.xlabel('sepal length [cm]')\nplt.ylabel('petal length [cm]')\nplt.title('SVM on Iris')\nplt.show()\n\n\n\n\n\n\n\n\n1D example\n\n\nfrom mlxtend.evaluate import plot_decision_regions\nimport matplotlib.pyplot as plt\nfrom sklearn import datasets\nfrom sklearn.svm import SVC\n\n# Loading some example data\niris = datasets.load_iris()\nX = iris.data[:, 2]\nX = X[:, None]\ny = iris.target\n\n# Training a classifier\nsvm = SVC(C=0.5, kernel='linear')\nsvm.fit(X,y)\n\n# Plotting decision regions\nplot_decision_regions(X, y, clf=svm, res=0.02, legend=2)\n\n# Adding axes annotations\nplt.xlabel('sepal length [cm]')\nplt.ylabel('petal length [cm]')\nplt.title('SVM on Iris')\nplt.show()\n\n\n\n\n\n\n\n\nHighlighting Test Data Points\n\n\nVia the \nX_highlight\n, a second dataset can be provided to highlight particular points in the dataset via a circle.\n\n\nfrom sklearn.cross_validation import train_test_split\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=0)\n\n# Training a classifier\nsvm = SVC(C=0.5, kernel='linear')\nsvm.fit(X_train, y_train)\n\n# Plotting decision regions\n\nplot_decision_regions(X, y, clf=svm, \n                  X_highlight=X_test, \n                  res=0.02, legend=2)\n\n# Adding axes annotations\nplt.xlabel('sepal length [standardized]')\nplt.ylabel('petal length [standardized]')\nplt.title('SVM on Iris')\nplt.show()\n\n\n\n\n\nFor more examples, please see this \nIPython Notebook\n.\n\n\n\n\n\nDefault Parameters\n\n\ndef plot_decision_regions(X, y, clf, X_highlight=None, res=0.02, cycle_marker=True, legend=1, cmap=None):\n    \"\"\"\n    Plots decision regions of a classifier.\n\n    Parameters\n    ----------\n    X : array-like, shape = [n_samples, n_features]\n      Feature Matrix.\n\n    y : array-like, shape = [n_samples]\n      True class labels.\n\n    clf : Classifier object. Must have a .predict method.\n\n    X_highlight : array-like, shape = [n_samples, n_features] (default: None)\n      An array with data points that are used to highlight samples in `X`.\n\n    res : float (default: 0.02)\n      Grid width. Lower values increase the resolution but\n      slow down the plotting.\n\n    cycle_marker : bool\n      Use different marker for each class.\n\n    legend : int\n      Integer to specify the legend location.\n      No legend if legend is 0.\n\n    cmap : Custom colormap object .\n\n    Returns\n    ---------\n    None\n\n    Examples\n    --------\n\n    from sklearn import datasets\n    from sklearn.svm import SVC\n\n    iris = datasets.load_iris()\n    X = iris.data[:, [0,2]]\n    y = iris.target\n\n    svm = SVC(C=1.0, kernel='linear')\n    svm.fit(X,y)\n\n    plot_decision_region(X, y, clf=svm, res=0.02, cycle_marker=True, legend=1)\n\n    plt.xlabel('sepal length [cm]')\n    plt.ylabel('petal length [cm]')\n    plt.title('SVM on Iris')\n    plt.show()\n\n    \"\"\"",
            "title": "Plot decision regions"
        },
        {
            "location": "/docs/evaluate/plot_decision_regions/#plotting-decision-regions",
            "text": "from mlxtend.evaluate import plot_decision_regions",
            "title": "Plotting Decision Regions"
        },
        {
            "location": "/docs/evaluate/plot_decision_regions/#2d-example",
            "text": "from mlxtend.evaluate import plot_decision_regions\nimport matplotlib.pyplot as plt\nfrom sklearn import datasets\nfrom sklearn.svm import SVC\n\n# Loading some example data\niris = datasets.load_iris()\nX = iris.data[:, [0,2]]\ny = iris.target\n\n# Training a classifier\nsvm = SVC(C=0.5, kernel='linear')\nsvm.fit(X,y)\n\n# Plotting decision regions\nplot_decision_regions(X, y, clf=svm, res=0.02, legend=2)\n\n# Adding axes annotations\nplt.xlabel('sepal length [cm]')\nplt.ylabel('petal length [cm]')\nplt.title('SVM on Iris')\nplt.show()",
            "title": "2D example"
        },
        {
            "location": "/docs/evaluate/plot_decision_regions/#1d-example",
            "text": "from mlxtend.evaluate import plot_decision_regions\nimport matplotlib.pyplot as plt\nfrom sklearn import datasets\nfrom sklearn.svm import SVC\n\n# Loading some example data\niris = datasets.load_iris()\nX = iris.data[:, 2]\nX = X[:, None]\ny = iris.target\n\n# Training a classifier\nsvm = SVC(C=0.5, kernel='linear')\nsvm.fit(X,y)\n\n# Plotting decision regions\nplot_decision_regions(X, y, clf=svm, res=0.02, legend=2)\n\n# Adding axes annotations\nplt.xlabel('sepal length [cm]')\nplt.ylabel('petal length [cm]')\nplt.title('SVM on Iris')\nplt.show()",
            "title": "1D example"
        },
        {
            "location": "/docs/evaluate/plot_decision_regions/#highlighting-test-data-points",
            "text": "Via the  X_highlight , a second dataset can be provided to highlight particular points in the dataset via a circle.  from sklearn.cross_validation import train_test_split\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=0)\n\n# Training a classifier\nsvm = SVC(C=0.5, kernel='linear')\nsvm.fit(X_train, y_train)\n\n# Plotting decision regions\n\nplot_decision_regions(X, y, clf=svm, \n                  X_highlight=X_test, \n                  res=0.02, legend=2)\n\n# Adding axes annotations\nplt.xlabel('sepal length [standardized]')\nplt.ylabel('petal length [standardized]')\nplt.title('SVM on Iris')\nplt.show()   For more examples, please see this  IPython Notebook .",
            "title": "Highlighting Test Data Points"
        },
        {
            "location": "/docs/evaluate/plot_decision_regions/#default-parameters",
            "text": "def plot_decision_regions(X, y, clf, X_highlight=None, res=0.02, cycle_marker=True, legend=1, cmap=None):\n    \"\"\"\n    Plots decision regions of a classifier.\n\n    Parameters\n    ----------\n    X : array-like, shape = [n_samples, n_features]\n      Feature Matrix.\n\n    y : array-like, shape = [n_samples]\n      True class labels.\n\n    clf : Classifier object. Must have a .predict method.\n\n    X_highlight : array-like, shape = [n_samples, n_features] (default: None)\n      An array with data points that are used to highlight samples in `X`.\n\n    res : float (default: 0.02)\n      Grid width. Lower values increase the resolution but\n      slow down the plotting.\n\n    cycle_marker : bool\n      Use different marker for each class.\n\n    legend : int\n      Integer to specify the legend location.\n      No legend if legend is 0.\n\n    cmap : Custom colormap object .\n\n    Returns\n    ---------\n    None\n\n    Examples\n    --------\n\n    from sklearn import datasets\n    from sklearn.svm import SVC\n\n    iris = datasets.load_iris()\n    X = iris.data[:, [0,2]]\n    y = iris.target\n\n    svm = SVC(C=1.0, kernel='linear')\n    svm.fit(X,y)\n\n    plot_decision_region(X, y, clf=svm, res=0.02, cycle_marker=True, legend=1)\n\n    plt.xlabel('sepal length [cm]')\n    plt.ylabel('petal length [cm]')\n    plt.title('SVM on Iris')\n    plt.show()\n\n    \"\"\"",
            "title": "Default Parameters"
        },
        {
            "location": "/docs/evaluate/plot_learning_curves/",
            "text": "mlxtend\n\nSebastian Raschka, last updated: 05/14/2015\n\n\n\n\n\nPlotting Learning Curves\n\n\n\n\nfrom mlxtend.evaluate import plot_learning_curves\n\n\n\n\nA function to plot learning curves for classifiers. Learning curves are extremely useful to analyze if a model is suffering from over- or under-fitting (high variance or high bias). The function can be imported via\n\n\n\n\n\nExample 1 - Training Samples\n\n\nfrom mlxtend.evaluate import plot_learning_curves\nfrom sklearn import datasets\nfrom sklearn.cross_validation import train_test_split\n\n# Loading some example data\niris = datasets.load_iris()\nX = iris.data\ny = iris.target\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.6, random_seed=2)\n\nfrom sklearn.tree import DecisionTreeClassifier\nimport numpy as np\n\nclf = DecisionTreeClassifier(max_depth=1)\n\nplot_learning_curves(X_train, y_train, X_test, y_test, clf, kind='training_size')\nplt.show()\n\nplot_learning_curves(X_train, y_train, X_test, y_test, clf, kind='n_features')\nplt.show()\n\n\n\n\n\n\n\n\nExample 2 - Features\n\n\nplot_learning_curves(X_train, y_train, X_test, y_test, clf, kind='n_features')\nplt.show()\n\n\n\n\n\nFor more examples, please see this \nIPython Notebook\n\n\n\n\n\nDefault Parameters\n\n\ndef plot_learning_curves(X_train, y_train, X_test, y_test, clf, kind='training_size',\n            marker='o', scoring='misclassification error', suppress_plot=False, print_model=True):\n    \"\"\"\n    Plots learning curves of a classifier.\n\n    Parameters\n    ----------\n    X_train : array-like, shape = [n_samples, n_features]\n      Feature matrix of the training dataset.\n\n    y_train : array-like, shape = [n_samples]\n      True class labels of the training dataset.\n\n    X_test : array-like, shape = [n_samples, n_features]\n      Feature matrix of the test dataset.\n\n    y_test : array-like, shape = [n_samples]\n      True class labels of the test dataset.\n\n    clf : Classifier object. Must have a .predict .fit method.\n\n    kind : str (default: 'training_size')\n      'training_size' or 'n_features'\n      Plots missclassifications vs. training size or number of features.\n\n    marker : str (default: 'o')\n      Marker for the line plot.\n\n    scoring : str (default: 'misclassification error')\n      If not 'accuracy', accepts the following metrics (from scikit-learn):\n      {'accuracy', 'average_precision', 'f1_micro', 'f1_macro',\n      'f1_weighted', 'f1_samples', 'log_loss', 'precision', 'recall', 'roc_auc',\n      'adjusted_rand_score', 'mean_absolute_error', 'mean_squared_error',\n      'median_absolute_error', 'r2'}\n\n    suppress_plot=False : bool (default: False)\n      Suppress matplotlib plots if True. Recommended\n      for testing purposes.\n\n    print_model : bool (default: True)\n      Print model parameters in plot title if True.\n\n    Returns\n    ---------\n    (training_error, test_error): tuple of lists\n    \"\"\"",
            "title": "Plot learning curves"
        },
        {
            "location": "/docs/evaluate/plot_learning_curves/#plotting-learning-curves",
            "text": "from mlxtend.evaluate import plot_learning_curves   A function to plot learning curves for classifiers. Learning curves are extremely useful to analyze if a model is suffering from over- or under-fitting (high variance or high bias). The function can be imported via",
            "title": "Plotting Learning Curves"
        },
        {
            "location": "/docs/evaluate/plot_learning_curves/#example-1-training-samples",
            "text": "from mlxtend.evaluate import plot_learning_curves\nfrom sklearn import datasets\nfrom sklearn.cross_validation import train_test_split\n\n# Loading some example data\niris = datasets.load_iris()\nX = iris.data\ny = iris.target\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.6, random_seed=2)\n\nfrom sklearn.tree import DecisionTreeClassifier\nimport numpy as np\n\nclf = DecisionTreeClassifier(max_depth=1)\n\nplot_learning_curves(X_train, y_train, X_test, y_test, clf, kind='training_size')\nplt.show()\n\nplot_learning_curves(X_train, y_train, X_test, y_test, clf, kind='n_features')\nplt.show()",
            "title": "Example 1 - Training Samples"
        },
        {
            "location": "/docs/evaluate/plot_learning_curves/#example-2-features",
            "text": "plot_learning_curves(X_train, y_train, X_test, y_test, clf, kind='n_features')\nplt.show()   For more examples, please see this  IPython Notebook",
            "title": "Example 2 - Features"
        },
        {
            "location": "/docs/evaluate/plot_learning_curves/#default-parameters",
            "text": "def plot_learning_curves(X_train, y_train, X_test, y_test, clf, kind='training_size',\n            marker='o', scoring='misclassification error', suppress_plot=False, print_model=True):\n    \"\"\"\n    Plots learning curves of a classifier.\n\n    Parameters\n    ----------\n    X_train : array-like, shape = [n_samples, n_features]\n      Feature matrix of the training dataset.\n\n    y_train : array-like, shape = [n_samples]\n      True class labels of the training dataset.\n\n    X_test : array-like, shape = [n_samples, n_features]\n      Feature matrix of the test dataset.\n\n    y_test : array-like, shape = [n_samples]\n      True class labels of the test dataset.\n\n    clf : Classifier object. Must have a .predict .fit method.\n\n    kind : str (default: 'training_size')\n      'training_size' or 'n_features'\n      Plots missclassifications vs. training size or number of features.\n\n    marker : str (default: 'o')\n      Marker for the line plot.\n\n    scoring : str (default: 'misclassification error')\n      If not 'accuracy', accepts the following metrics (from scikit-learn):\n      {'accuracy', 'average_precision', 'f1_micro', 'f1_macro',\n      'f1_weighted', 'f1_samples', 'log_loss', 'precision', 'recall', 'roc_auc',\n      'adjusted_rand_score', 'mean_absolute_error', 'mean_squared_error',\n      'median_absolute_error', 'r2'}\n\n    suppress_plot=False : bool (default: False)\n      Suppress matplotlib plots if True. Recommended\n      for testing purposes.\n\n    print_model : bool (default: True)\n      Print model parameters in plot title if True.\n\n    Returns\n    ---------\n    (training_error, test_error): tuple of lists\n    \"\"\"",
            "title": "Default Parameters"
        },
        {
            "location": "/docs/plotting/category_scatter/",
            "text": "mlxtend\n\nSebastian Raschka, last updated: 05/14/2015\n\n\n\n\n\nCategory Scatter\n\n\n\n\nfrom mlxtend.plotting import category_scatter\n\n\n\n\nA function to quickly produce a scatter plot colored by categories from a pandas \nDataFrame\n or NumPy \nndarray\n object.\n\n\n\n\n\nExample\n\n\nLoading an example dataset as pandas \nDataFrame\n:   \n\n\nimport pandas as pd\n\ndf = pd.read_csv('/Users/sebastian/Desktop/data.csv')\ndf.head()\n\n\n\n\n\nPlotting the data where the categories are determined by the unique values in the label column \nlabel_col\n. The \nx\n and \ny\n values are simply the column names of the DataFrame that we want to plot.\n\n\nimport matplotlib.pyplot as plt\nfrom mlxtend.plotting import category_scatter\n\ncategory_scatter(x='x', y='y', label_col='label', data=df)\n\nplt.legend(loc='best')\n\n\n\n\n\nSimilarly, we can also use NumPy arrays. E.g.,\n\n\nX =\n\narray([['class1', 10.0, 8.04],\n   ['class1', 8.0, 6.95],\n   ['class1', 13.2, 7.58],\n   ['class1', 9.0, 8.81],\n    ...\n   ['class4', 8.0, 5.56],\n   ['class4', 8.0, 7.91],\n   ['class4', 8.0, 6.89]], dtype=object)\n\n\n\nWhere the \nx\n, \ny\n, and \nlabel_col\n refer to the respective column indices in the array:\n\n\ncategory_scatter(x=1, y=2, label_col=0, data=df.values)\n\nplt.legend(loc='best')\n\n\n\n\n\n\n\n\nDefault Parameters\n\n\ndef category_scatter(x, y, label_col, data,\n            markers='sxo^v',\n            colors=('blue', 'green', 'red', 'purple', 'gray', 'cyan'),\n            alpha=0.7, markersize=20.0):\n\n    \"\"\"\n    Scatter plot to plot categories in different colors/markerstyles.\n\n    Parameters\n    ----------\n    x : str or int\n      DataFrame column name of the x-axis values or\n      integer for the numpy ndarray column index.\n\n    y : str\n      DataFrame column name of the y-axis values or\n      integer for the numpy ndarray column index\n\n    data : Pandas DataFrame object or NumPy ndarray.\n\n    markers : str\n      Markers that are cycled through the label category.\n\n    colors : tuple \n      Colors that are cycled through the label category.\n\n    alpha : float (default: 0.7)\n      Parameter to control the transparency.\n\n    markersize : float (default : 20.0)\n      Parameter to control the marker size.\n\n    Returns\n    ---------\n    None\n\n    \"\"\"",
            "title": "Category scatter"
        },
        {
            "location": "/docs/plotting/category_scatter/#category-scatter",
            "text": "from mlxtend.plotting import category_scatter   A function to quickly produce a scatter plot colored by categories from a pandas  DataFrame  or NumPy  ndarray  object.",
            "title": "Category Scatter"
        },
        {
            "location": "/docs/plotting/category_scatter/#example",
            "text": "Loading an example dataset as pandas  DataFrame :     import pandas as pd\n\ndf = pd.read_csv('/Users/sebastian/Desktop/data.csv')\ndf.head()   Plotting the data where the categories are determined by the unique values in the label column  label_col . The  x  and  y  values are simply the column names of the DataFrame that we want to plot.  import matplotlib.pyplot as plt\nfrom mlxtend.plotting import category_scatter\n\ncategory_scatter(x='x', y='y', label_col='label', data=df)\n\nplt.legend(loc='best')   Similarly, we can also use NumPy arrays. E.g.,  X =\n\narray([['class1', 10.0, 8.04],\n   ['class1', 8.0, 6.95],\n   ['class1', 13.2, 7.58],\n   ['class1', 9.0, 8.81],\n    ...\n   ['class4', 8.0, 5.56],\n   ['class4', 8.0, 7.91],\n   ['class4', 8.0, 6.89]], dtype=object)  Where the  x ,  y , and  label_col  refer to the respective column indices in the array:  category_scatter(x=1, y=2, label_col=0, data=df.values)\n\nplt.legend(loc='best')",
            "title": "Example"
        },
        {
            "location": "/docs/plotting/category_scatter/#default-parameters",
            "text": "def category_scatter(x, y, label_col, data,\n            markers='sxo^v',\n            colors=('blue', 'green', 'red', 'purple', 'gray', 'cyan'),\n            alpha=0.7, markersize=20.0):\n\n    \"\"\"\n    Scatter plot to plot categories in different colors/markerstyles.\n\n    Parameters\n    ----------\n    x : str or int\n      DataFrame column name of the x-axis values or\n      integer for the numpy ndarray column index.\n\n    y : str\n      DataFrame column name of the y-axis values or\n      integer for the numpy ndarray column index\n\n    data : Pandas DataFrame object or NumPy ndarray.\n\n    markers : str\n      Markers that are cycled through the label category.\n\n    colors : tuple \n      Colors that are cycled through the label category.\n\n    alpha : float (default: 0.7)\n      Parameter to control the transparency.\n\n    markersize : float (default : 20.0)\n      Parameter to control the marker size.\n\n    Returns\n    ---------\n    None\n\n    \"\"\"",
            "title": "Default Parameters"
        },
        {
            "location": "/docs/plotting/enrichment_plot/",
            "text": "mlxtend\n\nSebastian Raschka, last updated: 05/14/2015\n\n\n\n\n\nEnrichment Plot\n\n\n\n\nfrom mlxtend.plotting import enrichment_plot\n\n\n\n\nA function to plot step plots of cumulative counts.\n\n\n\n\n\nExample\n\n\nCreating an example  \nDataFrame\n:   \n\n\nimport pandas as pd\ns1 = [1.1, 1.5]\ns2 = [2.1, 1.8]\ns3 = [3.1, 2.1]\ns4 = [3.9, 2.5]\ndata = [s1, s2, s3, s4]\ndf = pd.DataFrame(data, columns=['X1', 'X2'])\ndf\n\n\n\n\n\nPlotting the enrichment plot. The y-axis can be interpreted as \"how many samples are less or equal to the corresponding x-axis label.\"\n\n\nfrom mlxtend.plotting import enrichment_plot\nenrichment_plot(df)\n\n\n\n\n\n\n\n\nDefault Parameters\n\n\n\ndef enrichment_plot(df, colors='bgrkcy', markers=' ', linestyles='-', alpha=0.5, lw=2,\n                    legend=True, where='post', grid=True, count_label='Count',\n                    xlim='auto', ylim='auto', invert_axes=False, ax=None):\n    \"\"\"\n    Function to plot stacked barplots\n\n    Parameters\n    ----------\n    df : pandas.DataFrame\n      A pandas DataFrame where columns represent the different categories.\n\n    colors: str (default: 'bgrcky')\n      The colors of the bars.\n\n    markers: str (default: ' ')\n      Matplotlib markerstyles, e.g,\n      'sov' for square,circle, and triangle markers.\n\n    linestyles: str (default: '-')\n      Matplotlib linestyles, e.g., \n      '-,--' to cycle normal and dashed lines. Note\n      that the different linestyles need to be separated by commas.\n\n    alpha: float (default: 0.5)\n      Transparency level from 0.0 to 1.0.\n\n    lw: int or float (default: 2)\n      Linewidth parameter.\n\n    legend: bool (default: True)\n      Plots legend if True.\n\n    where: {'post', 'pre', 'mid'} (default: 'post')\n      Starting location of the steps.\n\n    grid: bool (default: True)\n      Plots a grid if True.\n\n    count_label: str (default: 'Count')\n      Label for the \"Count\"-axis.\n\n    xlim: 'auto' or array-like [min, max]\n      Min and maximum position of the x-axis range.\n\n    ylim: 'auto' or array-like [min, max]\n      Min and maximum position of the y-axis range.\n\n    invert_axes: bool (default: False)\n      Plots count on the x-axis if True.\n\n    ax: matplotlib axis, optional (default: None)\n      Use this axis for plotting or make a new one otherwise\n\n    Returns\n    ----------\n    ax: matplotlib axis\n\n    \"\"\"",
            "title": "Enrichment plot"
        },
        {
            "location": "/docs/plotting/enrichment_plot/#enrichment-plot",
            "text": "from mlxtend.plotting import enrichment_plot   A function to plot step plots of cumulative counts.",
            "title": "Enrichment Plot"
        },
        {
            "location": "/docs/plotting/enrichment_plot/#example",
            "text": "Creating an example   DataFrame :     import pandas as pd\ns1 = [1.1, 1.5]\ns2 = [2.1, 1.8]\ns3 = [3.1, 2.1]\ns4 = [3.9, 2.5]\ndata = [s1, s2, s3, s4]\ndf = pd.DataFrame(data, columns=['X1', 'X2'])\ndf   Plotting the enrichment plot. The y-axis can be interpreted as \"how many samples are less or equal to the corresponding x-axis label.\"  from mlxtend.plotting import enrichment_plot\nenrichment_plot(df)",
            "title": "Example"
        },
        {
            "location": "/docs/plotting/enrichment_plot/#default-parameters",
            "text": "def enrichment_plot(df, colors='bgrkcy', markers=' ', linestyles='-', alpha=0.5, lw=2,\n                    legend=True, where='post', grid=True, count_label='Count',\n                    xlim='auto', ylim='auto', invert_axes=False, ax=None):\n    \"\"\"\n    Function to plot stacked barplots\n\n    Parameters\n    ----------\n    df : pandas.DataFrame\n      A pandas DataFrame where columns represent the different categories.\n\n    colors: str (default: 'bgrcky')\n      The colors of the bars.\n\n    markers: str (default: ' ')\n      Matplotlib markerstyles, e.g,\n      'sov' for square,circle, and triangle markers.\n\n    linestyles: str (default: '-')\n      Matplotlib linestyles, e.g., \n      '-,--' to cycle normal and dashed lines. Note\n      that the different linestyles need to be separated by commas.\n\n    alpha: float (default: 0.5)\n      Transparency level from 0.0 to 1.0.\n\n    lw: int or float (default: 2)\n      Linewidth parameter.\n\n    legend: bool (default: True)\n      Plots legend if True.\n\n    where: {'post', 'pre', 'mid'} (default: 'post')\n      Starting location of the steps.\n\n    grid: bool (default: True)\n      Plots a grid if True.\n\n    count_label: str (default: 'Count')\n      Label for the \"Count\"-axis.\n\n    xlim: 'auto' or array-like [min, max]\n      Min and maximum position of the x-axis range.\n\n    ylim: 'auto' or array-like [min, max]\n      Min and maximum position of the y-axis range.\n\n    invert_axes: bool (default: False)\n      Plots count on the x-axis if True.\n\n    ax: matplotlib axis, optional (default: None)\n      Use this axis for plotting or make a new one otherwise\n\n    Returns\n    ----------\n    ax: matplotlib axis\n\n    \"\"\"",
            "title": "Default Parameters"
        },
        {
            "location": "/docs/plotting/stacked_barplot/",
            "text": "mlxtend\n\nSebastian Raschka, last updated: 05/14/2015\n\n\n\n\n\nStacked Barplot\n\n\n\n\nfrom mlxtend.plotting import stacked_barplot\n\n\n\n\nA function to conveniently plot stacked bar plots in matplotlib using pandas \nDataFrame\ns. \n\n\n\n\n\nExample\n\n\nCreating an example  \nDataFrame\n:   \n\n\nimport pandas as pd\n\ns1 = [1.0, 2.0, 3.0, 4.0]\ns2 = [1.4, 2.1, 2.9, 5.1]\ns3 = [1.9, 2.2, 3.5, 4.1]\ns4 = [1.4, 2.5, 3.5, 4.2]\ndata = [s1, s2, s3, s4]\n\ndf = pd.DataFrame(data, columns=['X1', 'X2', 'X3', 'X4'])\ndf.columns = ['X1', 'X2', 'X3', 'X4']\ndf.index = ['Sample1', 'Sample2', 'Sample3', 'Sample4']\ndf\n\n\n\n\n\nPlotting the stacked barplot. By default, the index of the \nDataFrame\n is used as column labels, and the \nDataFrame\n columns are used for the plot legend.\n\n\nfrom mlxtend.plotting import stacked_barplot\n\nstacked_barplot(df, rotation=45)\n\n\n\n\n\nDefault Parameters\n\n\ndef stacked_barplot(df, bar_width='auto', colors='bgrcky', labels='index', rotation=90, legend=True):\n    \"\"\"\n    Function to plot stacked barplots\n\n    Parameters\n    ----------\n    df : pandas.DataFrame\n      A pandas DataFrame where the index denotes the\n      x-axis labels, and the columns contain the different\n      measurements for each row.\n\n    bar_width: 'auto' or float (default: 'auto')\n      Parameter to set the widths of the bars. if\n      'auto', the width is automatically determined by\n      the number of columns in the dataset.\n\n    colors: str (default: 'bgrcky')\n      The colors of the bars.\n\n    labels: 'index' or iterable (default: 'index')\n      If 'index', the DataFrame index will be used as\n      x-tick labels.\n\n    rotation: int (default: 90)\n      Parameter to rotate the x-axis labels.\n\n    legend: bool (default: True)\n      Parameter to plot the legend.\n\n    Returns\n    ----------\n    None\n\n    \"\"\"",
            "title": "Stacked barplot"
        },
        {
            "location": "/docs/plotting/stacked_barplot/#stacked-barplot",
            "text": "from mlxtend.plotting import stacked_barplot   A function to conveniently plot stacked bar plots in matplotlib using pandas  DataFrame s.",
            "title": "Stacked Barplot"
        },
        {
            "location": "/docs/plotting/stacked_barplot/#example",
            "text": "Creating an example   DataFrame :     import pandas as pd\n\ns1 = [1.0, 2.0, 3.0, 4.0]\ns2 = [1.4, 2.1, 2.9, 5.1]\ns3 = [1.9, 2.2, 3.5, 4.1]\ns4 = [1.4, 2.5, 3.5, 4.2]\ndata = [s1, s2, s3, s4]\n\ndf = pd.DataFrame(data, columns=['X1', 'X2', 'X3', 'X4'])\ndf.columns = ['X1', 'X2', 'X3', 'X4']\ndf.index = ['Sample1', 'Sample2', 'Sample3', 'Sample4']\ndf   Plotting the stacked barplot. By default, the index of the  DataFrame  is used as column labels, and the  DataFrame  columns are used for the plot legend.  from mlxtend.plotting import stacked_barplot\n\nstacked_barplot(df, rotation=45)",
            "title": "Example"
        },
        {
            "location": "/docs/plotting/stacked_barplot/#default-parameters",
            "text": "def stacked_barplot(df, bar_width='auto', colors='bgrcky', labels='index', rotation=90, legend=True):\n    \"\"\"\n    Function to plot stacked barplots\n\n    Parameters\n    ----------\n    df : pandas.DataFrame\n      A pandas DataFrame where the index denotes the\n      x-axis labels, and the columns contain the different\n      measurements for each row.\n\n    bar_width: 'auto' or float (default: 'auto')\n      Parameter to set the widths of the bars. if\n      'auto', the width is automatically determined by\n      the number of columns in the dataset.\n\n    colors: str (default: 'bgrcky')\n      The colors of the bars.\n\n    labels: 'index' or iterable (default: 'index')\n      If 'index', the DataFrame index will be used as\n      x-tick labels.\n\n    rotation: int (default: 90)\n      Parameter to rotate the x-axis labels.\n\n    legend: bool (default: True)\n      Parameter to plot the legend.\n\n    Returns\n    ----------\n    None\n\n    \"\"\"",
            "title": "Default Parameters"
        },
        {
            "location": "/docs/regression/plot_linear_regression_fit/",
            "text": "mlxtend\n\nSebastian Raschka, last updated: 05/14/2015\n\n\n\n\n\nPlotting Linear Regression Fits\n\n\n\n\nfrom mlxtend.regression import lin_regplot\n\n\n\n\nlin_regplot\n is a function to plot linear regression fits. \nBy default \nlin_regplot\n uses scikit-learn's \nlinear_model.LinearRegression\n to fit the model and SciPy's \nstats.pearsonr\n to calculate the correlation coefficient. \n\n\n\n\n\nExample\n\n\nimport matplotlib.pyplot as plt\nfrom mlxtend.regression import lin_regplot\nimport numpy as np\n\nX = np.array([4, 8, 13, 26, 31, 10, 8, 30, 18, 12, 20, 5, 28, 18, 6, 31, 12,\n   12, 27, 11, 6, 14, 25, 7, 13,4, 15, 21, 15])\n\ny = np.array([14, 24, 22, 59, 66, 25, 18, 60, 39, 32, 53, 18, 55, 41, 28, 61, 35,\n   36, 52, 23, 19, 25, 73, 16, 32, 14, 31, 43, 34])\n\nintercept, slope, corr_coeff = lin_regplot(X, y,)\nplt.show()\n\n\n\n\n\nDefault Parameters\n\n\ndef lin_regplot(X, \n             y, \n             model=LinearRegression(), \n             corr_func=pearsonr,\n             scattercolor='blue', \n             fit_style='k--', \n             legend=True,\n             xlim='auto'):\n    \"\"\"\n    Function to plot a linear regression line fit.\n\n\nParameters\n----------\nX : numpy array, shape (n_samples,)\n  Samples.\n\ny : numpy array, shape (n_samples,)\n  Target values\n\nmodel: object (default: sklearn.linear_model.LinearRegression)\n  Estimator object for regression. Must implement\n  a .fit() and .predict() method.\n\ncorr_func: function (default: scipy.stats.pearsonr)\n  function to calculate the regression\n  slope.\n\nscattercolor: string (default: blue)\n  Color of scatter plot points.\n\nfit_style: string (default: k--) \n  Style for the line fit.\n\nlegend: bool (default: True)\n  Plots legend with corr_coeff coef., \n  fit coef., and intercept values.\n\nxlim: array-like (x_min, x_max) or 'auto' (default: 'auto')\n  X-axis limits for the linear line fit.\n\nReturns\n----------\nintercept, slope, corr_coeff: float, float, float\n\n\"\"\"</pre>",
            "title": "Plot linear regression fit"
        },
        {
            "location": "/docs/regression/plot_linear_regression_fit/#plotting-linear-regression-fits",
            "text": "from mlxtend.regression import lin_regplot   lin_regplot  is a function to plot linear regression fits. \nBy default  lin_regplot  uses scikit-learn's  linear_model.LinearRegression  to fit the model and SciPy's  stats.pearsonr  to calculate the correlation coefficient.",
            "title": "Plotting Linear Regression Fits"
        },
        {
            "location": "/docs/regression/plot_linear_regression_fit/#example",
            "text": "import matplotlib.pyplot as plt\nfrom mlxtend.regression import lin_regplot\nimport numpy as np\n\nX = np.array([4, 8, 13, 26, 31, 10, 8, 30, 18, 12, 20, 5, 28, 18, 6, 31, 12,\n   12, 27, 11, 6, 14, 25, 7, 13,4, 15, 21, 15])\n\ny = np.array([14, 24, 22, 59, 66, 25, 18, 60, 39, 32, 53, 18, 55, 41, 28, 61, 35,\n   36, 52, 23, 19, 25, 73, 16, 32, 14, 31, 43, 34])\n\nintercept, slope, corr_coeff = lin_regplot(X, y,)\nplt.show()",
            "title": "Example"
        },
        {
            "location": "/docs/regression/plot_linear_regression_fit/#default-parameters",
            "text": "def lin_regplot(X, \n             y, \n             model=LinearRegression(), \n             corr_func=pearsonr,\n             scattercolor='blue', \n             fit_style='k--', \n             legend=True,\n             xlim='auto'):\n    \"\"\"\n    Function to plot a linear regression line fit.  Parameters\n----------\nX : numpy array, shape (n_samples,)\n  Samples.\n\ny : numpy array, shape (n_samples,)\n  Target values\n\nmodel: object (default: sklearn.linear_model.LinearRegression)\n  Estimator object for regression. Must implement\n  a .fit() and .predict() method.\n\ncorr_func: function (default: scipy.stats.pearsonr)\n  function to calculate the regression\n  slope.\n\nscattercolor: string (default: blue)\n  Color of scatter plot points.\n\nfit_style: string (default: k--) \n  Style for the line fit.\n\nlegend: bool (default: True)\n  Plots legend with corr_coeff coef., \n  fit coef., and intercept values.\n\nxlim: array-like (x_min, x_max) or 'auto' (default: 'auto')\n  X-axis limits for the linear line fit.\n\nReturns\n----------\nintercept, slope, corr_coeff: float, float, float\n\n\"\"\"</pre>",
            "title": "Default Parameters"
        },
        {
            "location": "/docs/regression/linear_regression/",
            "text": "mlxtend\nSebastian Raschka, last updated: 06/18/2015\n\n\nLinear Regression\n\n\n\n\nfrom mlxtend.regression import LinearRegression\n\n\n\n\nImplementation of a linear regression model for performing ordinary least squares regression using one of the following three approaches:\n\n\n\n\nnormal equation (closed-form solution)\n\n\ngradient descent\n\n\nstochastic gradient descent \n\n\n\n\nFor more usage examples please see the \nIPython Notebook\n.\n\n\n\n\n\n\n\nExample 1 - Normal Equation\n\n\nThe closed-form solution should be preferred for \"smaller\" datasets where calculating (a \"costly\") is not a concern. For very large datasets, or datasets where the inverse of [\nX\n^T \nX\n] may not exist (the matrix is non-invertible or singular, e.g., in case of perfect multicollinearity), the gradient descent or stochastic gradient descent approaches are to be preferred.\n\n\nThe linear function (linear regression model) is defined as:\n\n\n\n\nwhere \ny\n is the response variable, \nx\n is an \nm\n-dimensional sample vector, and \nw\n is the weight vector (vector of coefficients). Note that \nw_0\n represents the y-axis intercept of the model and therefore \nx_0\n=1.  \n\n\nUsing the closed-form solution (normal equation), we can calculate the weights of the model as follows:\n\n\n\n\nwhere \nX\n is the matrix of the data samples. After obtaining the weight, the intercept (w_0) can be calculated from the difference of the true and predicted average responses: \n\n\n\n\n>>> import numpy as np\n>>> X = np.array([ 1, 2, 3, 4, 5])[:, np.newaxis]\n>>> y = np.array([ 1, 2, 3, 4, 5])\n\n>>> import matplotlib.pyplot as plt\n>>> def lin_regplot(X, y, model):\n...     plt.scatter(X, y, c='blue')\n...     plt.plot(X, model.predict(X), color='red')    \n...     return\n\n>>> ne_lr = LinearRegression(solver='normal_equation')\n>>> ne_lr.fit(X, y)\n>>> print('Intercept: %.2f' % ne_lr.w_[0])\nIntercept: 0.00\n>>> print('Slope: %.2f' % ne_lr.w_[1])\nSlope: 1.00\n\nlin_regplot(X, y, ne_lr)\nplt.show()\n\n\n\n\n\n\n\nExample 2 - Gradient Descent\n\n\nUsing the gradient decent optimization algorithm, the weights are updated incrementally after each epoch (= pass over the training dataset).\n\n\n\n\n>>> gd_lr = LinearRegression(solver='gd', eta=0.005, epochs=1500, random_seed=0)\n>>> gd_lr.fit(X, y)\n>>> print('Intercept: %.2f' % gd_lr.w_[0])\nIntercept: 0.00\n>>> print('Slope: %.2f' % gd_lr.w_[1])\nSlope: 1.00\n\n\n\nVisualizing the cost to check for convergence and plotting the linear model:\n\n\n>>> plt.plot(range(1, gd_lr.epochs+1), gd_lr.cost_)\n>>> plt.xlabel('Epochs')\n>>> plt.ylabel('Cost')\n>>> plt.tight_layout()\n>>> plt.show()\n\n\n\n\n\n>>>  lin_regplot(X, y, gd_lr)\n>>>  plt.show()\n\n\n\n\n\nTip\n: I you are using gradient descent, consider standardizing the variables for better convergence of the algorithm.\n\n\n>>> X_std = (X - np.mean(X)) / X.std()\n>>> y_std = (y - np.mean(y)) / y.std()\n\n>>> gd_lr = LinearRegression(solver='gd', eta=0.1, epochs=10, random_seed=0)\n>>> gd_lr.fit(X_std, y_std)\n>>> print('Intercept: %.2f' % gd_lr.w_[0])\nIntercept: 0.00\n>>> print('Slope: %.2f' % gd_lr.w_[1])\nSlope: 1.00\n\n\n\n\n\n\n\n\n\nExample 3 - Stochastic Gradient Descent\n\n\nExample 2 explained \"batch\" gradient descent learning. The \"batch\" updates refers to the fact that the cost function is minimized based on the complete training data set. In contrast, stochastic gradient descent performs the weight update incrementally after each individual training sample.\n\n\nThe process of incrementally updating the weights is also called \"stochastic\" gradient descent since it approximates the minimization of the cost function. Although the stochastic gradient descent approach might sound inferior to gradient descent due its \"stochastic\" nature and the \"approximated\" direction (gradient), it can have certain advantages in practice. Often, stochastic gradient descent converges much faster than gradient descent since the updates are applied immediately after each training sample; stochastic gradient descent is computationally more efficient, especially for very large datasets. \n\n\n>>> sgd_lr = LinearRegression(solver='sgd', eta=0.1, epochs=10, random_seed=0)\n>>> sgd_lr.fit(X_std, y_std)\n>>> print('Intercept: %.2f' % sgd_lr.w_[0])\nIntercept: 0.00\n>>> print('Slope: %.2f' % sgd_lr.w_[1])\nSlope: 1.00\n\n>>> plt.plot(range(1, sgd_lr.epochs+1), sgd_lr.cost_)\n>>> plt.xlabel('Epochs')\n>>> plt.ylabel('Cost')\n>>> plt.tight_layout()\n>>> plt.show()\n\n\n\n\n\n>>> plt.plot(range(1, sgd_lr.epochs+1), sgd_lr.cost_)\n>>> plt.xlabel('Epochs')\n>>> plt.ylabel('Cost')\n>>> plt.tight_layout()\n>>> plt.show()\n\n\n\n \n\n\n\n\n\nDefault Parameters\n\n\n    class LinearRegression(object):\n        \"\"\" Ordinary least squares linear regression.\n\n        Parameters\n        ------------\n\n        solver : {'gd', 'sgd', 'normal_equation'} (default: 'normal_equation')\n          Method for solving the cost function. 'gd' for gradient descent, \n          'sgd' for stochastic gradient descent, or 'normal_equation' (default)\n          to solve the cost function analytically.\n\n        eta : float (default: 0.1)\n          Learning rate (between 0.0 and 1.0); \n          ignored if solver='normal_equation'.\n\n        epochs : int (default: 50)\n          Passes over the training dataset; \n          ignored if solver='normal_equation'.\n\n        shuffle : bool (default: False)\n            Shuffles training data every epoch if True to prevent circles;\n            ignored if solver='normal_equation'.\n\n        random_seed : int (default: None)\n            Set random state for shuffling and initializing the weights;\n            ignored if solver='normal_equation'.\n\n        zero_init_weight : bool (default: False)\n            If True, weights are initialized to zero instead of small random\n            numbers in the interval [0,1];\n            ignored if solver='normal_equation'\n\n        Attributes\n        -----------\n        w_ : 1d-array\n          Weights after fitting.\n\n        cost_ : list\n          Sum of squared errors after each epoch;\n          ignored if solver='normal_equation'\n\n        \"\"\"\n\n\n\n\n\n\nMethods\n\n\n    def fit(self, X, y, init_weights=True):\n        \"\"\" Fit training data.\n\n        Parameters\n        ----------\n        X : {array-like, sparse matrix}, shape = [n_samples, n_features]\n            Training vectors, where n_samples is the number of samples and\n            n_features is the number of features.\n\n        y : array-like, shape = [n_samples]\n            Target values.\n\n        init_weights : bool (default: True)\n            (Re)initializes weights to small random floats if True.\n\n        Returns\n        -------\n        self : object\n        \"\"\"\n\n\n\n    def predict(self, X):\n        \"\"\"\n        Predict target values for X.\n\n        Parameters\n        ----------\n        X : {array-like, sparse matrix}, shape = [n_samples, n_features]\n            Training vectors, where n_samples is the number of samples and\n            n_features is the number of features.\n\n        Returns\n        ----------\n        float : Predicted target value.\n\n        \"\"\"",
            "title": "Linear regression"
        },
        {
            "location": "/docs/regression/linear_regression/#linear-regression",
            "text": "from mlxtend.regression import LinearRegression   Implementation of a linear regression model for performing ordinary least squares regression using one of the following three approaches:   normal equation (closed-form solution)  gradient descent  stochastic gradient descent    For more usage examples please see the  IPython Notebook .",
            "title": "Linear Regression"
        },
        {
            "location": "/docs/regression/linear_regression/#example-1-normal-equation",
            "text": "The closed-form solution should be preferred for \"smaller\" datasets where calculating (a \"costly\") is not a concern. For very large datasets, or datasets where the inverse of [ X ^T  X ] may not exist (the matrix is non-invertible or singular, e.g., in case of perfect multicollinearity), the gradient descent or stochastic gradient descent approaches are to be preferred.  The linear function (linear regression model) is defined as:   where  y  is the response variable,  x  is an  m -dimensional sample vector, and  w  is the weight vector (vector of coefficients). Note that  w_0  represents the y-axis intercept of the model and therefore  x_0 =1.    Using the closed-form solution (normal equation), we can calculate the weights of the model as follows:   where  X  is the matrix of the data samples. After obtaining the weight, the intercept (w_0) can be calculated from the difference of the true and predicted average responses:    >>> import numpy as np\n>>> X = np.array([ 1, 2, 3, 4, 5])[:, np.newaxis]\n>>> y = np.array([ 1, 2, 3, 4, 5])\n\n>>> import matplotlib.pyplot as plt\n>>> def lin_regplot(X, y, model):\n...     plt.scatter(X, y, c='blue')\n...     plt.plot(X, model.predict(X), color='red')    \n...     return\n\n>>> ne_lr = LinearRegression(solver='normal_equation')\n>>> ne_lr.fit(X, y)\n>>> print('Intercept: %.2f' % ne_lr.w_[0])\nIntercept: 0.00\n>>> print('Slope: %.2f' % ne_lr.w_[1])\nSlope: 1.00\n\nlin_regplot(X, y, ne_lr)\nplt.show()",
            "title": "Example 1 - Normal Equation"
        },
        {
            "location": "/docs/regression/linear_regression/#example-2-gradient-descent",
            "text": "Using the gradient decent optimization algorithm, the weights are updated incrementally after each epoch (= pass over the training dataset).   >>> gd_lr = LinearRegression(solver='gd', eta=0.005, epochs=1500, random_seed=0)\n>>> gd_lr.fit(X, y)\n>>> print('Intercept: %.2f' % gd_lr.w_[0])\nIntercept: 0.00\n>>> print('Slope: %.2f' % gd_lr.w_[1])\nSlope: 1.00  Visualizing the cost to check for convergence and plotting the linear model:  >>> plt.plot(range(1, gd_lr.epochs+1), gd_lr.cost_)\n>>> plt.xlabel('Epochs')\n>>> plt.ylabel('Cost')\n>>> plt.tight_layout()\n>>> plt.show()   >>>  lin_regplot(X, y, gd_lr)\n>>>  plt.show()   Tip : I you are using gradient descent, consider standardizing the variables for better convergence of the algorithm.  >>> X_std = (X - np.mean(X)) / X.std()\n>>> y_std = (y - np.mean(y)) / y.std()\n\n>>> gd_lr = LinearRegression(solver='gd', eta=0.1, epochs=10, random_seed=0)\n>>> gd_lr.fit(X_std, y_std)\n>>> print('Intercept: %.2f' % gd_lr.w_[0])\nIntercept: 0.00\n>>> print('Slope: %.2f' % gd_lr.w_[1])\nSlope: 1.00",
            "title": "Example 2 - Gradient Descent"
        },
        {
            "location": "/docs/regression/linear_regression/#example-3-stochastic-gradient-descent",
            "text": "Example 2 explained \"batch\" gradient descent learning. The \"batch\" updates refers to the fact that the cost function is minimized based on the complete training data set. In contrast, stochastic gradient descent performs the weight update incrementally after each individual training sample.  The process of incrementally updating the weights is also called \"stochastic\" gradient descent since it approximates the minimization of the cost function. Although the stochastic gradient descent approach might sound inferior to gradient descent due its \"stochastic\" nature and the \"approximated\" direction (gradient), it can have certain advantages in practice. Often, stochastic gradient descent converges much faster than gradient descent since the updates are applied immediately after each training sample; stochastic gradient descent is computationally more efficient, especially for very large datasets.   >>> sgd_lr = LinearRegression(solver='sgd', eta=0.1, epochs=10, random_seed=0)\n>>> sgd_lr.fit(X_std, y_std)\n>>> print('Intercept: %.2f' % sgd_lr.w_[0])\nIntercept: 0.00\n>>> print('Slope: %.2f' % sgd_lr.w_[1])\nSlope: 1.00\n\n>>> plt.plot(range(1, sgd_lr.epochs+1), sgd_lr.cost_)\n>>> plt.xlabel('Epochs')\n>>> plt.ylabel('Cost')\n>>> plt.tight_layout()\n>>> plt.show()   >>> plt.plot(range(1, sgd_lr.epochs+1), sgd_lr.cost_)\n>>> plt.xlabel('Epochs')\n>>> plt.ylabel('Cost')\n>>> plt.tight_layout()\n>>> plt.show()",
            "title": "Example 3 - Stochastic Gradient Descent"
        },
        {
            "location": "/docs/regression/linear_regression/#default-parameters",
            "text": "class LinearRegression(object):\n        \"\"\" Ordinary least squares linear regression.\n\n        Parameters\n        ------------\n\n        solver : {'gd', 'sgd', 'normal_equation'} (default: 'normal_equation')\n          Method for solving the cost function. 'gd' for gradient descent, \n          'sgd' for stochastic gradient descent, or 'normal_equation' (default)\n          to solve the cost function analytically.\n\n        eta : float (default: 0.1)\n          Learning rate (between 0.0 and 1.0); \n          ignored if solver='normal_equation'.\n\n        epochs : int (default: 50)\n          Passes over the training dataset; \n          ignored if solver='normal_equation'.\n\n        shuffle : bool (default: False)\n            Shuffles training data every epoch if True to prevent circles;\n            ignored if solver='normal_equation'.\n\n        random_seed : int (default: None)\n            Set random state for shuffling and initializing the weights;\n            ignored if solver='normal_equation'.\n\n        zero_init_weight : bool (default: False)\n            If True, weights are initialized to zero instead of small random\n            numbers in the interval [0,1];\n            ignored if solver='normal_equation'\n\n        Attributes\n        -----------\n        w_ : 1d-array\n          Weights after fitting.\n\n        cost_ : list\n          Sum of squared errors after each epoch;\n          ignored if solver='normal_equation'\n\n        \"\"\"",
            "title": "Default Parameters"
        },
        {
            "location": "/docs/regression/linear_regression/#methods",
            "text": "def fit(self, X, y, init_weights=True):\n        \"\"\" Fit training data.\n\n        Parameters\n        ----------\n        X : {array-like, sparse matrix}, shape = [n_samples, n_features]\n            Training vectors, where n_samples is the number of samples and\n            n_features is the number of features.\n\n        y : array-like, shape = [n_samples]\n            Target values.\n\n        init_weights : bool (default: True)\n            (Re)initializes weights to small random floats if True.\n\n        Returns\n        -------\n        self : object\n        \"\"\"      def predict(self, X):\n        \"\"\"\n        Predict target values for X.\n\n        Parameters\n        ----------\n        X : {array-like, sparse matrix}, shape = [n_samples, n_features]\n            Training vectors, where n_samples is the number of samples and\n            n_features is the number of features.\n\n        Returns\n        ----------\n        float : Predicted target value.\n\n        \"\"\"",
            "title": "Methods"
        },
        {
            "location": "/docs/preprocessing/standardizing/",
            "text": "mlxtend\n\nSebastian Raschka, last updated: 06/02/2015\n\n\n\n\n\nStandardizing\n\n\n\n\nfrom mlxtend.preprocessing import standardizing\n\n\n\n\nA function to standardize columns in pandas DataFrames (or NumPy arrays) so that they have properties of a standard normal distribution (mean=0, standard deviation=1).\n\n\n\n\n\nExample\n\n\nfrom mlxtend.pandas import standardizing\n\n\n\n\n\nDefault Parameters\n\n\ndef standardizing(array, columns, ddof=0):\n    \"\"\"\n    Standardizing columns in pandas DataFrames.\n\n    Parameters\n    ----------\n    array : pandas DataFrame or NumPy ndarray, shape = [n_rows, n_columns].\n\n    columns : array-like, shape = [n_columns]\n      Array-like with column names, e.g., ['col1', 'col2', ...]\n      or column indices [0, 2, 4, ...]\n\n    ddof : int (default: 0)\n      Delta Degrees of Freedom. The divisor used in calculations \n      is N - ddof, where N represents the number of elements.\n\n    Returns\n    ----------\n    df_new: pandas DataFrame object.\n      Copy of the array or DataFrame with standardized columns.\n\n    \"\"\"",
            "title": "Standardizing"
        },
        {
            "location": "/docs/preprocessing/standardizing/#standardizing",
            "text": "from mlxtend.preprocessing import standardizing   A function to standardize columns in pandas DataFrames (or NumPy arrays) so that they have properties of a standard normal distribution (mean=0, standard deviation=1).",
            "title": "Standardizing"
        },
        {
            "location": "/docs/preprocessing/standardizing/#example",
            "text": "from mlxtend.pandas import standardizing",
            "title": "Example"
        },
        {
            "location": "/docs/preprocessing/standardizing/#default-parameters",
            "text": "def standardizing(array, columns, ddof=0):\n    \"\"\"\n    Standardizing columns in pandas DataFrames.\n\n    Parameters\n    ----------\n    array : pandas DataFrame or NumPy ndarray, shape = [n_rows, n_columns].\n\n    columns : array-like, shape = [n_columns]\n      Array-like with column names, e.g., ['col1', 'col2', ...]\n      or column indices [0, 2, 4, ...]\n\n    ddof : int (default: 0)\n      Delta Degrees of Freedom. The divisor used in calculations \n      is N - ddof, where N represents the number of elements.\n\n    Returns\n    ----------\n    df_new: pandas DataFrame object.\n      Copy of the array or DataFrame with standardized columns.\n\n    \"\"\"",
            "title": "Default Parameters"
        },
        {
            "location": "/docs/preprocessing/minmax_scaling/",
            "text": "mlxtend\n\nSebastian Raschka, last updated: 06/02/2015\n\n\n\n\n\nMinmax Scaling\n\n\n\n\nfrom mlxtend.preprocessing import minmax_scaling\n\n\n\n\nA function that applies minmax scaling to pandas DataFrame or NumPy array columns.\n\n\n\n\n\nExample\n\n\nfrom mlxtend.preprocessing import minmax_scaling\n\n\n\n\n\nDefault Parameters\n\n\ndef minmax_scaling(array, columns, min_val=0, max_val=1):\n    \"\"\"\n    Min max scaling for pandas DataFrames\n\n    Parameters\n    ----------\n    array : pandas DataFrame or NumPy ndarray, shape = [n_rows, n_columns].\n\n    columns : array-like, shape = [n_columns]\n      Array-like with column names, e.g., ['col1', 'col2', ...]\n      or column indices [0, 2, 4, ...]\n\n    min_val : `int` or `float`, optional (default=`0`)\n      minimum value after rescaling.\n\n    min_val : `int` or `float`, optional (default=`1`)\n      maximum value after rescaling.\n\n    Returns\n    ----------\n\n    df_new: pandas DataFrame object.\n      Copy of the array or DataFrame with rescaled columns.\n\n    \"\"\"",
            "title": "Minmax scaling"
        },
        {
            "location": "/docs/preprocessing/minmax_scaling/#minmax-scaling",
            "text": "from mlxtend.preprocessing import minmax_scaling   A function that applies minmax scaling to pandas DataFrame or NumPy array columns.",
            "title": "Minmax Scaling"
        },
        {
            "location": "/docs/preprocessing/minmax_scaling/#example",
            "text": "from mlxtend.preprocessing import minmax_scaling",
            "title": "Example"
        },
        {
            "location": "/docs/preprocessing/minmax_scaling/#default-parameters",
            "text": "def minmax_scaling(array, columns, min_val=0, max_val=1):\n    \"\"\"\n    Min max scaling for pandas DataFrames\n\n    Parameters\n    ----------\n    array : pandas DataFrame or NumPy ndarray, shape = [n_rows, n_columns].\n\n    columns : array-like, shape = [n_columns]\n      Array-like with column names, e.g., ['col1', 'col2', ...]\n      or column indices [0, 2, 4, ...]\n\n    min_val : `int` or `float`, optional (default=`0`)\n      minimum value after rescaling.\n\n    min_val : `int` or `float`, optional (default=`1`)\n      maximum value after rescaling.\n\n    Returns\n    ----------\n\n    df_new: pandas DataFrame object.\n      Copy of the array or DataFrame with rescaled columns.\n\n    \"\"\"",
            "title": "Default Parameters"
        },
        {
            "location": "/docs/preprocessing/mean_centerer/",
            "text": "mlxtend\n\nSebastian Raschka, last updated: 05/14/2015\n\n\n\n\n\nMeanCenterer\n\n\n\n\nfrom mlxtend.preprocessing import MeanCenterer\n\n\n\n\nA transformer class that performs column-based mean centering on a NumPy array.\n\n\n\n\n\nExample\n\n\nUse the \nfit\n method to fit the column means of a dataset (e.g., the training dataset) to a new \nMeanCenterer\n object. Then, call the \ntransform\n method on the same dataset to center it at the sample mean.\n\n\n>>> from mlxtend.preprocessing import MeanCenterer\n>>> X_train\narray([[1, 2, 3],\n   [4, 5, 6],\n   [7, 8, 9]])\n>>> mc = MeanCenterer().fit(X_train)\n>>> mc.transform(X_train)\narray([[-3, -3, -3],\n   [ 0,  0,  0],\n   [ 3,  3,  3]])\n\n\n\n\n\nTo use the same parameters that were used to center the training dataset, simply call the \ntransform\n method of the \nMeanCenterer\n instance on a new dataset (e.g., test dataset).\n\n\n>>> X_test \narray([[1, 1, 1],\n   [1, 1, 1],\n   [1, 1, 1]])\n>>> mc.transform(X_test)  \narray([[-3, -4, -5],\n   [-3, -4, -5],\n   [-3, -4, -5]])\n\n\n\n\n\nThe \nMeanCenterer\n also supports Python list objects, and the \nfit_transform\n method allows you to directly fit and center the dataset.\n\n\n>>> Z\n[1, 2, 3]\n>>> MeanCenterer().fit_transform(Z)\narray([-1,  0,  1])\n\n\n\n\n\nimport matplotlib.pyplot as plt\nimport numpy as np\n\nX = 2 * np.random.randn(100,2) + 5\n\nplt.scatter(X[:,0], X[:,1])\nplt.grid()\nplt.title('Random Gaussian data w. mean=5, sigma=2')\nplt.show()\n\nY = MeanCenterer.fit_transform(X)\nplt.scatter(Y[:,0], Y[:,1])\nplt.grid()\nplt.title('Data after mean centering')\nplt.show()\n\n\n\n\n\n\n\n\nDefault Parameters\n\n\nclass MeanCenterer(TransformerObj):\n    \"\"\"\n    Class for column centering of vectors and matrices.\n\n    Keyword arguments:\n        X: NumPy array object where each attribute/variable is\n           stored in an individual column. \n           Also accepts 1-dimensional Python list objects.\n\n    Class methods:\n        fit: Fits column means to MeanCenterer object.\n        transform: Uses column means from `fit` for mean centering.\n        fit_transform: Fits column means and performs mean centering.\n\n    The class methods `transform` and `fit_transform` return a new numpy array\n    object where the attributes are centered at the column means.\n\n    \"\"\"",
            "title": "Mean centerer"
        },
        {
            "location": "/docs/preprocessing/mean_centerer/#meancenterer",
            "text": "from mlxtend.preprocessing import MeanCenterer   A transformer class that performs column-based mean centering on a NumPy array.",
            "title": "MeanCenterer"
        },
        {
            "location": "/docs/preprocessing/mean_centerer/#example",
            "text": "Use the  fit  method to fit the column means of a dataset (e.g., the training dataset) to a new  MeanCenterer  object. Then, call the  transform  method on the same dataset to center it at the sample mean.  >>> from mlxtend.preprocessing import MeanCenterer\n>>> X_train\narray([[1, 2, 3],\n   [4, 5, 6],\n   [7, 8, 9]])\n>>> mc = MeanCenterer().fit(X_train)\n>>> mc.transform(X_train)\narray([[-3, -3, -3],\n   [ 0,  0,  0],\n   [ 3,  3,  3]])   To use the same parameters that were used to center the training dataset, simply call the  transform  method of the  MeanCenterer  instance on a new dataset (e.g., test dataset).  >>> X_test \narray([[1, 1, 1],\n   [1, 1, 1],\n   [1, 1, 1]])\n>>> mc.transform(X_test)  \narray([[-3, -4, -5],\n   [-3, -4, -5],\n   [-3, -4, -5]])   The  MeanCenterer  also supports Python list objects, and the  fit_transform  method allows you to directly fit and center the dataset.  >>> Z\n[1, 2, 3]\n>>> MeanCenterer().fit_transform(Z)\narray([-1,  0,  1])   import matplotlib.pyplot as plt\nimport numpy as np\n\nX = 2 * np.random.randn(100,2) + 5\n\nplt.scatter(X[:,0], X[:,1])\nplt.grid()\nplt.title('Random Gaussian data w. mean=5, sigma=2')\nplt.show()\n\nY = MeanCenterer.fit_transform(X)\nplt.scatter(Y[:,0], Y[:,1])\nplt.grid()\nplt.title('Data after mean centering')\nplt.show()",
            "title": "Example"
        },
        {
            "location": "/docs/preprocessing/mean_centerer/#default-parameters",
            "text": "class MeanCenterer(TransformerObj):\n    \"\"\"\n    Class for column centering of vectors and matrices.\n\n    Keyword arguments:\n        X: NumPy array object where each attribute/variable is\n           stored in an individual column. \n           Also accepts 1-dimensional Python list objects.\n\n    Class methods:\n        fit: Fits column means to MeanCenterer object.\n        transform: Uses column means from `fit` for mean centering.\n        fit_transform: Fits column means and performs mean centering.\n\n    The class methods `transform` and `fit_transform` return a new numpy array\n    object where the attributes are centered at the column means.\n\n    \"\"\"",
            "title": "Default Parameters"
        },
        {
            "location": "/docs/preprocessing/shuffle_arrays_unison/",
            "text": "mlxtend\n\nSebastian Raschka, last updated: 05/14/2015\n\n\n\n\n\nArray Unison Shuffling\n\n\n\n\nfrom mlxtend.preprocessing import shuffle_arrays_unison\n\n\n\n\nA function that shuffles 2 or more NumPy arrays in unison.\n\n\n\n\n\nExample\n\n\n>>> import numpy as np\n>>> from mlxtend.preprocessing import shuffle_arrays_unison\n>>> X1 = np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]])\n>>> y1 = np.array([1, 2, 3])\n>>> print(X1)\n[[1 2 3]\n[4 5 6]\n[7 8 9]]    \n>>> print(y1)\n[1 2 3]\n>>> X2, y2 = shuffle_arrays_unison(arrays=[X1, y1], random_seed=3)\n>>> print(X2)\n[[4 5 6]\n[1 2 3]\n[7 8 9]]\n>>> print(y1)\n[2 1 3]\n\n\n\nDefault Parameters\n\n\ndef shuffle_arrays_unison(arrays, random_state=None):\n    \"\"\"\n    Shuffle NumPy arrays in unison.\n\n    Parameters\n    ----------\n    arrays : array-like, shape = [n_arrays]\n      A list of NumPy arrays.\n\n    random_state : int\n      Sets the random state.\n\n    Returns\n    ----------\n    shuffled_arrays : A list of NumPy arrays after shuffling.\n\n    Examples\n    --------\n    >>> import numpy as np\n    >>> from mlxtend.preprocessing import shuffle_arrays_unison\n    >>> X1 = np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]])\n    >>> y1 = np.array([1, 2, 3])\n    >>> X2, y2 = shuffle_arrays_unison(arrays=[X1, y1], random_state=3)\n    >>> assert(X2.all() == np.array([[4, 5, 6], [1, 2, 3], [7, 8, 9]]).all())\n    >>> assert(y2.all() == np.array([2, 1, 3]).all())\n    >>>\n    \"\"\"",
            "title": "Shuffle arrays unison"
        },
        {
            "location": "/docs/preprocessing/shuffle_arrays_unison/#array-unison-shuffling",
            "text": "from mlxtend.preprocessing import shuffle_arrays_unison   A function that shuffles 2 or more NumPy arrays in unison.",
            "title": "Array Unison Shuffling"
        },
        {
            "location": "/docs/preprocessing/shuffle_arrays_unison/#example",
            "text": ">>> import numpy as np\n>>> from mlxtend.preprocessing import shuffle_arrays_unison\n>>> X1 = np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]])\n>>> y1 = np.array([1, 2, 3])\n>>> print(X1)\n[[1 2 3]\n[4 5 6]\n[7 8 9]]    \n>>> print(y1)\n[1 2 3]\n>>> X2, y2 = shuffle_arrays_unison(arrays=[X1, y1], random_seed=3)\n>>> print(X2)\n[[4 5 6]\n[1 2 3]\n[7 8 9]]\n>>> print(y1)\n[2 1 3]",
            "title": "Example"
        },
        {
            "location": "/docs/preprocessing/shuffle_arrays_unison/#default-parameters",
            "text": "def shuffle_arrays_unison(arrays, random_state=None):\n    \"\"\"\n    Shuffle NumPy arrays in unison.\n\n    Parameters\n    ----------\n    arrays : array-like, shape = [n_arrays]\n      A list of NumPy arrays.\n\n    random_state : int\n      Sets the random state.\n\n    Returns\n    ----------\n    shuffled_arrays : A list of NumPy arrays after shuffling.\n\n    Examples\n    --------\n    >>> import numpy as np\n    >>> from mlxtend.preprocessing import shuffle_arrays_unison\n    >>> X1 = np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]])\n    >>> y1 = np.array([1, 2, 3])\n    >>> X2, y2 = shuffle_arrays_unison(arrays=[X1, y1], random_state=3)\n    >>> assert(X2.all() == np.array([[4, 5, 6], [1, 2, 3], [7, 8, 9]]).all())\n    >>> assert(y2.all() == np.array([2, 1, 3]).all())\n    >>>\n    \"\"\"",
            "title": "Default Parameters"
        },
        {
            "location": "/docs/preprocessing/scikit-learn_dense_transformer/",
            "text": "mlxtend\n\nSebastian Raschka, last updated: 05/14/2015\n\n\n\n\n\nDense Transformer\n\n\n\n\nfrom mlxtend.preprocessing import DenseTransformer\n\n\n\n\nA simple transformer that converts a sparse into a dense numpy array, e.g., required for scikit-learn's \nPipeline\n when e.g,. \nCountVectorizers\n are used in combination with \nRandomForest\ns.\n\n\n\n\n\nExample\n\n\nfrom sklearn.pipeline import Pipeline\nfrom sklearn import metrics\nfrom sklearn.grid_search import GridSearchCV\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.feature_extraction.text import CountVectorizer\n\nfrom mlxtend.preprocessing import DenseTransformer\n\n\npipe_1 = Pipeline([\n    ('vect', CountVectorizer(analyzer='word',\n                      decode_error='replace',\n                      preprocessor=lambda text: re.sub('[^a-zA-Z]', ' ', text.lower()), \n                      stop_words=stopwords,) ),\n    ('to_dense', DenseTransformer()),\n    ('clf', RandomForestClassifier())\n])\n\nparameters_1 = dict(\n    clf__n_estimators=[50, 100, 200],\n    clf__max_features=['sqrt', 'log2', None],)\n\ngrid_search_1 = GridSearchCV(pipe_1, \n                           parameters_1, \n                           n_jobs=1, \n                           verbose=1,\n                           scoring=f1_scorer,\n                           cv=10)\n\n\nprint(\"Performing grid search...\")\nprint(\"pipeline:\", [name for name, _ in pipe_1.steps])\nprint(\"parameters:\")\ngrid_search_1.fit(X_train, y_train)\nprint(\"Best score: %0.3f\" % grid_search_1.best_score_)\nprint(\"Best parameters set:\")\nbest_parameters_1 = grid_search_1.best_estimator_.get_params()\nfor param_name in sorted(parameters_1.keys()):\n    print(\"\\t%s: %r\" % (param_name, best_parameters_1[param_name]))\n\n\n\nDefault Parameters",
            "title": "Scikit learn dense transformer"
        },
        {
            "location": "/docs/preprocessing/scikit-learn_dense_transformer/#dense-transformer",
            "text": "from mlxtend.preprocessing import DenseTransformer   A simple transformer that converts a sparse into a dense numpy array, e.g., required for scikit-learn's  Pipeline  when e.g,.  CountVectorizers  are used in combination with  RandomForest s.",
            "title": "Dense Transformer"
        },
        {
            "location": "/docs/preprocessing/scikit-learn_dense_transformer/#example",
            "text": "from sklearn.pipeline import Pipeline\nfrom sklearn import metrics\nfrom sklearn.grid_search import GridSearchCV\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.feature_extraction.text import CountVectorizer\n\nfrom mlxtend.preprocessing import DenseTransformer\n\n\npipe_1 = Pipeline([\n    ('vect', CountVectorizer(analyzer='word',\n                      decode_error='replace',\n                      preprocessor=lambda text: re.sub('[^a-zA-Z]', ' ', text.lower()), \n                      stop_words=stopwords,) ),\n    ('to_dense', DenseTransformer()),\n    ('clf', RandomForestClassifier())\n])\n\nparameters_1 = dict(\n    clf__n_estimators=[50, 100, 200],\n    clf__max_features=['sqrt', 'log2', None],)\n\ngrid_search_1 = GridSearchCV(pipe_1, \n                           parameters_1, \n                           n_jobs=1, \n                           verbose=1,\n                           scoring=f1_scorer,\n                           cv=10)\n\n\nprint(\"Performing grid search...\")\nprint(\"pipeline:\", [name for name, _ in pipe_1.steps])\nprint(\"parameters:\")\ngrid_search_1.fit(X_train, y_train)\nprint(\"Best score: %0.3f\" % grid_search_1.best_score_)\nprint(\"Best parameters set:\")\nbest_parameters_1 = grid_search_1.best_estimator_.get_params()\nfor param_name in sorted(parameters_1.keys()):\n    print(\"\\t%s: %r\" % (param_name, best_parameters_1[param_name]))",
            "title": "Example"
        },
        {
            "location": "/docs/preprocessing/scikit-learn_dense_transformer/#default-parameters",
            "text": "",
            "title": "Default Parameters"
        },
        {
            "location": "/docs/file_io/find_files/",
            "text": "mlxtend\n\nSebastian Raschka, last updated: 05/14/2015\n\n\n\n\n\nFind Files\n\n\n\n\nfrom mlxtend.file_io import find_files\n\n\n\n\nA function that finds files in a given directory based on substring matches and returns a list of the file names found.\n\n\n\n\n\nExample\n\n\n>>> from mlxtend.file_io import find_files\n\n>>> find_files('mlxtend', '/Users/sebastian/Desktop')\n['/Users/sebastian/Desktop/mlxtend-0.1.6.tar.gz', \n'/Users/sebastian/Desktop/mlxtend-0.1.7.tar.gz']\n\n\n\n\n\n\nDefault Parameters\n\n\ndef find_files(substring, path, recursive=False, check_ext=None, ignore_invisible=True): \n    \"\"\"\n    Function that finds files in a directory based on substring matching.\n\n    Parameters\n    ----------\n\n    substring : `str`\n      Substring of the file to be matched.\n\n    path : `str` \n      Path where to look.\n\n    recursive: `bool`, optional, (default=`False`)\n      If true, searches subdirectories recursively.\n\n    check_ext: `str`, optional, (default=`None`)\n      If string (e.g., '.txt'), only returns files that\n      match the specified file extension.\n\n    ignore_invisible : `bool`, optional, (default=`True`)\n      If `True`, ignores invisible files (i.e., files starting with a period).\n\n    Returns\n    ----------\n    results : `list`\n      List of the matched files.\n\n    \"\"\"",
            "title": "Find files"
        },
        {
            "location": "/docs/file_io/find_files/#find-files",
            "text": "from mlxtend.file_io import find_files   A function that finds files in a given directory based on substring matches and returns a list of the file names found.",
            "title": "Find Files"
        },
        {
            "location": "/docs/file_io/find_files/#example",
            "text": ">>> from mlxtend.file_io import find_files\n\n>>> find_files('mlxtend', '/Users/sebastian/Desktop')\n['/Users/sebastian/Desktop/mlxtend-0.1.6.tar.gz', \n'/Users/sebastian/Desktop/mlxtend-0.1.7.tar.gz']",
            "title": "Example"
        },
        {
            "location": "/docs/file_io/find_files/#default-parameters",
            "text": "def find_files(substring, path, recursive=False, check_ext=None, ignore_invisible=True): \n    \"\"\"\n    Function that finds files in a directory based on substring matching.\n\n    Parameters\n    ----------\n\n    substring : `str`\n      Substring of the file to be matched.\n\n    path : `str` \n      Path where to look.\n\n    recursive: `bool`, optional, (default=`False`)\n      If true, searches subdirectories recursively.\n\n    check_ext: `str`, optional, (default=`None`)\n      If string (e.g., '.txt'), only returns files that\n      match the specified file extension.\n\n    ignore_invisible : `bool`, optional, (default=`True`)\n      If `True`, ignores invisible files (i.e., files starting with a period).\n\n    Returns\n    ----------\n    results : `list`\n      List of the matched files.\n\n    \"\"\"",
            "title": "Default Parameters"
        },
        {
            "location": "/docs/file_io/find_filegroups/",
            "text": "mlxtend\n\nSebastian Raschka, 05/14/2015\n\n\n\n\n\nFind File Groups\n\n\n\n\nfrom mlxtend.file_io import find_filegroups\n\n\n\n\nA function that finds files that belong together (i.e., differ only by file extension) in different directories and collects them in a Python dictionary for further processing tasks. \n\n\n\n\n\nExample\n\n\n\n\nfrom mlxtend.file_io import find_filegroups\n\nd1 = os.path.join(master_path, 'dir_1')\nd2 = os.path.join(master_path, 'dir_2')\nd3 = os.path.join(master_path, 'dir_3')\n\nfind_filegroups(paths=[d1,d2,d3], substring='file_1')\n# Returns:\n# {'file_1': ['/Users/sebastian/github/mlxtend/tests/data/find_filegroups/dir_1/file_1.log', \n#             '/Users/sebastian/github/mlxtend/tests/data/find_filegroups/dir_2/file_1.csv', \n#             '/Users/sebastian/github/mlxtend/tests/data/find_filegroups/dir_3/file_1.txt']}\n#\n# Note: Setting `substring=''` would return a \n# dictionary of all file paths for \n# file_1.*, file_2.*, file_3.*\n\n\n\n\n\n\nDefault Parameters\n\n\ndef find_filegroups(paths, substring='', extensions=None, validity_check=True, ignore_invisible=True):\n    \"\"\"\n    Function that finds and groups files from different directories in a python dictionary.\n\n    Parameters\n    ----------\n    paths : `list` \n      Paths of the directories to be searched. Dictionary keys are build from\n      the first directory.\n\n    substring : `str`, optional, (default=`''`)\n      Substring that all files have to contain to be considered.\n\n    extensions : `list`, optional, (default=`None`)\n      `None` or `list` of allowed file extensions for each path. If provided, the number\n      of extensions must match the number of `paths`.\n\n    validity_check : `bool`, optional, (default=`True`)\n      If `True`, checks if all dictionary values have the same number of file paths. Prints\n      a warning and returns an empty dictionary if the validity check failed.\n\n    ignore_invisible : `bool`, optional, (default=`True`)\n      If `True`, ignores invisible files (i.e., files starting with a period).\n\n    Returns\n    ----------\n    groups : `dict`\n      Dictionary of files paths. Keys are the file names found in the first directory listed\n      in `paths` (without file extension).\n\n    \"\"\"",
            "title": "Find filegroups"
        },
        {
            "location": "/docs/file_io/find_filegroups/#find-file-groups",
            "text": "from mlxtend.file_io import find_filegroups   A function that finds files that belong together (i.e., differ only by file extension) in different directories and collects them in a Python dictionary for further processing tasks.",
            "title": "Find File Groups"
        },
        {
            "location": "/docs/file_io/find_filegroups/#example",
            "text": "from mlxtend.file_io import find_filegroups\n\nd1 = os.path.join(master_path, 'dir_1')\nd2 = os.path.join(master_path, 'dir_2')\nd3 = os.path.join(master_path, 'dir_3')\n\nfind_filegroups(paths=[d1,d2,d3], substring='file_1')\n# Returns:\n# {'file_1': ['/Users/sebastian/github/mlxtend/tests/data/find_filegroups/dir_1/file_1.log', \n#             '/Users/sebastian/github/mlxtend/tests/data/find_filegroups/dir_2/file_1.csv', \n#             '/Users/sebastian/github/mlxtend/tests/data/find_filegroups/dir_3/file_1.txt']}\n#\n# Note: Setting `substring=''` would return a \n# dictionary of all file paths for \n# file_1.*, file_2.*, file_3.*",
            "title": "Example"
        },
        {
            "location": "/docs/file_io/find_filegroups/#default-parameters",
            "text": "def find_filegroups(paths, substring='', extensions=None, validity_check=True, ignore_invisible=True):\n    \"\"\"\n    Function that finds and groups files from different directories in a python dictionary.\n\n    Parameters\n    ----------\n    paths : `list` \n      Paths of the directories to be searched. Dictionary keys are build from\n      the first directory.\n\n    substring : `str`, optional, (default=`''`)\n      Substring that all files have to contain to be considered.\n\n    extensions : `list`, optional, (default=`None`)\n      `None` or `list` of allowed file extensions for each path. If provided, the number\n      of extensions must match the number of `paths`.\n\n    validity_check : `bool`, optional, (default=`True`)\n      If `True`, checks if all dictionary values have the same number of file paths. Prints\n      a warning and returns an empty dictionary if the validity check failed.\n\n    ignore_invisible : `bool`, optional, (default=`True`)\n      If `True`, ignores invisible files (i.e., files starting with a period).\n\n    Returns\n    ----------\n    groups : `dict`\n      Dictionary of files paths. Keys are the file names found in the first directory listed\n      in `paths` (without file extension).\n\n    \"\"\"",
            "title": "Default Parameters"
        },
        {
            "location": "/docs/math/combinations/",
            "text": "mlxtend\n\nSebastian Raschka, last updated: 05/14/2015\n\n\n\n\n\nCombinations\n\n\n\n\nfrom mlxtend.math import num_combinations\n\n\n\n\nFunctions to calculate the number of combinations for creating subsequences of \nr\n elements out of a sequence with \nn\n elements.\n\n\nExample\n\n\nIn:\n\n\nfrom mlxtend.math import num_combinations\n\nc = num_combinations(n=20, r=8, with_replacement=False)\nprint('Number of ways to combine 20 elements into 8 subelements: %d' % c)\n\n\n\nOut:    \n\n\nNumber of ways to combine 20 elements into 8 subelements: 125970\n\n\n\nThis is especially useful in combination with \nitertools\n, e.g., in order to estimate the progress via \npyprind\n.\n\n\n\n\nDefault Parameters\n\n\ndef num_combinations(n, r, with_replacement=False):\n    \"\"\" \n    Function to calculate the number of possible combinations.\n\n    Parameters\n    ----------\n    n : `int`\n      Total number of items.\n\n    r : `int`\n      Number of elements of the target itemset.\n\n    with_replacement : `bool`, optional, (default=False)\n      Allows repeated elements if True.\n\n    Returns\n    ----------\n    comb : `int`\n      Number of possible combinations.\n\n    \"\"\"",
            "title": "Combinations"
        },
        {
            "location": "/docs/math/combinations/#combinations",
            "text": "from mlxtend.math import num_combinations   Functions to calculate the number of combinations for creating subsequences of  r  elements out of a sequence with  n  elements.",
            "title": "Combinations"
        },
        {
            "location": "/docs/math/combinations/#example",
            "text": "In:  from mlxtend.math import num_combinations\n\nc = num_combinations(n=20, r=8, with_replacement=False)\nprint('Number of ways to combine 20 elements into 8 subelements: %d' % c)  Out:      Number of ways to combine 20 elements into 8 subelements: 125970  This is especially useful in combination with  itertools , e.g., in order to estimate the progress via  pyprind .",
            "title": "Example"
        },
        {
            "location": "/docs/math/combinations/#default-parameters",
            "text": "def num_combinations(n, r, with_replacement=False):\n    \"\"\" \n    Function to calculate the number of possible combinations.\n\n    Parameters\n    ----------\n    n : `int`\n      Total number of items.\n\n    r : `int`\n      Number of elements of the target itemset.\n\n    with_replacement : `bool`, optional, (default=False)\n      Allows repeated elements if True.\n\n    Returns\n    ----------\n    comb : `int`\n      Number of possible combinations.\n\n    \"\"\"",
            "title": "Default Parameters"
        },
        {
            "location": "/docs/math/permutations/",
            "text": "mlxtend\n\nSebastian Raschka, last updated: 05/14/2015\n\n\n\n\n\nPermutations\n\n\n\n\nfrom mlxtend.math import num_permutations\n\n\n\n\nFunctions to calculate the number of permutations for creating subsequences of \nr\n elements out of a sequence with \nn\n elements.\n\n\n\n\n\nExample\n\n\nIn:\n\n\nfrom mlxtend.math import num_permutations\n\nd = num_permutations(n=20, r=8, with_replacement=False)\nprint('Number of ways to permute 20 elements into 8 subelements: %d' % d)\n\n\n\nOut:    \n\n\nNumber of ways to permute 20 elements into 8 subelements: 5079110400\n\n\n\nThis is especially useful in combination with \nitertools\n, e.g., in order to estimate the progress via \npyprind\n.\n\n\n\n\nDefault Parameters\n\n\ndef num_permutations(n, r, with_replacement=False):\n    \"\"\" \n    Function to calculate the number of possible permutations.\n\n    Parameters\n    ----------\n    n : `int`\n      Total number of items.\n\n    r : `int`\n      Number of elements of the target itemset.\n\n    with_replacement : `bool`, optional, (default=False)\n      Allows repeated elements if True.\n\n    Returns\n    ----------\n    permut : `int`\n      Number of possible permutations.\n\n    \"\"\"",
            "title": "Permutations"
        },
        {
            "location": "/docs/math/permutations/#permutations",
            "text": "from mlxtend.math import num_permutations   Functions to calculate the number of permutations for creating subsequences of  r  elements out of a sequence with  n  elements.",
            "title": "Permutations"
        },
        {
            "location": "/docs/math/permutations/#example",
            "text": "In:  from mlxtend.math import num_permutations\n\nd = num_permutations(n=20, r=8, with_replacement=False)\nprint('Number of ways to permute 20 elements into 8 subelements: %d' % d)  Out:      Number of ways to permute 20 elements into 8 subelements: 5079110400  This is especially useful in combination with  itertools , e.g., in order to estimate the progress via  pyprind .",
            "title": "Example"
        },
        {
            "location": "/docs/math/permutations/#default-parameters",
            "text": "def num_permutations(n, r, with_replacement=False):\n    \"\"\" \n    Function to calculate the number of possible permutations.\n\n    Parameters\n    ----------\n    n : `int`\n      Total number of items.\n\n    r : `int`\n      Number of elements of the target itemset.\n\n    with_replacement : `bool`, optional, (default=False)\n      Allows repeated elements if True.\n\n    Returns\n    ----------\n    permut : `int`\n      Number of possible permutations.\n\n    \"\"\"",
            "title": "Default Parameters"
        },
        {
            "location": "/docs/text/generalize_names/",
            "text": "mlxtend\n\nSebastian Raschka, last updated: 05/14/2015\n\n\n\n\n\nName Generalization\n\n\n\n\nfrom mlxtend.text import generalize_names\n\n\n\n\nA function that converts a name into a general format \n<last_name><separator><firstname letter(s)> (all lowercase)\n, which is useful if data is collected from different sources and is supposed to be compared or merged based on name identifiers. E.g., if names are stored in a pandas \nDataFrame\n column, the apply function can be used to generalize names: \ndf['name'] = df['name'].apply(generalize_names)\n\n\n\n\n\nExamples\n\n\nfrom mlxtend.text import generalize_names\n\n# defaults\n>>> generalize_names('Pozo, Jos\u00e9 \u00c1ngel')\n'pozo j'\n>>> generalize_names('Pozo, Jos\u00e9 \u00c1ngel') \n'pozo j'\n>>> assert(generalize_names('Jos\u00e9 \u00c1ngel Pozo') \n'pozo j' \n>>> generalize_names('Jos\u00e9 Pozo')\n'pozo j'\n\n# optional parameters\n>>> generalize_names(\"Eto'o, Samuel\", firstname_output_letters=2)\n'etoo sa'\n>>> generalize_names(\"Eto'o, Samuel\", firstname_output_letters=0)\n'etoo'\n>>> generalize_names(\"Eto'o, Samuel\", output_sep=', ')\n'etoo, s'\n\n\n\n\n\n\nDefault Parameters\n\n\ndef generalize_names(name, output_sep=' ', firstname_output_letters=1):\n    \"\"\"\n    Function that outputs a person's name in the format\n    \n (all lowercase)\n\n    Parameters\n    ----------\n    name : `str`\n      Name of the player\n\n    output_sep : `str` (default: ' ')\n      String for separating last name and first name in the output.\n\n    firstname_output_letters : `int`\n      Number of letters in the abbreviated first name.\n\n    Returns\n    ----------\n    gen_name : `str`\n      The generalized name.\n\n    \"\"\"",
            "title": "Generalize names"
        },
        {
            "location": "/docs/text/generalize_names/#name-generalization",
            "text": "from mlxtend.text import generalize_names   A function that converts a name into a general format  <last_name><separator><firstname letter(s)> (all lowercase) , which is useful if data is collected from different sources and is supposed to be compared or merged based on name identifiers. E.g., if names are stored in a pandas  DataFrame  column, the apply function can be used to generalize names:  df['name'] = df['name'].apply(generalize_names)",
            "title": "Name Generalization"
        },
        {
            "location": "/docs/text/generalize_names/#examples",
            "text": "from mlxtend.text import generalize_names\n\n# defaults\n>>> generalize_names('Pozo, Jos\u00e9 \u00c1ngel')\n'pozo j'\n>>> generalize_names('Pozo, Jos\u00e9 \u00c1ngel') \n'pozo j'\n>>> assert(generalize_names('Jos\u00e9 \u00c1ngel Pozo') \n'pozo j' \n>>> generalize_names('Jos\u00e9 Pozo')\n'pozo j'\n\n# optional parameters\n>>> generalize_names(\"Eto'o, Samuel\", firstname_output_letters=2)\n'etoo sa'\n>>> generalize_names(\"Eto'o, Samuel\", firstname_output_letters=0)\n'etoo'\n>>> generalize_names(\"Eto'o, Samuel\", output_sep=', ')\n'etoo, s'",
            "title": "Examples"
        },
        {
            "location": "/docs/text/generalize_names/#default-parameters",
            "text": "def generalize_names(name, output_sep=' ', firstname_output_letters=1):\n    \"\"\"\n    Function that outputs a person's name in the format\n      (all lowercase)\n\n    Parameters\n    ----------\n    name : `str`\n      Name of the player\n\n    output_sep : `str` (default: ' ')\n      String for separating last name and first name in the output.\n\n    firstname_output_letters : `int`\n      Number of letters in the abbreviated first name.\n\n    Returns\n    ----------\n    gen_name : `str`\n      The generalized name.\n\n    \"\"\"",
            "title": "Default Parameters"
        },
        {
            "location": "/docs/text/generalize_names_duplcheck/",
            "text": "mlxtend\n\nSebastian Raschka, last updated: 05/14/2015\n\n\n\n\n\nName Generalization and Duplicates\n\n\n\n\nfrom mlxtend.text import generalize_names_duplcheck\n\n\n\n\nNote\n that using \ngeneralize_names\n with few \nfirstname_output_letters\n can result in duplicate entries. E.g., if your dataset contains the names \"Adam Johnson\" and \"Andrew Johnson\", the default setting (i.e., 1 first name letter) will produce the generalized name \"johnson a\" in both cases.\n\n\nOne solution is to increase the number of first name letters in the output by setting the parameter \nfirstname_output_letters\n to a value larger than 1. \n\n\nAn alternative solution is to use the \ngeneralize_names_duplcheck\n function if you are working with pandas DataFrames. \n\n\nThe  \ngeneralize_names_duplcheck\n function can be imported via\n\n\nfrom mlxtend.text import generalize_names_duplcheck\n\n\n\nBy default,  \ngeneralize_names_duplcheck\n will apply  \ngeneralize_names\n to a pandas DataFrame column with the minimum number of first name letters and append as many first name letters as necessary until no duplicates are present in the given DataFrame column. An example dataset column that contains the names  \n\n\n\n\n\nExamples\n\n\nReading in a CSV file that has column \nName\n for which we want to generalize the names:\n\n\n\n\nSamuel Eto'o\n\n\nAdam Johnson\n\n\nAndrew Johnson\n\n\n\n\n\n\ndf = pd.read_csv(path)\n\n\n\nApplying \ngeneralize_names_duplcheck\n to generate a new DataFrame with the generalized names without duplicates:          \n\n\ndf_new = generalize_names_duplcheck(df=df, col_name='Name')\n\n\n\n\n\netoo s\n\n\njohnson ad\n\n\njohnson an\n\n\n\n\n\n\n\nDefault Parameters\n\n\ndef generalize_names_duplcheck(df, col_name):\n    \"\"\"\n    Applies mlxtend.text.generalize_names to a DataFrame with 1 first name letter\n    by default and uses more first name letters if duplicates are detected.\n\n    Parameters\n    ----------\n    df : `pandas.DataFrame`\n      DataFrame that contains a column where generalize_names should be applied.\n\n    col_name : `str`\n      Name of the DataFrame column where `generalize_names` function should be applied to.\n\n    Returns\n    ----------\n    df_new : `str`\n      New DataFrame object where generalize_names function has been applied without duplicates.\n\n    \"\"\"",
            "title": "Generalize names duplcheck"
        },
        {
            "location": "/docs/text/generalize_names_duplcheck/#name-generalization-and-duplicates",
            "text": "from mlxtend.text import generalize_names_duplcheck   Note  that using  generalize_names  with few  firstname_output_letters  can result in duplicate entries. E.g., if your dataset contains the names \"Adam Johnson\" and \"Andrew Johnson\", the default setting (i.e., 1 first name letter) will produce the generalized name \"johnson a\" in both cases.  One solution is to increase the number of first name letters in the output by setting the parameter  firstname_output_letters  to a value larger than 1.   An alternative solution is to use the  generalize_names_duplcheck  function if you are working with pandas DataFrames.   The   generalize_names_duplcheck  function can be imported via  from mlxtend.text import generalize_names_duplcheck  By default,   generalize_names_duplcheck  will apply   generalize_names  to a pandas DataFrame column with the minimum number of first name letters and append as many first name letters as necessary until no duplicates are present in the given DataFrame column. An example dataset column that contains the names",
            "title": "Name Generalization and Duplicates"
        },
        {
            "location": "/docs/text/generalize_names_duplcheck/#examples",
            "text": "Reading in a CSV file that has column  Name  for which we want to generalize the names:   Samuel Eto'o  Adam Johnson  Andrew Johnson    df = pd.read_csv(path)  Applying  generalize_names_duplcheck  to generate a new DataFrame with the generalized names without duplicates:            df_new = generalize_names_duplcheck(df=df, col_name='Name')   etoo s  johnson ad  johnson an",
            "title": "Examples"
        },
        {
            "location": "/docs/text/generalize_names_duplcheck/#default-parameters",
            "text": "def generalize_names_duplcheck(df, col_name):\n    \"\"\"\n    Applies mlxtend.text.generalize_names to a DataFrame with 1 first name letter\n    by default and uses more first name letters if duplicates are detected.\n\n    Parameters\n    ----------\n    df : `pandas.DataFrame`\n      DataFrame that contains a column where generalize_names should be applied.\n\n    col_name : `str`\n      Name of the DataFrame column where `generalize_names` function should be applied to.\n\n    Returns\n    ----------\n    df_new : `str`\n      New DataFrame object where generalize_names function has been applied without duplicates.\n\n    \"\"\"",
            "title": "Default Parameters"
        },
        {
            "location": "/docs/text/tokenizer/",
            "text": "mlxtend\n\nSebastian Raschka, last updated: 05/20/2015\n\n\nTokenizer\n\n\nEmoticons\n\n\n\n\nfrom mlxtend.text import extract_emoticons\n\n\n\n\nA function that uses regular expressions to return a list ofemoticons from a text.\n\n\nExample:\n\n\n>>> extract_emoticons('</a>This :) is :( a test :-)!')\n[':)', ':(', ':-)']\n\n\n\nWords and Emoticons\n\n\n\n\nfrom mlxtend.text import extract_words_and_emoticons\n\n\n\n\nA function that uses regular expressions to return a list of words and emoticons from a text.\n\n\nExample:\n\n\n>>> extract_words_and_emoticons('</a>This :) is :( a test :-)!')\n['this', 'is', 'a', 'test', ':)', ':(', ':-)']",
            "title": "Tokenizer"
        },
        {
            "location": "/docs/text/tokenizer/#tokenizer",
            "text": "",
            "title": "Tokenizer"
        },
        {
            "location": "/docs/text/tokenizer/#emoticons",
            "text": "from mlxtend.text import extract_emoticons   A function that uses regular expressions to return a list ofemoticons from a text.  Example:  >>> extract_emoticons('</a>This :) is :( a test :-)!')\n[':)', ':(', ':-)']",
            "title": "Emoticons"
        },
        {
            "location": "/docs/text/tokenizer/#words-and-emoticons",
            "text": "from mlxtend.text import extract_words_and_emoticons   A function that uses regular expressions to return a list of words and emoticons from a text.  Example:  >>> extract_words_and_emoticons('</a>This :) is :( a test :-)!')\n['this', 'is', 'a', 'test', ':)', ':(', ':-)']",
            "title": "Words and Emoticons"
        },
        {
            "location": "/docs/data/iris/",
            "text": "mlxtend\n\nSebastian Raschka, last updated: 06/07/2015\n\n\n\n\n\nIris\n\n\n\n\nfrom mlxtend.data import iris_data\n\n\n\n\nA function that loads the iris dataset into NumPy arrays.\n\n\nSource:\nhttps://archive.ics.uci.edu/ml/datasets/Iris\n \n\n\n\n\nBache, K. & Lichman, M. (2013). UCI Machine Learning Repository. Irvine, CA: University of California, School of Information and Computer Science.\n\n\n\n\n\n\n\n\n\n\niris.csv\n\n\n\n\n\n\n\n\n\n\n\n\nSamples\n\n\n150\n\n\n\n\n\n\nFeatures\n\n\n4\n\n\n\n\n\n\nClasses\n\n\n3\n\n\n\n\n\n\nData Set Characteristics:\n\n\nMultivariate\n\n\n\n\n\n\nAttribute Characteristics:\n\n\nReal\n\n\n\n\n\n\nAssociated Tasks:\n\n\nClassification\n\n\n\n\n\n\nMissing Values\n\n\nNone\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ncolumn\n\n\nattribute\n\n\n\n\n\n\n\n\n\n\n1)\n\n\nsepal length in cm\n\n\n\n\n\n\n2)\n\n\nsepal width in cm\n\n\n\n\n\n\n3)\n\n\npetal length in cm\n\n\n\n\n\n\n4)\n\n\npetal width in cm\n\n\n\n\n\n\n5)\n\n\nclass label\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nclass\n\n\nsamples\n\n\n\n\n\n\n\n\n\n\nIris-setosa\n\n\n50\n\n\n\n\n\n\nIris-versicolor\n\n\n50\n\n\n\n\n\n\nIris-virginica\n\n\n50\n\n\n\n\n\n\n\n\nCreator: R.A. Fisher (1936)\n\n\n\n\n\nExample\n\n\n>>> from mlxtend.data import iris_data\n>>> X, y = iris_data()\n\n\n\n\n\n\nDefault Parameters\n\n\ndef iris_data():\n    \"\"\"Iris flower dataset.\n\n    Returns\n    --------\n    X, y : [n_samples, n_features], [n_class_labels]\n      X is the feature matrix with 150 flower samples as rows,\n      and the 3 feature columns sepal length, sepal width,\n      petal length, and petal width.\n      y is a 1-dimensional array of the class labels where\n      0 = setosa, 1 = versicolor, 2 = virginica.\n      Reference: https://archive.ics.uci.edu/ml/datasets/Iris\n\n     \"\"\"",
            "title": "Iris"
        },
        {
            "location": "/docs/data/iris/#iris",
            "text": "from mlxtend.data import iris_data   A function that loads the iris dataset into NumPy arrays.  Source: https://archive.ics.uci.edu/ml/datasets/Iris     Bache, K. & Lichman, M. (2013). UCI Machine Learning Repository. Irvine, CA: University of California, School of Information and Computer Science.      iris.csv       Samples  150    Features  4    Classes  3    Data Set Characteristics:  Multivariate    Attribute Characteristics:  Real    Associated Tasks:  Classification    Missing Values  None        column  attribute      1)  sepal length in cm    2)  sepal width in cm    3)  petal length in cm    4)  petal width in cm    5)  class label        class  samples      Iris-setosa  50    Iris-versicolor  50    Iris-virginica  50     Creator: R.A. Fisher (1936)",
            "title": "Iris"
        },
        {
            "location": "/docs/data/iris/#example",
            "text": ">>> from mlxtend.data import iris_data\n>>> X, y = iris_data()",
            "title": "Example"
        },
        {
            "location": "/docs/data/iris/#default-parameters",
            "text": "def iris_data():\n    \"\"\"Iris flower dataset.\n\n    Returns\n    --------\n    X, y : [n_samples, n_features], [n_class_labels]\n      X is the feature matrix with 150 flower samples as rows,\n      and the 3 feature columns sepal length, sepal width,\n      petal length, and petal width.\n      y is a 1-dimensional array of the class labels where\n      0 = setosa, 1 = versicolor, 2 = virginica.\n      Reference: https://archive.ics.uci.edu/ml/datasets/Iris\n\n     \"\"\"",
            "title": "Default Parameters"
        },
        {
            "location": "/docs/data/wine/",
            "text": "mlxtend\n\nSebastian Raschka, last updated: 06/07/2015\n\n\n\n\n\nwine\n\n\n\n\nfrom mlxtend.data import wine_data\n\n\n\n\nA function that loads the wine dataset into NumPy arrays.\n\n\nSource: \nhttps://archive.ics.uci.edu/ml/datasets/Wine\n\n\n\n\nBache, K. & Lichman, M. (2013). UCI Machine Learning Repository. Irvine, CA: University of California, School of Information and Computer Science.\n\n\n\n\n\n\n\n\n\n\nwine.csv.csv\n\n\n\n\n\n\n\n\n\n\n\n\nSamples\n\n\n178\n\n\n\n\n\n\nFeatures\n\n\n13\n\n\n\n\n\n\nClasses\n\n\n3\n\n\n\n\n\n\nData Set Characteristics:\n\n\nMultivariate\n\n\n\n\n\n\nAttribute Characteristics:\n\n\nInteger, Real\n\n\n\n\n\n\nAssociated Tasks:\n\n\nClassification\n\n\n\n\n\n\nMissing Values\n\n\nNone\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ncolumn\n\n\nattribute\n\n\n\n\n\n\n\n\n\n\n1)\n\n\nClass Label\n\n\n\n\n\n\n2)\n\n\nAlcohol\n\n\n\n\n\n\n3)\n\n\nMalic acid\n\n\n\n\n\n\n4)\n\n\nAsh\n\n\n\n\n\n\n5)\n\n\nAlcalinity of ash\n\n\n\n\n\n\n6)\n\n\nMagnesium\n\n\n\n\n\n\n7)\n\n\nTotal phenols\n\n\n\n\n\n\n8)\n\n\nFlavanoids\n\n\n\n\n\n\n9)\n\n\nNonflavanoid phenols\n\n\n\n\n\n\n10)\n\n\nProanthocyanins\n\n\n\n\n\n\n11)\n\n\nintensity\n\n\n\n\n\n\n12)\n\n\nHue\n\n\n\n\n\n\n13)\n\n\nOD280/OD315 of diluted wines\n\n\n\n\n\n\n14)\n\n\nProline\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nclass\n\n\nsamples\n\n\n\n\n\n\n\n\n\n\n0\n\n\n59\n\n\n\n\n\n\n1\n\n\n71\n\n\n\n\n\n\n2\n\n\n48\n\n\n\n\n\n\n\n\nOriginal Owners: \n\n\n\n\nForina, M. et al, PARVUS - \nAn Extendible Package for Data Exploration, Classification and Correlation. \nInstitute of Pharmaceutical and Food Analysis and Technologies, Via Brigata Salerno, \n16147 Genoa, Italy. \n\n\n\n\n\n\n\nExample\n\n\n>>> from mlxtend.data import wine_data\n>>> X, y = wine_data()\n\n\n\n\n\n\nDefault Parameters\n\n\ndef wine_data():\n    \"\"\"Iris flower dataset.\n\n    Returns\n    --------\n    X, y : [n_samples, n_features], [n_class_labels]\n      X is the feature matrix with 178 wine samples as rows\n      and 13 feature columns.\n      y is a 1-dimensional array of the 3 class labels 0, 1, 2\n      Source: https://archive.ics.uci.edu/ml/datasets/Wine\n\n    \"\"\"",
            "title": "Wine"
        },
        {
            "location": "/docs/data/wine/#wine",
            "text": "from mlxtend.data import wine_data   A function that loads the wine dataset into NumPy arrays.  Source:  https://archive.ics.uci.edu/ml/datasets/Wine   Bache, K. & Lichman, M. (2013). UCI Machine Learning Repository. Irvine, CA: University of California, School of Information and Computer Science.      wine.csv.csv       Samples  178    Features  13    Classes  3    Data Set Characteristics:  Multivariate    Attribute Characteristics:  Integer, Real    Associated Tasks:  Classification    Missing Values  None        column  attribute      1)  Class Label    2)  Alcohol    3)  Malic acid    4)  Ash    5)  Alcalinity of ash    6)  Magnesium    7)  Total phenols    8)  Flavanoids    9)  Nonflavanoid phenols    10)  Proanthocyanins    11)  intensity    12)  Hue    13)  OD280/OD315 of diluted wines    14)  Proline        class  samples      0  59    1  71    2  48     Original Owners:    Forina, M. et al, PARVUS - \nAn Extendible Package for Data Exploration, Classification and Correlation. \nInstitute of Pharmaceutical and Food Analysis and Technologies, Via Brigata Salerno, \n16147 Genoa, Italy.",
            "title": "wine"
        },
        {
            "location": "/docs/data/wine/#example",
            "text": ">>> from mlxtend.data import wine_data\n>>> X, y = wine_data()",
            "title": "Example"
        },
        {
            "location": "/docs/data/wine/#default-parameters",
            "text": "def wine_data():\n    \"\"\"Iris flower dataset.\n\n    Returns\n    --------\n    X, y : [n_samples, n_features], [n_class_labels]\n      X is the feature matrix with 178 wine samples as rows\n      and 13 feature columns.\n      y is a 1-dimensional array of the 3 class labels 0, 1, 2\n      Source: https://archive.ics.uci.edu/ml/datasets/Wine\n\n    \"\"\"",
            "title": "Default Parameters"
        },
        {
            "location": "/docs/data/autompg/",
            "text": "mlxtend\n\nSebastian Raschka, last updated: 06/07/2015\n\n\n\n\n\nAuto MPG\n\n\n\n\nfrom mlxtend.data import autompg_data\n\n\n\n\nA function that loads the autompg dataset into NumPy arrays.\n\n\nSource: \nhttps://archive.ics.uci.edu/ml/datasets/Auto+MPG\n\n\nThe Auto-MPG dataset for regression analysis. The target (\ny\n) is defined as the miles per gallon (mpg) for 392 automobiles (6 rows containing \"NaN\"s have been removed. The 8 feature columns are:\n\n\n\n\ncylinders: multi-valued discrete \n\n\ndisplacement: continuous \n\n\nhorsepower: continuous \n\n\nweight: continuous \n\n\nacceleration: continuous \n\n\nmodel year: multi-valued discrete \n\n\norigin: multi-valued discrete \n\n\ncar name: string (unique for each instance)\n\n\n\n\n\n\nQuinlan,R. (1993). Combining Instance-Based and Model-Based Learning. In Proceedings on the Tenth International Conference of Machine Learning, 236-243, University of Massachusetts, Amherst. Morgan Kaufmann.\n\n\n\n\n\n\n\nExample\n\n\n>>> from mlxtend.data import autompg_data\n>>> X, y = autompg_data()\n\n\n\n\n\n\nDefault Parameters\n\n\ndef autompg_data():\n    \"\"\"Auto MPG dataset.\n\n    Returns\n    --------\n    X, y : [n_samples, n_features], [n_targets]\n      X is the feature matrix with 392 auto samples as rows\n      and 8 feature columns (6 rows with NaNs removed).\n      y is a 1-dimensional array of the target MPG values.\n      Source: https://archive.ics.uci.edu/ml/datasets/Auto+MPG\n\n    \"\"\"",
            "title": "Autompg"
        },
        {
            "location": "/docs/data/autompg/#auto-mpg",
            "text": "from mlxtend.data import autompg_data   A function that loads the autompg dataset into NumPy arrays.  Source:  https://archive.ics.uci.edu/ml/datasets/Auto+MPG  The Auto-MPG dataset for regression analysis. The target ( y ) is defined as the miles per gallon (mpg) for 392 automobiles (6 rows containing \"NaN\"s have been removed. The 8 feature columns are:   cylinders: multi-valued discrete   displacement: continuous   horsepower: continuous   weight: continuous   acceleration: continuous   model year: multi-valued discrete   origin: multi-valued discrete   car name: string (unique for each instance)    Quinlan,R. (1993). Combining Instance-Based and Model-Based Learning. In Proceedings on the Tenth International Conference of Machine Learning, 236-243, University of Massachusetts, Amherst. Morgan Kaufmann.",
            "title": "Auto MPG"
        },
        {
            "location": "/docs/data/autompg/#example",
            "text": ">>> from mlxtend.data import autompg_data\n>>> X, y = autompg_data()",
            "title": "Example"
        },
        {
            "location": "/docs/data/autompg/#default-parameters",
            "text": "def autompg_data():\n    \"\"\"Auto MPG dataset.\n\n    Returns\n    --------\n    X, y : [n_samples, n_features], [n_targets]\n      X is the feature matrix with 392 auto samples as rows\n      and 8 feature columns (6 rows with NaNs removed).\n      y is a 1-dimensional array of the target MPG values.\n      Source: https://archive.ics.uci.edu/ml/datasets/Auto+MPG\n\n    \"\"\"",
            "title": "Default Parameters"
        },
        {
            "location": "/docs/data/mnist/",
            "text": "mlxtend\n\nSebastian Raschka, last updated: 06/23/2015\n\n\n\n\n\nMNIST\n\n\n\n\nfrom mlxtend.data import mnist_data\n\n\n\n\nA function that loads 5000 shuffled and labeled training samples from the MNIST (handwritten digits) dataset into NumPy arrays. \n\n\nThe feature matrix \nX\n consists of 5000 rows where each row represents the unrolled 784 pixel feature vector of the 28x28 pixel images.\n\nThe unique class labels in \ny\n are the integers 0-9 corresponding to the respective digits in the feature matrix.\n\n\nDataset source: \nhttp://yann.lecun.com/exdb/mnist/\n\n\n\n\nY. LeCun and C. Cortes. Mnist handwritten digit database. AT&T Labs [Online]. Available: http://yann. lecun. com/exdb/mnist, 2010.\n\n\n\n\n\n\n\nExample\n\n\n>>> from mlxtend.data import mnist_data\n>>> X, y = mnist_data()\n\n\n\nVisualizing the images:\n\n\n>>> import matplotlib.pyplot as plt\n>>> def plot_digit(X, y, idx):\n...    img = X[idx].reshape(28,28)\n...    plt.imshow(img, cmap='Greys',  interpolation='nearest')\n...    plt.title('true label: %d' % y[idx])\n...    plt.show()\n>>> plot_digit(X, y, 4)\n\n\n\n\n\n\n\n\nDefault Parameters\n\n\ndef mnist_data():\n    \"\"\"5000 samples from the MNIST handwritten digits datast.\n    Data Source: http://yann.lecun.com/exdb/mnist/\n\n    Returns\n    --------\n    X, y : [n_samples, n_features], [n_class_labels]\n      X is the feature matrix with 5000 image samples as rows,\n      each row consists of 28x28 pixels that were unrolled into\n      784 pixel feature vectors.\n      y contains the 10 unique class labels 0-9.\n\n    \"\"\"",
            "title": "Mnist"
        },
        {
            "location": "/docs/data/mnist/#mnist",
            "text": "from mlxtend.data import mnist_data   A function that loads 5000 shuffled and labeled training samples from the MNIST (handwritten digits) dataset into NumPy arrays.   The feature matrix  X  consists of 5000 rows where each row represents the unrolled 784 pixel feature vector of the 28x28 pixel images. \nThe unique class labels in  y  are the integers 0-9 corresponding to the respective digits in the feature matrix.  Dataset source:  http://yann.lecun.com/exdb/mnist/   Y. LeCun and C. Cortes. Mnist handwritten digit database. AT&T Labs [Online]. Available: http://yann. lecun. com/exdb/mnist, 2010.",
            "title": "MNIST"
        },
        {
            "location": "/docs/data/mnist/#example",
            "text": ">>> from mlxtend.data import mnist_data\n>>> X, y = mnist_data()  Visualizing the images:  >>> import matplotlib.pyplot as plt\n>>> def plot_digit(X, y, idx):\n...    img = X[idx].reshape(28,28)\n...    plt.imshow(img, cmap='Greys',  interpolation='nearest')\n...    plt.title('true label: %d' % y[idx])\n...    plt.show()\n>>> plot_digit(X, y, 4)",
            "title": "Example"
        },
        {
            "location": "/docs/data/mnist/#default-parameters",
            "text": "def mnist_data():\n    \"\"\"5000 samples from the MNIST handwritten digits datast.\n    Data Source: http://yann.lecun.com/exdb/mnist/\n\n    Returns\n    --------\n    X, y : [n_samples, n_features], [n_class_labels]\n      X is the feature matrix with 5000 image samples as rows,\n      each row consists of 28x28 pixels that were unrolled into\n      784 pixel feature vectors.\n      y contains the 10 unique class labels 0-9.\n\n    \"\"\"",
            "title": "Default Parameters"
        },
        {
            "location": "/changelog/",
            "text": "0.3.0\n\n\n\n\nSequential Feature Selection algorithms: \nSFS\n, \nSFFS\n, and \nSFBS\n\n\n\n\n0.2.9\n\n\n\n\nChanged \nregularization\n & \nlambda\n parameters in \nLogisticRegression\n to single parameter \nl2_lambda\n\n\n\n\n0.2.8\n\n\n\n\nAPI changes:\n\n\nmlxtend.sklearn.EnsembleClassifier\n -> \nmlxtend.classifier.EnsembleClassifier\n\n\nmlxtend.sklearn.ColumnSelector\n -> \nmlxtend.feature_selection.ColumnSelector\n\n\nmlxtend.sklearn.DenseTransformer\n -> \nmlxtend.preprocessing.DenseTransformer\n\n\nmlxtend.pandas.standardizing\n ->  \nmlxtend.preprocessing.standardizing\n \n\n\nmlxtend.pandas.minmax_scaling\n ->  \nmlxtend.preprocessing.minmax_scaling\n\n\nmlxtend.matplotlib\n -> \nmlxtend.plotting\n \n\n\n\n\n\n\nAdded momentum learning parameter (alpha coefficient) to \nmlxtend.classifier.NeuralNetMLP\n.\n\n\nAdded adaptive learning rate (decrease constant) to \nmlxtend.classifier.NeuralNetMLP\n.\n\n\nmlxtend.pandas.minmax_scaling\n became \nmlxtend.preprocessing.minmax_scaling\n  and also supports NumPy arrays now\n\n\nmlxtend.pandas.standardizing\n became \nmlxtend.preprocessing.standardizing\n and now supports both NumPy arrays and pandas DataFrames; also, now \nddof\n parameters to set the degrees of freedom when calculating the standard deviation\n\n\n\n\n0.2.7\n\n\n\n\nAdded multilayer perceptron (feedforward artificial neural network) classifier as \nmlxtend.classifier.NeuralNetMLP\n.\n\n\nAdded 5000 labeled trainingsamples from the MNIST handwritten digits dataset to \nmlxtend.data\n\n\n\n\n0.2.6\n\n\n\n\nAdded ordinary least square regression using different solvers (gradient and stochastic gradient descent, and the closed form solution (normal equation)\n\n\nAdded option for random weight initialization to logistic regression classifier and updated l2 regularization\n\n\nAdded \nwine\n dataset to \nmlxtend.data\n \n\n\nAdded \ninvert_axes\n parameter \nmlxtend.matplotlib.enrichtment_plot\n to optionally plot the \"Count\" on the x-axis\n\n\nNew \nverbose\n parameter for \nmlxtend.sklearn.EnsembleClassifier\n by \nAlejandro C. Bahnsen\n\n\nAdded \nmlxtend.pandas.standardizing\n to standardize columns in a Pandas DataFrame\n\n\nAdded parameters \nlinestyles\n and \nmarkers\n to \nmlxtend.matplotlib.enrichment_plot\n\n\nmlxtend.regression.lin_regplot\n automatically adds np.newaxis and works w. python lists\n\n\nAdded tokenizers: \nmlxtend.text.extract_emoticons\n and \nmlxtend.text.extract_words_and_emoticons\n\n\n\n\n0.2.5\n\n\n\n\nAdded Sequential Backward Selection (mlxtend.sklearn.SBS)\n\n\nAdded \nX_highlight\n parameter to \nmlxtend.evaluate.plot_decision_regions\n for highlighting test data points.\n\n\nAdded mlxtend.regression.lin_regplot to plot the fitted line from linear regression.\n\n\nAdded mlxtend.matplotlib.stacked_barplot to conveniently produce stacked barplots using pandas \nDataFrame\ns.\n\n\nAdded mlxtend.matplotlib.enrichment_plot\n\n\n\n\n0.2.4\n\n\n\n\nAdded \nscoring\n to \nmlxtend.evaluate.learning_curves\n (by user pfsq)\n\n\nFixed setup.py bug caused by the missing README.html file\n\n\nmatplotlib.category_scatter for pandas DataFrames and Numpy arrays\n\n\n\n\n0.2.3\n\n\n\n\nAdded Logistic regression\n\n\nGradient descent and stochastic gradient descent perceptron was changed\n  to Adaline (Adaptive Linear Neuron)\n\n\nPerceptron and Adaline for {0, 1} classes\n\n\nAdded \nmlxtend.preprocessing.shuffle_arrays_unison\n function to \n  shuffle one or more NumPy arrays.\n\n\nAdded shuffle and random seed parameter to stochastic gradient descent classifier.\n\n\nAdded \nrstrip\n parameter to \nmlxtend.file_io.find_filegroups\n to allow trimming of base names.\n\n\nAdded \nignore_substring\n parameter to \nmlxtend.file_io.find_filegroups\n and \nfind_files\n.\n\n\nReplaced .rstrip in \nmlxtend.file_io.find_filegroups\n with more robust regex.\n\n\nGridsearch support for \nmlxtend.sklearn.EnsembleClassifier\n\n\n\n\n0.2.2\n\n\n\n\nImproved robustness of EnsembleClassifier.\n\n\nExtended plot_decision_regions() functionality for plotting 1D decision boundaries.\n\n\nFunction matplotlib.plot_decision_regions was reorganized  to evaluate.plot_decision_regions .\n\n\nevaluate.plot_learning_curves() function added.\n\n\nAdded Rosenblatt, gradient descent, and stochastic gradient descent perceptrons.\n\n\n\n\n0.2.1\n\n\n\n\nAdded mlxtend.pandas.minmax_scaling - a function to rescale pandas DataFrame columns.\n\n\nSlight update to the EnsembleClassifier interface (additional \nvoting\n parameter)\n\n\nFixed EnsembleClassifier to return correct class labels if class labels are not\n  integers from 0 to n.\n\n\nAdded new matplotlib function to plot decision regions of classifiers.\n\n\n\n\n0.2.0\n\n\n\n\nImproved mlxtend.text.generalize_duplcheck to remove duplicates and prevent endless looping issue.\n\n\nAdded \nrecursive\n search parameter to mlxtend.file_io.find_files.\n\n\nAdded \ncheck_ext\n parameter mlxtend.file_io.find_files to search based on file extensions.\n\n\nDefault parameter to ignore invisible files for mlxtend.file_io.find.\n\n\nAdded \ntransform\n and \nfit_transform\n to the \nEnsembleClassifier\n.\n\n\nAdded mlxtend.file_io.find_filegroups function.\n\n\n\n\n0.1.9\n\n\n\n\nImplemented scikit-learn EnsembleClassifier (majority voting rule) class.\n\n\n\n\n0.1.8\n\n\n\n\nImprovements to mlxtend.text.generalize_names to handle certain Dutch last name prefixes (van, van der, de, etc.).\n\n\nAdded mlxtend.text.generalize_name_duplcheck function to apply mlxtend.text.generalize_names function to a pandas DataFrame without creating duplicates.\n\n\n\n\n0.1.7\n\n\n\n\nAdded text utilities with name generalization function.\n\n\nAdded  and file_io utilities.\n\n\n\n\n0.1.6\n\n\n\n\nAdded combinations and permutations estimators.\n\n\n\n\n0.1.5\n\n\n\n\nAdded \nDenseTransformer\n for pipelines and grid search.\n\n\n\n\n0.1.4\n\n\n\n\nmean_centering\n function is now a Class that creates \nMeanCenterer\n objects\n  that can be used to fit data via the \nfit\n method, and center data at the column\n  means via the \ntransform\n and \nfit_transform\n method.\n\n\n\n\n0.1.3\n\n\n\n\nAdded \npreprocessing\n module and \nmean_centering\n function.\n\n\n\n\n0.1.2\n\n\n\n\nAdded \nmatplotlib\n utilities and \nremove_borders\n function.\n\n\n\n\n0.1.1\n\n\n\n\nSimplified code for ColumnSelector.",
            "title": "Changelog"
        },
        {
            "location": "/changelog/#030",
            "text": "Sequential Feature Selection algorithms:  SFS ,  SFFS , and  SFBS",
            "title": "0.3.0"
        },
        {
            "location": "/changelog/#029",
            "text": "Changed  regularization  &  lambda  parameters in  LogisticRegression  to single parameter  l2_lambda",
            "title": "0.2.9"
        },
        {
            "location": "/changelog/#028",
            "text": "API changes:  mlxtend.sklearn.EnsembleClassifier  ->  mlxtend.classifier.EnsembleClassifier  mlxtend.sklearn.ColumnSelector  ->  mlxtend.feature_selection.ColumnSelector  mlxtend.sklearn.DenseTransformer  ->  mlxtend.preprocessing.DenseTransformer  mlxtend.pandas.standardizing  ->   mlxtend.preprocessing.standardizing    mlxtend.pandas.minmax_scaling  ->   mlxtend.preprocessing.minmax_scaling  mlxtend.matplotlib  ->  mlxtend.plotting      Added momentum learning parameter (alpha coefficient) to  mlxtend.classifier.NeuralNetMLP .  Added adaptive learning rate (decrease constant) to  mlxtend.classifier.NeuralNetMLP .  mlxtend.pandas.minmax_scaling  became  mlxtend.preprocessing.minmax_scaling   and also supports NumPy arrays now  mlxtend.pandas.standardizing  became  mlxtend.preprocessing.standardizing  and now supports both NumPy arrays and pandas DataFrames; also, now  ddof  parameters to set the degrees of freedom when calculating the standard deviation",
            "title": "0.2.8"
        },
        {
            "location": "/changelog/#027",
            "text": "Added multilayer perceptron (feedforward artificial neural network) classifier as  mlxtend.classifier.NeuralNetMLP .  Added 5000 labeled trainingsamples from the MNIST handwritten digits dataset to  mlxtend.data",
            "title": "0.2.7"
        },
        {
            "location": "/changelog/#026",
            "text": "Added ordinary least square regression using different solvers (gradient and stochastic gradient descent, and the closed form solution (normal equation)  Added option for random weight initialization to logistic regression classifier and updated l2 regularization  Added  wine  dataset to  mlxtend.data    Added  invert_axes  parameter  mlxtend.matplotlib.enrichtment_plot  to optionally plot the \"Count\" on the x-axis  New  verbose  parameter for  mlxtend.sklearn.EnsembleClassifier  by  Alejandro C. Bahnsen  Added  mlxtend.pandas.standardizing  to standardize columns in a Pandas DataFrame  Added parameters  linestyles  and  markers  to  mlxtend.matplotlib.enrichment_plot  mlxtend.regression.lin_regplot  automatically adds np.newaxis and works w. python lists  Added tokenizers:  mlxtend.text.extract_emoticons  and  mlxtend.text.extract_words_and_emoticons",
            "title": "0.2.6"
        },
        {
            "location": "/changelog/#025",
            "text": "Added Sequential Backward Selection (mlxtend.sklearn.SBS)  Added  X_highlight  parameter to  mlxtend.evaluate.plot_decision_regions  for highlighting test data points.  Added mlxtend.regression.lin_regplot to plot the fitted line from linear regression.  Added mlxtend.matplotlib.stacked_barplot to conveniently produce stacked barplots using pandas  DataFrame s.  Added mlxtend.matplotlib.enrichment_plot",
            "title": "0.2.5"
        },
        {
            "location": "/changelog/#024",
            "text": "Added  scoring  to  mlxtend.evaluate.learning_curves  (by user pfsq)  Fixed setup.py bug caused by the missing README.html file  matplotlib.category_scatter for pandas DataFrames and Numpy arrays",
            "title": "0.2.4"
        },
        {
            "location": "/changelog/#023",
            "text": "Added Logistic regression  Gradient descent and stochastic gradient descent perceptron was changed\n  to Adaline (Adaptive Linear Neuron)  Perceptron and Adaline for {0, 1} classes  Added  mlxtend.preprocessing.shuffle_arrays_unison  function to \n  shuffle one or more NumPy arrays.  Added shuffle and random seed parameter to stochastic gradient descent classifier.  Added  rstrip  parameter to  mlxtend.file_io.find_filegroups  to allow trimming of base names.  Added  ignore_substring  parameter to  mlxtend.file_io.find_filegroups  and  find_files .  Replaced .rstrip in  mlxtend.file_io.find_filegroups  with more robust regex.  Gridsearch support for  mlxtend.sklearn.EnsembleClassifier",
            "title": "0.2.3"
        },
        {
            "location": "/changelog/#022",
            "text": "Improved robustness of EnsembleClassifier.  Extended plot_decision_regions() functionality for plotting 1D decision boundaries.  Function matplotlib.plot_decision_regions was reorganized  to evaluate.plot_decision_regions .  evaluate.plot_learning_curves() function added.  Added Rosenblatt, gradient descent, and stochastic gradient descent perceptrons.",
            "title": "0.2.2"
        },
        {
            "location": "/changelog/#021",
            "text": "Added mlxtend.pandas.minmax_scaling - a function to rescale pandas DataFrame columns.  Slight update to the EnsembleClassifier interface (additional  voting  parameter)  Fixed EnsembleClassifier to return correct class labels if class labels are not\n  integers from 0 to n.  Added new matplotlib function to plot decision regions of classifiers.",
            "title": "0.2.1"
        },
        {
            "location": "/changelog/#020",
            "text": "Improved mlxtend.text.generalize_duplcheck to remove duplicates and prevent endless looping issue.  Added  recursive  search parameter to mlxtend.file_io.find_files.  Added  check_ext  parameter mlxtend.file_io.find_files to search based on file extensions.  Default parameter to ignore invisible files for mlxtend.file_io.find.  Added  transform  and  fit_transform  to the  EnsembleClassifier .  Added mlxtend.file_io.find_filegroups function.",
            "title": "0.2.0"
        },
        {
            "location": "/changelog/#019",
            "text": "Implemented scikit-learn EnsembleClassifier (majority voting rule) class.",
            "title": "0.1.9"
        },
        {
            "location": "/changelog/#018",
            "text": "Improvements to mlxtend.text.generalize_names to handle certain Dutch last name prefixes (van, van der, de, etc.).  Added mlxtend.text.generalize_name_duplcheck function to apply mlxtend.text.generalize_names function to a pandas DataFrame without creating duplicates.",
            "title": "0.1.8"
        },
        {
            "location": "/changelog/#017",
            "text": "Added text utilities with name generalization function.  Added  and file_io utilities.",
            "title": "0.1.7"
        },
        {
            "location": "/changelog/#016",
            "text": "Added combinations and permutations estimators.",
            "title": "0.1.6"
        },
        {
            "location": "/changelog/#015",
            "text": "Added  DenseTransformer  for pipelines and grid search.",
            "title": "0.1.5"
        },
        {
            "location": "/changelog/#014",
            "text": "mean_centering  function is now a Class that creates  MeanCenterer  objects\n  that can be used to fit data via the  fit  method, and center data at the column\n  means via the  transform  and  fit_transform  method.",
            "title": "0.1.4"
        },
        {
            "location": "/changelog/#013",
            "text": "Added  preprocessing  module and  mean_centering  function.",
            "title": "0.1.3"
        },
        {
            "location": "/changelog/#012",
            "text": "Added  matplotlib  utilities and  remove_borders  function.",
            "title": "0.1.2"
        },
        {
            "location": "/changelog/#011",
            "text": "Simplified code for ColumnSelector.",
            "title": "0.1.1"
        },
        {
            "location": "/installation/",
            "text": "Installing mlxtend\n\n\nTo install \nmlxtend\n, just execute  \n\n\npip install mlxtend\n\n\n\nThe \nmlxtend\n version on PyPI may always one step behind; you can install the latest development version from the GitHub repository by executing\n\n\npip install git+git://github.com/rasbt/mlxtend.git#egg=mlxtend\n\n\n\nAlternatively, you download the package manually from the Python Package Index \nhttps://pypi.python.org/pypi/mlxtend\n, unzip it, navigate into the package, and use the command:\n\n\npython setup.py install",
            "title": "Installation"
        },
        {
            "location": "/installation/#installing-mlxtend",
            "text": "To install  mlxtend , just execute    pip install mlxtend  The  mlxtend  version on PyPI may always one step behind; you can install the latest development version from the GitHub repository by executing  pip install git+git://github.com/rasbt/mlxtend.git#egg=mlxtend  Alternatively, you download the package manually from the Python Package Index  https://pypi.python.org/pypi/mlxtend , unzip it, navigate into the package, and use the command:  python setup.py install",
            "title": "Installing mlxtend"
        },
        {
            "location": "/contributing/",
            "text": "How to Contribute\n\n\nI would be very happy about any kind of contributions that help to improve and extend the functionality of mlxtend.\n\n\n\n\n\n\nQuick contributor checklist\n\n\n[ ]  Open a new \"issue\" on GitHub to discuss the new feature / bugfix\n\n[ ]  Create and checkout a new topic branch \n\n[ ]  Implement new feature or apply bugfix\n\n[ ]  Add appropriate unit test functions\n\n[ ]  Run \nnosetests -sv\n and make sure that all unit tests pass\n\n[ ]  Add a note about the change to the \n./docs/sources/CHANGELOG.md\n file\n\n[ ]  Modify documentation in \n./docs/sources/\n if appropriate\n\n[ ]  Push the topic branch to the server and create a pull request\n\n\n\n\nGetting Started\n\n\n\n\nIf you don't have a \nGitHub\n account yet, please create one to contribute to this project.\n\n\nPlease submit a ticket for your issue to discuss the fix or new feature before too much time and effort is spent for the implementation.\n\n\n\n\n\n\n\n\nFork the \nmlxtend\n repository from the GitHub web interface.\n\n\n\n\n\n\n\n\nClone the \nmlxtend\n repository to your local machine\n\n\ngit clone https://github.com/your_username/mlxtend.git\n\n\n\n\n\n\n\n\n\n\n\n\nMaking Changes\n\n\n\n\nPlease avoid working directly on the master branch but create a new feature branch:\n\n\ngit branch new_feature\n\n\ngit checkout new_feature\n\n\n\n\n\n\nWhen you make changes, please provide meaningful \ncommit\n messages:\n\n\ngit add edited_file\n \n\n\ngit commit -m 'my note'\n \n\n\n\n\n\n\nMake an entry in the \nmlxtend/docs/source/changelog.md\n file.\n\n\nAdd tests to the \nmlxtend/tests\n directory.\n\n\nRun all tests (e.g., via \nnosetests\n  or \npytest\n).\n\n\nIf it is a new feature, it would be nice (but not necessary) if you could update the documentation.\n\n\nPush your changes to a topic branch:\n\n\ngit push -u origin my_feature\n\n\n\n\n\n\nSubmit a \npull request\n from your forked repository via the GitHub web interface.\n\n\n\n\n\n\n\n\n\nNotes for Developers\n\n\nBuilding the documentation\n\n\nThe documentation is build using \nmkdocs\n. \nIf you are adding a new document, please add it to the \n~/github/mlxtend/docs/mkdocs.yml\n file.\n\n\nIn order to view the documentation locally, execute \nmkdocs serve\n from the \nmlxtend/docs\n directory.\n\n\nFor example,\n\n\n~/github/mlxtend/docs >mkdocs serve\n\n\n\nIf everything looks fine, the documentation can be build by executing\n\n\n~/github/mlxtend/docs >mkdocs build",
            "title": "Contributing"
        },
        {
            "location": "/contributing/#how-to-contribute",
            "text": "I would be very happy about any kind of contributions that help to improve and extend the functionality of mlxtend.",
            "title": "How to Contribute"
        },
        {
            "location": "/contributing/#quick-contributor-checklist",
            "text": "[ ]  Open a new \"issue\" on GitHub to discuss the new feature / bugfix \n[ ]  Create and checkout a new topic branch  \n[ ]  Implement new feature or apply bugfix \n[ ]  Add appropriate unit test functions \n[ ]  Run  nosetests -sv  and make sure that all unit tests pass \n[ ]  Add a note about the change to the  ./docs/sources/CHANGELOG.md  file \n[ ]  Modify documentation in  ./docs/sources/  if appropriate \n[ ]  Push the topic branch to the server and create a pull request",
            "title": "Quick contributor checklist"
        },
        {
            "location": "/contributing/#getting-started",
            "text": "If you don't have a  GitHub  account yet, please create one to contribute to this project.  Please submit a ticket for your issue to discuss the fix or new feature before too much time and effort is spent for the implementation.     Fork the  mlxtend  repository from the GitHub web interface.     Clone the  mlxtend  repository to your local machine  git clone https://github.com/your_username/mlxtend.git",
            "title": "Getting Started"
        },
        {
            "location": "/contributing/#making-changes",
            "text": "Please avoid working directly on the master branch but create a new feature branch:  git branch new_feature  git checkout new_feature    When you make changes, please provide meaningful  commit  messages:  git add edited_file    git commit -m 'my note'      Make an entry in the  mlxtend/docs/source/changelog.md  file.  Add tests to the  mlxtend/tests  directory.  Run all tests (e.g., via  nosetests   or  pytest ).  If it is a new feature, it would be nice (but not necessary) if you could update the documentation.  Push your changes to a topic branch:  git push -u origin my_feature    Submit a  pull request  from your forked repository via the GitHub web interface.",
            "title": "Making Changes"
        },
        {
            "location": "/contributing/#notes-for-developers",
            "text": "",
            "title": "Notes for Developers"
        },
        {
            "location": "/contributing/#building-the-documentation",
            "text": "The documentation is build using  mkdocs . \nIf you are adding a new document, please add it to the  ~/github/mlxtend/docs/mkdocs.yml  file.  In order to view the documentation locally, execute  mkdocs serve  from the  mlxtend/docs  directory.  For example,  ~/github/mlxtend/docs >mkdocs serve  If everything looks fine, the documentation can be build by executing  ~/github/mlxtend/docs >mkdocs build",
            "title": "Building the documentation"
        }
    ]
}
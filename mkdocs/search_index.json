{
    "docs": [
        {
            "location": "/",
            "text": "A library consisting of useful tools and extensions for the day-to-day data science tasks.\n\n\nSebastian Raschka 2014-2015\n\n\nCurrent version: 0.2.6\n\n\n\n\nLinks\n\n\n\n\nDocumentation: \nhttp://rasbt.github.io/mlxtend/\n\n\nSource code repository: \nhttps://github.com/rasbt/mlxtend\n\n\nPyPI: \nhttps://pypi.python.org/pypi/mlxtend",
            "title": "Home"
        },
        {
            "location": "/#links",
            "text": "Documentation:  http://rasbt.github.io/mlxtend/  Source code repository:  https://github.com/rasbt/mlxtend  PyPI:  https://pypi.python.org/pypi/mlxtend",
            "title": "Links"
        },
        {
            "location": "/docs/classifier/adaline/",
            "text": "mlxtend\n\nSebastian Raschka, 05/14/2015\n\n\nAdaline\n\n\nImplementation of Adaline (Adaptive Linear Neuron; a single-layer artificial neural network) using the Widrow-Hoff delta rule. [2].\n\n\n[2] B. Widrow, M. E. Hoff, et al. Adaptive switching circuits. 1960.\n\n\n\n\nFor more usage examples please see the \nIPython Notebook\n.\n\n\nA detailed explanation about the Adaline learning algorithm can be found here \nArtificial Neurons and Single-Layer Neural Networks\n- How Machine Learning Algorithms Work Part 1\n.\n\n\n\n\n\nExample 1 - Stochastic Gradient Descent\n\n\nfrom mlxtend.data import iris_data\nfrom mlxtend.evaluate import plot_decision_regions\nfrom mlxtend.classifier import Adaline\nimport matplotlib.pyplot as plt\n\n# Loading Data\n\nX, y = iris_data()\nX = X[:, [0, 3]] # sepal length and petal width\nX = X[0:100] # class 0 and class 1\ny = y[0:100] # class 0 and class 1\n\n# standardize\nX[:,0] = (X[:,0] - X[:,0].mean()) / X[:,0].std()\nX[:,1] = (X[:,1] - X[:,1].mean()) / X[:,1].std()\n\n\nada = Adaline(epochs=30, eta=0.01, learning='sgd', random_seed=1)\nada.fit(X, y)\nplot_decision_regions(X, y, clf=ada)\nplt.title('Adaline - Stochastic Gradient Descent')\nplt.show()\n\nplt.plot(range(len(ada.cost_)), ada.cost_, marker='o')\nplt.xlabel('Iterations')\nplt.ylabel('Cost')\nplt.show()\n\n\n\n\n\n\n\n\n\n\nDefault Parameters\n\n\nclass Adaline(object):\n    \"\"\" ADAptive LInear NEuron classifier.\n\n    Parameters\n    ------------\n    eta : float\n      Learning rate (between 0.0 and 1.0)\n\n    epochs : int\n      Passes over the training dataset.\n\n    learning : str (default: sgd)\n      Gradient decent (gd) or stochastic gradient descent (sgd)\n\n    shuffle : bool (default: False)\n        Shuffles training data every epoch if True to prevent circles.\n\n    random_seed : int (default: None)\n        Set random state for shuffling and initializing the weights.\n\n\n    Attributes\n    -----------\n    w_ : 1d-array\n      Weights after fitting.\n\n    cost_ : list\n      Sum of squared errors after each epoch.\n\n    \"\"\"\n\n\n\n\n\n\nMethods\n\n\n    def fit(self, X, y, init_weights=True):\n        \"\"\" Fit training data.\n\n        Parameters\n        ----------\n        X : {array-like, sparse matrix}, shape = [n_samples, n_features]\n            Training vectors, where n_samples is the number of samples and\n            n_features is the number of features.\n\n        y : array-like, shape = [n_samples]\n            Target values.\n\n        init_weights : bool (default: True)\n            (Re)initializes weights to small random floats if True.\n\n        Returns\n        -------\n        self : object\n        \"\"\"\n\n\n\n    def predict(self, X):\n        \"\"\"\n        Predict class labels for X.\n\n        Parameters\n        ----------\n        X : {array-like, sparse matrix}, shape = [n_samples, n_features]\n            Training vectors, where n_samples is the number of samples and\n            n_features is the number of features.\n\n        Returns\n        ----------\n        class : int\n          Predicted class label.\n\n        \"\"\"",
            "title": "Adaline"
        },
        {
            "location": "/docs/classifier/adaline/#adaline",
            "text": "Implementation of Adaline (Adaptive Linear Neuron; a single-layer artificial neural network) using the Widrow-Hoff delta rule. [2].  [2] B. Widrow, M. E. Hoff, et al. Adaptive switching circuits. 1960.   For more usage examples please see the  IPython Notebook .  A detailed explanation about the Adaline learning algorithm can be found here  Artificial Neurons and Single-Layer Neural Networks\n- How Machine Learning Algorithms Work Part 1 .   Example 1 - Stochastic Gradient Descent  from mlxtend.data import iris_data\nfrom mlxtend.evaluate import plot_decision_regions\nfrom mlxtend.classifier import Adaline\nimport matplotlib.pyplot as plt\n\n# Loading Data\n\nX, y = iris_data()\nX = X[:, [0, 3]] # sepal length and petal width\nX = X[0:100] # class 0 and class 1\ny = y[0:100] # class 0 and class 1\n\n# standardize\nX[:,0] = (X[:,0] - X[:,0].mean()) / X[:,0].std()\nX[:,1] = (X[:,1] - X[:,1].mean()) / X[:,1].std()\n\n\nada = Adaline(epochs=30, eta=0.01, learning='sgd', random_seed=1)\nada.fit(X, y)\nplot_decision_regions(X, y, clf=ada)\nplt.title('Adaline - Stochastic Gradient Descent')\nplt.show()\n\nplt.plot(range(len(ada.cost_)), ada.cost_, marker='o')\nplt.xlabel('Iterations')\nplt.ylabel('Cost')\nplt.show()     Default Parameters  class Adaline(object):\n    \"\"\" ADAptive LInear NEuron classifier.\n\n    Parameters\n    ------------\n    eta : float\n      Learning rate (between 0.0 and 1.0)\n\n    epochs : int\n      Passes over the training dataset.\n\n    learning : str (default: sgd)\n      Gradient decent (gd) or stochastic gradient descent (sgd)\n\n    shuffle : bool (default: False)\n        Shuffles training data every epoch if True to prevent circles.\n\n    random_seed : int (default: None)\n        Set random state for shuffling and initializing the weights.\n\n\n    Attributes\n    -----------\n    w_ : 1d-array\n      Weights after fitting.\n\n    cost_ : list\n      Sum of squared errors after each epoch.\n\n    \"\"\"   Methods      def fit(self, X, y, init_weights=True):\n        \"\"\" Fit training data.\n\n        Parameters\n        ----------\n        X : {array-like, sparse matrix}, shape = [n_samples, n_features]\n            Training vectors, where n_samples is the number of samples and\n            n_features is the number of features.\n\n        y : array-like, shape = [n_samples]\n            Target values.\n\n        init_weights : bool (default: True)\n            (Re)initializes weights to small random floats if True.\n\n        Returns\n        -------\n        self : object\n        \"\"\"      def predict(self, X):\n        \"\"\"\n        Predict class labels for X.\n\n        Parameters\n        ----------\n        X : {array-like, sparse matrix}, shape = [n_samples, n_features]\n            Training vectors, where n_samples is the number of samples and\n            n_features is the number of features.\n\n        Returns\n        ----------\n        class : int\n          Predicted class label.\n\n        \"\"\"",
            "title": "Adaline"
        },
        {
            "location": "/docs/classifier/perceptron/",
            "text": "mlxtend\n\nSebastian Raschka, 05/14/2015\n\n\n\n\n\nPerceptron\n\n\nImplementation of a Perceptron (single-layer artificial neural network) using the Rosenblatt Perceptron Rule [1].\n\n\n[1] F. Rosenblatt. The perceptron, a perceiving and recognizing automaton Project Para. Cornell Aeronautical Laboratory, 1957.\n\n\n\n\nFor more usage examples please see the \nIPython Notebook\n.\n\n\nA detailed explanation about the perceptron learning algorithm can be found here \nArtificial Neurons and Single-Layer Neural Networks\n- How Machine Learning Algorithms Work Part 1\n.\n\n\n\n\n\nExample\n\n\nfrom mlxtend.data import iris_data\nfrom mlxtend.evaluate import plot_decision_regions\nfrom mlxtend.classifier import Perceptron\nimport matplotlib.pyplot as plt\n\n# Loading Data\n\nX, y = iris_data()\nX = X[:, [0, 3]] # sepal length and petal width\nX = X[0:100] # class 0 and class 1\ny = y[0:100] # class 0 and class 1\n\n# standardize\nX[:,0] = (X[:,0] - X[:,0].mean()) / X[:,0].std()\nX[:,1] = (X[:,1] - X[:,1].mean()) / X[:,1].std()\n\n\n# Rosenblatt Perceptron\n\nppn = Perceptron(epochs=15, eta=0.01, random_seed=1)\nppn.fit(X, y)\n\nplot_decision_regions(X, y, clf=ppn)\nplt.title('Perceptron - Rosenblatt Perceptron Rule')\nplt.show()\n\nprint(ppn.w_)\n\nplt.plot(range(len(ppn.cost_)), ppn.cost_)\nplt.xlabel('Iterations')\nplt.ylabel('Missclassifications')\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\nDefault Parameters\n\n\nclass Perceptron(object):\n    \"\"\"Perceptron classifier.\n\n    Parameters\n    ------------\n    eta : float\n      Learning rate (between 0.0 and 1.0)\n\n    epochs : int\n      Passes over the training dataset.\n\n    random_state : int\n      Random state for initializing random weights.\n\n    Attributes\n    -----------\n    w_ : 1d-array\n      Weights after fitting.\n\n    cost_ : list\n      Number of misclassifications in every epoch.\n\n    \"\"\"\n\n\n\n\n\n\nMethods\n\n\n    def fit(self, X, y, init_weights=True):\n        \"\"\" Fit training data.\n\n        Parameters\n        ----------\n        X : {array-like, sparse matrix}, shape = [n_samples, n_features]\n            Training vectors, where n_samples is the number of samples and\n            n_features is the number of features.\n\n        y : array-like, shape = [n_samples]\n            Target values.\n\n        init_weights : bool (default: True)\n            (Re)initializes weights to small random floats if True.\n\n        shuffle : bool (default: False)\n            Shuffles training data every epoch if True to prevent circles.\n\n        random_seed : int (default: None)\n            Set random state for shuffling and initializing the weights.\n\n        Returns\n        -------\n        self : object\n\n        \"\"\"\n\n\n\n    def predict(self, X):\n        \"\"\" Predict class labels for X.\n\n        Parameters\n        ----------\n        X : {array-like, sparse matrix}, shape = [n_samples, n_features]\n            Training vectors, where n_samples is the number of samples and\n            n_features is the number of features.\n        Returns\n        ----------\n        class : int\n          Predicted class label.\n\n        \"\"\"",
            "title": "Perceptron"
        },
        {
            "location": "/docs/classifier/perceptron/#perceptron",
            "text": "Implementation of a Perceptron (single-layer artificial neural network) using the Rosenblatt Perceptron Rule [1].  [1] F. Rosenblatt. The perceptron, a perceiving and recognizing automaton Project Para. Cornell Aeronautical Laboratory, 1957.   For more usage examples please see the  IPython Notebook .  A detailed explanation about the perceptron learning algorithm can be found here  Artificial Neurons and Single-Layer Neural Networks\n- How Machine Learning Algorithms Work Part 1 .   Example  from mlxtend.data import iris_data\nfrom mlxtend.evaluate import plot_decision_regions\nfrom mlxtend.classifier import Perceptron\nimport matplotlib.pyplot as plt\n\n# Loading Data\n\nX, y = iris_data()\nX = X[:, [0, 3]] # sepal length and petal width\nX = X[0:100] # class 0 and class 1\ny = y[0:100] # class 0 and class 1\n\n# standardize\nX[:,0] = (X[:,0] - X[:,0].mean()) / X[:,0].std()\nX[:,1] = (X[:,1] - X[:,1].mean()) / X[:,1].std()\n\n\n# Rosenblatt Perceptron\n\nppn = Perceptron(epochs=15, eta=0.01, random_seed=1)\nppn.fit(X, y)\n\nplot_decision_regions(X, y, clf=ppn)\nplt.title('Perceptron - Rosenblatt Perceptron Rule')\nplt.show()\n\nprint(ppn.w_)\n\nplt.plot(range(len(ppn.cost_)), ppn.cost_)\nplt.xlabel('Iterations')\nplt.ylabel('Missclassifications')\nplt.show()      Default Parameters  class Perceptron(object):\n    \"\"\"Perceptron classifier.\n\n    Parameters\n    ------------\n    eta : float\n      Learning rate (between 0.0 and 1.0)\n\n    epochs : int\n      Passes over the training dataset.\n\n    random_state : int\n      Random state for initializing random weights.\n\n    Attributes\n    -----------\n    w_ : 1d-array\n      Weights after fitting.\n\n    cost_ : list\n      Number of misclassifications in every epoch.\n\n    \"\"\"   Methods      def fit(self, X, y, init_weights=True):\n        \"\"\" Fit training data.\n\n        Parameters\n        ----------\n        X : {array-like, sparse matrix}, shape = [n_samples, n_features]\n            Training vectors, where n_samples is the number of samples and\n            n_features is the number of features.\n\n        y : array-like, shape = [n_samples]\n            Target values.\n\n        init_weights : bool (default: True)\n            (Re)initializes weights to small random floats if True.\n\n        shuffle : bool (default: False)\n            Shuffles training data every epoch if True to prevent circles.\n\n        random_seed : int (default: None)\n            Set random state for shuffling and initializing the weights.\n\n        Returns\n        -------\n        self : object\n\n        \"\"\"      def predict(self, X):\n        \"\"\" Predict class labels for X.\n\n        Parameters\n        ----------\n        X : {array-like, sparse matrix}, shape = [n_samples, n_features]\n            Training vectors, where n_samples is the number of samples and\n            n_features is the number of features.\n        Returns\n        ----------\n        class : int\n          Predicted class label.\n\n        \"\"\"",
            "title": "Perceptron"
        },
        {
            "location": "/docs/classifier/logistic_regression/",
            "text": "mlxtend\n\nSebastian Raschka, 05/14/2015\n\n\n\n\n\nLogistic Regression\n\n\nImplementation of Logistic Regression  with different learning rules: Gradient descent and stochastic gradient descent.\n\n\n\n\nFor more usage examples please see the \nIPython Notebook\n.\n\n\nA more detailed article about the algorithms is in preparation.\n\n\n\n\n\nExample 1 - Stochastic Gradient Descent\n\n\nfrom mlxtend.data import iris_data\nfrom mlxtend.evaluate import plot_decision_regions\nfrom mlxtend.classifier import LogisticRegression\nimport matplotlib.pyplot as plt\n\n# Loading Data\n\nX, y = iris_data()\nX = X[:, [0, 3]] # sepal length and petal width\nX = X[0:100] # class 0 and class 1\ny = y[0:100] # class 0 and class 1\n\n# standardize\nX[:,0] = (X[:,0] - X[:,0].mean()) / X[:,0].std()\nX[:,1] = (X[:,1] - X[:,1].mean()) / X[:,1].std()\n\n\n\nlr = LogisticRegression(eta=0.01, epochs=100, learning='sgd')\nlr.fit(X, y)\n\nplot_decision_regions(X, y, clf=lr)\nplt.title('Logistic Regression - Stochastic Gradient Descent')\nplt.show()\n\nprint(lr.w_)\n\nplt.plot(range(len(lr.cost_)), lr.cost_)\nplt.xlabel('Iterations')\nplt.ylabel('Missclassifications')\nplt.show()\n\n\n\n\n\n\n\n\n\n\nDefault Parameters\n\n\nclass LogisticRegression(object):\n    \"\"\"Logistic regression classifier.\n\n    Parameters\n    ------------\n    eta : float\n      Learning rate (between 0.0 and 1.0)\n\n    epochs : int\n      Passes over the training dataset.\n\n    learning : str (default: sgd)\n      Learning rule, sgd (stochastic gradient descent)\n      or gd (gradient descent).\n\n    lambda_ : float\n      Regularization parameter for L2 regularization.\n      No regularization if lambda_=0.0.\n\n    shuffle : bool (default: False)\n        Shuffles training data every epoch if True to prevent circles.\n\n    random_seed : int (default: None)\n        Set random state for shuffling and initializing the weights.\n\n    Attributes\n    -----------\n    w_ : 1d-array\n      Weights after fitting.\n\n    cost_ : list\n      List of floats with sum of squared error cost (sgd or gd) for every\n      epoch.\n\n    \"\"\"\n\n\n\n\n\n\nMethods\n\n\n    def fit(self, X, y, init_weights=True):\n        \"\"\" Fit training data.\n\n        Parameters\n        ----------\n        X : {array-like, sparse matrix}, shape = [n_samples, n_features]\n            Training vectors, where n_samples is the number of samples and\n            n_features is the number of features.\n\n        y : array-like, shape = [n_samples]\n            Target values.\n\n        init_weights : bool (default: True)\n            (Re)initializes weights to small random floats if True.\n\n        Returns\n        -------\n        self : object\n\n        \"\"\"\n\n\n\n    def predict(self, X):\n        \"\"\"\n        Predict class labels for X.\n\n        Parameters\n        ----------\n        X : {array-like, sparse matrix}, shape = [n_samples, n_features]\n            Training vectors, where n_samples is the number of samples and\n            n_features is the number of features.\n\n        Returns\n        ----------\n        class : int\n          Predicted class label.\n\n        \"\"\"\n\n\n\n    def activation(self, X):\n        \"\"\"\n        Predict class probabilities for X.\n\n        Parameters\n        ----------\n        X : {array-like, sparse matrix}, shape = [n_samples, n_features]\n            Training vectors, where n_samples is the number of samples and\n            n_features is the number of features.\n\n        Returns\n        ----------\n          Class 1 probability : float\n\n        \"\"\"",
            "title": "Logistic regression"
        },
        {
            "location": "/docs/classifier/logistic_regression/#logistic-regression",
            "text": "Implementation of Logistic Regression  with different learning rules: Gradient descent and stochastic gradient descent.   For more usage examples please see the  IPython Notebook .  A more detailed article about the algorithms is in preparation.   Example 1 - Stochastic Gradient Descent  from mlxtend.data import iris_data\nfrom mlxtend.evaluate import plot_decision_regions\nfrom mlxtend.classifier import LogisticRegression\nimport matplotlib.pyplot as plt\n\n# Loading Data\n\nX, y = iris_data()\nX = X[:, [0, 3]] # sepal length and petal width\nX = X[0:100] # class 0 and class 1\ny = y[0:100] # class 0 and class 1\n\n# standardize\nX[:,0] = (X[:,0] - X[:,0].mean()) / X[:,0].std()\nX[:,1] = (X[:,1] - X[:,1].mean()) / X[:,1].std()\n\n\n\nlr = LogisticRegression(eta=0.01, epochs=100, learning='sgd')\nlr.fit(X, y)\n\nplot_decision_regions(X, y, clf=lr)\nplt.title('Logistic Regression - Stochastic Gradient Descent')\nplt.show()\n\nprint(lr.w_)\n\nplt.plot(range(len(lr.cost_)), lr.cost_)\nplt.xlabel('Iterations')\nplt.ylabel('Missclassifications')\nplt.show()     Default Parameters  class LogisticRegression(object):\n    \"\"\"Logistic regression classifier.\n\n    Parameters\n    ------------\n    eta : float\n      Learning rate (between 0.0 and 1.0)\n\n    epochs : int\n      Passes over the training dataset.\n\n    learning : str (default: sgd)\n      Learning rule, sgd (stochastic gradient descent)\n      or gd (gradient descent).\n\n    lambda_ : float\n      Regularization parameter for L2 regularization.\n      No regularization if lambda_=0.0.\n\n    shuffle : bool (default: False)\n        Shuffles training data every epoch if True to prevent circles.\n\n    random_seed : int (default: None)\n        Set random state for shuffling and initializing the weights.\n\n    Attributes\n    -----------\n    w_ : 1d-array\n      Weights after fitting.\n\n    cost_ : list\n      List of floats with sum of squared error cost (sgd or gd) for every\n      epoch.\n\n    \"\"\"   Methods      def fit(self, X, y, init_weights=True):\n        \"\"\" Fit training data.\n\n        Parameters\n        ----------\n        X : {array-like, sparse matrix}, shape = [n_samples, n_features]\n            Training vectors, where n_samples is the number of samples and\n            n_features is the number of features.\n\n        y : array-like, shape = [n_samples]\n            Target values.\n\n        init_weights : bool (default: True)\n            (Re)initializes weights to small random floats if True.\n\n        Returns\n        -------\n        self : object\n\n        \"\"\"      def predict(self, X):\n        \"\"\"\n        Predict class labels for X.\n\n        Parameters\n        ----------\n        X : {array-like, sparse matrix}, shape = [n_samples, n_features]\n            Training vectors, where n_samples is the number of samples and\n            n_features is the number of features.\n\n        Returns\n        ----------\n        class : int\n          Predicted class label.\n\n        \"\"\"      def activation(self, X):\n        \"\"\"\n        Predict class probabilities for X.\n\n        Parameters\n        ----------\n        X : {array-like, sparse matrix}, shape = [n_samples, n_features]\n            Training vectors, where n_samples is the number of samples and\n            n_features is the number of features.\n\n        Returns\n        ----------\n          Class 1 probability : float\n\n        \"\"\"",
            "title": "Logistic Regression"
        },
        {
            "location": "/docs/pandas/minmax_scaling/",
            "text": "mlxtend\n\nSebastian Raschka, 05/14/2015\n\n\n\n\n\nMinmax Scaling\n\n\nA function that applies minmax scaling to pandas DataFrame columns.\n\n\n\n\n\nExample\n\n\nfrom mlxtend.pandas import minmax_scaling\n\n\n\n\n\nDefault Parameters\n\n\ndef minmax_scaling(df, columns, min_val=0, max_val=1):\n    \"\"\"\n    Min max scaling for pandas DataFrames\n\n    Parameters\n    ----------\n    df : pandas DataFrame object.\n\n    columns : array-like, shape = [n_columns]\n      Array-like with pandas DataFrame column names, e.g., ['col1', 'col2', ...]\n\n    min_val : `int` or `float`, optional (default=`0`)\n      minimum value after rescaling.\n\n    min_val : `int` or `float`, optional (default=`1`)\n      maximum value after rescaling.\n\n    Returns\n    ----------\n\n    df_new: pandas DataFrame object.\n      Copy of the DataFrame with rescaled columns.\n\n    \"\"\"",
            "title": "Minmax scaling"
        },
        {
            "location": "/docs/pandas/minmax_scaling/#minmax-scaling",
            "text": "A function that applies minmax scaling to pandas DataFrame columns.",
            "title": "Minmax Scaling"
        },
        {
            "location": "/docs/pandas/minmax_scaling/#example",
            "text": "from mlxtend.pandas import minmax_scaling",
            "title": "Example"
        },
        {
            "location": "/docs/pandas/minmax_scaling/#default-parameters",
            "text": "def minmax_scaling(df, columns, min_val=0, max_val=1):\n    \"\"\"\n    Min max scaling for pandas DataFrames\n\n    Parameters\n    ----------\n    df : pandas DataFrame object.\n\n    columns : array-like, shape = [n_columns]\n      Array-like with pandas DataFrame column names, e.g., ['col1', 'col2', ...]\n\n    min_val : `int` or `float`, optional (default=`0`)\n      minimum value after rescaling.\n\n    min_val : `int` or `float`, optional (default=`1`)\n      maximum value after rescaling.\n\n    Returns\n    ----------\n\n    df_new: pandas DataFrame object.\n      Copy of the DataFrame with rescaled columns.\n\n    \"\"\"",
            "title": "Default Parameters"
        },
        {
            "location": "/docs/pandas/standardizing/",
            "text": "mlxtend\n\nSebastian Raschka, 05/14/2015\n\n\n\n\n\nStandardizing\n\n\nA function to standardize columns in pandas DataFrames so that they have properties of a standard normal distribution (mean=0, standard deviation=1).\n\n\n\n\n\nExample\n\n\nfrom mlxtend.pandas import standardizing\n\n\n\n\n\nDefault Parameters\n\n\ndef standardizing(df, columns):\n    \"\"\"\n    Standardizing columns in pandas DataFrames.\n\n    Parameters\n    ----------\n    df : pandas DataFrame object.\n\n    columns : array-like, shape = [n_columns]\n      Array-like with pandas DataFrame column names, e.g., ['col1', 'col2', ...]\n\n    Returns\n    ----------\n\n    df_new: pandas DataFrame object.\n      Copy of the DataFrame with standardized columns.\n\n    \"\"\"",
            "title": "Standardizing"
        },
        {
            "location": "/docs/pandas/standardizing/#standardizing",
            "text": "A function to standardize columns in pandas DataFrames so that they have properties of a standard normal distribution (mean=0, standard deviation=1).",
            "title": "Standardizing"
        },
        {
            "location": "/docs/pandas/standardizing/#example",
            "text": "from mlxtend.pandas import standardizing",
            "title": "Example"
        },
        {
            "location": "/docs/pandas/standardizing/#default-parameters",
            "text": "def standardizing(df, columns):\n    \"\"\"\n    Standardizing columns in pandas DataFrames.\n\n    Parameters\n    ----------\n    df : pandas DataFrame object.\n\n    columns : array-like, shape = [n_columns]\n      Array-like with pandas DataFrame column names, e.g., ['col1', 'col2', ...]\n\n    Returns\n    ----------\n\n    df_new: pandas DataFrame object.\n      Copy of the DataFrame with standardized columns.\n\n    \"\"\"",
            "title": "Default Parameters"
        },
        {
            "location": "/docs/sklearn/sequential_backward_selection/",
            "text": "mlxtend\n\nSebastian Raschka, 05/14/2015\n\n\n\n\n\nSequential Backward Selection\n\n\nSequential Backward Selection (SBS) is  a classic feature selection algorithm -- a greedy search algorithm -- that has been developed as a suboptimal solution to the computationally often not feasible exhaustive search. In a nutshell, SBS removes one feature at the time based on the classifier performance until a feature subset of the desired size \nk\n is reached. \n\n\nNote that SBS is different from the \nrecursive feature elimination (RFE)\n that is implemented in scikit-learn.\n RFE sequentially removes features based on the feature weights whereas SBS removes features based on the model performance.\nMore detailed explanations about the algorithms and examples can be found in \nthis IPython notebook\n.\n\n\n\n\n\nExample\n\n\nInput:\n\n\nfrom mlxtend.sklearn import SBS\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.datasets import load_iris\n\niris = load_iris()\nX = iris.data\ny = iris.target\n\nknn = KNeighborsClassifier(n_neighbors=4)\n\nsbs = SBS(knn, k_features=2, scoring='accuracy', cv=5)\nsbs.fit(X, y)\n\nprint('Indices of selected features:', sbs.indices_)\nprint('CV score of selected subset:', sbs.k_score_)\nprint('New feature subset:')\nsbs.transform(X)[0:5]\n\n\n\nOutput:\n\n\nIndices of selected features: (0, 3)\nCV score of selected subset: 0.96\nNew feature subset:\narray([[ 5.1,  0.2],\n   [ 4.9,  0.2],\n   [ 4.7,  0.2],\n   [ 4.6,  0.2],\n   [ 5. ,  0.2]])\n\n\n\n\n\n\n\nAs demonstrated below, the SBS algorithm can be a useful alternative to dimensionality reduction techniques to reduce overfitting and where the original features need to be preserved:\n\n\nimport matplotlib.pyplot as plt\nfrom sklearn.preprocessing import StandardScaler\n\nscr = StandardScaler()\nX_std = scr.fit_transform(X)\n\nknn = KNeighborsClassifier(n_neighbors=4)\n\n# selecting features\nsbs = SBS(knn, k_features=1, scoring='accuracy', cv=5)\nsbs.fit(X_std, y)\n\n# plotting performance of feature subsets\nk_feat = [len(k) for k in sbs.subsets_]\n\nplt.plot(k_feat, sbs.scores_, marker='o')\nplt.ylabel('Accuracy')\nplt.xlabel('Number of features')\nplt.show()\n\n\n\n\n\n\nMore examples -- including how to use \nSBS\n in scikit-learn's \nGridSearch\n can be found in \nthis IPython notebook\n.\n\n\n\n\n\nDefault Parameters\n\n\nclass SBS(BaseEstimator, MetaEstimatorMixin):\n    \"\"\" Sequential Backward Selection for feature selection.\n\n    Parameters\n    ----------\n    clfs : scikit-learn estimator object\n\n    k_features : int\n      Number of features to select where k_features.\n\n    scoring : str, (default='accuracy')\n      Scoring metric for the cross validation scorer.\n\n    cv : int (default: 5)\n      Number of folds in StratifiedKFold.\n\n    n_jobs : int (default: 1)\n      The number of CPUs to use for cross validation. -1 means 'all CPUs'.\n\n    Attributes\n    ----------\n    indices_ : array-like, shape = [n_predictions]\n      Indices of the selected subsets.\n\n    k_score_ : float\n      Cross validation mean scores of the selected subset\n\n    subsets_ : list of tuples\n      Indices of the sequentially selected subsets.\n\n    scores_ : list\n      Cross validation mean scores of the sequentially selected subsets.\n\n    Examples\n    --------\n    >>> from sklearn.neighbors import KNeighborsClassifier\n    >>> from sklearn.datasets import load_iris\n    >>> iris = load_iris()\n    >>> X = iris.data\n    >>> y = iris.target\n    >>> knn = KNeighborsClassifier(n_neighbors=4)\n    >>> sbs = SBS(knn, k_features=2, scoring='accuracy', cv=5)\n    >>> sbs = sbs.fit(X, y)\n    >>> sbs.indices_\n    (0, 3)\n    >>> sbs.k_score_\n    0.96\n    >>> sbs.transform(X)\n    array([[ 5.1,  0.2],\n       [ 4.9,  0.2],\n       [ 4.7,  0.2],\n       [ 4.6,  0.2],\n       [ 5. ,  0.2]])\n\n    \"\"\"",
            "title": "Sequential backward selection"
        },
        {
            "location": "/docs/sklearn/sequential_backward_selection/#sequential-backward-selection",
            "text": "Sequential Backward Selection (SBS) is  a classic feature selection algorithm -- a greedy search algorithm -- that has been developed as a suboptimal solution to the computationally often not feasible exhaustive search. In a nutshell, SBS removes one feature at the time based on the classifier performance until a feature subset of the desired size  k  is reached.   Note that SBS is different from the  recursive feature elimination (RFE)  that is implemented in scikit-learn.  RFE sequentially removes features based on the feature weights whereas SBS removes features based on the model performance.\nMore detailed explanations about the algorithms and examples can be found in  this IPython notebook .   Example  Input:  from mlxtend.sklearn import SBS\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.datasets import load_iris\n\niris = load_iris()\nX = iris.data\ny = iris.target\n\nknn = KNeighborsClassifier(n_neighbors=4)\n\nsbs = SBS(knn, k_features=2, scoring='accuracy', cv=5)\nsbs.fit(X, y)\n\nprint('Indices of selected features:', sbs.indices_)\nprint('CV score of selected subset:', sbs.k_score_)\nprint('New feature subset:')\nsbs.transform(X)[0:5]  Output:  Indices of selected features: (0, 3)\nCV score of selected subset: 0.96\nNew feature subset:\narray([[ 5.1,  0.2],\n   [ 4.9,  0.2],\n   [ 4.7,  0.2],\n   [ 4.6,  0.2],\n   [ 5. ,  0.2]])    As demonstrated below, the SBS algorithm can be a useful alternative to dimensionality reduction techniques to reduce overfitting and where the original features need to be preserved:  import matplotlib.pyplot as plt\nfrom sklearn.preprocessing import StandardScaler\n\nscr = StandardScaler()\nX_std = scr.fit_transform(X)\n\nknn = KNeighborsClassifier(n_neighbors=4)\n\n# selecting features\nsbs = SBS(knn, k_features=1, scoring='accuracy', cv=5)\nsbs.fit(X_std, y)\n\n# plotting performance of feature subsets\nk_feat = [len(k) for k in sbs.subsets_]\n\nplt.plot(k_feat, sbs.scores_, marker='o')\nplt.ylabel('Accuracy')\nplt.xlabel('Number of features')\nplt.show()   \nMore examples -- including how to use  SBS  in scikit-learn's  GridSearch  can be found in  this IPython notebook .",
            "title": "Sequential Backward Selection"
        },
        {
            "location": "/docs/sklearn/sequential_backward_selection/#default-parameters",
            "text": "class SBS(BaseEstimator, MetaEstimatorMixin):\n    \"\"\" Sequential Backward Selection for feature selection.\n\n    Parameters\n    ----------\n    clfs : scikit-learn estimator object\n\n    k_features : int\n      Number of features to select where k_features.\n\n    scoring : str, (default='accuracy')\n      Scoring metric for the cross validation scorer.\n\n    cv : int (default: 5)\n      Number of folds in StratifiedKFold.\n\n    n_jobs : int (default: 1)\n      The number of CPUs to use for cross validation. -1 means 'all CPUs'.\n\n    Attributes\n    ----------\n    indices_ : array-like, shape = [n_predictions]\n      Indices of the selected subsets.\n\n    k_score_ : float\n      Cross validation mean scores of the selected subset\n\n    subsets_ : list of tuples\n      Indices of the sequentially selected subsets.\n\n    scores_ : list\n      Cross validation mean scores of the sequentially selected subsets.\n\n    Examples\n    --------\n    >>> from sklearn.neighbors import KNeighborsClassifier\n    >>> from sklearn.datasets import load_iris\n    >>> iris = load_iris()\n    >>> X = iris.data\n    >>> y = iris.target\n    >>> knn = KNeighborsClassifier(n_neighbors=4)\n    >>> sbs = SBS(knn, k_features=2, scoring='accuracy', cv=5)\n    >>> sbs = sbs.fit(X, y)\n    >>> sbs.indices_\n    (0, 3)\n    >>> sbs.k_score_\n    0.96\n    >>> sbs.transform(X)\n    array([[ 5.1,  0.2],\n       [ 4.9,  0.2],\n       [ 4.7,  0.2],\n       [ 4.6,  0.2],\n       [ 5. ,  0.2]])\n\n    \"\"\"",
            "title": "Default Parameters"
        },
        {
            "location": "/docs/sklearn/column_selector/",
            "text": "mlxtend\n\nSebastian Raschka, 05/14/2015\n\n\n\n\n\nColumnSelector for Custom Feature Selection\n\n\nA feature selector for scikit-learn's Pipeline class that returns specified columns from a NumPy array; extremely useful in combination with scikit-learn's \nPipeline\n in cross-validation.\n\n\n\n\nAn example usage\n of the \nColumnSelector\n used in a pipeline for cross-validation on the Iris dataset.\n\n\n\n\n\n\n\nExample in \nPipeline\n:\n\n\nfrom mlxtend.sklearn import ColumnSelector\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.preprocessing import StandardScaler\n\nclf_2col = Pipeline(steps=[\n    ('scaler', StandardScaler()),\n    ('reduce_dim', ColumnSelector(cols=(1,3))),    # extracts column 2 and 4\n    ('classifier', GaussianNB())   \n    ])\n\n\n\nColumnSelector\n has a \ntransform\n method that is used to select and return columns (features) from a NumPy array so that it can be used in the \nPipeline\n like other \ntransformation\n classes. \n\n\n### original data\n\nprint('First 3 rows before:\\n', X_train[:3,:])\nFirst 3 rows before:\n[[ 4.5  2.3  1.3  0.3]\n[ 6.7  3.3  5.7  2.1]\n[ 5.7  3.   4.2  1.2]]\n\n### after selection\n\ncols = ColumnExtractor(cols=(1,3)).transform(X_train)\nprint('First 3 rows:\\n', cols[:3,:])\n\nFirst 3 rows:\n[[ 2.3  0.3]\n[ 3.3  2.1]\n[ 3.   1.2]]\n\n\n\n\n\n\nDefault Parameters",
            "title": "Column selector"
        },
        {
            "location": "/docs/sklearn/column_selector/#columnselector-for-custom-feature-selection",
            "text": "A feature selector for scikit-learn's Pipeline class that returns specified columns from a NumPy array; extremely useful in combination with scikit-learn's  Pipeline  in cross-validation.   An example usage  of the  ColumnSelector  used in a pipeline for cross-validation on the Iris dataset.    Example in  Pipeline :  from mlxtend.sklearn import ColumnSelector\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.preprocessing import StandardScaler\n\nclf_2col = Pipeline(steps=[\n    ('scaler', StandardScaler()),\n    ('reduce_dim', ColumnSelector(cols=(1,3))),    # extracts column 2 and 4\n    ('classifier', GaussianNB())   \n    ])  ColumnSelector  has a  transform  method that is used to select and return columns (features) from a NumPy array so that it can be used in the  Pipeline  like other  transformation  classes.   ### original data\n\nprint('First 3 rows before:\\n', X_train[:3,:])\nFirst 3 rows before:\n[[ 4.5  2.3  1.3  0.3]\n[ 6.7  3.3  5.7  2.1]\n[ 5.7  3.   4.2  1.2]]\n\n### after selection\n\ncols = ColumnExtractor(cols=(1,3)).transform(X_train)\nprint('First 3 rows:\\n', cols[:3,:])\n\nFirst 3 rows:\n[[ 2.3  0.3]\n[ 3.3  2.1]\n[ 3.   1.2]]",
            "title": "ColumnSelector for Custom Feature Selection"
        },
        {
            "location": "/docs/sklearn/column_selector/#default-parameters",
            "text": "",
            "title": "Default Parameters"
        },
        {
            "location": "/docs/sklearn/ensemble_classifier/",
            "text": "mlxtend\n\nSebastian Raschka, last updated: 05/14/2015\n\n\n\n\n\nMajority Rule Ensemble Classifier\n\n\nAnd ensemble classifier that predicts class labels based on a majority voting rule (hard voting) or average predicted probabilities (soft voting).\n\n\nDecision regions plotted for 4 different classifiers:   \n\n\n\n\nPlease see the \nIPython Notebook\n for a detailed explanation and examples.\n\n\nThe \nEnsembleClassifier\n will likely be included in the scikit-learn library as \nVotingClassifier\n at some point, and during this implementation process, the \nEnsembleClassifier\n has been slightly improved based on valuable feedback from the scikit-learn community.\n\n\n\n\n\nCross-validation Example\n\n\nInput:\n\n\nfrom mlxtend.sklearn import EnsembleClassifier\nfrom sklearn import cross_validation\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.naive_bayes import GaussianNB \nfrom sklearn.ensemble import RandomForestClassifier\nimport numpy as np\nfrom sklearn import datasets\n\niris = datasets.load_iris()\nX, y = iris.data[:, 1:3], iris.target\n\nnp.random.seed(123)\n\n################################\n# Initialize classifiers\n################################\n\nclf1 = LogisticRegression()\nclf2 = RandomForestClassifier()\nclf3 = GaussianNB()\n\n################################\n# Initialize EnsembleClassifier\n################################\n\n# hard voting    \neclf = EnsembleClassifier(clfs=[clf1, clf2, clf3], voting='hard')\n\n# soft voting (uniform weights)\n# eclf = EnsembleClassifier(clfs=[clf1, clf2, clf3], voting='soft')\n\n# soft voting with different weights\n# eclf = EnsembleClassifier(clfs=[clf1, clf2, clf3], voting='soft', weights=[1,2,10])\n\n\n\n################################\n# 5-fold Cross-Validation\n################################\n\nfor clf, label in zip([clf1, clf2, clf3, eclf], ['Logistic Regression', 'Random Forest', 'naive Bayes', 'Ensemble']):\n\n    scores = cross_validation.cross_val_score(clf, X, y, cv=5, scoring='accuracy')\n    print(\"Accuracy: %0.2f (+/- %0.2f) [%s]\" % (scores.mean(), scores.std(), label))\n\n\n\nOutput:\n\n\nAccuracy: 0.90 (+/- 0.05) [Logistic Regression]\nAccuracy: 0.92 (+/- 0.05) [Random Forest]\nAccuracy: 0.91 (+/- 0.04) [naive Bayes]\nAccuracy: 0.95 (+/- 0.05) [Ensemble]\n\n\n\n\n\n\n\nGridSearch Example\n\n\nThe \nEnsembleClassifier\n van also be used in combination with scikit-learns gridsearch module:\n\n\nfrom sklearn.grid_search import GridSearchCV\n\nclf1 = LogisticRegression(random_state=1)\nclf2 = RandomForestClassifier(random_state=1)\nclf3 = GaussianNB()\neclf = EnsembleClassifier(clfs=[clf1, clf2, clf3], voting='soft')\n\nparams = {'logisticregression__C': [1.0, 100.0],\n      'randomforestclassifier__n_estimators': [20, 200],}\n\ngrid = GridSearchCV(estimator=eclf, param_grid=params, cv=5)\ngrid.fit(iris.data, iris.target)\n\nfor params, mean_score, scores in grid.grid_scores_:\n    print(\"%0.3f (+/-%0.03f) for %r\"\n        % (mean_score, scores.std() / 2, params))\n\n\n\nOutput:\n\n\n0.953 (+/-0.013) for {'randomforestclassifier__n_estimators': 20, 'logisticregression__C': 1.0}\n0.960 (+/-0.012) for {'randomforestclassifier__n_estimators': 200, 'logisticregression__C': 1.0}\n0.960 (+/-0.012) for {'randomforestclassifier__n_estimators': 20, 'logisticregression__C': 100.0}\n0.953 (+/-0.017) for {'randomforestclassifier__n_estimators': 200, 'logisticregression__C': 100.0}\n\n\n\n\n\n\nDefault Parameters\n\n\nclass EnsembleClassifier(BaseEstimator, ClassifierMixin, TransformerMixin):\n    \"\"\" Soft Voting/Majority Rule classifier for unfitted clfs.\n\n\nParameters\n----------\nclfs : array-like, shape = [n_classifiers]\n  A list of classifiers.\n  Invoking the `fit` method on the `VotingClassifier` will fit clones\n  of those original classifiers that will be stored in the class attribute\n  `self.clfs_`.\n\nvoting : str, {'hard', 'soft'} (default='hard')\n  If 'hard', uses predicted class labels for majority rule voting.\n  Else if 'soft', predicts the class label based on the argmax of\n  the sums of the predicted probalities, which is recommended for\n  an ensemble of well-calibrated classifiers.\n\nweights : array-like, shape = [n_classifiers], optional (default=`None`)\n  Sequence of weights (`float` or `int`) to weight the occurances of\n  predicted class labels (`hard` voting) or class probabilities\n  before averaging (`soft` voting). Uses uniform weights if `None`.\n\nAttributes\n----------\nclasses_ : array-like, shape = [n_predictions]\n\nExamples\n--------\n\n import numpy as np\n\n from sklearn.linear_model import LogisticRegression\n\n from sklearn.naive_bayes import GaussianNB\n\n from sklearn.ensemble import RandomForestClassifier\n\n clf1 = LogisticRegression(random_state=1)\n\n clf2 = RandomForestClassifier(random_state=1)\n\n clf3 = GaussianNB()\n\n X = np.array([[-1, -1], [-2, -1], [-3, -2], [1, 1], [2, 1], [3, 2]])\n\n y = np.array([1, 1, 1, 2, 2, 2])\n\n eclf1 = VotingClassifier(clfs=[clf1, clf2, clf3], voting='hard')\n\n eclf1 = eclf1.fit(X, y)\n\n print(eclf1.predict(X))\n[1 1 1 2 2 2]\n\n eclf2 = VotingClassifier(clfs=[clf1, clf2, clf3], voting='soft')\n\n eclf2 = eclf2.fit(X, y)\n\n print(eclf2.predict(X))\n[1 1 1 2 2 2]\n\n eclf3 = VotingClassifier(clfs=[clf1, clf2, clf3],\n...                          voting='soft', weights=[2,1,1])\n\n eclf3 = eclf3.fit(X, y)\n\n print(eclf3.predict(X))\n[1 1 1 2 2 2]\n\n\n\"\"\"\n/pre\n\n\n\n\n\n\n\nMethods\n\n\n    def fit(self, X, y):\n        \"\"\" Fit the clfs.\n\n        Parameters\n        ----------\n        X : {array-like, sparse matrix}, shape = [n_samples, n_features]\n            Training vectors, where n_samples is the number of samples and\n            n_features is the number of features.\n\n        y : array-like, shape = [n_samples]\n            Target values.\n\n        Returns\n        -------\n        self : object\n        \"\"\"\n\n\n\n    def predict(self, X):\n        \"\"\" Predict class labels for X.\n\n        Parameters\n        ----------\n        X : {array-like, sparse matrix}, shape = [n_samples, n_features]\n            Training vectors, where n_samples is the number of samples and\n            n_features is the number of features.\n\n        Returns\n        ----------\n        maj : array-like, shape = [n_samples]\n            Predicted class labels.\n        \"\"\"\n\n\n\npredict_proba(self, X):\n        \"\"\" Predict class probabilities for X.\n\n        Parameters\n        ----------\n        X : {array-like, sparse matrix}, shape = [n_samples, n_features]\n            Training vectors, where n_samples is the number of samples and\n            n_features is the number of features.\n\n        Returns\n        ----------\n        avg : array-like, shape = [n_samples, n_classes]\n            Weighted average probability for each class per sample.\n        \"\"\"\n\n\n\n    def transform(self, X):\n        \"\"\" Return class labels or probabilities for X for each estimator.\n\n        Parameters\n        ----------\n        X : {array-like, sparse matrix}, shape = [n_samples, n_features]\n            Training vectors, where n_samples is the number of samples and\n            n_features is the number of features.\n\n        Returns\n        -------\n        If `voting='soft'`:\n          array-like = [n_classifiers, n_samples, n_classes]\n            Class probabilties calculated by each classifier.\n        If `voting='hard'`:\n          array-like = [n_classifiers, n_samples]\n            Class labels predicted by each classifier.\n        \"\"\"",
            "title": "Ensemble classifier"
        },
        {
            "location": "/docs/sklearn/ensemble_classifier/#majority-rule-ensemble-classifier",
            "text": "And ensemble classifier that predicts class labels based on a majority voting rule (hard voting) or average predicted probabilities (soft voting).  Decision regions plotted for 4 different classifiers:      Please see the  IPython Notebook  for a detailed explanation and examples.  The  EnsembleClassifier  will likely be included in the scikit-learn library as  VotingClassifier  at some point, and during this implementation process, the  EnsembleClassifier  has been slightly improved based on valuable feedback from the scikit-learn community.",
            "title": "Majority Rule Ensemble Classifier"
        },
        {
            "location": "/docs/sklearn/ensemble_classifier/#cross-validation-example",
            "text": "Input:  from mlxtend.sklearn import EnsembleClassifier\nfrom sklearn import cross_validation\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.naive_bayes import GaussianNB \nfrom sklearn.ensemble import RandomForestClassifier\nimport numpy as np\nfrom sklearn import datasets\n\niris = datasets.load_iris()\nX, y = iris.data[:, 1:3], iris.target\n\nnp.random.seed(123)\n\n################################\n# Initialize classifiers\n################################\n\nclf1 = LogisticRegression()\nclf2 = RandomForestClassifier()\nclf3 = GaussianNB()\n\n################################\n# Initialize EnsembleClassifier\n################################\n\n# hard voting    \neclf = EnsembleClassifier(clfs=[clf1, clf2, clf3], voting='hard')\n\n# soft voting (uniform weights)\n# eclf = EnsembleClassifier(clfs=[clf1, clf2, clf3], voting='soft')\n\n# soft voting with different weights\n# eclf = EnsembleClassifier(clfs=[clf1, clf2, clf3], voting='soft', weights=[1,2,10])\n\n\n\n################################\n# 5-fold Cross-Validation\n################################\n\nfor clf, label in zip([clf1, clf2, clf3, eclf], ['Logistic Regression', 'Random Forest', 'naive Bayes', 'Ensemble']):\n\n    scores = cross_validation.cross_val_score(clf, X, y, cv=5, scoring='accuracy')\n    print(\"Accuracy: %0.2f (+/- %0.2f) [%s]\" % (scores.mean(), scores.std(), label))  Output:  Accuracy: 0.90 (+/- 0.05) [Logistic Regression]\nAccuracy: 0.92 (+/- 0.05) [Random Forest]\nAccuracy: 0.91 (+/- 0.04) [naive Bayes]\nAccuracy: 0.95 (+/- 0.05) [Ensemble]",
            "title": "Cross-validation Example"
        },
        {
            "location": "/docs/sklearn/ensemble_classifier/#gridsearch-example",
            "text": "The  EnsembleClassifier  van also be used in combination with scikit-learns gridsearch module:  from sklearn.grid_search import GridSearchCV\n\nclf1 = LogisticRegression(random_state=1)\nclf2 = RandomForestClassifier(random_state=1)\nclf3 = GaussianNB()\neclf = EnsembleClassifier(clfs=[clf1, clf2, clf3], voting='soft')\n\nparams = {'logisticregression__C': [1.0, 100.0],\n      'randomforestclassifier__n_estimators': [20, 200],}\n\ngrid = GridSearchCV(estimator=eclf, param_grid=params, cv=5)\ngrid.fit(iris.data, iris.target)\n\nfor params, mean_score, scores in grid.grid_scores_:\n    print(\"%0.3f (+/-%0.03f) for %r\"\n        % (mean_score, scores.std() / 2, params))  Output:  0.953 (+/-0.013) for {'randomforestclassifier__n_estimators': 20, 'logisticregression__C': 1.0}\n0.960 (+/-0.012) for {'randomforestclassifier__n_estimators': 200, 'logisticregression__C': 1.0}\n0.960 (+/-0.012) for {'randomforestclassifier__n_estimators': 20, 'logisticregression__C': 100.0}\n0.953 (+/-0.017) for {'randomforestclassifier__n_estimators': 200, 'logisticregression__C': 100.0}",
            "title": "GridSearch Example"
        },
        {
            "location": "/docs/sklearn/ensemble_classifier/#default-parameters",
            "text": "class EnsembleClassifier(BaseEstimator, ClassifierMixin, TransformerMixin):\n    \"\"\" Soft Voting/Majority Rule classifier for unfitted clfs.  Parameters\n----------\nclfs : array-like, shape = [n_classifiers]\n  A list of classifiers.\n  Invoking the `fit` method on the `VotingClassifier` will fit clones\n  of those original classifiers that will be stored in the class attribute\n  `self.clfs_`.\n\nvoting : str, {'hard', 'soft'} (default='hard')\n  If 'hard', uses predicted class labels for majority rule voting.\n  Else if 'soft', predicts the class label based on the argmax of\n  the sums of the predicted probalities, which is recommended for\n  an ensemble of well-calibrated classifiers.\n\nweights : array-like, shape = [n_classifiers], optional (default=`None`)\n  Sequence of weights (`float` or `int`) to weight the occurances of\n  predicted class labels (`hard` voting) or class probabilities\n  before averaging (`soft` voting). Uses uniform weights if `None`.\n\nAttributes\n----------\nclasses_ : array-like, shape = [n_predictions]\n\nExamples\n--------  import numpy as np  from sklearn.linear_model import LogisticRegression  from sklearn.naive_bayes import GaussianNB  from sklearn.ensemble import RandomForestClassifier  clf1 = LogisticRegression(random_state=1)  clf2 = RandomForestClassifier(random_state=1)  clf3 = GaussianNB()  X = np.array([[-1, -1], [-2, -1], [-3, -2], [1, 1], [2, 1], [3, 2]])  y = np.array([1, 1, 1, 2, 2, 2])  eclf1 = VotingClassifier(clfs=[clf1, clf2, clf3], voting='hard')  eclf1 = eclf1.fit(X, y)  print(eclf1.predict(X))\n[1 1 1 2 2 2]  eclf2 = VotingClassifier(clfs=[clf1, clf2, clf3], voting='soft')  eclf2 = eclf2.fit(X, y)  print(eclf2.predict(X))\n[1 1 1 2 2 2]  eclf3 = VotingClassifier(clfs=[clf1, clf2, clf3],\n...                          voting='soft', weights=[2,1,1])  eclf3 = eclf3.fit(X, y)  print(eclf3.predict(X))\n[1 1 1 2 2 2] \n\"\"\" /pre",
            "title": "Default Parameters"
        },
        {
            "location": "/docs/sklearn/ensemble_classifier/#methods",
            "text": "def fit(self, X, y):\n        \"\"\" Fit the clfs.\n\n        Parameters\n        ----------\n        X : {array-like, sparse matrix}, shape = [n_samples, n_features]\n            Training vectors, where n_samples is the number of samples and\n            n_features is the number of features.\n\n        y : array-like, shape = [n_samples]\n            Target values.\n\n        Returns\n        -------\n        self : object\n        \"\"\"      def predict(self, X):\n        \"\"\" Predict class labels for X.\n\n        Parameters\n        ----------\n        X : {array-like, sparse matrix}, shape = [n_samples, n_features]\n            Training vectors, where n_samples is the number of samples and\n            n_features is the number of features.\n\n        Returns\n        ----------\n        maj : array-like, shape = [n_samples]\n            Predicted class labels.\n        \"\"\"  predict_proba(self, X):\n        \"\"\" Predict class probabilities for X.\n\n        Parameters\n        ----------\n        X : {array-like, sparse matrix}, shape = [n_samples, n_features]\n            Training vectors, where n_samples is the number of samples and\n            n_features is the number of features.\n\n        Returns\n        ----------\n        avg : array-like, shape = [n_samples, n_classes]\n            Weighted average probability for each class per sample.\n        \"\"\"      def transform(self, X):\n        \"\"\" Return class labels or probabilities for X for each estimator.\n\n        Parameters\n        ----------\n        X : {array-like, sparse matrix}, shape = [n_samples, n_features]\n            Training vectors, where n_samples is the number of samples and\n            n_features is the number of features.\n\n        Returns\n        -------\n        If `voting='soft'`:\n          array-like = [n_classifiers, n_samples, n_classes]\n            Class probabilties calculated by each classifier.\n        If `voting='hard'`:\n          array-like = [n_classifiers, n_samples]\n            Class labels predicted by each classifier.\n        \"\"\"",
            "title": "Methods"
        },
        {
            "location": "/docs/sklearn/dense_transformer/",
            "text": "mlxtend\n\nSebastian Raschka, 05/14/2015\n\n\n\n\n\nDense Transformer\n\n\nA simple transformer that converts a sparse into a dense numpy array, e.g., required for scikit-learn's \nPipeline\n when e.g,. \nCountVectorizers\n are used in combination with \nRandomForest\ns.\n\n\n\n\n\nExample\n\n\nfrom sklearn.pipeline import Pipeline\nfrom sklearn import metrics\nfrom sklearn.grid_search import GridSearchCV\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.feature_extraction.text import CountVectorizer\n\nfrom mlxtend.sklearn import DenseTransformer\n\n\npipe_1 = Pipeline([\n    ('vect', CountVectorizer(analyzer='word',\n                      decode_error='replace',\n                      preprocessor=lambda text: re.sub('[^a-zA-Z]', ' ', text.lower()), \n                      stop_words=stopwords,) ),\n    ('to_dense', DenseTransformer()),\n    ('clf', RandomForestClassifier())\n])\n\nparameters_1 = dict(\n    clf__n_estimators=[50, 100, 200],\n    clf__max_features=['sqrt', 'log2', None],)\n\ngrid_search_1 = GridSearchCV(pipe_1, \n                           parameters_1, \n                           n_jobs=1, \n                           verbose=1,\n                           scoring=f1_scorer,\n                           cv=10)\n\n\nprint(\"Performing grid search...\")\nprint(\"pipeline:\", [name for name, _ in pipe_1.steps])\nprint(\"parameters:\")\ngrid_search_1.fit(X_train, y_train)\nprint(\"Best score: %0.3f\" % grid_search_1.best_score_)\nprint(\"Best parameters set:\")\nbest_parameters_1 = grid_search_1.best_estimator_.get_params()\nfor param_name in sorted(parameters_1.keys()):\n    print(\"\\t%s: %r\" % (param_name, best_parameters_1[param_name]))\n\n\n\nDefault Parameters",
            "title": "Dense transformer"
        },
        {
            "location": "/docs/sklearn/dense_transformer/#dense-transformer",
            "text": "A simple transformer that converts a sparse into a dense numpy array, e.g., required for scikit-learn's  Pipeline  when e.g,.  CountVectorizers  are used in combination with  RandomForest s.",
            "title": "Dense Transformer"
        },
        {
            "location": "/docs/sklearn/dense_transformer/#example",
            "text": "from sklearn.pipeline import Pipeline\nfrom sklearn import metrics\nfrom sklearn.grid_search import GridSearchCV\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.feature_extraction.text import CountVectorizer\n\nfrom mlxtend.sklearn import DenseTransformer\n\n\npipe_1 = Pipeline([\n    ('vect', CountVectorizer(analyzer='word',\n                      decode_error='replace',\n                      preprocessor=lambda text: re.sub('[^a-zA-Z]', ' ', text.lower()), \n                      stop_words=stopwords,) ),\n    ('to_dense', DenseTransformer()),\n    ('clf', RandomForestClassifier())\n])\n\nparameters_1 = dict(\n    clf__n_estimators=[50, 100, 200],\n    clf__max_features=['sqrt', 'log2', None],)\n\ngrid_search_1 = GridSearchCV(pipe_1, \n                           parameters_1, \n                           n_jobs=1, \n                           verbose=1,\n                           scoring=f1_scorer,\n                           cv=10)\n\n\nprint(\"Performing grid search...\")\nprint(\"pipeline:\", [name for name, _ in pipe_1.steps])\nprint(\"parameters:\")\ngrid_search_1.fit(X_train, y_train)\nprint(\"Best score: %0.3f\" % grid_search_1.best_score_)\nprint(\"Best parameters set:\")\nbest_parameters_1 = grid_search_1.best_estimator_.get_params()\nfor param_name in sorted(parameters_1.keys()):\n    print(\"\\t%s: %r\" % (param_name, best_parameters_1[param_name]))",
            "title": "Example"
        },
        {
            "location": "/docs/sklearn/dense_transformer/#default-parameters",
            "text": "",
            "title": "Default Parameters"
        },
        {
            "location": "/docs/evaluate/plot_decision_regions/",
            "text": "mlxtend\n\nSebastian Raschka, 05/14/2015\n\n\n\n\n\nPlotting Decision Regions\n\n\n\n\n\n2D example\n\n\nfrom mlxtend.evaluate import plot_decision_regions\nimport matplotlib.pyplot as plt\nfrom sklearn import datasets\nfrom sklearn.svm import SVC\n\n# Loading some example data\niris = datasets.load_iris()\nX = iris.data[:, [0,2]]\ny = iris.target\n\n# Training a classifier\nsvm = SVC(C=0.5, kernel='linear')\nsvm.fit(X,y)\n\n# Plotting decision regions\nplot_decision_regions(X, y, clf=svm, res=0.02, legend=2)\n\n# Adding axes annotations\nplt.xlabel('sepal length [cm]')\nplt.ylabel('petal length [cm]')\nplt.title('SVM on Iris')\nplt.show()\n\n\n\n\n\n\n\n\n1D example\n\n\nfrom mlxtend.evaluate import plot_decision_regions\nimport matplotlib.pyplot as plt\nfrom sklearn import datasets\nfrom sklearn.svm import SVC\n\n# Loading some example data\niris = datasets.load_iris()\nX = iris.data[:, 2]\nX = X[:, None]\ny = iris.target\n\n# Training a classifier\nsvm = SVC(C=0.5, kernel='linear')\nsvm.fit(X,y)\n\n# Plotting decision regions\nplot_decision_regions(X, y, clf=svm, res=0.02, legend=2)\n\n# Adding axes annotations\nplt.xlabel('sepal length [cm]')\nplt.ylabel('petal length [cm]')\nplt.title('SVM on Iris')\nplt.show()\n\n\n\n\n\n\n\n\nHighlighting Test Data Points\n\n\nVia the \nX_highlight\n, a second dataset can be provided to highlight particular points in the dataset via a circle.\n\n\nfrom sklearn.cross_validation import train_test_split\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=0)\n\n# Training a classifier\nsvm = SVC(C=0.5, kernel='linear')\nsvm.fit(X_train, y_train)\n\n# Plotting decision regions\n\nplot_decision_regions(X, y, clf=svm, \n                  X_highlight=X_test, \n                  res=0.02, legend=2)\n\n# Adding axes annotations\nplt.xlabel('sepal length [standardized]')\nplt.ylabel('petal length [standardized]')\nplt.title('SVM on Iris')\nplt.show()\n\n\n\n\n\nFor more examples, please see this \nIPython Notebook\n.\n\n\n\n\n\nDefault Parameters\n\n\ndef plot_decision_regions(X, y, clf, X_highlight=None, res=0.02, cycle_marker=True, legend=1):\n    \"\"\"\n    Plots decision regions of a classifier.\n\n    Parameters\n    ----------\n    X : array-like, shape = [n_samples, n_features]\n      Feature Matrix.\n\n    y : array-like, shape = [n_samples]\n      True class labels.\n\n    clf : Classifier object. Must have a .predict method.\n\n    X_highlight : array-like, shape = [n_samples, n_features] (default: None)\n      An array with data points that are used to highlight samples in `X`.\n\n    res : float (default: 0.02)\n      Grid width. Lower values increase the resolution but\n      slow down the plotting.\n\n    cycle_marker : bool\n      Use different marker for each class.\n\n    legend : int\n      Integer to specify the legend location.\n      No legend if legend is 0.\n\n    cmap : Custom colormap object.\n      Uses matplotlib.cm.rainbow if None.\n\n    Returns\n    ---------\n    None\n\n    Examples\n    --------\n\n    from sklearn import datasets\n    from sklearn.svm import SVC\n\n    iris = datasets.load_iris()\n    X = iris.data[:, [0,2]]\n    y = iris.target\n\n    svm = SVC(C=1.0, kernel='linear')\n    svm.fit(X,y)\n\n    plot_decision_region(X, y, clf=svm, res=0.02, cycle_marker=True, legend=1)\n\n    plt.xlabel('sepal length [cm]')\n    plt.ylabel('petal length [cm]')\n    plt.title('SVM on Iris')\n    plt.show()\n\n    \"\"\"",
            "title": "Plot decision regions"
        },
        {
            "location": "/docs/evaluate/plot_decision_regions/#plotting-decision-regions",
            "text": "",
            "title": "Plotting Decision Regions"
        },
        {
            "location": "/docs/evaluate/plot_decision_regions/#2d-example",
            "text": "from mlxtend.evaluate import plot_decision_regions\nimport matplotlib.pyplot as plt\nfrom sklearn import datasets\nfrom sklearn.svm import SVC\n\n# Loading some example data\niris = datasets.load_iris()\nX = iris.data[:, [0,2]]\ny = iris.target\n\n# Training a classifier\nsvm = SVC(C=0.5, kernel='linear')\nsvm.fit(X,y)\n\n# Plotting decision regions\nplot_decision_regions(X, y, clf=svm, res=0.02, legend=2)\n\n# Adding axes annotations\nplt.xlabel('sepal length [cm]')\nplt.ylabel('petal length [cm]')\nplt.title('SVM on Iris')\nplt.show()",
            "title": "2D example"
        },
        {
            "location": "/docs/evaluate/plot_decision_regions/#1d-example",
            "text": "from mlxtend.evaluate import plot_decision_regions\nimport matplotlib.pyplot as plt\nfrom sklearn import datasets\nfrom sklearn.svm import SVC\n\n# Loading some example data\niris = datasets.load_iris()\nX = iris.data[:, 2]\nX = X[:, None]\ny = iris.target\n\n# Training a classifier\nsvm = SVC(C=0.5, kernel='linear')\nsvm.fit(X,y)\n\n# Plotting decision regions\nplot_decision_regions(X, y, clf=svm, res=0.02, legend=2)\n\n# Adding axes annotations\nplt.xlabel('sepal length [cm]')\nplt.ylabel('petal length [cm]')\nplt.title('SVM on Iris')\nplt.show()",
            "title": "1D example"
        },
        {
            "location": "/docs/evaluate/plot_decision_regions/#highlighting-test-data-points",
            "text": "Via the  X_highlight , a second dataset can be provided to highlight particular points in the dataset via a circle.  from sklearn.cross_validation import train_test_split\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=0)\n\n# Training a classifier\nsvm = SVC(C=0.5, kernel='linear')\nsvm.fit(X_train, y_train)\n\n# Plotting decision regions\n\nplot_decision_regions(X, y, clf=svm, \n                  X_highlight=X_test, \n                  res=0.02, legend=2)\n\n# Adding axes annotations\nplt.xlabel('sepal length [standardized]')\nplt.ylabel('petal length [standardized]')\nplt.title('SVM on Iris')\nplt.show()   For more examples, please see this  IPython Notebook .",
            "title": "Highlighting Test Data Points"
        },
        {
            "location": "/docs/evaluate/plot_decision_regions/#default-parameters",
            "text": "def plot_decision_regions(X, y, clf, X_highlight=None, res=0.02, cycle_marker=True, legend=1):\n    \"\"\"\n    Plots decision regions of a classifier.\n\n    Parameters\n    ----------\n    X : array-like, shape = [n_samples, n_features]\n      Feature Matrix.\n\n    y : array-like, shape = [n_samples]\n      True class labels.\n\n    clf : Classifier object. Must have a .predict method.\n\n    X_highlight : array-like, shape = [n_samples, n_features] (default: None)\n      An array with data points that are used to highlight samples in `X`.\n\n    res : float (default: 0.02)\n      Grid width. Lower values increase the resolution but\n      slow down the plotting.\n\n    cycle_marker : bool\n      Use different marker for each class.\n\n    legend : int\n      Integer to specify the legend location.\n      No legend if legend is 0.\n\n    cmap : Custom colormap object.\n      Uses matplotlib.cm.rainbow if None.\n\n    Returns\n    ---------\n    None\n\n    Examples\n    --------\n\n    from sklearn import datasets\n    from sklearn.svm import SVC\n\n    iris = datasets.load_iris()\n    X = iris.data[:, [0,2]]\n    y = iris.target\n\n    svm = SVC(C=1.0, kernel='linear')\n    svm.fit(X,y)\n\n    plot_decision_region(X, y, clf=svm, res=0.02, cycle_marker=True, legend=1)\n\n    plt.xlabel('sepal length [cm]')\n    plt.ylabel('petal length [cm]')\n    plt.title('SVM on Iris')\n    plt.show()\n\n    \"\"\"",
            "title": "Default Parameters"
        },
        {
            "location": "/docs/evaluate/plot_learning_curves/",
            "text": "mlxtend\n\nSebastian Raschka, 05/14/2015\n\n\n\n\n\nPlotting Learning Curves\n\n\nA function to plot learning curves for classifiers. Learning curves are extremely useful to analyze if a model is suffering from over- or under-fitting (high variance or high bias). The function can be imported via\n\n\n\n\n\nExample 1 - Training Samples\n\n\nfrom mlxtend.evaluate import plot_learning_curves\nfrom sklearn import datasets\nfrom sklearn.cross_validation import train_test_split\n\n# Loading some example data\niris = datasets.load_iris()\nX = iris.data\ny = iris.target\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.6, random_seed=2)\n\nfrom sklearn.tree import DecisionTreeClassifier\nimport numpy as np\n\nclf = DecisionTreeClassifier(max_depth=1)\n\nplot_learning_curves(X_train, y_train, X_test, y_test, clf, kind='training_size')\nplt.show()\n\nplot_learning_curves(X_train, y_train, X_test, y_test, clf, kind='n_features')\nplt.show()\n\n\n\n\n\n\n\n\nExample 2 - Features\n\n\nplot_learning_curves(X_train, y_train, X_test, y_test, clf, kind='n_features')\nplt.show()\n\n\n\n\n\nFor more examples, please see this \nIPython Notebook\n\n\n\n\n\nDefault Parameters\n\n\ndef plot_learning_curves(X_train, y_train, X_test, y_test, clf, kind='training_size',\n            marker='o', scoring='misclassification error', suppress_plot=False, print_model=True):\n    \"\"\"\n    Plots learning curves of a classifier.\n\n    Parameters\n    ----------\n    X_train : array-like, shape = [n_samples, n_features]\n      Feature matrix of the training dataset.\n\n    y_train : array-like, shape = [n_samples]\n      True class labels of the training dataset.\n\n    X_test : array-like, shape = [n_samples, n_features]\n      Feature matrix of the test dataset.\n\n    y_test : array-like, shape = [n_samples]\n      True class labels of the test dataset.\n\n    clf : Classifier object. Must have a .predict .fit method.\n\n    kind : str (default: 'training_size')\n      'training_size' or 'n_features'\n      Plots missclassifications vs. training size or number of features.\n\n    marker : str (default: 'o')\n      Marker for the line plot.\n\n    scoring : str (default: 'misclassification error')\n      If not 'accuracy', accepts the following metrics (from scikit-learn):\n      {'accuracy', 'average_precision', 'f1_micro', 'f1_macro',\n      'f1_weighted', 'f1_samples', 'log_loss', 'precision', 'recall', 'roc_auc',\n      'adjusted_rand_score', 'mean_absolute_error', 'mean_squared_error',\n      'median_absolute_error', 'r2'}\n\n    suppress_plot=False : bool (default: False)\n      Suppress matplotlib plots if True. Recommended\n      for testing purposes.\n\n    print_model : bool (default: True)\n      Print model parameters in plot title if True.\n\n    Returns\n    ---------\n    (training_error, test_error): tuple of lists\n    \"\"\"",
            "title": "Plot learning curves"
        },
        {
            "location": "/docs/evaluate/plot_learning_curves/#plotting-learning-curves",
            "text": "A function to plot learning curves for classifiers. Learning curves are extremely useful to analyze if a model is suffering from over- or under-fitting (high variance or high bias). The function can be imported via",
            "title": "Plotting Learning Curves"
        },
        {
            "location": "/docs/evaluate/plot_learning_curves/#example-1-training-samples",
            "text": "from mlxtend.evaluate import plot_learning_curves\nfrom sklearn import datasets\nfrom sklearn.cross_validation import train_test_split\n\n# Loading some example data\niris = datasets.load_iris()\nX = iris.data\ny = iris.target\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.6, random_seed=2)\n\nfrom sklearn.tree import DecisionTreeClassifier\nimport numpy as np\n\nclf = DecisionTreeClassifier(max_depth=1)\n\nplot_learning_curves(X_train, y_train, X_test, y_test, clf, kind='training_size')\nplt.show()\n\nplot_learning_curves(X_train, y_train, X_test, y_test, clf, kind='n_features')\nplt.show()",
            "title": "Example 1 - Training Samples"
        },
        {
            "location": "/docs/evaluate/plot_learning_curves/#example-2-features",
            "text": "plot_learning_curves(X_train, y_train, X_test, y_test, clf, kind='n_features')\nplt.show()   For more examples, please see this  IPython Notebook",
            "title": "Example 2 - Features"
        },
        {
            "location": "/docs/evaluate/plot_learning_curves/#default-parameters",
            "text": "def plot_learning_curves(X_train, y_train, X_test, y_test, clf, kind='training_size',\n            marker='o', scoring='misclassification error', suppress_plot=False, print_model=True):\n    \"\"\"\n    Plots learning curves of a classifier.\n\n    Parameters\n    ----------\n    X_train : array-like, shape = [n_samples, n_features]\n      Feature matrix of the training dataset.\n\n    y_train : array-like, shape = [n_samples]\n      True class labels of the training dataset.\n\n    X_test : array-like, shape = [n_samples, n_features]\n      Feature matrix of the test dataset.\n\n    y_test : array-like, shape = [n_samples]\n      True class labels of the test dataset.\n\n    clf : Classifier object. Must have a .predict .fit method.\n\n    kind : str (default: 'training_size')\n      'training_size' or 'n_features'\n      Plots missclassifications vs. training size or number of features.\n\n    marker : str (default: 'o')\n      Marker for the line plot.\n\n    scoring : str (default: 'misclassification error')\n      If not 'accuracy', accepts the following metrics (from scikit-learn):\n      {'accuracy', 'average_precision', 'f1_micro', 'f1_macro',\n      'f1_weighted', 'f1_samples', 'log_loss', 'precision', 'recall', 'roc_auc',\n      'adjusted_rand_score', 'mean_absolute_error', 'mean_squared_error',\n      'median_absolute_error', 'r2'}\n\n    suppress_plot=False : bool (default: False)\n      Suppress matplotlib plots if True. Recommended\n      for testing purposes.\n\n    print_model : bool (default: True)\n      Print model parameters in plot title if True.\n\n    Returns\n    ---------\n    (training_error, test_error): tuple of lists\n    \"\"\"",
            "title": "Default Parameters"
        },
        {
            "location": "/docs/matplotlib/category_scatter/",
            "text": "mlxtend\n\nSebastian Raschka, last updated: 05/14/2015\n\n\n\n\n\nCategory Scatter\n\n\nA function to quickly produce a scatter plot colored by categories from a pandas \nDataFrame\n or NumPy \nndarray\n object.\n\n\n\n\n\nExample\n\n\nLoading an example dataset as pandas \nDataFrame\n:   \n\n\nimport pandas as pd\n\ndf = pd.read_csv('/Users/sebastian/Desktop/data.csv')\ndf.head()\n\n\n\n\n\nPlotting the data where the categories are determined by the unique values in the label column \nlabel_col\n. The \nx\n and \ny\n values are simply the column names of the DataFrame that we want to plot.\n\n\nimport matplotlib.pyplot as plt\nfrom mlxtend.matplotlib import category_scatter\n\ncategory_scatter(x='x', y='y', label_col='label', data=df)\n\nplt.legend(loc='best')\n\n\n\n\n\nSimilarly, we can also use NumPy arrays. E.g.,\n\n\nX =\n\narray([['class1', 10.0, 8.04],\n   ['class1', 8.0, 6.95],\n   ['class1', 13.2, 7.58],\n   ['class1', 9.0, 8.81],\n    ...\n   ['class4', 8.0, 5.56],\n   ['class4', 8.0, 7.91],\n   ['class4', 8.0, 6.89]], dtype=object)\n\n\n\nWhere the \nx\n, \ny\n, and \nlabel_col\n refer to the respective column indices in the array:\n\n\ncategory_scatter(x=1, y=2, label_col=0, data=df.values)\n\nplt.legend(loc='best')\n\n\n\n\n\n\n\n\nDefault Parameters\n\n\ndef category_scatter(x, y, label_col, data,\n            markers='sxo^v',\n            colors=('blue', 'green', 'red', 'purple', 'gray', 'cyan'),\n            alpha=0.7, markersize=20.0):\n\n    \"\"\"\n    Scatter plot to plot categories in different colors/markerstyles.\n\n    Parameters\n    ----------\n    x : str or int\n      DataFrame column name of the x-axis values or\n      integer for the numpy ndarray column index.\n\n    y : str\n      DataFrame column name of the y-axis values or\n      integer for the numpy ndarray column index\n\n    data : Pandas DataFrame object or NumPy ndarray.\n\n    markers : str\n      Markers that are cycled through the label category.\n\n    colors : tuple \n      Colors that are cycled through the label category.\n\n    alpha : float (default: 0.7)\n      Parameter to control the transparency.\n\n    markersize : float (default : 20.0)\n      Parameter to control the marker size.\n\n    Returns\n    ---------\n    None\n\n    \"\"\"",
            "title": "Category scatter"
        },
        {
            "location": "/docs/matplotlib/category_scatter/#category-scatter",
            "text": "A function to quickly produce a scatter plot colored by categories from a pandas  DataFrame  or NumPy  ndarray  object.",
            "title": "Category Scatter"
        },
        {
            "location": "/docs/matplotlib/category_scatter/#example",
            "text": "Loading an example dataset as pandas  DataFrame :     import pandas as pd\n\ndf = pd.read_csv('/Users/sebastian/Desktop/data.csv')\ndf.head()   Plotting the data where the categories are determined by the unique values in the label column  label_col . The  x  and  y  values are simply the column names of the DataFrame that we want to plot.  import matplotlib.pyplot as plt\nfrom mlxtend.matplotlib import category_scatter\n\ncategory_scatter(x='x', y='y', label_col='label', data=df)\n\nplt.legend(loc='best')   Similarly, we can also use NumPy arrays. E.g.,  X =\n\narray([['class1', 10.0, 8.04],\n   ['class1', 8.0, 6.95],\n   ['class1', 13.2, 7.58],\n   ['class1', 9.0, 8.81],\n    ...\n   ['class4', 8.0, 5.56],\n   ['class4', 8.0, 7.91],\n   ['class4', 8.0, 6.89]], dtype=object)  Where the  x ,  y , and  label_col  refer to the respective column indices in the array:  category_scatter(x=1, y=2, label_col=0, data=df.values)\n\nplt.legend(loc='best')",
            "title": "Example"
        },
        {
            "location": "/docs/matplotlib/category_scatter/#default-parameters",
            "text": "def category_scatter(x, y, label_col, data,\n            markers='sxo^v',\n            colors=('blue', 'green', 'red', 'purple', 'gray', 'cyan'),\n            alpha=0.7, markersize=20.0):\n\n    \"\"\"\n    Scatter plot to plot categories in different colors/markerstyles.\n\n    Parameters\n    ----------\n    x : str or int\n      DataFrame column name of the x-axis values or\n      integer for the numpy ndarray column index.\n\n    y : str\n      DataFrame column name of the y-axis values or\n      integer for the numpy ndarray column index\n\n    data : Pandas DataFrame object or NumPy ndarray.\n\n    markers : str\n      Markers that are cycled through the label category.\n\n    colors : tuple \n      Colors that are cycled through the label category.\n\n    alpha : float (default: 0.7)\n      Parameter to control the transparency.\n\n    markersize : float (default : 20.0)\n      Parameter to control the marker size.\n\n    Returns\n    ---------\n    None\n\n    \"\"\"",
            "title": "Default Parameters"
        },
        {
            "location": "/docs/matplotlib/enrichment_plot/",
            "text": "mlxtend\n\nSebastian Raschka, last updated: 05/14/2015\n\n\n\n\n\nEnrichment Plot\n\n\nA function to plot step plots of cumulative counts.\n\n\n\n\n\nExample\n\n\nCreating an example  \nDataFrame\n:   \n\n\nimport pandas as pd\ns1 = [1.1, 1.5]\ns2 = [2.1, 1.8]\ns3 = [3.1, 2.1]\ns4 = [3.9, 2.5]\ndata = [s1, s2, s3, s4]\ndf = pd.DataFrame(data, columns=['X1', 'X2'])\ndf\n\n\n\n\n\nPlotting the enrichment plot. The y-axis can be interpreted as \"how many samples are less or equal to the corresponding x-axis label.\"\n\n\nfrom mlxtend.matplotlib import enrichment_plot\nenrichment_plot(df)\n\n\n\n\n\n\n\n\nDefault Parameters\n\n\n\ndef enrichment_plot(df, colors='bgrkcy', markers=' ', linestyles='-', alpha=0.5, lw=2,\n                    legend=True, where='post', grid=True, ylabel='Count',\n                    xlim='auto', ylim='auto'):\n    \"\"\"\n    Function to plot stacked barplots\n\n    Parameters\n    ----------\n    df : pandas.DataFrame\n      A pandas DataFrame where columns represent the different categories.\n\n    colors: str (default: 'bgrcky')\n      The colors of the bars.\n\n    markers: str (default: ' ')\n      Matplotlib markerstyles, e.g,\n      'sov' for square,circle, and triangle markers.\n\n    linestyles: str (default: '-')\n      Matplotlib linestyles, e.g., \n      '-,--' to cycle normal and dashed lines. Note\n      that the different linestyles need to be separated by commas.\n\n    alpha: float (default: 0.5)\n      Transparency level from 0.0 to 1.0.\n\n    lw: int or float (default: 2)\n      Linewidth parameter.\n\n    legend: bool (default: True)\n      Plots legend if True.\n\n    where: {'post', 'pre', 'mid'} (default: 'post')\n      Starting location of the steps.\n\n    grid: bool (default: True)\n      Plots a grid if True.\n\n    ylabel: str (default: 'Count')\n      y-axis label.\n\n    xlim: 'auto' or array-like [min, max]\n      Min and maximum position of the x-axis range.\n\n    ylim: 'auto' or array-like [min, max]\n      Min and maximum position of the y-axis range.\n\n    Returns\n    ----------\n    None\n\n    \"\"\"",
            "title": "Enrichment plot"
        },
        {
            "location": "/docs/matplotlib/enrichment_plot/#enrichment-plot",
            "text": "A function to plot step plots of cumulative counts.",
            "title": "Enrichment Plot"
        },
        {
            "location": "/docs/matplotlib/enrichment_plot/#example",
            "text": "Creating an example   DataFrame :     import pandas as pd\ns1 = [1.1, 1.5]\ns2 = [2.1, 1.8]\ns3 = [3.1, 2.1]\ns4 = [3.9, 2.5]\ndata = [s1, s2, s3, s4]\ndf = pd.DataFrame(data, columns=['X1', 'X2'])\ndf   Plotting the enrichment plot. The y-axis can be interpreted as \"how many samples are less or equal to the corresponding x-axis label.\"  from mlxtend.matplotlib import enrichment_plot\nenrichment_plot(df)",
            "title": "Example"
        },
        {
            "location": "/docs/matplotlib/enrichment_plot/#default-parameters",
            "text": "def enrichment_plot(df, colors='bgrkcy', markers=' ', linestyles='-', alpha=0.5, lw=2,\n                    legend=True, where='post', grid=True, ylabel='Count',\n                    xlim='auto', ylim='auto'):\n    \"\"\"\n    Function to plot stacked barplots\n\n    Parameters\n    ----------\n    df : pandas.DataFrame\n      A pandas DataFrame where columns represent the different categories.\n\n    colors: str (default: 'bgrcky')\n      The colors of the bars.\n\n    markers: str (default: ' ')\n      Matplotlib markerstyles, e.g,\n      'sov' for square,circle, and triangle markers.\n\n    linestyles: str (default: '-')\n      Matplotlib linestyles, e.g., \n      '-,--' to cycle normal and dashed lines. Note\n      that the different linestyles need to be separated by commas.\n\n    alpha: float (default: 0.5)\n      Transparency level from 0.0 to 1.0.\n\n    lw: int or float (default: 2)\n      Linewidth parameter.\n\n    legend: bool (default: True)\n      Plots legend if True.\n\n    where: {'post', 'pre', 'mid'} (default: 'post')\n      Starting location of the steps.\n\n    grid: bool (default: True)\n      Plots a grid if True.\n\n    ylabel: str (default: 'Count')\n      y-axis label.\n\n    xlim: 'auto' or array-like [min, max]\n      Min and maximum position of the x-axis range.\n\n    ylim: 'auto' or array-like [min, max]\n      Min and maximum position of the y-axis range.\n\n    Returns\n    ----------\n    None\n\n    \"\"\"",
            "title": "Default Parameters"
        },
        {
            "location": "/docs/matplotlib/stacked_barplot/",
            "text": "mlxtend\n\nSebastian Raschka, last updated: 05/14/2015\n\n\n\n\n\nStacked Barplot\n\n\nA function to conveniently plot stacked bar plots in matplotlib using pandas \nDataFrame\ns. \n\n\n\n\n\nExample\n\n\nCreating an example  \nDataFrame\n:   \n\n\nimport pandas as pd\n\ns1 = [1.0, 2.0, 3.0, 4.0]\ns2 = [1.4, 2.1, 2.9, 5.1]\ns3 = [1.9, 2.2, 3.5, 4.1]\ns4 = [1.4, 2.5, 3.5, 4.2]\ndata = [s1, s2, s3, s4]\n\ndf = pd.DataFrame(data, columns=['X1', 'X2', 'X3', 'X4'])\ndf.columns = ['X1', 'X2', 'X3', 'X4']\ndf.index = ['Sample1', 'Sample2', 'Sample3', 'Sample4']\ndf\n\n\n\n\n\nPlotting the stacked barplot. By default, the index of the \nDataFrame\n is used as column labels, and the \nDataFrame\n columns are used for the plot legend.\n\n\nfrom mlxtend.matplotlib import stacked_barplot\n\nstacked_barplot(df, rotation=45)\n\n\n\n\n\nDefault Parameters\n\n\ndef stacked_barplot(df, bar_width='auto', colors='bgrcky', labels='index', rotation=90, legend=True):\n    \"\"\"\n    Function to plot stacked barplots\n\n    Parameters\n    ----------\n    df : pandas.DataFrame\n      A pandas DataFrame where the index denotes the\n      x-axis labels, and the columns contain the different\n      measurements for each row.\n\n    bar_width: 'auto' or float (default: 'auto')\n      Parameter to set the widths of the bars. if\n      'auto', the width is automatically determined by\n      the number of columns in the dataset.\n\n    colors: str (default: 'bgrcky')\n      The colors of the bars.\n\n    labels: 'index' or iterable (default: 'index')\n      If 'index', the DataFrame index will be used as\n      x-tick labels.\n\n    rotation: int (default: 90)\n      Parameter to rotate the x-axis labels.\n\n    legend: bool (default: True)\n      Parameter to plot the legend.\n\n    Returns\n    ----------\n    None\n\n    \"\"\"",
            "title": "Stacked barplot"
        },
        {
            "location": "/docs/matplotlib/stacked_barplot/#stacked-barplot",
            "text": "A function to conveniently plot stacked bar plots in matplotlib using pandas  DataFrame s.",
            "title": "Stacked Barplot"
        },
        {
            "location": "/docs/matplotlib/stacked_barplot/#example",
            "text": "Creating an example   DataFrame :     import pandas as pd\n\ns1 = [1.0, 2.0, 3.0, 4.0]\ns2 = [1.4, 2.1, 2.9, 5.1]\ns3 = [1.9, 2.2, 3.5, 4.1]\ns4 = [1.4, 2.5, 3.5, 4.2]\ndata = [s1, s2, s3, s4]\n\ndf = pd.DataFrame(data, columns=['X1', 'X2', 'X3', 'X4'])\ndf.columns = ['X1', 'X2', 'X3', 'X4']\ndf.index = ['Sample1', 'Sample2', 'Sample3', 'Sample4']\ndf   Plotting the stacked barplot. By default, the index of the  DataFrame  is used as column labels, and the  DataFrame  columns are used for the plot legend.  from mlxtend.matplotlib import stacked_barplot\n\nstacked_barplot(df, rotation=45)",
            "title": "Example"
        },
        {
            "location": "/docs/matplotlib/stacked_barplot/#default-parameters",
            "text": "def stacked_barplot(df, bar_width='auto', colors='bgrcky', labels='index', rotation=90, legend=True):\n    \"\"\"\n    Function to plot stacked barplots\n\n    Parameters\n    ----------\n    df : pandas.DataFrame\n      A pandas DataFrame where the index denotes the\n      x-axis labels, and the columns contain the different\n      measurements for each row.\n\n    bar_width: 'auto' or float (default: 'auto')\n      Parameter to set the widths of the bars. if\n      'auto', the width is automatically determined by\n      the number of columns in the dataset.\n\n    colors: str (default: 'bgrcky')\n      The colors of the bars.\n\n    labels: 'index' or iterable (default: 'index')\n      If 'index', the DataFrame index will be used as\n      x-tick labels.\n\n    rotation: int (default: 90)\n      Parameter to rotate the x-axis labels.\n\n    legend: bool (default: True)\n      Parameter to plot the legend.\n\n    Returns\n    ----------\n    None\n\n    \"\"\"",
            "title": "Default Parameters"
        },
        {
            "location": "/docs/regression/plot_linear_regression_fit/",
            "text": "mlxtend\n\nSebastian Raschka, 05/14/2015\n\n\n\n\n\nPlotting Linear Regression Fits\n\n\nlin_regplot\n is a function to plot linear regression fits. \nBy default \nlin_regplot\n uses scikit-learn's \nlinear_model.LinearRegression\n to fit the model and SciPy's \nstats.pearsonr\n to calculate the correlation coefficient. \n\n\n\n\n\nExample\n\n\nimport matplotlib.pyplot as plt\nfrom mlxtend.regression import lin_regplot\nimport numpy as np\n\nX = np.array([4, 8, 13, 26, 31, 10, 8, 30, 18, 12, 20, 5, 28, 18, 6, 31, 12,\n   12, 27, 11, 6, 14, 25, 7, 13,4, 15, 21, 15])\n\ny = np.array([14, 24, 22, 59, 66, 25, 18, 60, 39, 32, 53, 18, 55, 41, 28, 61, 35,\n   36, 52, 23, 19, 25, 73, 16, 32, 14, 31, 43, 34])\n\nintercept, slope, corr_coeff = lin_regplot(X, y,)\nplt.show()\n\n\n\n\n\nDefault Parameters\n\n\ndef lin_regplot(X, \n             y, \n             model=LinearRegression(), \n             corr_func=pearsonr,\n             scattercolor='blue', \n             fit_style='k--', \n             legend=True,\n             xlim='auto'):\n    \"\"\"\n    Function to plot a linear regression line fit.\n\n\nParameters\n----------\nX : numpy array, shape (n_samples,)\n  Samples.\n\ny : numpy array, shape (n_samples,)\n  Target values\n\nmodel: object (default: sklearn.linear_model.LinearRegression)\n  Estimator object for regression. Must implement\n  a .fit() and .predict() method.\n\ncorr_func: function (default: scipy.stats.pearsonr)\n  function to calculate the regression\n  slope.\n\nscattercolor: string (default: blue)\n  Color of scatter plot points.\n\nfit_style: string (default: k--) \n  Style for the line fit.\n\nlegend: bool (default: True)\n  Plots legend with corr_coeff coef., \n  fit coef., and intercept values.\n\nxlim: array-like (x_min, x_max) or 'auto' (default: 'auto')\n  X-axis limits for the linear line fit.\n\nReturns\n----------\nintercept, slope, corr_coeff: float, float, float\n\n\"\"\"\n/pre",
            "title": "Plot linear regression fit"
        },
        {
            "location": "/docs/regression/plot_linear_regression_fit/#plotting-linear-regression-fits",
            "text": "lin_regplot  is a function to plot linear regression fits. \nBy default  lin_regplot  uses scikit-learn's  linear_model.LinearRegression  to fit the model and SciPy's  stats.pearsonr  to calculate the correlation coefficient.",
            "title": "Plotting Linear Regression Fits"
        },
        {
            "location": "/docs/regression/plot_linear_regression_fit/#example",
            "text": "import matplotlib.pyplot as plt\nfrom mlxtend.regression import lin_regplot\nimport numpy as np\n\nX = np.array([4, 8, 13, 26, 31, 10, 8, 30, 18, 12, 20, 5, 28, 18, 6, 31, 12,\n   12, 27, 11, 6, 14, 25, 7, 13,4, 15, 21, 15])\n\ny = np.array([14, 24, 22, 59, 66, 25, 18, 60, 39, 32, 53, 18, 55, 41, 28, 61, 35,\n   36, 52, 23, 19, 25, 73, 16, 32, 14, 31, 43, 34])\n\nintercept, slope, corr_coeff = lin_regplot(X, y,)\nplt.show()",
            "title": "Example"
        },
        {
            "location": "/docs/regression/plot_linear_regression_fit/#default-parameters",
            "text": "def lin_regplot(X, \n             y, \n             model=LinearRegression(), \n             corr_func=pearsonr,\n             scattercolor='blue', \n             fit_style='k--', \n             legend=True,\n             xlim='auto'):\n    \"\"\"\n    Function to plot a linear regression line fit.  Parameters\n----------\nX : numpy array, shape (n_samples,)\n  Samples.\n\ny : numpy array, shape (n_samples,)\n  Target values\n\nmodel: object (default: sklearn.linear_model.LinearRegression)\n  Estimator object for regression. Must implement\n  a .fit() and .predict() method.\n\ncorr_func: function (default: scipy.stats.pearsonr)\n  function to calculate the regression\n  slope.\n\nscattercolor: string (default: blue)\n  Color of scatter plot points.\n\nfit_style: string (default: k--) \n  Style for the line fit.\n\nlegend: bool (default: True)\n  Plots legend with corr_coeff coef., \n  fit coef., and intercept values.\n\nxlim: array-like (x_min, x_max) or 'auto' (default: 'auto')\n  X-axis limits for the linear line fit.\n\nReturns\n----------\nintercept, slope, corr_coeff: float, float, float\n\n\"\"\" /pre",
            "title": "Default Parameters"
        },
        {
            "location": "/docs/preprocessing/mean_centerer/",
            "text": "mlxtend\n\nSebastian Raschka, last updated: 05/14/2015\n\n\n\n\n\nMeanCenterer\n\n\nA transformer class that performs column-based mean centering on a NumPy array.\n\n\n\n\n\nExample\n\n\nUse the \nfit\n method to fit the column means of a dataset (e.g., the training dataset) to a new \nMeanCenterer\n object. Then, call the \ntransform\n method on the same dataset to center it at the sample mean.\n\n\n from mlxtend.preprocessing import MeanCenterer\n\n X_train\narray([[1, 2, 3],\n   [4, 5, 6],\n   [7, 8, 9]])\n\n mc = MeanCenterer().fit(X_train)\n\n mc.transform(X_train)\narray([[-3, -3, -3],\n   [ 0,  0,  0],\n   [ 3,  3,  3]])\n\n\n\n\n\nTo use the same parameters that were used to center the training dataset, simply call the \ntransform\n method of the \nMeanCenterer\n instance on a new dataset (e.g., test dataset).\n\n\n X_test \narray([[1, 1, 1],\n   [1, 1, 1],\n   [1, 1, 1]])\n\n mc.transform(X_test)  \narray([[-3, -4, -5],\n   [-3, -4, -5],\n   [-3, -4, -5]])\n\n\n\n\n\nThe \nMeanCenterer\n also supports Python list objects, and the \nfit_transform\n method allows you to directly fit and center the dataset.\n\n\n Z\n[1, 2, 3]\n\n MeanCenterer().fit_transform(Z)\narray([-1,  0,  1])\n\n\n\n\n\nimport matplotlib.pyplot as plt\nimport numpy as np\n\nX = 2 * np.random.randn(100,2) + 5\n\nplt.scatter(X[:,0], X[:,1])\nplt.grid()\nplt.title('Random Gaussian data w. mean=5, sigma=2')\nplt.show()\n\nY = MeanCenterer.fit_transform(X)\nplt.scatter(Y[:,0], Y[:,1])\nplt.grid()\nplt.title('Data after mean centering')\nplt.show()\n\n\n\n\n\n\n\n\nDefault Parameters\n\n\nclass MeanCenterer(TransformerObj):\n    \"\"\"\n    Class for column centering of vectors and matrices.\n\n    Keyword arguments:\n        X: NumPy array object where each attribute/variable is\n           stored in an individual column. \n           Also accepts 1-dimensional Python list objects.\n\n    Class methods:\n        fit: Fits column means to MeanCenterer object.\n        transform: Uses column means from `fit` for mean centering.\n        fit_transform: Fits column means and performs mean centering.\n\n    The class methods `transform` and `fit_transform` return a new numpy array\n    object where the attributes are centered at the column means.\n\n    \"\"\"",
            "title": "Mean centerer"
        },
        {
            "location": "/docs/preprocessing/mean_centerer/#meancenterer",
            "text": "A transformer class that performs column-based mean centering on a NumPy array.",
            "title": "MeanCenterer"
        },
        {
            "location": "/docs/preprocessing/mean_centerer/#example",
            "text": "Use the  fit  method to fit the column means of a dataset (e.g., the training dataset) to a new  MeanCenterer  object. Then, call the  transform  method on the same dataset to center it at the sample mean.   from mlxtend.preprocessing import MeanCenterer  X_train\narray([[1, 2, 3],\n   [4, 5, 6],\n   [7, 8, 9]])  mc = MeanCenterer().fit(X_train)  mc.transform(X_train)\narray([[-3, -3, -3],\n   [ 0,  0,  0],\n   [ 3,  3,  3]])   To use the same parameters that were used to center the training dataset, simply call the  transform  method of the  MeanCenterer  instance on a new dataset (e.g., test dataset).   X_test \narray([[1, 1, 1],\n   [1, 1, 1],\n   [1, 1, 1]])  mc.transform(X_test)  \narray([[-3, -4, -5],\n   [-3, -4, -5],\n   [-3, -4, -5]])   The  MeanCenterer  also supports Python list objects, and the  fit_transform  method allows you to directly fit and center the dataset.   Z\n[1, 2, 3]  MeanCenterer().fit_transform(Z)\narray([-1,  0,  1])   import matplotlib.pyplot as plt\nimport numpy as np\n\nX = 2 * np.random.randn(100,2) + 5\n\nplt.scatter(X[:,0], X[:,1])\nplt.grid()\nplt.title('Random Gaussian data w. mean=5, sigma=2')\nplt.show()\n\nY = MeanCenterer.fit_transform(X)\nplt.scatter(Y[:,0], Y[:,1])\nplt.grid()\nplt.title('Data after mean centering')\nplt.show()",
            "title": "Example"
        },
        {
            "location": "/docs/preprocessing/mean_centerer/#default-parameters",
            "text": "class MeanCenterer(TransformerObj):\n    \"\"\"\n    Class for column centering of vectors and matrices.\n\n    Keyword arguments:\n        X: NumPy array object where each attribute/variable is\n           stored in an individual column. \n           Also accepts 1-dimensional Python list objects.\n\n    Class methods:\n        fit: Fits column means to MeanCenterer object.\n        transform: Uses column means from `fit` for mean centering.\n        fit_transform: Fits column means and performs mean centering.\n\n    The class methods `transform` and `fit_transform` return a new numpy array\n    object where the attributes are centered at the column means.\n\n    \"\"\"",
            "title": "Default Parameters"
        },
        {
            "location": "/docs/preprocessing/shuffle_arrays_unison/",
            "text": "mlxtend\n\nSebastian Raschka, 05/14/2015\n\n\n\n\n\nArray Unison Shuffling\n\n\nA function that shuffles 2 or more NumPy arrays in unison.\n\n\n\n\n\nExample\n\n\n import numpy as np\n\n from mlxtend.preprocessing import shuffle_arrays_unison\n\n X1 = np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]])\n\n y1 = np.array([1, 2, 3])\n\n print(X1)\n[[1 2 3]\n[4 5 6]\n[7 8 9]]    \n\n print(y1)\n[1 2 3]\n\n X2, y2 = shuffle_arrays_unison(arrays=[X1, y1], random_seed=3)\n\n print(X2)\n[[4 5 6]\n[1 2 3]\n[7 8 9]]\n\n print(y1)\n[2 1 3]\n\n\n\nDefault Parameters\n\n\ndef shuffle_arrays_unison(arrays, random_state=None):\n    \"\"\"\n    Shuffle NumPy arrays in unison.\n\n    Parameters\n    ----------\n    arrays : array-like, shape = [n_arrays]\n      A list of NumPy arrays.\n\n    random_state : int\n      Sets the random state.\n\n    Returns\n    ----------\n    shuffled_arrays : A list of NumPy arrays after shuffling.\n\n    Examples\n    --------\n    >>> import numpy as np\n    >>> from mlxtend.preprocessing import shuffle_arrays_unison\n    >>> X1 = np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]])\n    >>> y1 = np.array([1, 2, 3])\n    >>> X2, y2 = shuffle_arrays_unison(arrays=[X1, y1], random_state=3)\n    >>> assert(X2.all() == np.array([[4, 5, 6], [1, 2, 3], [7, 8, 9]]).all())\n    >>> assert(y2.all() == np.array([2, 1, 3]).all())\n    >>>\n    \"\"\"",
            "title": "Shuffle arrays unison"
        },
        {
            "location": "/docs/preprocessing/shuffle_arrays_unison/#array-unison-shuffling",
            "text": "A function that shuffles 2 or more NumPy arrays in unison.",
            "title": "Array Unison Shuffling"
        },
        {
            "location": "/docs/preprocessing/shuffle_arrays_unison/#example",
            "text": "import numpy as np  from mlxtend.preprocessing import shuffle_arrays_unison  X1 = np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]])  y1 = np.array([1, 2, 3])  print(X1)\n[[1 2 3]\n[4 5 6]\n[7 8 9]]      print(y1)\n[1 2 3]  X2, y2 = shuffle_arrays_unison(arrays=[X1, y1], random_seed=3)  print(X2)\n[[4 5 6]\n[1 2 3]\n[7 8 9]]  print(y1)\n[2 1 3]",
            "title": "Example"
        },
        {
            "location": "/docs/preprocessing/shuffle_arrays_unison/#default-parameters",
            "text": "def shuffle_arrays_unison(arrays, random_state=None):\n    \"\"\"\n    Shuffle NumPy arrays in unison.\n\n    Parameters\n    ----------\n    arrays : array-like, shape = [n_arrays]\n      A list of NumPy arrays.\n\n    random_state : int\n      Sets the random state.\n\n    Returns\n    ----------\n    shuffled_arrays : A list of NumPy arrays after shuffling.\n\n    Examples\n    --------\n    >>> import numpy as np\n    >>> from mlxtend.preprocessing import shuffle_arrays_unison\n    >>> X1 = np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]])\n    >>> y1 = np.array([1, 2, 3])\n    >>> X2, y2 = shuffle_arrays_unison(arrays=[X1, y1], random_state=3)\n    >>> assert(X2.all() == np.array([[4, 5, 6], [1, 2, 3], [7, 8, 9]]).all())\n    >>> assert(y2.all() == np.array([2, 1, 3]).all())\n    >>>\n    \"\"\"",
            "title": "Default Parameters"
        },
        {
            "location": "/docs/file_io/find_files/",
            "text": "mlxtend\n\nSebastian Raschka, 05/14/2015\n\n\n\n\n\nFind Files\n\n\nA function that finds files in a given directory based on substring matches and returns a list of the file names found.\n\n\n\n\n\nExample\n\n\nfrom mlxtend.file_io import find_files\n\n\n find_files('mlxtend', '/Users/sebastian/Desktop')\n['/Users/sebastian/Desktop/mlxtend-0.1.6.tar.gz', \n'/Users/sebastian/Desktop/mlxtend-0.1.7.tar.gz']\n\n\n\n\n\n\nDefault Parameters\n\n\ndef find_files(substring, path, recursive=False, check_ext=None, ignore_invisible=True): \n    \"\"\"\n    Function that finds files in a directory based on substring matching.\n\n    Parameters\n    ----------\n\n    substring : `str`\n      Substring of the file to be matched.\n\n    path : `str` \n      Path where to look.\n\n    recursive: `bool`, optional, (default=`False`)\n      If true, searches subdirectories recursively.\n\n    check_ext: `str`, optional, (default=`None`)\n      If string (e.g., '.txt'), only returns files that\n      match the specified file extension.\n\n    ignore_invisible : `bool`, optional, (default=`True`)\n      If `True`, ignores invisible files (i.e., files starting with a period).\n\n    Returns\n    ----------\n    results : `list`\n      List of the matched files.\n\n    \"\"\"",
            "title": "Find files"
        },
        {
            "location": "/docs/file_io/find_files/#find-files",
            "text": "A function that finds files in a given directory based on substring matches and returns a list of the file names found.",
            "title": "Find Files"
        },
        {
            "location": "/docs/file_io/find_files/#example",
            "text": "from mlxtend.file_io import find_files  find_files('mlxtend', '/Users/sebastian/Desktop')\n['/Users/sebastian/Desktop/mlxtend-0.1.6.tar.gz', \n'/Users/sebastian/Desktop/mlxtend-0.1.7.tar.gz']",
            "title": "Example"
        },
        {
            "location": "/docs/file_io/find_files/#default-parameters",
            "text": "def find_files(substring, path, recursive=False, check_ext=None, ignore_invisible=True): \n    \"\"\"\n    Function that finds files in a directory based on substring matching.\n\n    Parameters\n    ----------\n\n    substring : `str`\n      Substring of the file to be matched.\n\n    path : `str` \n      Path where to look.\n\n    recursive: `bool`, optional, (default=`False`)\n      If true, searches subdirectories recursively.\n\n    check_ext: `str`, optional, (default=`None`)\n      If string (e.g., '.txt'), only returns files that\n      match the specified file extension.\n\n    ignore_invisible : `bool`, optional, (default=`True`)\n      If `True`, ignores invisible files (i.e., files starting with a period).\n\n    Returns\n    ----------\n    results : `list`\n      List of the matched files.\n\n    \"\"\"",
            "title": "Default Parameters"
        },
        {
            "location": "/docs/file_io/find_filegroups/",
            "text": "mlxtend\n\nSebastian Raschka, 05/14/2015\n\n\n\n\n\nFind File Groups\n\n\nA function that finds files that belong together (i.e., differ only by file extension) in different directories and collects them in a Python dictionary for further processing tasks. \n\n\n\n\n\nExample\n\n\n\n\nfrom mlxtend.file_io import find_filegroups\n\nd1 = os.path.join(master_path, 'dir_1')\nd2 = os.path.join(master_path, 'dir_2')\nd3 = os.path.join(master_path, 'dir_3')\n\nfind_filegroups(paths=[d1,d2,d3], substring='file_1')\n# Returns:\n# {'file_1': ['/Users/sebastian/github/mlxtend/tests/data/find_filegroups/dir_1/file_1.log', \n#             '/Users/sebastian/github/mlxtend/tests/data/find_filegroups/dir_2/file_1.csv', \n#             '/Users/sebastian/github/mlxtend/tests/data/find_filegroups/dir_3/file_1.txt']}\n#\n# Note: Setting `substring=''` would return a \n# dictionary of all file paths for \n# file_1.*, file_2.*, file_3.*\n\n\n\n\n\n\nDefault Parameters\n\n\ndef find_filegroups(paths, substring='', extensions=None, validity_check=True, ignore_invisible=True):\n    \"\"\"\n    Function that finds and groups files from different directories in a python dictionary.\n\n    Parameters\n    ----------\n    paths : `list` \n      Paths of the directories to be searched. Dictionary keys are build from\n      the first directory.\n\n    substring : `str`, optional, (default=`''`)\n      Substring that all files have to contain to be considered.\n\n    extensions : `list`, optional, (default=`None`)\n      `None` or `list` of allowed file extensions for each path. If provided, the number\n      of extensions must match the number of `paths`.\n\n    validity_check : `bool`, optional, (default=`True`)\n      If `True`, checks if all dictionary values have the same number of file paths. Prints\n      a warning and returns an empty dictionary if the validity check failed.\n\n    ignore_invisible : `bool`, optional, (default=`True`)\n      If `True`, ignores invisible files (i.e., files starting with a period).\n\n    Returns\n    ----------\n    groups : `dict`\n      Dictionary of files paths. Keys are the file names found in the first directory listed\n      in `paths` (without file extension).\n\n    \"\"\"",
            "title": "Find filegroups"
        },
        {
            "location": "/docs/file_io/find_filegroups/#find-file-groups",
            "text": "A function that finds files that belong together (i.e., differ only by file extension) in different directories and collects them in a Python dictionary for further processing tasks.",
            "title": "Find File Groups"
        },
        {
            "location": "/docs/file_io/find_filegroups/#example",
            "text": "from mlxtend.file_io import find_filegroups\n\nd1 = os.path.join(master_path, 'dir_1')\nd2 = os.path.join(master_path, 'dir_2')\nd3 = os.path.join(master_path, 'dir_3')\n\nfind_filegroups(paths=[d1,d2,d3], substring='file_1')\n# Returns:\n# {'file_1': ['/Users/sebastian/github/mlxtend/tests/data/find_filegroups/dir_1/file_1.log', \n#             '/Users/sebastian/github/mlxtend/tests/data/find_filegroups/dir_2/file_1.csv', \n#             '/Users/sebastian/github/mlxtend/tests/data/find_filegroups/dir_3/file_1.txt']}\n#\n# Note: Setting `substring=''` would return a \n# dictionary of all file paths for \n# file_1.*, file_2.*, file_3.*",
            "title": "Example"
        },
        {
            "location": "/docs/file_io/find_filegroups/#default-parameters",
            "text": "def find_filegroups(paths, substring='', extensions=None, validity_check=True, ignore_invisible=True):\n    \"\"\"\n    Function that finds and groups files from different directories in a python dictionary.\n\n    Parameters\n    ----------\n    paths : `list` \n      Paths of the directories to be searched. Dictionary keys are build from\n      the first directory.\n\n    substring : `str`, optional, (default=`''`)\n      Substring that all files have to contain to be considered.\n\n    extensions : `list`, optional, (default=`None`)\n      `None` or `list` of allowed file extensions for each path. If provided, the number\n      of extensions must match the number of `paths`.\n\n    validity_check : `bool`, optional, (default=`True`)\n      If `True`, checks if all dictionary values have the same number of file paths. Prints\n      a warning and returns an empty dictionary if the validity check failed.\n\n    ignore_invisible : `bool`, optional, (default=`True`)\n      If `True`, ignores invisible files (i.e., files starting with a period).\n\n    Returns\n    ----------\n    groups : `dict`\n      Dictionary of files paths. Keys are the file names found in the first directory listed\n      in `paths` (without file extension).\n\n    \"\"\"",
            "title": "Default Parameters"
        },
        {
            "location": "/docs/math/combinations/",
            "text": "mlxtend\n\nSebastian Raschka, last updated: 05/14/2015\n\n\n\n\n\nCombinations\n\n\nFunctions to calculate the number of combinations for creating subsequences of \nr\n elements out of a sequence with \nn\n elements.\n\n\nExample\n\n\nIn:\n\n\nfrom mlxtend.math import num_combinations\n\nc = num_combinations(n=20, r=8, with_replacement=False)\nprint('Number of ways to combine 20 elements into 8 subelements: %d' % c)\n\n\n\nOut:    \n\n\nNumber of ways to combine 20 elements into 8 subelements: 125970\n\n\n\nThis is especially useful in combination with \nitertools\n, e.g., in order to estimate the progress via \npyprind\n.\n\n\n\n\nDefault Parameters\n\n\ndef num_combinations(n, r, with_replacement=False):\n    \"\"\" \n    Function to calculate the number of possible combinations.\n\n    Parameters\n    ----------\n    n : `int`\n      Total number of items.\n\n    r : `int`\n      Number of elements of the target itemset.\n\n    with_replacement : `bool`, optional, (default=False)\n      Allows repeated elements if True.\n\n    Returns\n    ----------\n    comb : `int`\n      Number of possible combinations.\n\n    \"\"\"",
            "title": "Combinations"
        },
        {
            "location": "/docs/math/combinations/#combinations",
            "text": "Functions to calculate the number of combinations for creating subsequences of  r  elements out of a sequence with  n  elements.",
            "title": "Combinations"
        },
        {
            "location": "/docs/math/combinations/#example",
            "text": "In:  from mlxtend.math import num_combinations\n\nc = num_combinations(n=20, r=8, with_replacement=False)\nprint('Number of ways to combine 20 elements into 8 subelements: %d' % c)  Out:      Number of ways to combine 20 elements into 8 subelements: 125970  This is especially useful in combination with  itertools , e.g., in order to estimate the progress via  pyprind .",
            "title": "Example"
        },
        {
            "location": "/docs/math/combinations/#default-parameters",
            "text": "def num_combinations(n, r, with_replacement=False):\n    \"\"\" \n    Function to calculate the number of possible combinations.\n\n    Parameters\n    ----------\n    n : `int`\n      Total number of items.\n\n    r : `int`\n      Number of elements of the target itemset.\n\n    with_replacement : `bool`, optional, (default=False)\n      Allows repeated elements if True.\n\n    Returns\n    ----------\n    comb : `int`\n      Number of possible combinations.\n\n    \"\"\"",
            "title": "Default Parameters"
        },
        {
            "location": "/docs/math/permutations/",
            "text": "mlxtend\n\nSebastian Raschka, last updated: 05/14/2015\n\n\n\n\n\nPermutations\n\n\nFunctions to calculate the number of permutations for creating subsequences of \nr\n elements out of a sequence with \nn\n elements.\n\n\n\n\n\nExample\n\n\nIn:\n\n\nfrom mlxtend.math import num_permutations\n\nd = num_permutations(n=20, r=8, with_replacement=False)\nprint('Number of ways to permute 20 elements into 8 subelements: %d' % d)\n\n\n\nOut:    \n\n\nNumber of ways to permute 20 elements into 8 subelements: 5079110400\n\n\n\nThis is especially useful in combination with \nitertools\n, e.g., in order to estimate the progress via \npyprind\n.\n\n\n\n\nDefault Parameters\n\n\ndef num_permutations(n, r, with_replacement=False):\n    \"\"\" \n    Function to calculate the number of possible permutations.\n\n    Parameters\n    ----------\n    n : `int`\n      Total number of items.\n\n    r : `int`\n      Number of elements of the target itemset.\n\n    with_replacement : `bool`, optional, (default=False)\n      Allows repeated elements if True.\n\n    Returns\n    ----------\n    permut : `int`\n      Number of possible permutations.\n\n    \"\"\"",
            "title": "Permutations"
        },
        {
            "location": "/docs/math/permutations/#permutations",
            "text": "Functions to calculate the number of permutations for creating subsequences of  r  elements out of a sequence with  n  elements.",
            "title": "Permutations"
        },
        {
            "location": "/docs/math/permutations/#example",
            "text": "In:  from mlxtend.math import num_permutations\n\nd = num_permutations(n=20, r=8, with_replacement=False)\nprint('Number of ways to permute 20 elements into 8 subelements: %d' % d)  Out:      Number of ways to permute 20 elements into 8 subelements: 5079110400  This is especially useful in combination with  itertools , e.g., in order to estimate the progress via  pyprind .",
            "title": "Example"
        },
        {
            "location": "/docs/math/permutations/#default-parameters",
            "text": "def num_permutations(n, r, with_replacement=False):\n    \"\"\" \n    Function to calculate the number of possible permutations.\n\n    Parameters\n    ----------\n    n : `int`\n      Total number of items.\n\n    r : `int`\n      Number of elements of the target itemset.\n\n    with_replacement : `bool`, optional, (default=False)\n      Allows repeated elements if True.\n\n    Returns\n    ----------\n    permut : `int`\n      Number of possible permutations.\n\n    \"\"\"",
            "title": "Default Parameters"
        },
        {
            "location": "/docs/text/generalize_names/",
            "text": "mlxtend\n\nSebastian Raschka, 05/14/2015\n\n\n\n\n\nName Generalization\n\n\nA function that converts a name into a general format \nlast_name\nseparator\nfirstname letter(s)\n (all lowercase)\n, which is useful if data is collected from different sources and is supposed to be compared or merged based on name identifiers. E.g., if names are stored in a pandas \nDataFrame\n column, the apply function can be used to generalize names: \ndf['name'] = df['name'].apply(generalize_names)\n\n\n\n\n\nExamples\n\n\nfrom mlxtend.text import generalize_names\n\n# defaults\n\n generalize_names('Pozo, Jos\u00e9 \u00c1ngel')\n'pozo j'\n\n generalize_names('Pozo, Jos\u00e9 \u00c1ngel') \n'pozo j'\n\n assert(generalize_names('Jos\u00e9 \u00c1ngel Pozo') \n'pozo j' \n\n generalize_names('Jos\u00e9 Pozo')\n'pozo j'\n\n# optional parameters\n\n generalize_names(\"Eto'o, Samuel\", firstname_output_letters=2)\n'etoo sa'\n\n generalize_names(\"Eto'o, Samuel\", firstname_output_letters=0)\n'etoo'\n\n generalize_names(\"Eto'o, Samuel\", output_sep=', ')\n'etoo, s'\n\n\n\n\n\n\nDefault Parameters\n\n\ndef generalize_names(name, output_sep=' ', firstname_output_letters=1):\n    \"\"\"\n    Function that outputs a person's name in the format\n    \n (all lowercase)\n\n    Parameters\n    ----------\n    name : `str`\n      Name of the player\n\n    output_sep : `str` (default: ' ')\n      String for separating last name and first name in the output.\n\n    firstname_output_letters : `int`\n      Number of letters in the abbreviated first name.\n\n    Returns\n    ----------\n    gen_name : `str`\n      The generalized name.\n\n    \"\"\"",
            "title": "Generalize names"
        },
        {
            "location": "/docs/text/generalize_names/#name-generalization",
            "text": "A function that converts a name into a general format  last_name separator firstname letter(s)  (all lowercase) , which is useful if data is collected from different sources and is supposed to be compared or merged based on name identifiers. E.g., if names are stored in a pandas  DataFrame  column, the apply function can be used to generalize names:  df['name'] = df['name'].apply(generalize_names)",
            "title": "Name Generalization"
        },
        {
            "location": "/docs/text/generalize_names/#examples",
            "text": "from mlxtend.text import generalize_names\n\n# defaults  generalize_names('Pozo, Jos\u00e9 \u00c1ngel')\n'pozo j'  generalize_names('Pozo, Jos\u00e9 \u00c1ngel') \n'pozo j'  assert(generalize_names('Jos\u00e9 \u00c1ngel Pozo') \n'pozo j'   generalize_names('Jos\u00e9 Pozo')\n'pozo j'\n\n# optional parameters  generalize_names(\"Eto'o, Samuel\", firstname_output_letters=2)\n'etoo sa'  generalize_names(\"Eto'o, Samuel\", firstname_output_letters=0)\n'etoo'  generalize_names(\"Eto'o, Samuel\", output_sep=', ')\n'etoo, s'",
            "title": "Examples"
        },
        {
            "location": "/docs/text/generalize_names/#default-parameters",
            "text": "def generalize_names(name, output_sep=' ', firstname_output_letters=1):\n    \"\"\"\n    Function that outputs a person's name in the format\n      (all lowercase)\n\n    Parameters\n    ----------\n    name : `str`\n      Name of the player\n\n    output_sep : `str` (default: ' ')\n      String for separating last name and first name in the output.\n\n    firstname_output_letters : `int`\n      Number of letters in the abbreviated first name.\n\n    Returns\n    ----------\n    gen_name : `str`\n      The generalized name.\n\n    \"\"\"",
            "title": "Default Parameters"
        },
        {
            "location": "/docs/text/generalize_names_duplcheck/",
            "text": "mlxtend\n\nSebastian Raschka, 05/14/2015\n\n\n\n\n\nName Generalization and Duplicates\n\n\nNote\n that using \ngeneralize_names\n with few \nfirstname_output_letters\n can result in duplicate entries. E.g., if your dataset contains the names \"Adam Johnson\" and \"Andrew Johnson\", the default setting (i.e., 1 first name letter) will produce the generalized name \"johnson a\" in both cases.\n\n\nOne solution is to increase the number of first name letters in the output by setting the parameter \nfirstname_output_letters\n to a value larger than 1. \n\n\nAn alternative solution is to use the \ngeneralize_names_duplcheck\n function if you are working with pandas DataFrames. \n\n\nThe  \ngeneralize_names_duplcheck\n function can be imported via\n\n\nfrom mlxtend.text import generalize_names_duplcheck\n\n\n\nBy default,  \ngeneralize_names_duplcheck\n will apply  \ngeneralize_names\n to a pandas DataFrame column with the minimum number of first name letters and append as many first name letters as necessary until no duplicates are present in the given DataFrame column. An example dataset column that contains the names  \n\n\n\n\n\nExamples\n\n\nReading in a CSV file that has column \nName\n for which we want to generalize the names:\n\n\n\n\nSamuel Eto'o\n\n\nAdam Johnson\n\n\nAndrew Johnson\n\n\n\n\n\n\ndf = pd.read_csv(path)\n\n\n\nApplying \ngeneralize_names_duplcheck\n to generate a new DataFrame with the generalized names without duplicates:          \n\n\ndf_new = generalize_names_duplcheck(df=df, col_name='Name')\n\n\n\n\n\netoo s\n\n\njohnson ad\n\n\njohnson an\n\n\n\n\n\n\n\nDefault Parameters\n\n\ndef generalize_names_duplcheck(df, col_name):\n    \"\"\"\n    Applies mlxtend.text.generalize_names to a DataFrame with 1 first name letter\n    by default and uses more first name letters if duplicates are detected.\n\n    Parameters\n    ----------\n    df : `pandas.DataFrame`\n      DataFrame that contains a column where generalize_names should be applied.\n\n    col_name : `str`\n      Name of the DataFrame column where `generalize_names` function should be applied to.\n\n    Returns\n    ----------\n    df_new : `str`\n      New DataFrame object where generalize_names function has been applied without duplicates.\n\n    \"\"\"",
            "title": "Generalize names duplcheck"
        },
        {
            "location": "/docs/text/generalize_names_duplcheck/#name-generalization-and-duplicates",
            "text": "Note  that using  generalize_names  with few  firstname_output_letters  can result in duplicate entries. E.g., if your dataset contains the names \"Adam Johnson\" and \"Andrew Johnson\", the default setting (i.e., 1 first name letter) will produce the generalized name \"johnson a\" in both cases.  One solution is to increase the number of first name letters in the output by setting the parameter  firstname_output_letters  to a value larger than 1.   An alternative solution is to use the  generalize_names_duplcheck  function if you are working with pandas DataFrames.   The   generalize_names_duplcheck  function can be imported via  from mlxtend.text import generalize_names_duplcheck  By default,   generalize_names_duplcheck  will apply   generalize_names  to a pandas DataFrame column with the minimum number of first name letters and append as many first name letters as necessary until no duplicates are present in the given DataFrame column. An example dataset column that contains the names",
            "title": "Name Generalization and Duplicates"
        },
        {
            "location": "/docs/text/generalize_names_duplcheck/#examples",
            "text": "Reading in a CSV file that has column  Name  for which we want to generalize the names:   Samuel Eto'o  Adam Johnson  Andrew Johnson    df = pd.read_csv(path)  Applying  generalize_names_duplcheck  to generate a new DataFrame with the generalized names without duplicates:            df_new = generalize_names_duplcheck(df=df, col_name='Name')   etoo s  johnson ad  johnson an",
            "title": "Examples"
        },
        {
            "location": "/docs/text/generalize_names_duplcheck/#default-parameters",
            "text": "def generalize_names_duplcheck(df, col_name):\n    \"\"\"\n    Applies mlxtend.text.generalize_names to a DataFrame with 1 first name letter\n    by default and uses more first name letters if duplicates are detected.\n\n    Parameters\n    ----------\n    df : `pandas.DataFrame`\n      DataFrame that contains a column where generalize_names should be applied.\n\n    col_name : `str`\n      Name of the DataFrame column where `generalize_names` function should be applied to.\n\n    Returns\n    ----------\n    df_new : `str`\n      New DataFrame object where generalize_names function has been applied without duplicates.\n\n    \"\"\"",
            "title": "Default Parameters"
        },
        {
            "location": "/changelog/",
            "text": "0.2.6\n\n\n\n\nAdded \nmlxtend.pandas.standardizing\n to standardize columns in a Pandas DataFrame\n\n\nAdded parameters \nlinestyles\n and \nmarkers\n to \nmlxtend.matplotlib.enrichment_plot\n\n\nmlxtend.regression.lin_regplot\n automatically adds np.newaxis and works w. python lists\n\n\n\n\n0.2.5\n\n\n\n\nAdded Sequential Backward Selection (mlxtend.sklearn.SBS)\n\n\nAdded \nX_highlight\n parameter to \nmlxtend.evaluate.plot_decision_regions\n for highlighting test data points.\n\n\nAdded mlxtend.regression.lin_regplot to plot the fitted line from linear regression.\n\n\nAdded mlxtend.matplotlib.stacked_barplot to conveniently produce stacked barplots using pandas \nDataFrame\ns.\n\n\nAdded mlxtend.matplotlib.enrichment_plot\n\n\n\n\n0.2.4\n\n\n\n\nAdded \nscoring\n to \nmlxtend.evaluate.learning_curves\n (by user pfsq)\n\n\nFixed setup.py bug caused by the missing README.html file\n\n\nmatplotlib.category_scatter for pandas DataFrames and Numpy arrays\n\n\n\n\n0.2.3\n\n\n\n\nAdded Logistic regression\n\n\nGradient descent and stochastic gradient descent perceptron was changed\n  to Adaline (Adaptive Linear Neuron)\n\n\nPerceptron and Adaline for {0, 1} classes\n\n\nAdded \nmlxtend.preprocessing.shuffle_arrays_unison\n function to \n  shuffle one or more NumPy arrays.\n\n\nAdded shuffle and random seed parameter to stochastic gradient descent classifier.\n\n\nAdded \nrstrip\n parameter to \nmlxtend.file_io.find_filegroups\n to allow trimming of base names.\n\n\nAdded \nignore_substring\n parameter to \nmlxtend.file_io.find_filegroups\n and \nfind_files\n.\n\n\nReplaced .rstrip in \nmlxtend.file_io.find_filegroups\n with more robust regex.\n\n\nGridsearch support for \nmlxtend.sklearn.EnsembleClassifier\n\n\n\n\n0.2.2\n\n\n\n\nImproved robustness of EnsembleClassifier.\n\n\nExtended plot_decision_regions() functionality for plotting 1D decision boundaries.\n\n\nFunction matplotlib.plot_decision_regions was reorganized  to evaluate.plot_decision_regions .\n\n\nevaluate.plot_learning_curves() function added.\n\n\nAdded Rosenblatt, gradient descent, and stochastic gradient descent perceptrons.\n\n\n\n\n0.2.1\n\n\n\n\nAdded mlxtend.pandas.minmax_scaling - a function to rescale pandas DataFrame columns.\n\n\nSlight update to the EnsembleClassifier interface (additional \nvoting\n parameter)\n\n\nFixed EnsembleClassifier to return correct class labels if class labels are not\n  integers from 0 to n.\n\n\nAdded new matplotlib function to plot decision regions of classifiers.\n\n\n\n\n0.2.0\n\n\n\n\nImproved mlxtend.text.generalize_duplcheck to remove duplicates and prevent endless looping issue.\n\n\nAdded \nrecursive\n search parameter to mlxtend.file_io.find_files.\n\n\nAdded \ncheck_ext\n parameter mlxtend.file_io.find_files to search based on file extensions.\n\n\nDefault parameter to ignore invisible files for mlxtend.file_io.find.\n\n\nAdded \ntransform\n and \nfit_transform\n to the \nEnsembleClassifier\n.\n\n\nAdded mlxtend.file_io.find_filegroups function.\n\n\n\n\n0.1.9\n\n\n\n\nImplemented scikit-learn EnsembleClassifier (majority voting rule) class.\n\n\n\n\n0.1.8\n\n\n\n\nImprovements to mlxtend.text.generalize_names to handle certain Dutch last name prefixes (van, van der, de, etc.).\n\n\nAdded mlxtend.text.generalize_name_duplcheck function to apply mlxtend.text.generalize_names function to a pandas DataFrame without creating duplicates.\n\n\n\n\n0.1.7\n\n\n\n\nAdded text utilities with name generalization function.\n\n\nAdded  and file_io utilities.\n\n\n\n\n0.1.6\n\n\n\n\nAdded combinations and permutations estimators.\n\n\n\n\n0.1.5\n\n\n\n\nAdded \nDenseTransformer\n for pipelines and grid search.\n\n\n\n\n0.1.4\n\n\n\n\nmean_centering\n function is now a Class that creates \nMeanCenterer\n objects\n  that can be used to fit data via the \nfit\n method, and center data at the column\n  means via the \ntransform\n and \nfit_transform\n method.\n\n\n\n\n0.1.3\n\n\n\n\nAdded \npreprocessing\n module and \nmean_centering\n function.\n\n\n\n\n0.1.2\n\n\n\n\nAdded \nmatplotlib\n utilities and \nremove_borders\n function.\n\n\n\n\n0.1.1\n\n\n\n\nSimplified code for ColumnSelector.",
            "title": "Changelog"
        },
        {
            "location": "/installation/",
            "text": "Installing mlxtend\n\n\nYou can use the following command to install \nmlxtend\n:\n\n\npip install mlxtend\n\n or  \n\n\neasy_install mlxtend\n  \n\n\nAlternatively, you download the package manually from the Python Package Index \nhttps://pypi.python.org/pypi/mlxtend\n, unzip it, navigate into the package, and use the command:\n\n\npython setup.py install",
            "title": "Installation"
        },
        {
            "location": "/installation/#installing-mlxtend",
            "text": "You can use the following command to install  mlxtend :  pip install mlxtend \n or    easy_install mlxtend     Alternatively, you download the package manually from the Python Package Index  https://pypi.python.org/pypi/mlxtend , unzip it, navigate into the package, and use the command:  python setup.py install",
            "title": "Installing mlxtend"
        }
    ]
}